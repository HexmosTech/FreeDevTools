<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytest - pytest usage</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/jammy/+package/python3-pytest">python3-pytest_6.2.5-1ubuntu2_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       pytest - pytest usage

</pre><h4><b>CALLING</b> <b>PYTEST-3</b> <b>THROUGH</b> <b>PYTHON</b> <b>-M</b> <b>PYTEST-3</b></h4><pre>
       You can invoke testing through the Python interpreter from the command line:

          python -m pytest [...]

       This is almost equivalent to invoking the command line script <b>pytest</b> <b>[...]</b>  directly, except that calling
       via <b>python</b> will also add the current directory to <b>sys.path</b>.

</pre><h4><b>POSSIBLE</b> <b>EXIT</b> <b>CODES</b></h4><pre>
       Running <b>pytest</b> can result in six different exit codes:

       <b>Exit</b> <b>code</b> <b>0</b>
              All tests were collected and passed successfully

       <b>Exit</b> <b>code</b> <b>1</b>
              Tests were collected and run but some of the tests failed

       <b>Exit</b> <b>code</b> <b>2</b>
              Test execution was interrupted by the user

       <b>Exit</b> <b>code</b> <b>3</b>
              Internal error happened while executing tests

       <b>Exit</b> <b>code</b> <b>4</b>
              pytest command line usage error

       <b>Exit</b> <b>code</b> <b>5</b>
              No tests were collected

       They  are  represented  by the <b>pytest.ExitCode</b> enum. The exit codes being a part of the public API can be
       imported and accessed directly using:

          from pytest import ExitCode

       <b>NOTE:</b>
          If you would like to customize the exit code in some scenarios, specially when no tests are collected,
          consider using the <u>pytest-custom_exit_code</u> plugin.

</pre><h4><b>GETTING</b> <b>HELP</b> <b>ON</b> <b>VERSION,</b> <b>OPTION</b> <b>NAMES,</b> <b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
          pytest --version   # shows where pytest was imported from
          pytest --fixtures  # show available builtin function arguments
          pytest -h | --help # show help on command line and config file options

       The full command-line flags can be found in the reference.

</pre><h4><b>STOPPING</b> <b>AFTER</b> <b>THE</b> <b>FIRST</b> <b>(OR</b> <b>N)</b> <b>FAILURES</b></h4><pre>
       To stop the testing process after the first (N) failures:

          pytest -x           # stop after first failure
          pytest --maxfail=2  # stop after two failures

</pre><h4><b>SPECIFYING</b> <b>TESTS</b> <b>/</b> <b>SELECTING</b> <b>TESTS</b></h4><pre>
       Pytest supports several ways to run and select tests from the command-line.

       <b>Run</b> <b>tests</b> <b>in</b> <b>a</b> <b>module</b>

          pytest test_mod.py

       <b>Run</b> <b>tests</b> <b>in</b> <b>a</b> <b>directory</b>

          pytest testing/

       <b>Run</b> <b>tests</b> <b>by</b> <b>keyword</b> <b>expressions</b>

          pytest -k "MyClass and not method"

       This will run tests which contain names that match the given <u>string</u> <u>expression</u> (case-insensitive),  which
       can  include  Python  operators  that  use  filenames,  class names and function names as variables.  The
       example above will run <b>TestMyClass.test_something</b>  but not <b>TestMyClass.test_method_simple</b>.

       <b>Run</b> <b>tests</b> <b>by</b> <b>node</b> <b>ids</b>

       Each collected test is assigned a unique  <b>nodeid</b>  which  consist  of  the  module  filename  followed  by
       specifiers  like  class  names,  function  names  and  parameters  from  parametrization, separated by <b>::</b>
       characters.

       To run a specific test within a module:

          pytest test_mod.py::test_func

       Another example specifying a test method in the command line:

          pytest test_mod.py::TestClass::test_method

       <b>Run</b> <b>tests</b> <b>by</b> <b>marker</b> <b>expressions</b>

          pytest -m slow

       Will run all tests which are decorated with the <b>@pytest.mark.slow</b> decorator.

       For more information see marks.

       <b>Run</b> <b>tests</b> <b>from</b> <b>packages</b>

          pytest --pyargs pkg.testing

       This will import <b>pkg.testing</b> and use its filesystem location to find and run tests from.

</pre><h4><b>MODIFYING</b> <b>PYTHON</b> <b>TRACEBACK</b> <b>PRINTING</b></h4><pre>
       Examples for modifying traceback printing:

          pytest --showlocals # show local variables in tracebacks
          pytest -l           # show local variables (shortcut)

          pytest --tb=auto    # (default) 'long' tracebacks for the first and last
                               # entry, but 'short' style for the other entries
          pytest --tb=long    # exhaustive, informative traceback formatting
          pytest --tb=short   # shorter traceback format
          pytest --tb=line    # only one line per failure
          pytest --tb=native  # Python standard library formatting
          pytest --tb=no      # no traceback at all

       The <b>--full-trace</b> causes very long traces to be printed on error (longer than <b>--tb=long</b>). It also  ensures
       that a stack trace is printed on <b>KeyboardInterrupt</b> (Ctrl+C).  This is very useful if the tests are taking
       too long and you interrupt them with Ctrl+C to find out where the tests are <u>hanging</u>. By default no output
       will be shown (because KeyboardInterrupt is caught by pytest). By using this option you make sure a trace
       is shown.

</pre><h4><b>DETAILED</b> <b>SUMMARY</b> <b>REPORT</b></h4><pre>
       The  <b>-r</b> flag can be used to display a "short test summary info" at the end of the test session, making it
       easy in large test suites to get a clear picture of all failures, skips, xfails, etc.

       It defaults to <b>fE</b> to list failures and errors.

       Example:

          # content of test_example.py
          import pytest

          @pytest.fixture
          def error_fixture():
              assert 0

          def test_ok():
              print("ok")

          def test_fail():
              assert 0

          def test_error(error_fixture):
              pass

          def test_skip():
              pytest.skip("skipping this test")

          def test_xfail():
              pytest.xfail("xfailing this test")

          @pytest.mark.xfail(reason="always xfail")
          def test_xpass():
              pass

          $ pytest -ra
          =========================== test session starts ============================
          platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
          cachedir: $PYTHON_PREFIX/.pytest_cache
          rootdir: $REGENDOC_TMPDIR
          collected 6 items

          test_example.py .FEsxX                                               [100%]

          ================================== ERRORS ==================================
          _______________________ ERROR at setup of test_error _______________________

              @pytest.fixture
              def error_fixture():
          &gt;       assert 0
          E       assert 0

          test_example.py:6: AssertionError
          ================================= FAILURES =================================
          ________________________________ test_fail _________________________________

              def test_fail():
          &gt;       assert 0
          E       assert 0

          test_example.py:14: AssertionError
          ========================= short test summary info ==========================
          SKIPPED [1] test_example.py:22: skipping this test
          XFAIL test_example.py::test_xfail
            reason: xfailing this test
          XPASS test_example.py::test_xpass always xfail
          ERROR test_example.py::test_error - assert 0
          FAILED test_example.py::test_fail - assert 0
          == 1 failed, 1 passed, 1 skipped, 1 xfailed, 1 xpassed, 1 error in 0.12s ===

       The <b>-r</b> options accepts a number of characters after it, with <b>a</b> used above meaning "all except passes".

       Here is the full list of available characters that can be used:

          • <b>f</b> - failed

          • <b>E</b> - error

          • <b>s</b> - skipped

          • <b>x</b> - xfailed

          • <b>X</b> - xpassed

          • <b>p</b> - passed

          • <b>P</b> - passed with output

       Special characters for (de)selection of groups:

          • <b>a</b> - all except <b>pP</b>

          • <b>A</b> - all

          • <b>N</b> - none, this can be used to display nothing (since <b>fE</b> is the default)

       More than one character can be used, so for example to  only  see  failed  and  skipped  tests,  you  can
       execute:

          $ pytest -rfs
          =========================== test session starts ============================
          platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
          cachedir: $PYTHON_PREFIX/.pytest_cache
          rootdir: $REGENDOC_TMPDIR
          collected 6 items

          test_example.py .FEsxX                                               [100%]

          ================================== ERRORS ==================================
          _______________________ ERROR at setup of test_error _______________________

              @pytest.fixture
              def error_fixture():
          &gt;       assert 0
          E       assert 0

          test_example.py:6: AssertionError
          ================================= FAILURES =================================
          ________________________________ test_fail _________________________________

              def test_fail():
          &gt;       assert 0
          E       assert 0

          test_example.py:14: AssertionError
          ========================= short test summary info ==========================
          FAILED test_example.py::test_fail - assert 0
          SKIPPED [1] test_example.py:22: skipping this test
          == 1 failed, 1 passed, 1 skipped, 1 xfailed, 1 xpassed, 1 error in 0.12s ===

       Using <b>p</b> lists the passing tests, whilst <b>P</b> adds an extra section "PASSES" with those tests that passed but
       had captured output:

          $ pytest -rpP
          =========================== test session starts ============================
          platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
          cachedir: $PYTHON_PREFIX/.pytest_cache
          rootdir: $REGENDOC_TMPDIR
          collected 6 items

          test_example.py .FEsxX                                               [100%]

          ================================== ERRORS ==================================
          _______________________ ERROR at setup of test_error _______________________

              @pytest.fixture
              def error_fixture():
          &gt;       assert 0
          E       assert 0

          test_example.py:6: AssertionError
          ================================= FAILURES =================================
          ________________________________ test_fail _________________________________

              def test_fail():
          &gt;       assert 0
          E       assert 0

          test_example.py:14: AssertionError
          ================================== PASSES ==================================
          _________________________________ test_ok __________________________________
          --------------------------- Captured stdout call ---------------------------
          ok
          ========================= short test summary info ==========================
          PASSED test_example.py::test_ok
          == 1 failed, 1 passed, 1 skipped, 1 xfailed, 1 xpassed, 1 error in 0.12s ===

</pre><h4><b>DROPPING</b> <b>TO</b> <b>PDB</b> <b>(PYTHON</b> <b>DEBUGGER)</b> <b>ON</b> <b>FAILURES</b></h4><pre>
       Python  comes  with  a builtin Python debugger called <u>PDB</u>.  <b>pytest</b> allows one to drop into the <u>PDB</u> prompt
       via a command line option:

          pytest --pdb

       This will invoke the Python debugger on every failure (or KeyboardInterrupt).  Often you might only  want
       to do this for the first failing test to understand a certain failure situation:

          pytest -x --pdb   # drop to PDB on first failure, then end test session
          pytest --pdb --maxfail=3  # drop to PDB for first three failures

       Note  that  on  any  failure  the  exception  information  is stored on <b>sys.last_value</b>, <b>sys.last_type</b> and
       <b>sys.last_traceback</b>. In interactive use, this allows one to drop into postmortem debugging with any  debug
       tool. One can also manually access the exception information, for example:

          &gt;&gt;&gt; import sys
          &gt;&gt;&gt; sys.last_traceback.tb_lineno
          42
          &gt;&gt;&gt; sys.last_value
          AssertionError('assert result == "ok"',)

</pre><h4><b>DROPPING</b> <b>TO</b> <b>PDB</b> <b>(PYTHON</b> <b>DEBUGGER)</b> <b>AT</b> <b>THE</b> <b>START</b> <b>OF</b> <b>A</b> <b>TEST</b></h4><pre>
       <b>pytest</b>  allows  one  to drop into the <u>PDB</u> prompt immediately at the start of each test via a command line
       option:

          pytest --trace

       This will invoke the Python debugger at the start of every test.

</pre><h4><b>SETTING</b> <b>BREAKPOINTS</b></h4><pre>
       To set a breakpoint in your code use the native Python <b>import</b> <b>pdb;pdb.set_trace()</b> call in your  code  and
       pytest automatically disables its output capture for that test:

       • Output capture in other tests is not affected.

       • Any prior test output that has already been captured and will be processed as such.

       • Output capture gets resumed when ending the debugger session (via the <b>continue</b> command).

</pre><h4><b>USING</b> <b>THE</b> <b>BUILTIN</b> <b>BREAKPOINT</b> <b>FUNCTION</b></h4><pre>
       Python  3.7 introduces a builtin <b>breakpoint()</b> function.  Pytest supports the use of <b>breakpoint()</b> with the
       following behaviours:

          • When <b>breakpoint()</b> is called and <b>PYTHONBREAKPOINT</b> is set to the default value, pytest  will  use  the
            custom internal PDB trace UI instead of the system default <b>Pdb</b>.

          • When tests are complete, the system will default back to the system <b>Pdb</b> trace UI.

          • With  <b>--pdb</b>  passed  to  pytest, the custom internal Pdb trace UI is used with both <b>breakpoint()</b> and
            failed tests/unhandled exceptions.

          • <b>--pdbcls</b> can be used to specify a custom debugger class.

</pre><h4><b>PROFILING</b> <b>TEST</b> <b>EXECUTION</b> <b>DURATION</b></h4><pre>
       Changed in version 6.0.

       To get a list of the slowest 10 test durations over 1.0s long:

          pytest --durations=10 --durations-min=1.0

       By default, pytest will not show test durations that are too small (&lt;0.005s) unless <b>-vv</b> is passed on  the
       command-line.

</pre><h4><b>FAULT</b> <b>HANDLER</b></h4><pre>
       New in version 5.0.

       The <u>faulthandler</u> standard module can be used to dump Python tracebacks on a segfault or after a timeout.

       The  module  is  automatically  enabled  for  pytest  runs, unless the <b>-p</b> <b>no:faulthandler</b> is given on the
       command-line.

       Also the <b>faulthandler_timeout=X</b> configuration option can be used to dump the traceback of all threads  if
       a test takes longer than <b>X</b> seconds to finish (not available on Windows).

       <b>NOTE:</b>
          This  functionality  has  been integrated from the external <u>pytest-faulthandler</u> plugin, with two small
          differences:

          • To disable it, use <b>-p</b> <b>no:faulthandler</b> instead of <b>--no-faulthandler</b>: the former can be used with  any
            plugin, so it saves one option.

          • The  <b>--faulthandler-timeout</b>  command-line  option  has become the <b>faulthandler_timeout</b> configuration
            option. It can still be configured from the command-line using <b>-o</b> <b>faulthandler_timeout=X</b>.

</pre><h4><b>WARNING</b> <b>ABOUT</b> <b>UNRAISABLE</b> <b>EXCEPTIONS</b> <b>AND</b> <b>UNHANDLED</b> <b>THREAD</b> <b>EXCEPTIONS</b></h4><pre>
       New in version 6.2.

       <b>NOTE:</b>
          These features only work on Python&gt;=3.8.

       Unhandled exceptions are exceptions that are raised in a situation in which they cannot  propagate  to  a
       caller. The most common case is an exception raised in a <b>__</b><u>del</u><b>__</b> implementation.

       Unhandled  thread  exceptions  are  exceptions  raised in a <u>Thread</u> but not handled, causing the thread to
       terminate uncleanly.

       Both types of exceptions are normally considered bugs, but may go unnoticed because they don't cause  the
       program itself to crash. Pytest detects these conditions and issues a warning that is visible in the test
       run summary.

       The  plugins  are  automatically  enabled  for  pytest  runs,  unless  the <b>-p</b> <b>no:unraisableexception</b> (for
       unraisable exceptions) and <b>-p</b> <b>no:threadexception</b>  (for  thread  exceptions)  options  are  given  on  the
       command-line.

       The  warnings  may  be  silenced  selectivly  using  the pytest.mark.filterwarnings ref mark. The warning
       categories are <b>pytest.PytestUnraisableExceptionWarning</b> and <b>pytest.PytestUnhandledThreadExceptionWarning</b>.

</pre><h4><b>CREATING</b> <b>JUNITXML</b> <b>FORMAT</b> <b>FILES</b></h4><pre>
       To create result files which can be read by <u>Jenkins</u> or other Continuous  integration  servers,  use  this
       invocation:

          pytest --junitxml=path

       to create an XML file at <b>path</b>.

       To  set  the  name of the root test suite xml item, you can configure the <b>junit_suite_name</b> option in your
       config file:

          [pytest]
          junit_suite_name = my_suite

       New in version 4.0.

       JUnit XML specification seems to indicate that <b>"time"</b> attribute should report total test execution times,
       including setup and teardown (<u>1</u>, <u>2</u>).  It is the default pytest behavior. To report  just  call  durations
       instead, configure the <b>junit_duration_report</b> option like this:

          [pytest]
          junit_duration_report = call

   <b>record_property</b>
       If you want to log additional information for a test, you can use the <b>record_property</b> fixture:

          def test_function(record_property):
              record_property("example_key", 1)
              assert True

       This will add an extra property <b>example_key="1"</b> to the generated <b>testcase</b> tag:

          &lt;testcase classname="test_function" file="test_function.py" line="0" name="test_function" time="0.0009"&gt;
            &lt;properties&gt;
              &lt;property name="example_key" value="1" /&gt;
            &lt;/properties&gt;
          &lt;/testcase&gt;

       Alternatively, you can integrate this functionality with custom markers:

          # content of conftest.py

          def pytest_collection_modifyitems(session, config, items):
              for item in items:
                  for marker in item.iter_markers(name="test_id"):
                      test_id = marker.args[0]
                      item.user_properties.append(("test_id", test_id))

       And in your tests:

          # content of test_function.py
          import pytest

          @<a href="../man1501/pytest.mark.test_id.1501.html">pytest.mark.test_id</a>(1501)
          def test_function():
              assert True

       Will result in:

          &lt;testcase classname="test_function" file="test_function.py" line="0" name="test_function" time="0.0009"&gt;
            &lt;properties&gt;
              &lt;property name="test_id" value="1501" /&gt;
            &lt;/properties&gt;
          &lt;/testcase&gt;

       <b>WARNING:</b>
          Please  note  that  using this feature will break schema verifications for the latest JUnitXML schema.
          This might be a problem when used with some CI servers.

   <b>record_xml_attribute</b>
       To add an additional xml attribute to a testcase element, you can use <b>record_xml_attribute</b> fixture.  This
       can also be used to override existing values:

          def test_function(record_xml_attribute):
              record_xml_attribute("assertions", "REQ-1234")
              record_xml_attribute("classname", "custom_classname")
              print("hello world")
              assert True

       Unlike  <b>record_property</b>,  this  will  not  add  a new child element.  Instead, this will add an attribute
       <b>assertions="REQ-1234"</b> inside  the  generated  <b>testcase</b>  tag  and  override  the  default  <b>classname</b>  with
       <b>"classname=custom_classname"</b>:

          &lt;testcase classname="custom_classname" file="test_function.py" line="0" name="test_function" time="0.003" assertions="REQ-1234"&gt;
              &lt;system-out&gt;
                  hello world
              &lt;/system-out&gt;
          &lt;/testcase&gt;

       <b>WARNING:</b>
          <b>record_xml_attribute</b> is an experimental feature, and its interface might be replaced by something more
          powerful and general in future versions. The functionality per-se will be kept, however.

          Using  this  over  <b>record_xml_property</b> can help when using ci tools to parse the xml report.  However,
          some parsers are quite strict about the elements and attributes that are allowed.  Many tools  use  an
          xsd schema (like the example below) to validate incoming xml.  Make sure you are using attribute names
          that are allowed by your parser.

          Below is the Scheme used by Jenkins to validate the XML report:

              &lt;xs:element name="testcase"&gt;
                  &lt;xs:complexType&gt;
                      &lt;xs:sequence&gt;
                          &lt;xs:element ref="skipped" minOccurs="0" maxOccurs="1"/&gt;
                          &lt;xs:element ref="error" minOccurs="0" maxOccurs="unbounded"/&gt;
                          &lt;xs:element ref="failure" minOccurs="0" maxOccurs="unbounded"/&gt;
                          &lt;xs:element ref="system-out" minOccurs="0" maxOccurs="unbounded"/&gt;
                          &lt;xs:element ref="system-err" minOccurs="0" maxOccurs="unbounded"/&gt;
                      &lt;/xs:sequence&gt;
                      &lt;xs:attribute name="name" type="xs:string" use="required"/&gt;
                      &lt;xs:attribute name="assertions" type="xs:string" use="optional"/&gt;
                      &lt;xs:attribute name="time" type="xs:string" use="optional"/&gt;
                      &lt;xs:attribute name="classname" type="xs:string" use="optional"/&gt;
                      &lt;xs:attribute name="status" type="xs:string" use="optional"/&gt;
                  &lt;/xs:complexType&gt;
              &lt;/xs:element&gt;

       <b>WARNING:</b>
          Please  note  that  using this feature will break schema verifications for the latest JUnitXML schema.
          This might be a problem when used with some CI servers.

   <b>record_testsuite_property</b>
       New in version 4.5.

       If you want to add a properties node at the test-suite level, which  may  contains  properties  that  are
       relevant to all tests, you can use the <b>record_testsuite_property</b> session-scoped fixture:

       The <b>record_testsuite_property</b> session-scoped fixture can be used to add properties relevant to all tests.

          import pytest

          @pytest.fixture(scope="session", autouse=True)
          def log_global_env_facts(record_testsuite_property):
              record_testsuite_property("ARCH", "PPC")
              record_testsuite_property("STORAGE_TYPE", "CEPH")

          class TestMe:
              def test_foo(self):
                  assert True

       The fixture is a callable which receives <b>name</b> and <b>value</b> of a <b>&lt;property&gt;</b> tag added at the test-suite level
       of the generated xml:

          &lt;testsuite errors="0" failures="0" name="pytest" skipped="0" tests="1" time="0.006"&gt;
            &lt;properties&gt;
              &lt;property name="ARCH" value="PPC"/&gt;
              &lt;property name="STORAGE_TYPE" value="CEPH"/&gt;
            &lt;/properties&gt;
            &lt;testcase classname="test_me.TestMe" file="test_me.py" line="16" name="test_foo" time="0.000243663787842"/&gt;
          &lt;/testsuite&gt;

       <b>name</b> must be a string, <b>value</b> will be converted to a string and properly xml-escaped.

       The  generated  XML  is  compatible  with  the  latest  <b>xunit</b>  standard,  contrary to <u>record_property</u> and
       <u>record_xml_attribute</u>.

</pre><h4><b>CREATING</b> <b>RESULTLOG</b> <b>FORMAT</b> <b>FILES</b></h4><pre>
       To create plain-text machine-readable result files you can issue:

          pytest --resultlog=path

       and look at the content at the <b>path</b> location.  Such files are used e.g.  by the  <u>PyPy-test</u>  web  page  to
       show test results over several revisions.

       <b>WARNING:</b>
          This option is rarely used and is scheduled for removal in pytest 6.0.

          If you use this option, consider using the new <u>pytest-reportlog</u> plugin instead.

          See <u>the</u> <u>deprecation</u> <u>docs</u> for more information.

</pre><h4><b>SENDING</b> <b>TEST</b> <b>REPORT</b> <b>TO</b> <b>ONLINE</b> <b>PASTEBIN</b> <b>SERVICE</b></h4><pre>
       <b>Creating</b> <b>a</b> <b>URL</b> <b>for</b> <b>each</b> <b>test</b> <b>failure</b>:

          pytest --pastebin=failed

       This  will submit test run information to a remote Paste service and provide a URL for each failure.  You
       may select tests as usual or add for example <b>-x</b> if you only want to send one particular failure.

       <b>Creating</b> <b>a</b> <b>URL</b> <b>for</b> <b>a</b> <b>whole</b> <b>test</b> <b>session</b> <b>log</b>:

          pytest --pastebin=all

       Currently only pasting to the <u><a href="http://bpaste.net">http://bpaste.net</a></u> service is implemented.

       Changed in version 5.2.

       If creating the URL fails for any reason, a warning is generated  instead  of  failing  the  entire  test
       suite.

</pre><h4><b>EARLY</b> <b>LOADING</b> <b>PLUGINS</b></h4><pre>
       You can early-load plugins (internal and external) explicitly in the command-line with the <b>-p</b> option:

          pytest -p mypluginmodule

       The option receives a <b>name</b> parameter, which can be:

       • A full module dotted name, for example <b>myproject.plugins</b>. This dotted name must be importable.

       • The  entry-point name of a plugin. This is the name passed to <b>setuptools</b> when the plugin is registered.
         For example to early-load the <u>pytest-cov</u> plugin you can use:

            pytest -p pytest_cov

</pre><h4><b>DISABLING</b> <b>PLUGINS</b></h4><pre>
       To disable loading specific plugins at invocation time, use the <b>-p</b> option together with the prefix <b>no:</b>.

       Example: to disable loading the plugin <b>doctest</b>, which is responsible for  executing  doctest  tests  from
       text files, invoke pytest like this:

          pytest -p no:doctest

</pre><h4><b>CALLING</b> <b>PYTEST-3</b> <b>FROM</b> <b>PYTHON</b> <b>CODE</b></h4><pre>
       You can invoke <b>pytest</b> from Python code directly:

          pytest.main()

       this  acts  as if you would call "pytest" from the command line.  It will not raise <b>SystemExit</b> but return
       the exitcode instead.  You can pass in options and arguments:

          pytest.main(["-x", "mytestdir"])

       You can specify additional plugins to <b>pytest.main</b>:

          # content of myinvoke.py
          import pytest

          class MyPlugin:
              def pytest_sessionfinish(self):
                  print("*** test run reporting finishing")

          pytest.main(["-qq"], plugins=[MyPlugin()])

       Running it will show that <b>MyPlugin</b> was added and its hook was invoked:

          $ python myinvoke.py
          .FEsxX.                                                              [100%]*** test run reporting finishing

          ================================== ERRORS ==================================
          _______________________ ERROR at setup of test_error _______________________

              @pytest.fixture
              def error_fixture():
          &gt;       assert 0
          E       assert 0

          test_example.py:6: AssertionError
          ================================= FAILURES =================================
          ________________________________ test_fail _________________________________

              def test_fail():
          &gt;       assert 0
          E       assert 0

          test_example.py:14: AssertionError
          ========================= short test summary info ==========================
          FAILED test_example.py::test_fail - assert 0
          ERROR test_example.py::test_error - assert 0

       <b>NOTE:</b>
          Calling <b>pytest.main()</b> will result in importing your tests and any modules that they import. Due to the
          caching mechanism of python's import system, making subsequent calls to <b>pytest.main()</b>  from  the  same
          process  will  not  reflect changes to those files between the calls. For this reason, making multiple
          calls to <b>pytest.main()</b> from the  same  process  (in  order  to  re-run  tests,  for  example)  is  not
          recommended.

</pre><h4><b>AUTHOR</b></h4><pre>
       holger krekel at merlinux eu

</pre><h4><b>COPYRIGHT</b></h4><pre>
       2015–2020, holger krekel and pytest-dev team

6.2.5                                             Feb 09, 2022                                       <u><a href="../man1/PYTEST-3.1.html">PYTEST-3</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>