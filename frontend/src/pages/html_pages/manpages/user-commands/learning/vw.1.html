<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vw - Vowpal Wabbit -- fast online learning tool</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/noble/+package/vowpal-wabbit">vowpal-wabbit_8.6.1.dfsg1-1build3_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       vw - Vowpal Wabbit -- fast online learning tool

</pre><h4><b>DESCRIPTION</b></h4><pre>
   <b>VW</b> <b>options:</b>
       <b>--ring_size</b> arg
              size of example ring

       <b>--onethread</b>
              Disable parse thread

   <b>Update</b> <b>options:</b>
       <b>-l</b> [ <b>--learning_rate</b> ] arg
              Set learning rate

       <b>--power_t</b> arg
              t power value

       <b>--decay_learning_rate</b> arg
              Set Decay factor for learning_rate between passes

       <b>--initial_t</b> arg
              initial t value

       <b>--feature_mask</b> arg
              Use  existing  regressor  to  determine  which parameters may be updated.  If no initial_regressor
              given, also used for initial weights.

   <b>Weight</b> <b>options:</b>
       <b>-i</b> [ <b>--initial_regressor</b> ] arg
              Initial regressor(s)

       <b>--initial_weight</b> arg
              Set all weights to an initial value of arg.

       <b>--random_weights</b> arg
              make initial weights random

       <b>--normal_weights</b> arg
              make initial weights normal

       <b>--truncated_normal_weights</b> arg
              make initial weights truncated normal

       <b>--sparse_weights</b>
              Use a sparse datastructure for weights

       <b>--input_feature_regularizer</b> arg
              Per feature regularization input file

   <b>Parallelization</b> <b>options:</b>
       <b>--span_server</b> arg
              Location of server for setting up spanning tree

       <b>--threads</b>
              Enable multi-threading

       <b>--unique_id</b> arg (=0)
              unique id used for cluster parallel jobs

       <b>--total</b> arg (=1)
              total number of nodes used in cluster parallel job

       <b>--node</b> arg (=0)
              node number in cluster parallel job

   <b>Diagnostic</b> <b>options:</b>
       <b>--version</b>
              Version information

       <b>-a</b> [ <b>--audit</b> ]
              print weights of features

       <b>-P</b> [ <b>--progress</b> ] arg
              Progress update frequency. int: additive, float: multiplicative

       <b>--quiet</b>
              Don't output disgnostics and progress updates

       <b>-h</b> [ <b>--help</b> ]
              Look here: <a href="http://hunch.net/~vw/">http://hunch.net/~vw/</a> and click on Tutorial.

   <b>Random</b> <b>Seed</b> <b>option:</b>
       <b>--random_seed</b> arg
              seed random number generator

   <b>Feature</b> <b>options:</b>
       <b>--hash</b> arg
              how to hash the features. Available options: strings, all

       <b>--hash_seed</b> arg (=0)
              seed for hash function

       <b>--ignore</b> arg
              ignore namespaces beginning with character &lt;arg&gt;

       <b>--ignore_linear</b> arg
              ignore namespaces beginning with character &lt;arg&gt; for linear terms only

       <b>--keep</b> arg
              keep namespaces beginning with character &lt;arg&gt;

       <b>--redefine</b> arg
              redefine namespaces beginning with characters of string S as namespace N.  &lt;arg&gt; shall be in  form
              'N:=S'  where := is operator. Empty N or S are treated as default namespace. Use ':' as a wildcard
              in S.

       <b>-b</b> [ <b>--bit_precision</b> ] arg
              number of bits in the feature table

       <b>--noconstant</b>
              Don't add a constant feature

       <b>-C</b> [ <b>--constant</b> ] arg
              Set initial value of constant

       <b>--ngram</b> arg
              Generate N grams. To generate N grams for a single namespace 'foo', arg should be fN.

       <b>--skips</b> arg
              Generate skips in N grams. This in conjunction  with  the  ngram  tag  can  be  used  to  generate
              generalized n-skip-k-gram. To generate n-skips for a single namespace 'foo', arg should be fN.

       <b>--feature_limit</b> arg
              limit to N features. To apply to a single namespace 'foo', arg should be fN

       <b>--affix</b> arg
              generate  prefixes/suffixes  of features; argument '+2a,-3b,+1' means generate 2-char prefixes for
              namespace a, 3-char suffixes for b and 1 char prefixes for default namespace

       <b>--spelling</b> arg
              compute spelling features for a give namespace (use '_' for default namespace)

       <b>--dictionary</b> arg
              read a dictionary for additional features (arg either 'x:file' or just 'file')

       <b>--dictionary_path</b> arg
              look in this directory for dictionaries; defaults to current directory or env{PATH}

       <b>--interactions</b> arg
              Create feature interactions of any level between namespaces.

       <b>--permutations</b>
              Use permutations instead of combinations for feature interactions of same namespace.

       <b>--leave_duplicate_interactions</b>
              Don't remove interactions with duplicate combinations of namespaces.  For ex. this is a duplicate:
              '-q ab <b>-q</b> ba' and a lot more in '-q ::'.

       <b>-q</b> [ <b>--quadratic</b> ] arg
              Create and use quadratic features

       <b>--q</b>: arg
              : corresponds to a wildcard for all printable characters

       <b>--cubic</b> arg
              Create and use cubic features

   <b>Example</b> <b>options:</b>
       <b>-t</b> [ <b>--testonly</b> ]
              Ignore label information and just test

       <b>--holdout_off</b>
              no holdout data in multiple passes

       <b>--holdout_period</b> arg (=10)
              holdout period for test only

       <b>--holdout_after</b> arg
              holdout after n training examples, default off (disables holdout_period)

       <b>--early_terminate</b> arg (=3)
              Specify the number of passes tolerated when holdout loss doesn't decrease before early termination

       <b>--passes</b> arg
              Number of Training Passes

       <b>--initial_pass_length</b> arg
              initial number of examples per pass

       <b>--examples</b> arg
              number of examples to parse

       <b>--min_prediction</b> arg
              Smallest prediction to output

       <b>--max_prediction</b> arg
              Largest prediction to output

       <b>--sort_features</b>
              turn this on to disregard order in which features have been defined. This  will  lead  to  smaller
              cache sizes

       <b>--loss_function</b> arg (=squared)
              Specify  the  loss  function  to  be  used,  uses squared by default. Currently available ones are
              squared, classic, hinge, logistic, quantile and poisson.

       <b>--quantile_tau</b> arg (=0.5)
              Parameter \tau associated with Quantile loss. Defaults to 0.5

       <b>--l1</b> arg
              l_1 lambda

       <b>--l2</b> arg
              l_2 lambda

       <b>--no_bias_regularization</b> arg
              no bias in regularization

       <b>--named_labels</b> arg
              use names for labels (multiclass, etc.)  rather than integers,  argument  specified  all  possible
              labels, comma-sep, eg "--named_labels Noun,Verb,Adj,Punc"

   <b>Output</b> <b>model:</b>
       <b>-f</b> [ <b>--final_regressor</b> ] arg
              Final regressor

       <b>--readable_model</b> arg
              Output human-readable final regressor with numeric features

       <b>--invert_hash</b> arg
              Output human-readable final regressor with feature names.  Computationally expensive.

       <b>--save_resume</b>
              save extra state so learning can be resumed later with new data

       <b>--preserve_performance_counters</b>
              reset performance counters when warmstarting

       <b>--save_per_pass</b>
              Save the model after every pass over data

       <b>--output_feature_regularizer_binary</b> arg
              Per feature regularization output file

       <b>--output_feature_regularizer_text</b> arg Per feature regularization output file,
              in text

       <b>--id</b> arg
              User supplied ID embedded into the final regressor

   <b>Output</b> <b>options:</b>
       <b>-p</b> [ <b>--predictions</b> ] arg
              File to output predictions to

       <b>-r</b> [ <b>--raw_predictions</b> ] arg
              File to output unnormalized predictions to

   <b>Audit</b> <b>Regressor:</b>
       <b>--audit_regressor</b> arg
              stores  feature  names  and  their  regressor values. Same dataset must be used for both regressor
              training and this mode.

   <b>Search</b> <b>options:</b>
       <b>--search</b> arg
              Use learning to search, argument=maximum action id or 0 for LDF

       <b>--search_task</b> arg
              the search task (use "--search_task list" to get a list of available tasks)

       <b>--search_metatask</b> arg
              the search metatask (use "--search_metatask list" to get a list of available metatasks)

       <b>--search_interpolation</b> arg
              at what level should interpolation happen? [*data|policy]

       <b>--search_rollout</b> arg
              how should rollouts be executed?  [policy|oracle|*mix_per_state|mix_p

              er_roll|none]

       <b>--search_rollin</b> arg
              how should past trajectories be generated? [policy|oracle|*mix_per_stat e|mix_per_roll]

       <b>--search_passes_per_policy</b> arg (=1)
              number of passes per policy (only valid for search_interpolation=policy)

       <b>--search_beta</b> arg (=0.5)
              interpolation rate for policies (only valid for search_interpolation=policy)

       <b>--search_alpha</b> arg (=1.00000001e-10)
              annealed beta = 1-(1-alpha)^t (only valid for search_interpolation=data)

       <b>--search_total_nb_policies</b> arg
              if we are going to train the policies through multiple separate calls to vw, we  need  to  specify
              this parameter and tell vw how many policies are eventually going to be trained

       <b>--search_trained_nb_policies</b> arg
              the number of trained policies in a file

       <b>--search_allowed_transitions</b> arg
              read file of allowed transitions [def: all transitions are allowed]

       <b>--search_subsample_time</b> arg
              instead  of  training  at all timesteps, use a subset. if value in (0,1), train on a random v%. if
              v&gt;=1, train on precisely v steps per example, if v&lt;=-1, use active learning

       <b>--search_neighbor_features</b> arg
              copy features from neighboring lines.  argument looks like: '-1:a,+2' meaning copy  previous  line
              namespace a and next next line from namespace _unnamed_, where ',' separates them

       <b>--search_rollout_num_steps</b> arg
              how  many  calls  of  "loss"  before  we  stop  really predicting on rollouts and switch to oracle
              (default means "infinite")

       <b>--search_history_length</b> arg (=1)
              some tasks allow you to specify how much history their depend on; specify that here

       <b>--search_no_caching</b>
              turn off the built-in caching ability (makes things slower, but technically more safe)

       <b>--search_xv</b>
              train two separate policies, alternating prediction/learning

       <b>--search_perturb_oracle</b> arg (=0)
              perturb the oracle on rollin with this probability

       <b>--search_linear_ordering</b>
              insist on generating examples in linear order (def: hoopla permutation)

       <b>--search_active_verify</b> arg
              verify that active learning is doing the right thing (arg = multiplier, should be =  cost_range  *
              range_c)

       <b>--search_save_every_k_runs</b> arg
              save model every k runs

   <b>Experience</b> <b>Replay:</b>
       <b>--replay_c</b> arg
              use  experience  replay  at  a  specified level [b=classification/regression, m=multiclass, c=cost
              sensitive] with specified buffer size

       <b>--replay_c_count</b> arg (=1)
              how many times (in expectation) should each example be played (default: 1 = permuting)

   <b>Explore</b> <b>evaluation:</b>
       <b>--explore_eval</b>
              Evaluate explore_eval adf policies

       <b>--multiplier</b> arg
              Multiplier used to make all rejection sample probabilities &lt;= 1

   <b>Make</b> <b>Multiclass</b> <b>into</b> <b>Contextual</b> <b>Bandit:</b>
       <b>--cbify</b> arg
              Convert multiclass on &lt;k&gt; classes into a contextual bandit problem

       <b>--cbify_cs</b>
              consume cost-sensitive classification examples instead of multiclass

       <b>--loss0</b> arg (=0)
              loss for correct label

       <b>--loss1</b> arg (=1)
              loss for incorrect label

   <b>Contextual</b> <b>Bandit</b> <b>Exploration</b> <b>with</b> <b>Action</b> <b>Dependent</b> <b>Features:</b>
       <b>--cb_explore_adf</b>
              Online explore-exploit for a contextual bandit problem with multiline action dependent features

       <b>--first</b> arg
              tau-first exploration

       <b>--epsilon</b> arg
              epsilon-greedy exploration

       <b>--bag</b> arg
              bagging-based exploration

       <b>--cover</b> arg
              Online cover based exploration

       <b>--psi</b> arg (=1)
              disagreement parameter for cover

       <b>--nounif</b>
              do not explore uniformly on zero-probability actions in cover

       <b>--softmax</b>
              softmax exploration

       <b>--regcb</b>
              RegCB-elim exploration

       <b>--regcbopt</b>
              RegCB optimistic exploration

       <b>--mellowness</b> arg (=0.100000001)
              RegCB mellowness parameter c_0. Default 0.1

       <b>--greedify</b>
              always update first policy once in bagging

       <b>--cb_min_cost</b> arg (=0)
              lower bound on cost

       <b>--cb_max_cost</b> arg (=1)
              upper bound on cost

       <b>--first_only</b>
              Only explore the first action in a tie-breaking event

       <b>--lambda</b> arg (=-1)
              parameter for softmax

   <b>Contextual</b> <b>Bandit</b> <b>Exploration:</b>
       <b>--cb_explore</b> arg
              Online explore-exploit for a &lt;k&gt; action contextual bandit problem

       <b>--first</b> arg
              tau-first exploration

       <b>--epsilon</b> arg (=0.0500000007)
              epsilon-greedy exploration

       <b>--bag</b> arg
              bagging-based exploration

       <b>--cover</b> arg
              Online cover based exploration

       <b>--psi</b> arg (=1)
              disagreement parameter for cover

   <b>Multiworld</b> <b>Testing</b> <b>Options:</b>
       <b>--multiworld_test</b> arg
              Evaluate features as a policies

       <b>--learn</b> arg
              Do Contextual Bandit learning on &lt;n&gt; classes.

       <b>--exclude_eval</b>
              Discard mwt policy features before learning

   <b>Contextual</b> <b>Bandit</b> <b>with</b> <b>Action</b> <b>Dependent</b> <b>Features:</b>
       <b>--cb_adf</b>
              Do Contextual Bandit learning with multiline action dependent features.

       <b>--rank_all</b>
              Return actions sorted by score order

       <b>--no_predict</b>
              Do not do a prediction when training

       <b>--cb_type</b> arg (=ips)
              contextual bandit method to use in {ips,dm,dr, mtr}

   <b>Contextual</b> <b>Bandit</b> <b>Options:</b>
       <b>--cb</b> arg
              Use contextual bandit learning with &lt;k&gt; costs

       <b>--cb_type</b> arg (=dr)
              contextual bandit method to use in {ips,dm,dr}

       <b>--eval</b> Evaluate a policy rather than optimizing.

   <b>Cost</b> <b>Sensitive</b> <b>One</b> <b>Against</b> <b>All</b> <b>with</b> <b>Label</b> <b>Dependent</b> <b>Features:</b>
       <b>--csoaa_ldf</b> arg
              Use one-against-all multiclass learning with label dependent features.

       <b>--ldf_override</b> arg
              Override singleline or multiline from csoaa_ldf or wap_ldf, eg if stored in file

       <b>--csoaa_rank</b>
              Return actions sorted by score order

       <b>--probabilities</b>
              predict probabilites of all classes

       <b>--wap_ldf</b> arg
              Use weighted all-pairs multiclass learning with label dependent features.

              Specify singleline or multiline.

   <b>Interact</b> <b>via</b> <b>elementwise</b> <b>multiplication:</b>
       <b>--interact</b> arg
              Put weights on feature products from namespaces &lt;n1&gt; and &lt;n2&gt;

   <b>Cost</b> <b>Sensitive</b> <b>One</b> <b>Against</b> <b>All:</b>
       <b>--csoaa</b> arg
              One-against-all multiclass with &lt;k&gt; costs

   <b>Cost-sensitive</b> <b>Active</b> <b>Learning:</b>
       <b>--cs_active</b> arg
              Cost-sensitive active learning with &lt;k&gt; costs

       <b>--simulation</b>
              cost-sensitive active learning simulation mode

       <b>--baseline</b>
              cost-sensitive active learning baseline

       <b>--domination</b>
              cost-sensitive active learning use domination. Default 1

       <b>--mellowness</b> arg (=0.100000001)
              mellowness parameter c_0. Default 0.1.

       <b>--range_c</b> arg (=0.5)
              parameter controlling the threshold for per-label cost uncertainty. Default 0.5.

       <b>--max_labels</b> arg (=18446744073709551615)
              maximum number of label queries.

       <b>--min_labels</b> arg (=18446744073709551615)
              minimum number of label queries.

       <b>--cost_max</b> arg (=1)
              cost upper bound. Default 1.

       <b>--cost_min</b> arg (=0)
              cost lower bound. Default 0.

       <b>--csa_debug</b>
              print debug stuff for cs_active

   <b>Multilabel</b> <b>One</b> <b>Against</b> <b>All:</b>
       <b>--multilabel_oaa</b> arg
              One-against-all multilabel with &lt;k&gt; labels

   <b>importance</b> <b>weight</b> <b>classes:</b>
       <b>--classweight</b> arg
              importance weight multiplier for class

   <b>Recall</b> <b>Tree:</b>
       <b>--recall_tree</b> arg
              Use online tree for multiclass

       <b>--max_candidates</b> arg
              maximum number of labels per leaf in the tree

       <b>--bern_hyper</b> arg (=1)
              recall tree depth penalty

       <b>--max_depth</b> arg
              maximum depth of the tree, default log_2 (#classes)

       <b>--node_only</b> arg (=0)
              only use node features, not full path features

       <b>--randomized_routing</b> arg (=0)
              randomized routing

   <b>Logarithmic</b> <b>Time</b> <b>Multiclass</b> <b>Tree:</b>
       <b>--log_multi</b> arg
              Use online tree for multiclass

       <b>--no_progress</b>
              disable progressive validation

       <b>--swap_resistance</b> arg (=4)
              higher = more resistance to swap, default=4

   <b>Error</b> <b>Correcting</b> <b>Tournament</b> <b>Options:</b>
       <b>--ect</b> arg
              Error correcting tournament with &lt;k&gt; labels

       <b>--error</b> arg (=0)
              errors allowed by ECT

   <b>Boosting:</b>
       <b>--boosting</b> arg
              Online boosting with &lt;N&gt; weak learners

       <b>--gamma</b> arg (=0.100000001)
              weak learner's edge (=0.1), used only by online BBM

       <b>--alg</b> arg (=BBM)
              specify the boosting algorithm: BBM (default), logistic (AdaBoost.OL.W), adaptive (AdaBoost.OL)

   <b>One</b> <b>Against</b> <b>All</b> <b>Options:</b>
       <b>--oaa</b> arg
              One-against-all multiclass with &lt;k&gt; labels

       <b>--oaa_subsample</b> arg
              subsample this number of negative examples when learning

       <b>--probabilities</b>
              predict probabilites of all classes

       <b>--scores</b>
              output raw scores per class

   <b>Top</b> <b>K:</b>
       <b>--top</b> arg
              top k recommendation

   <b>Experience</b> <b>Replay:</b>
       <b>--replay_m</b> arg
              use experience replay at a  specified  level  [b=classification/regression,  m=multiclass,  c=cost
              sensitive] with specified buffer size

       <b>--replay_m_count</b> arg (=1)
              how many times (in expectation) should each example be played (default: 1 = permuting)

   <b>Binary</b> <b>loss:</b>
       <b>--binary</b>
              report loss as binary classification on <b>-1</b>,1

   <b>Bootstrap:</b>
       <b>--bootstrap</b> arg
              k-way bootstrap by online importance resampling

       <b>--bs_type</b> arg
              prediction type {mean,vote}

   <b>scorer</b> <b>options:</b>
       <b>--link</b> arg (=identity)
              Specify the link function: identity, logistic, glf1 or poisson

   <b>Stagewise</b> <b>polynomial</b> <b>options:</b>
       <b>--stage_poly</b>
              use stagewise polynomial feature learning

       <b>--sched_exponent</b> arg (=1)
              exponent controlling quantity of included features

       <b>--batch_sz</b> arg (=1000)
              multiplier on batch size before including more features

       <b>--batch_sz_no_doubling</b>
              batch_sz does not double

   <b>Low</b> <b>Rank</b> <b>Quadratics</b> <b>FA:</b>
       <b>--lrqfa</b> arg
              use low rank quadratic features with field aware weights

   <b>Low</b> <b>Rank</b> <b>Quadratics:</b>
       <b>--lrq</b> arg
              use low rank quadratic features

       <b>--lrqdropout</b>
              use dropout training for low rank quadratic features

   <b>Autolink:</b>
       <b>--autolink</b> arg
              create link function with polynomial d

   <b>Marginal:</b>
       <b>--marginal</b> arg
              substitute marginal label estimates for ids

       <b>--initial_denominator</b> arg (=1)
              initial denominator

       <b>--initial_numerator</b> arg (=0.5)
              initial numerator

       <b>--compete</b>
              enable competition with marginal features

       <b>--update_before_learn</b> arg (=0)
              update marginal values before learning

       <b>--unweighted_marginals</b> arg (=0)
              ignore importance weights when computing marginals

       <b>--decay</b> arg (=0)
              decay multiplier per event (1e-3 for example)

   <b>Matrix</b> <b>Factorization</b> <b>Reduction:</b>
       <b>--new_mf</b> arg
              rank for reduction-based matrix factorization

   <b>Neural</b> <b>Network:</b>
       <b>--nn</b> arg
              Sigmoidal feedforward network with &lt;k&gt; hidden units

       <b>--inpass</b>
              Train or test sigmoidal feedforward network with input passthrough.

       <b>--multitask</b>
              Share hidden layer across all reduced tasks.

       <b>--dropout</b>
              Train or test sigmoidal feedforward network using dropout.

       <b>--meanfield</b>
              Train or test sigmoidal feedforward network using mean field.

   <b>Confidence:</b>
       <b>--confidence</b>
              Get confidence for binary predictions

       <b>--confidence_after_training</b>
              Confidence after training

   <b>Active</b> <b>Learning</b> <b>with</b> <b>Cover:</b>
       <b>--active_cover</b>
              enable active learning with cover

       <b>--mellowness</b> arg (=8)
              active learning mellowness parameter c_0. Default 8.

       <b>--alpha</b> arg (=1)
              active learning variance upper bound parameter alpha. Default 1.

       <b>--beta_scale</b> arg (=3.1622777)
              active learning variance upper bound parameter beta_scale. Default <a href="../man10/sqrt.10.html">sqrt</a>(10).

       <b>--cover</b> arg (=12)
              cover size. Default 12.

       <b>--oracular</b>
              Use Oracular-CAL style query or not.  Default false.

   <b>Active</b> <b>Learning:</b>
       <b>--active</b>
              enable active learning

       <b>--simulation</b>
              active learning simulation mode

       <b>--mellowness</b> arg (=8)
              active learning mellowness parameter c_0. Default 8

   <b>Experience</b> <b>Replay:</b>
       <b>--replay_b</b> arg
              use  experience  replay  at  a  specified level [b=classification/regression, m=multiclass, c=cost
              sensitive] with specified buffer size

       <b>--replay_b_count</b> arg (=1)
              how many times (in expectation) should each example be played (default: 1 = permuting)

   <b>Baseline</b> <b>options:</b>
       <b>--baseline</b>
              Learn an additive baseline (from constant features) and a residual separately in regression.

       <b>--lr_multiplier</b> arg
              learning rate multiplier for baseline model

       <b>--global_only</b>
              use separate example with only global constant for baseline predictions

       <b>--check_enabled</b>
              only use baseline when the example contains enabled flag

   <b>OjaNewton</b> <b>options:</b>
       <b>--OjaNewton</b>
              Online Newton with Oja's Sketch

       <b>--sketch_size</b> arg (=10)
              size of sketch

       <b>--epoch_size</b> arg (=1)
              size of epoch

       <b>--alpha</b> arg (=1)
              mutiplicative constant for indentiy

       <b>--alpha_inverse</b> arg
              one over alpha, similar to learning rate

       <b>--learning_rate_cnt</b> arg (=2)
              constant for the learning rate 1/t

       <b>--normalize</b> arg (=1)
              normalize the features or not

       <b>--random_init</b> arg (=1)
              randomize initialization of Oja or not

   <b>LBFGS</b> <b>and</b> <b>Conjugate</b> <b>Gradient</b> <b>options:</b>
       <b>--conjugate_gradient</b>
              use conjugate gradient based optimization

       <b>--bfgs</b> use bfgs optimization

       <b>--hessian_on</b>
              use second derivative in line search

       <b>--mem</b> arg (=15)
              memory in bfgs

       <b>--termination</b> arg (=0.00100000005)
              Termination threshold

   <b>Latent</b> <b>Dirichlet</b> <b>Allocation:</b>
       <b>--lda</b> arg
              Run lda with &lt;int&gt; topics

       <b>--lda_alpha</b> arg (=0.100000001)
              Prior on sparsity of per-document topic weights

       <b>--lda_rho</b> arg (=0.100000001)
              Prior on sparsity of topic distributions

       <b>--lda_D</b> arg (=10000)
              Number of documents

       <b>--lda_epsilon</b> arg (=0.00100000005)
              Loop convergence threshold

       <b>--minibatch</b> arg (=1)
              Minibatch size, for LDA

       <b>--math-mode</b> arg (=0)
              Math mode: simd, accuracy, fast-approx

       <b>--metrics</b> arg (=0)
              Compute metrics

   <b>Noop</b> <b>Learner:</b>
       <b>--noop</b> do no learning

   <b>Print</b> <b>psuedolearner:</b>
       <b>--print</b>
              print examples

   <b>Gradient</b> <b>Descent</b> <b>Matrix</b> <b>Factorization:</b>
       <b>--rank</b> arg
              rank for matrix factorization.

   <b>Network</b> <b>sending:</b>
       <b>--sendto</b> arg
              send examples to &lt;host&gt;

   <b>Stochastic</b> <b>Variance</b> <b>Reduced</b> <b>Gradient:</b>
       <b>--svrg</b> Streaming Stochastic Variance Reduced Gradient

       <b>--stage_size</b> arg (=1)
              Number of passes per SVRG stage

   <b>Follow</b> <b>the</b> <b>Regularized</b> <b>Leader:</b>
       <b>--ftrl</b> FTRL: Follow the Proximal Regularized Leader

       <b>--ftrl_alpha</b> arg (=0.00499999989)
              Learning rate for FTRL optimization

       <b>--ftrl_beta</b> arg (=0.100000001)
              FTRL beta parameter

       <b>--pistol</b>
              FTRL: Parameter-free Stochastic Learning

       <b>--ftrl_alpha</b> arg (=1)
              Learning rate for FTRL optimization

       <b>--ftrl_beta</b> arg (=0.5)
              FTRL beta parameter

   <b>Kernel</b> <b>SVM:</b>
       <b>--ksvm</b> kernel svm

       <b>--reprocess</b> arg (=1)
              number of reprocess steps for LASVM

       <b>--pool_greedy</b>
              use greedy selection on mini pools

       <b>--para_active</b>
              do parallel active learning

       <b>--pool_size</b> arg (=1)
              size of pools for active learning

       <b>--subsample</b> arg (=1)
              number of items to subsample from the pool

       <b>--kernel</b> arg (=linear)
              type of kernel (rbf or linear (default))

       <b>--bandwidth</b> arg (=1)
              bandwidth of rbf kernel

       <b>--degree</b> arg (=2)
              degree of poly kernel

       <b>--lambda</b> arg
              saving regularization for test time

   <b>Gradient</b> <b>Descent</b> <b>options:</b>
       <b>--sgd</b>  use regular stochastic gradient descent update.

       <b>--adaptive</b>
              use adaptive, individual learning rates.

       <b>--adax</b> use adaptive learning rates with x^2 instead of g^2x^2

       <b>--invariant</b>
              use safe/importance aware updates.

       <b>--normalized</b>
              use per feature normalized updates

       <b>--sparse_l2</b> arg (=0)
              use per feature normalized updates

       <b>--l1_state</b> arg (=0)
              use per feature normalized updates

       <b>--l2_state</b> arg (=1)
              use per feature normalized updates

   <b>Input</b> <b>options:</b>
       <b>-d</b> [ <b>--data</b> ] arg
              Example Set

       <b>--daemon</b>
              persistent daemon mode on port 26542

       <b>--foreground</b>
              in persistent daemon mode, do not run in the background

       <b>--port</b> arg
              port to listen on; use 0 to pick unused port

       <b>--num_children</b> arg
              number of children for persistent daemon mode

       <b>--pid_file</b> arg
              Write pid file in persistent daemon mode

       <b>--port_file</b> arg
              Write port used in persistent daemon mode

       <b>-c</b> [ <b>--cache</b> ]
              Use a cache.  The default is &lt;data&gt;.cache

       <b>--cache_file</b> arg
              The location(s) of cache_file.

       <b>--json</b> Enable JSON parsing.

       <b>--dsjson</b>
              Enable Decision Service JSON parsing.

       <b>-k</b> [ <b>--kill_cache</b> ]
              do not reuse existing cache: create a new one always

       <b>--compressed</b>
              use gzip format whenever possible. If a cache  file  is  being  created,  this  option  creates  a
              compressed   cache   file.   A  mixture  of  raw-text  &amp;  compressed  inputs  are  supported  with
              autodetection.

       <b>--no_stdin</b>
              do not default to reading from stdin

vw 8.6.1                                          December 2020                                            <u><a href="../man1/VW.1.html">VW</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>