<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>scrun - an OCI runtime proxy for Slurm.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/slurm-client">slurm-client_24.11.3-2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       <b>scrun</b> - an OCI runtime proxy for Slurm.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>Create</b> <b>Operation</b>
              <b>scrun</b> [<u>GLOBAL</u> <u>OPTIONS</u>...] <u>create</u> [<u>CREATE</u> <u>OPTIONS</u>] &lt;<u>container-id</u>&gt;

              Prepares a new container with container-id in current working directory.

       <b>Start</b> <b>Operation</b>
              <b>scrun</b> [<u>GLOBAL</u> <u>OPTIONS</u>...] <u>start</u> &lt;<u>container-id</u>&gt;

              Request to start and run container in job.

       <b>Query</b> <b>State</b> <b>Operation</b>
              <b>scrun</b> [<u>GLOBAL</u> <u>OPTIONS</u>...] <u>state</u> &lt;<u>container-id</u>&gt;

              Output OCI defined JSON state of container.

       <b>Kill</b> <b>Operation</b>
              <b>scrun</b> [<u>GLOBAL</u> <u>OPTIONS</u>...] <u>kill</u> &lt;<u>container-id</u>&gt; [<u>signal</u>]

              Send signal (default: SIGTERM) to container.

       <b>Delete</b> <b>Operation</b>
              <b>scrun</b> [<u>GLOBAL</u> <u>OPTIONS</u>...] <u>delete</u> [<u>DELETE</u> <u>OPTIONS</u>] &lt;<u>container-id</u>&gt;

              Release any resources held by container locally and remotely.

       Perform OCI runtime operations against <u>container-id</u> per:
       https://github.com/opencontainers/runtime-spec/blob/main/runtime.md

       <b>scrun</b>  attempts  to  mimic  the  commandline behavior as closely as possible to <b>crun</b> and <b>runc</b> in order to
       maintain in place replacement compatibility with <b>DOCKER</b> and <b>podman</b>. All commandline  arguments  for  <b>crun</b>
       and <b>runc</b> will be accepted for compatibility but may be ignored depending on their applicability.

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>scrun</b>  is  an  OCI  runtime  proxy  for Slurm. It acts as a common interface to <b>DOCKER</b> or <b>podman</b> to allow
       container operations to be executed under Slurm as jobs.  <b>scrun</b>  will  accept  all  commands  as  an  OCI
       compliant  runtime  but will proxy the container and all STDIO to Slurm for scheduling and execution. The
       containers will be executed remotely on Slurm compute nodes according to settings in <b><a href="../man5/oci.conf.5.html">oci.conf</a></b>(5).

       <b>scrun</b> requires all containers to be OCI image compliant per:
       https://github.com/opencontainers/image-spec/blob/main/spec.md

</pre><h4><b>RETURN</b> <b>VALUE</b></h4><pre>
       On successful operation, <b>scrun</b> will return 0. For any other condition  <b>scrun</b>  will  return  any  non-zero
       number to denote a error.

</pre><h4><b>GLOBAL</b> <b>OPTIONS</b></h4><pre>
       <b>--cgroup-manager</b>
              Ignored.

       <b>--debug</b>
              Activate debug level logging.

       <b>-f</b> &lt;<u>slurm_conf_path</u>&gt;
              Use specified slurm.conf for configuration.
              Default: sysconfdir from <b>configure</b> during compilation

       <b>--usage</b>
              Show quick help on how to call <b>scrun</b>

       <b>--log-format</b>=&lt;<u>json|text</u>&gt;
              Optional select format for logging. May be "json" or "text".
              Default: text

       <b>--root</b>=&lt;<u>root_path</u>&gt;
              Path  to spool directory to communication sockets and temporary directories and files. This should
              be a tmpfs and should be cleared on reboot.
              Default: <a href="file:/run/user/">/run/user/</a><u>{user_id}</u>/scrun/

       <b>--rootless</b>
              Ignored. All <b>scrun</b> commands are always rootless.

       <b>--systemd-cgroup</b>
              Ignored.

       <b>-v</b>     Increase logging verbosity. Multiple -v's increase verbosity.

       <b>-V</b>, <b>--version</b>
              Print version information and exit.

</pre><h4><b>CREATE</b> <b>OPTIONS</b></h4><pre>
       <b>-b</b> &lt;<u>bundle_path</u>&gt;, <b>--bundle</b>=&lt;<u>bundle_path</u>&gt;
              Path to the root of the bundle directory.
              Default: caller's working directory

       <b>--console-socket</b>=&lt;<u>console_socket_path</u>&gt;
              Optional path to an AF_UNIX socket which will receive a file descriptor referencing the master end
              of the console's pseudoterminal.
              Default: <u>ignored</u>

       <b>--no-pivot</b>
              Ignored.

       <b>--no-new-keyring</b>
              Ignored.

       <b>--pid-file</b>=&lt;<u>pid_file_path</u>&gt;
              Specify the file to lock and populate with process ID.
              Default: <u>ignored</u>

       <b>--preserve-fds</b>
              Ignored.

</pre><h4><b>DELETE</b> <b>OPTIONS</b></h4><pre>
       <b>--force</b>
              Ignored. All delete requests are forced and will kill any running jobs.

</pre><h4><b>INPUT</b> <b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
       <b>SCRUN_DEBUG</b>=&lt;quiet|fatal|error|info|verbose|debug|debug2|debug3|debug4|debug5&gt;
              Set logging level.

       <b>SCRUN_STDERR_DEBUG</b>=&lt;quiet|fatal|error|info|verbose|debug|debug2|debug3|debug4|debug5&gt;
              Set logging level for standard error output only.

       <b>SCRUN_SYSLOG_DEBUG</b>=&lt;quiet|fatal|error|info|verbose|debug|debug2|debug3|debug4|debug5&gt;
              Set logging level for syslogging only.

       <b>SCRUN_FILE_DEBUG</b>=&lt;quiet|fatal|error|info|verbose|debug|debug2|debug3|debug4|debug5&gt;
              Set logging level for log file only.

</pre><h4><b>JOB</b> <b>INPUT</b> <b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
       <b>SCRUN_ACCOUNT</b>
              See <b>SLURM_ACCOUNT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_ACCTG_FREQ</b>
              See <b>SLURM_ACCTG_FREQ</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_BURST_BUFFER</b>
              See <b>SLURM_BURST_BUFFER</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CLUSTER_CONSTRAINT</b>
              See <b>SLURM_CLUSTER_CONSTRAINT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CLUSTERS</b>
              See <b>SLURM_CLUSTERS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CONSTRAINT</b>
              See <b>SLURM_CONSTRAINT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SLURM_CORE_SPEC</b>
              See <b>SLURM_ACCOUNT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CPU_BIND</b>
              See <b>SLURM_CPU_BIND</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CPU_FREQ_REQ</b>
              See <b>SLURM_CPU_FREQ_REQ</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CPUS_PER_GPU</b>
              See <b>SLURM_CPUS_PER_GPU</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_CPUS_PER_TASK</b>
              See <b>SRUN_CPUS_PER_TASK</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_DELAY_BOOT</b>
              See <b>SLURM_DELAY_BOOT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_DEPENDENCY</b>
              See <b>SLURM_DEPENDENCY</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_DISTRIBUTION</b>
              See <b>SLURM_DISTRIBUTION</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_EPILOG</b>
              See <b>SLURM_EPILOG</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_EXACT</b>
              See <b>SLURM_EXACT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_EXCLUSIVE</b>
              See <b>SLURM_EXCLUSIVE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GPU_BIND</b>
              See <b>SLURM_GPU_BIND</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GPU_FREQ</b>
              See <b>SLURM_GPU_FREQ</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GPUS</b>
              See <b>SLURM_GPUS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GPUS_PER_NODE</b>
              See <b>SLURM_GPUS_PER_NODE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GPUS_PER_SOCKET</b>
              See <b>SLURM_GPUS_PER_SOCKET</b> from <b><a href="../man1/salloc.1.html">salloc</a></b>(1).

       <b>SCRUN_GPUS_PER_TASK</b>
              See <b>SLURM_GPUS_PER_TASK</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GRES_FLAGS</b>
              See <b>SLURM_GRES_FLAGS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_GRES</b>
              See <b>SLURM_GRES</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_HINT</b>
              See <b>SLURM_HIST</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_JOB_NAME</b>
              See <b>SLURM_JOB_NAME</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_JOB_NODELIST</b>
              See <b>SLURM_JOB_NODELIST</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_JOB_NUM_NODES</b>
              See <b>SLURM_JOB_NUM_NODES</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_LABELIO</b>
              See <b>SLURM_LABELIO</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_MEM_BIND</b>
              See <b>SLURM_MEM_BIND</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_MEM_PER_CPU</b>
              See <b>SLURM_MEM_PER_CPU</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_MEM_PER_GPU</b>
              See <b>SLURM_MEM_PER_GPU</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_MEM_PER_NODE</b>
              See <b>SLURM_MEM_PER_NODE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_MPI_TYPE</b>
              See <b>SLURM_MPI_TYPE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NCORES_PER_SOCKET</b>
              See <b>SLURM_NCORES_PER_SOCKET</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NETWORK</b>
              See <b>SLURM_NETWORK</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NSOCKETS_PER_NODE</b>
              See <b>SLURM_NSOCKETS_PER_NODE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NTASKS</b>
              See <b>SLURM_NTASKS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NTASKS_PER_CORE</b>
              See <b>SLURM_NTASKS_PER_CORE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NTASKS_PER_GPU</b>
              See <b>SLURM_NTASKS_PER_GPU</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NTASKS_PER_NODE</b>
              See <b>SLURM_NTASKS_PER_NODE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_NTASKS_PER_TRES</b>
              See <b>SLURM_NTASKS_PER_TRES</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_OPEN_MODE</b>
              See <b>SLURM_MODE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_OVERCOMMIT</b>
              See <b>SLURM_OVERCOMMIT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_OVERLAP</b>
              See <b>SLURM_OVERLAP</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_PARTITION</b>
              See <b>SLURM_PARTITION</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_POWER</b>
              See <b>SLURM_POWER</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_PROFILE</b>
              See <b>SLURM_PROFILE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_PROLOG</b>
              See <b>SLURM_PROLOG</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_QOS</b>
              See <b>SLURM_QOS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_REMOTE_CWD</b>
              See <b>SLURM_REMOTE_CWD</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_REQ_SWITCH</b>
              See <b>SLURM_REQ_SWITCH</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_RESERVATION</b>
              See <b>SLURM_RESERVATION</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_SIGNAL</b>
              See <b>SLURM_SIGNAL</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_SLURMD_DEBUG</b>
              See <b>SLURMD_DEBUG</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_SPREAD_JOB</b>
              See <b>SLURM_SPREAD_JOB</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_TASK_EPILOG</b>
              See <b>SLURM_TASK_EPILOG</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_TASK_PROLOG</b>
              See <b>SLURM_TASK_PROLOG</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_THREAD_SPEC</b>
              See <b>SLURM_THREAD_SPEC</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_THREADS_PER_CORE</b>
              See <b>SLURM_THREADS_PER_CORE</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_THREADS</b>
              See <b>SLURM_THREADS</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_TIMELIMIT</b>
              See <b>SLURM_TIMELIMIT</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_TRES_BIND</b>
              Same as <b>--tres-bind</b>

       <b>SCRUN_TRES_PER_TASK</b>
              See <b>SLURM_TRES_PER_TASK</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_UNBUFFEREDIO</b>
              See <b>SLURM_UNBUFFEREDIO</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_USE_MIN_NODES</b>
              See <b>SLURM_USE_MIN_NODES</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_WAIT4SWITCH</b>
              See <b>SLURM_WAIT4SWITCH</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_WCKEY</b>
              See <b>SLURM_WCKEY</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

       <b>SCRUN_WORKING_DIR</b>
              See <b>SLURM_WORKING_DIR</b> from <b><a href="../man1/srun.1.html">srun</a></b>(1).

</pre><h4><b>OUTPUT</b> <b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
       <b>SCRUN_OCI_VERSION</b>
              Advertised version of OCI compliance of container.

       <b>SCRUN_CONTAINER_ID</b>
              Value based as <u>container_id</u> during create operation.

       <b>SCRUN_PID</b>
              PID of process used to monitor and control container on allocation node.

       <b>SCRUN_BUNDLE</b>
              Path to container bundle directory.

       <b>SCRUN_SUBMISSION_BUNDLE</b>
              Path to container bundle directory before modification by Lua script.

       <b>SCRUN_ANNOTATION_*</b>
              List of annotations from container's config.json.

       <b>SCRUN_PID_FILE</b>
              Path to pid file that is locked and populated with PID of scrun.

       <b>SCRUN_SOCKET</b>
              Path to control socket for scrun.

       <b>SCRUN_SPOOL_DIR</b>
              Path to workspace for all temporary files for current container. Purged by deletion operation.

       <b>SCRUN_SUBMISSION_CONFIG_FILE</b>
              Path to container's config.json file at time of submission.

       <b>SCRUN_USER</b>
              Name of user that called create operation.

       <b>SCRUN_USER_ID</b>
              Numeric ID of user that called create operation.

       <b>SCRUN_GROUP</b>
              Name of user's primary group that called create operation.

       <b>SCRUN_GROUP_ID</b>
              Numeric ID of user primary group that called create operation.

       <b>SCRUN_ROOT</b>
              See <b>--root</b>.

       <b>SCRUN_ROOTFS_PATH</b>
              Path to container's root directory.

       <b>SCRUN_SUBMISSION_ROOTFS_PATH</b>
              Path to container's root directory at submission time.

       <b>SCRUN_LOG_FILE</b>
              Path to scrun's log file during create operation.

       <b>SCRUN_LOG_FORMAT</b>
              Log format type during create operation.

</pre><h4><b>JOB</b> <b>OUTPUT</b> <b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
       <b>SLURM_*_HET_GROUP_#</b>
              For a heterogeneous job  allocation,  the  environment  variables  are  set  separately  for  each
              component.

       <b>SLURM_CLUSTER_NAME</b>
              Name of the cluster on which the job is executing.

       <b>SLURM_CONTAINER</b>
              OCI Bundle for job.

       <b>SLURM_CONTAINER_ID</b>
              OCI id for job.

       <b>SLURM_CPUS_PER_GPU</b>
              Number of CPUs requested per allocated GPU.

       <b>SLURM_CPUS_PER_TASK</b>
              Number of CPUs requested per task.

       <b>SLURM_DIST_PLANESIZE</b>
              Plane distribution size. Only set for plane distributions.

       <b>SLURM_DISTRIBUTION</b>
              Distribution type for the allocated jobs.

       <b>SLURM_GPU_BIND</b>
              Requested binding of tasks to GPU.

       <b>SLURM_GPU_FREQ</b>
              Requested GPU frequency.

       <b>SLURM_GPUS</b>
              Number of GPUs requested.

       <b>SLURM_GPUS_PER_NODE</b>
              Requested GPU count per allocated node.

       <b>SLURM_GPUS_PER_SOCKET</b>
              Requested GPU count per allocated socket.

       <b>SLURM_GPUS_PER_TASK</b>
              Requested GPU count per allocated task.

       <b>SLURM_HET_SIZE</b>
              Set to count of components in heterogeneous job.

       <b>SLURM_JOB_ACCOUNT</b>
              Account name associated of the job allocation.

       <b>SLURM_JOB_CPUS_PER_NODE</b>
              Count  of  CPUs  available  to  the  job  on  the  nodes  in  the  allocation,  using  the  format
              <u>CPU_count</u>[(x<u>number_of_nodes</u>)][,<u>CPU_count</u>     [(x<u>number_of_nodes</u>)]     ...].      For      example:
              SLURM_JOB_CPUS_PER_NODE='72(x2),36'  indicates  that  on  the first and second nodes (as listed by
              SLURM_JOB_NODELIST) the allocation has 72 CPUs, while the third  node  has  36  CPUs.   <b>NOTE</b>:  The
              <b>select/linear</b>  plugin  allocates  entire  nodes to jobs, so the value indicates the total count of
              CPUs on allocated nodes. The <b>select/cons_tres</b> plugin allocates individual CPUs to  jobs,  so  this
              number indicates the number of CPUs allocated to the job.

       <b>SLURM_JOB_END_TIME</b>
              The UNIX timestamp for a job's projected end time.

       <b>SLURM_JOB_GPUS</b>
              The  global  GPU IDs of the GPUs allocated to this job. The GPU IDs are not relative to any device
              cgroup, even if devices are constrained with task/cgroup.  Only set in batch and interactive jobs.

       <b>SLURM_JOB_ID</b>
              The ID of the job allocation.

       <b>SLURM_JOB_NODELIST</b>
              List of nodes allocated to the job.

       <b>SLURM_JOB_NUM_NODES</b>
              Total number of nodes in the job allocation.

       <b>SLURM_JOB_PARTITION</b>
              Name of the partition in which the job is running.

       <b>SLURM_JOB_QOS</b>
              Quality Of Service (QOS) of the job allocation.

       <b>SLURM_JOB_RESERVATION</b>
              Advanced reservation containing the job allocation, if any.

       <b>SLURM_JOB_START_TIME</b>
              UNIX timestamp for a job's start time.

       <b>SLURM_MEM_BIND</b>
              Bind tasks to memory.

       <b>SLURM_MEM_BIND_LIST</b>
              Set to bit mask used for memory binding.

       <b>SLURM_MEM_BIND_PREFER</b>
              Set to "prefer" if the <b>SLURM_MEM_BIND</b> option includes the prefer option.

       <b>SLURM_MEM_BIND_SORT</b>
              Sort free cache pages (run zonesort on Intel KNL nodes)

       <b>SLURM_MEM_BIND_TYPE</b>
              Set to the memory binding type specified with the  <b>SLURM_MEM_BIND</b>  option.   Possible  values  are
              "none", "rank", "map_map", "mask_mem" and "local".

       <b>SLURM_MEM_BIND_VERBOSE</b>
              Set  to  "verbose"  if  the  <b>SLURM_MEM_BIND</b>  option  includes  the verbose option.  Set to "quiet"
              otherwise.

       <b>SLURM_MEM_PER_CPU</b>
              Minimum memory required per usable allocated CPU.

       <b>SLURM_MEM_PER_GPU</b>
              Requested memory per allocated GPU.

       <b>SLURM_MEM_PER_NODE</b>
              Specify the real memory required per node.

       <b>SLURM_NTASKS</b>
              Specify the number of tasks to run.

       <b>SLURM_NTASKS_PER_CORE</b>
              Request the maximum <u>ntasks</u> be invoked on each core.

       <b>SLURM_NTASKS_PER_GPU</b>
              Request that there are <u>ntasks</u> tasks invoked for every GPU.

       <b>SLURM_NTASKS_PER_NODE</b>
              Request that <u>ntasks</u> be invoked on each node.

       <b>SLURM_NTASKS_PER_SOCKET</b>
              Request the maximum <u>ntasks</u> be invoked on each socket.

       <b>SLURM_OVERCOMMIT</b>
              Overcommit resources.

       <b>SLURM_PROFILE</b>
              Enables detailed data collection by the acct_gather_profile plugin.

       <b>SLURM_SHARDS_ON_NODE</b>
              Number of GPU Shards available to the step on this node.

       <b>SLURM_SUBMIT_HOST</b>
              The hostname of the computer from which <b>scrun</b> was invoked.

       <b>SLURM_TASKS_PER_NODE</b>
              Number of tasks to be initiated on each node. Values are comma separated and in the same order  as
              SLURM_JOB_NODELIST.   If two or more consecutive nodes are to have the same task count, that count
              is   followed   by   "(x#)"   where    "#"    is    the    repetition    count.    For    example,
              "SLURM_TASKS_PER_NODE=2(x3),1"  indicates  that  the first three nodes will each execute two tasks
              and the fourth node will execute one task.

       <b>SLURM_THREADS_PER_CORE</b>
              This is only set if <b>--threads-per-core</b> or <b>SCRUN_THREADS_PER_CORE</b> were specified. The value will be
              set to the value specified by  <b>--threads-per-core</b>  or  <b>SCRUN_THREADS_PER_CORE</b>.  This  is  used  by
              subsequent srun calls within the job allocation.

       <b>SLURM_TRES_PER_TASK</b>
              Set  to  the  value  of <b>--tres-per-task</b>. If <b>--cpus-per-task</b> or <b>--gpus-per-task</b> is specified, it is
              also set in <b>SLURM_TRES_PER_TASK</b> as if it were specified in <b>--tres-per-task</b>.

</pre><h4><b>SCRUN.LUA</b></h4><pre>
       /etc/slurm/<b>scrun.lua</b> must be present on any node where  <b>scrun</b>  will  be  invoked.  <b>scrun.lua</b>  must  be  a
       compliant <b>lua</b> script.

   <b>Required</b> <b>functions</b>
       The following functions must be defined.

       • function <b>slurm_scrun_stage_in</b>(<b>id</b>, <b>bundle</b>, <b>spool_dir</b>, <b>config_file</b>, <b>job_id</b>, <b>user_id</b>, <b>group_id</b>, <b>job_env</b>)
              Called  right  after job allocation to stage container into job node(s). Must return <u>SLURM.success</u>
              or job will be cancelled. It is required that function will prepare the container for execution on
              job node(s) as required to run as configured in <b><a href="../man1/oci.conf.1.html">oci.conf</a></b>(1). The function may  block  as  long  as
              required until container has been fully prepared (up to the job's max wall time).

           <b>id</b>     Container ID

           <b>bundle</b> OCI bundle path

           <b>spool_dir</b>
                  Temporary working directory for container

           <b>config_file</b>
                  Path to config.json for container

           <b>job_id</b> <u>jobid</u> of job allocation

           <b>user_id</b>
                  Resolved  numeric user id of job allocation. It is generally expected that the lua script will
                  be executed inside of a user namespace running under the <u><a href="../man0/root.0.html">root</a>(0)</u> user.

           <b>group_id</b>
                  Resolved numeric group id of job allocation. It is generally expected that the lua script will
                  be executed inside of a user namespace running under the <u><a href="../man0/root.0.html">root</a>(0)</u> group.

           <b>job_env</b>
                  Table with each entry of Key=Value or Value of each environment variable of the job.

       • function <b>slurm_scrun_stage_out</b>(<b>id</b>, <b>bundle</b>, <b>orig_bundle</b>, <b>root_path</b>, <b>orig_root_path</b>, <b>spool_dir</b>,
       <b>config_file</b>, <b>jobid</b>, <b>user_id</b>, <b>group_id</b>)
              Called right after container step completes to stage  out  files  from  job  nodes.   Must  return
              <u>SLURM.success</u>  or  job  will be cancelled. It is required that function will pull back any changes
              and cleanup the container on job node(s).  The function  may  block  as  long  as  required  until
              container has been fully prepared (up to the job's max wall time).

           <b>id</b>     Container ID

           <b>bundle</b> OCI bundle path

           <b>orig_bundle</b>
                  Originally submitted OCI bundle path before modification by <b>set_bundle_path</b>().

           <b>root_path</b>
                  Path to directory root of container contents.

           <b>orig_root_path</b>
                  Original path to directory root of container contents before modification by <b>set_root_path</b>().

           <b>spool_dir</b>
                  Temporary working directory for container

           <b>config_file</b>
                  Path to config.json for container

           <b>job_id</b> <u>jobid</u> of job allocation

           <b>user_id</b>
                  Resolved  numeric user id of job allocation. It is generally expected that the lua script will
                  be executed inside of a user namespace running under the <u><a href="../man0/root.0.html">root</a>(0)</u> user.

           <b>group_id</b>
                  Resolved numeric group id of job allocation. It is generally expected that the lua script will
                  be executed inside of a user namespace running under the <u><a href="../man0/root.0.html">root</a>(0)</u> group.

   <b>Provided</b> <b>functions</b>
       The following functions are provided for any Lua function to call as needed.

       • <b>slurm.set_bundle_path</b>(<u>PATH</u>)
              Called to notify <b>scrun</b> to use <u>PATH</u> as new OCI container bundle path. Depending on  the  filesystem
              layout, cloning the container bundle may be required to allow execution on job nodes.

       • <b>slurm.set_root_path</b>(<u>PATH</u>)
              Called  to  notify  <b>scrun</b>  to  use  <u>PATH</u>  as  new container root filesystem path. Depending on the
              filesystem layout, cloning the container bundle may be required to allow execution on  job  nodes.
              Script must also update #/root/path in config.json when changing root path.

       • <u>STATUS</u>,<u>OUTPUT</u> = <b>slurm.remote_command</b>(<u>SCRIPT</u>)
              Run <u>SCRIPT</u> in new job step on all job nodes. Returns numeric job status as <u>STATUS</u> and job stdio as
              <u>OUTPUT</u>. Blocks until <u>SCRIPT</u> exits.

       • <u>STATUS</u>,<u>OUTPUT</u> = <b>slurm.allocator_command</b>(<u>SCRIPT</u>)
              Run <u>SCRIPT</u> as forked child process of <b>scrun</b>. Returns numeric job status as <u>STATUS</u> and job stdio as
              <u>OUTPUT</u>. Blocks until <u>SCRIPT</u> exits.

       • <b>slurm.log</b>(<u>MSG</u>, <u>LEVEL</u>)
              Log <u>MSG</u> at log <u>LEVEL</u>. Valid range of values for <u>LEVEL</u> is [0, 4].

       • <b>slurm.error</b>(<u>MSG</u>)
              Log error <u>MSG</u>.

       • <b>slurm.log_error</b>(<u>MSG</u>)
              Log error <u>MSG</u>.

       • <b>slurm.log_info</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level INFO.

       • <b>slurm.log_verbose</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level VERBOSE.

       • <b>slurm.log_verbose</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level VERBOSE.

       • <b>slurm.log_debug</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level DEBUG.

       • <b>slurm.log_debug2</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level DEBUG2.

       • <b>slurm.log_debug3</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level DEBUG3.

       • <b>slurm.log_debug4</b>(<u>MSG</u>)
              Log <u>MSG</u> at log level DEBUG4.

       • <u>MINUTES</u> = <b>slurm.time_str2mins</b>(<u>TIME_STRING</u>)
              Parse <u>TIME_STRING</u> into number of minutes as <u>MINUTES</u>. Valid formats:

               • days-[hours[:minutes[:seconds]]]

               • hours:minutes:seconds

               • minutes[:seconds]

               • -1

               • INFINITE

               • UNLIMITED

   <b>Example</b> <b>scrun.lua</b> scripts
       Full Container staging example using rsync:
              This full example will stage a container as given by <b>docker</b> or <b>podman</b>. The container's config.json
              is  modified  to remove unwanted functions that may cause the container run to under <b>crun</b> or <b>runc</b>.
              The script uses <b>rsync</b> to move  the  container  to  a  shared  filesystem  under  the  <u>scratch_path</u>
              variable.

              <b>NOTE</b>: Support for JSON in liblua must generally be installed before Slurm is compiled. scrun.lua's
              syntax  and ability to load JSON support should be tested by directly calling the script using <b>lua</b>
              outside of Slurm.

              local json = require 'json'
              local open = io.open
              local scratch_path = "<a href="file:/run/user/">/run/user/</a>"

              local function read_file(path)
                   local file = open(path, "rb")
                   if not file then return nil end
                   local content = file:read "*all"
                   file:close()
                   return content
              end

              local function write_file(path, contents)
                   local file = open(path, "wb")
                   if not file then return nil end
                   file:write(contents)
                   file:close()
                   return
              end

              function slurm_scrun_stage_in(id, bundle, spool_dir, config_file, job_id, user_id, group_id, job_env)
                   slurm.log_debug(string.format("stage_in(%s, %s, %s, %s, %d, %d, %d)",
                               id, bundle, spool_dir, config_file, job_id, user_id, group_id))

                   local status, output, user, rc
                   local config = json.decode(read_file(config_file))
                   local src_rootfs = config["root"]["path"]
                   rc, user = slurm.allocator_command(string.format("id -un %d", user_id))
                   user = string.gsub(user, "%s+", "")
                   local root = scratch_path..math.floor(user_id).."/slurm/scrun/"
                   local dst_bundle = root.."/"..id.."/"
                   local dst_config = root.."/"..id.."/config.json"
                   local dst_rootfs = root.."/"..id.."/rootfs/"

                   if string.sub(src_rootfs, 1, 1) ~= "/"
                   then
                        -- always use absolute path
                        src_rootfs = string.format("%s/%s", bundle, src_rootfs)
                   end

                   status, output = slurm.allocator_command("mkdir -p "..dst_rootfs)
                   if (status ~= 0)
                   then
                        slurm.log_info(string.format("mkdir(%s) failed %u: %s",
                                    dst_rootfs, status, output))
                        return slurm.ERROR
                   end

                   status, output = slurm.allocator_command(string.format("<a href="file:///usr/lib/w3m/cgi-bin/w3mman2html.cgi?env">/usr/bin/env</a> rsync --exclude sys --exclude proc --numeric-ids --delete-after --ignore-errors --stats -a -- %s/ %s/", src_rootfs, dst_rootfs))
                   if (status ~= 0)
                   then
                        -- rsync can fail due to permissions which may not matter
                        slurm.log_info(string.format("WARNING: rsync failed: %s", output))
                   end

                   slurm.set_bundle_path(dst_bundle)
                   slurm.set_root_path(dst_rootfs)

                   config["root"]["path"] = dst_rootfs

                   -- Always force user namespace support in container or runc will reject
                   local process_user_id = 0
                   local process_group_id = 0

                   if ((config["process"] ~= nil) and (config["process"]["user"] ~= nil))
                   then
                        -- resolve out user in the container
                        if (config["process"]["user"]["uid"] ~= nil)
                        then
                             process_user_id=config["process"]["user"]["uid"]
                        else
                             process_user_id=0
                        end

                        -- resolve out group in the container
                        if (config["process"]["user"]["gid"] ~= nil)
                        then
                             process_group_id=config["process"]["user"]["gid"]
                        else
                             process_group_id=0
                        end

                        -- purge additionalGids as they are not supported in rootless
                        if (config["process"]["user"]["additionalGids"] ~= nil)
                        then
                             config["process"]["user"]["additionalGids"] = nil
                        end
                   end

                   if (config["linux"] ~= nil)
                   then
                        -- force user namespace to always be defined for rootless mode
                        local found = false
                        if (config["linux"]["namespaces"] == nil)
                        then
                             config["linux"]["namespaces"] = {}
                        else
                             for _, namespace in ipairs(config["linux"]["namespaces"]) do
                                  if (namespace["type"] == "user")
                                  then
                                       found=true
                                       break
                                  end
                             end
                        end
                        if (found == false)
                        then
                             table.insert(config["linux"]["namespaces"], {type= "user"})
                        end

                        -- Provide default user map as root if one not provided
                        if (true or config["linux"]["uidMappings"] == nil)
                        then
                             config["linux"]["uidMappings"] =
                                  {{containerID=process_user_id, hostID=math.floor(user_id), size=1}}
                        end

                        -- Provide default group map as root if one not provided
                        -- mappings fail with build???
                        if (true or config["linux"]["gidMappings"] == nil)
                        then
                             config["linux"]["gidMappings"] =
                                  {{containerID=process_group_id, hostID=math.floor(group_id), size=1}}
                        end

                        -- disable trying to use a specific cgroup
                        config["linux"]["cgroupsPath"] = nil
                   end

                   if (config["mounts"] ~= nil)
                   then
                        -- Find and remove any user/group settings in mounts
                        for _, mount in ipairs(config["mounts"]) do
                             local opts = {}

                             if (mount["options"] ~= nil)
                             then
                                  for _, opt in ipairs(mount["options"]) do
                                       if ((string.sub(opt, 1, 4) ~= "gid=") and (string.sub(opt, 1, 4) ~= "uid="))
                                       then
                                            table.insert(opts, opt)
                                       end
                                  end
                             end

                             if (opts ~= nil and #opts &gt; 0)
                             then
                                  mount["options"] = opts
                             else
                                  mount["options"] = nil
                             end
                        end

                        -- Remove all bind mounts by copying files into rootfs
                        local mounts = {}
                        for i, mount in ipairs(config["mounts"]) do
                             if ((mount["type"] ~= nil) and (mount["type"] == "bind") and (string.sub(mount["source"], 1, 4) ~= "<a href="file:/sys">/sys</a>") and (string.sub(mount["source"], 1, 5) ~= "<a href="file:/proc">/proc</a>"))
                             then
                                  status, output = slurm.allocator_command(string.format("<a href="file:///usr/lib/w3m/cgi-bin/w3mman2html.cgi?env">/usr/bin/env</a> rsync --numeric-ids --ignore-errors --stats -a -- %s %s", mount["source"], dst_rootfs..mount["destination"]))
                                  if (status ~= 0)
                                  then
                                       -- rsync can fail due to permissions which may not matter
                                       slurm.log_info("rsync failed")
                                  end
                             else
                                  table.insert(mounts, mount)
                             end
                        end
                        config["mounts"] = mounts
                   end

                   -- Force version to one compatible with older runc/crun at risk of new features silently failing
                   config["ociVersion"] = "1.0.0"

                   -- Merge in Job environment into container -- this is optional!
                   if (config["process"]["env"] == nil)
                   then
                        config["process"]["env"] = {}
                   end
                   for _, env in ipairs(job_env) do
                        table.insert(config["process"]["env"], env)
                   end

                   -- Remove all prestart hooks to squash any networking attempts
                   if ((config["hooks"] ~= nil) and (config["hooks"]["prestart"] ~= nil))
                   then
                        config["hooks"]["prestart"] = nil
                   end

                   -- Remove all rlimits
                   if ((config["process"] ~= nil) and (config["process"]["rlimits"] ~= nil))
                   then
                        config["process"]["rlimits"] = nil
                   end

                   write_file(dst_config, json.encode(config))
                   slurm.log_info("created: "..dst_config)

                   return slurm.SUCCESS
              end

              function slurm_scrun_stage_out(id, bundle, orig_bundle, root_path, orig_root_path, spool_dir, config_file, jobid, user_id, group_id)
                   if (root_path == nil)
                   then
                        root_path = ""
                   end

                   slurm.log_debug(string.format("stage_out(%s, %s, %s, %s, %s, %s, %s, %d, %d, %d)",
                               id, bundle, orig_bundle, root_path, orig_root_path, spool_dir, config_file, jobid, user_id, group_id))

                   if (bundle == orig_bundle)
                   then
                        slurm.log_info(string.format("skipping stage_out as bundle=orig_bundle=%s", bundle))
                        return slurm.SUCCESS
                   end

                   status, output = slurm.allocator_command(string.format("<a href="file:///usr/lib/w3m/cgi-bin/w3mman2html.cgi?env">/usr/bin/env</a> rsync --numeric-ids --delete-after --ignore-errors --stats -a -- %s/ %s/", root_path, orig_root_path))
                   if (status ~= 0)
                   then
                        -- rsync can fail due to permissions which may not matter
                        slurm.log_info("rsync failed")
                   else
                        -- cleanup temporary after they have been synced backed to source
                        slurm.allocator_command(string.format("<a href="file:///usr/lib/w3m/cgi-bin/w3mman2html.cgi?rm">/usr/bin/rm</a> --preserve-root=all --one-file-system -dr -- %s", bundle))
                   end

                   return slurm.SUCCESS
              end

              slurm.log_info("initialized scrun.lua")

              return slurm.SUCCESS

</pre><h4><b>SIGNALS</b></h4><pre>
       <b>SIGINT</b> Attempt to gracefully cancel any related jobs (if any) and cleanup.

       <b>SIGCHLD</b>
              Wait for all children, cleanup anchor and gracefully shutdown.

</pre><h4><b>COPYING</b></h4><pre>
       Copyright (C) 2023 SchedMD LLC.

       This   file   is   part   of   Slurm,   a   resource    management    program.     For    details,    see
       &lt;https://slurm.schedmd.com/&gt;.

       Slurm  is  free  software;  you  can  redistribute it and/or modify it under the terms of the GNU General
       Public License as published by the Free Software Foundation; either version 2 of the License, or (at your
       option) any later version.

       Slurm is distributed in the hope that it will be useful, but  WITHOUT  ANY  WARRANTY;  without  even  the
       implied  warranty  of  MERCHANTABILITY  or  FITNESS  FOR A PARTICULAR PURPOSE. See the GNU General Public
       License for more details.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man1/slurm.1.html">slurm</a></b>(1), <b><a href="../man5/oci.conf.5.html">oci.conf</a></b>(5), <b><a href="../man1/srun.1.html">srun</a></b>(1), <b>crun</b>, <b>runc</b>, <b>DOCKER</b> and <b>podman</b>

January 2025                                     Slurm Commands                                         <u><a href="../man1/scrun.1.html">scrun</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>