<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>hyperfine - hyperfine</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/hyperfine">hyperfine_1.19.0-1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       hyperfine - hyperfine

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>hyperfine</b> [<u>OPTIONS</u>] <u>&lt;command&gt;</u>...

</pre><h4><b>DESCRIPTION</b></h4><pre>
       A command-line benchmarking tool.

   <b>Arguments:</b>
              &lt;command&gt;...

              The  command  to  benchmark.  This  can be the name of an executable, a command line like "grep <b>-i</b>
              todo" or a shell command like "sleep 0.5 &amp;&amp; echo test". The latter is only available if the  shell
              is not explicitly disabled via '--shell=none'. If multiple commands are given, hyperfine will show
              a comparison of the respective runtimes.

</pre><h4><b>OPTIONS</b></h4><pre>

       <b>-w</b>, <b>--warmup</b> &lt;NUM&gt;

              Perform  NUM  warmup  runs before the actual benchmark. This can be used to fill (disk) caches for
              I/O-heavy programs.

       <b>-m</b>, <b>--min-runs</b> &lt;NUM&gt;

              Perform at least NUM runs for each command (default: 10).

       <b>-M</b>, <b>--max-runs</b> &lt;NUM&gt;

              Perform at most NUM runs for each command. By default, there is no limit.

       <b>-r</b>, <b>--runs</b> &lt;NUM&gt;

              Perform  exactly  NUM  runs  for  each  command.  If  this  option  is  not  specified,  hyperfine
              automatically determines the number of runs.

       <b>-s</b>, <b>--setup</b> &lt;CMD&gt;

              Execute  CMD  before  each set of timing runs. This is useful for compiling your software with the
              provided parameters, or to do any other work that should happen once before a series of  benchmark
              runs, not every time as would happen with the <b>--prepare</b> option.

       <b>--reference</b> &lt;CMD&gt;

              The  reference  command  for  the  relative  comparison  of results. If this is unset, results are
              compared with the fastest command as reference.

       <b>-p</b>, <b>--prepare</b> &lt;CMD&gt;

              Execute CMD before each timing run. This is useful for clearing disk  caches,  for  example.   The
              <b>--prepare</b>  option can be specified once for all commands or multiple times, once for each command.
              In the latter case, each preparation command will be run  prior  to  the  corresponding  benchmark
              command.

       <b>-C</b>, <b>--conclude</b> &lt;CMD&gt;

              Execute CMD after each timing run. This is useful for killing long-running processes started (e.g.
              a  web server started in <b>--prepare</b>), for example.  The <b>--conclude</b> option can be specified once for
              all commands or multiple times, once for each command. In the latter case, each  conclude  command
              will be run after the corresponding benchmark command.

       <b>-c</b>, <b>--cleanup</b> &lt;CMD&gt;

              Execute  CMD  after  the  completion  of  all  benchmarking runs for each individual command to be
              benchmarked. This is useful if the commands to be benchmarked produce artifacts that  need  to  be
              cleaned up.

       <b>-P</b>, <b>--parameter-scan</b> &lt;VAR&gt; &lt;MIN&gt; &lt;MAX&gt;

              Perform  benchmark  runs for each value in the range MIN..MAX. Replaces the string '{VAR}' in each
              command by the current parameter value.

       Example:
              hyperfine <b>-P</b> threads 1 8 'make <b>-j</b> {threads}'

              This performs benchmarks for 'make <b>-j</b> 1', 'make <b>-j</b> 2', ???, 'make <b>-j</b> 8'.

              To have the value increase following different patterns, use shell arithmetics.

              Example: hyperfine <b>-P</b> size 0 3 'sleep $((2**{size}))'

              This performs benchmarks with power of 2 increases: 'sleep 1', 'sleep  2',  'sleep  4',  ???   The
              exact syntax may vary depending on your shell and OS.

       <b>-D</b>, <b>--parameter-step-size</b> &lt;DELTA&gt;

              This  argument  requires <b>--parameter-scan</b> to be specified as well.  Traverse the range MIN..MAX in
              steps of DELTA.

       Example:
              hyperfine <b>-P</b> delay 0.3 0.7 <b>-D</b> 0.2 'sleep {delay}'

              This performs benchmarks for 'sleep 0.3', 'sleep 0.5' and 'sleep 0.7'.

       <b>-L</b>, <b>--parameter-list</b> &lt;VAR&gt; &lt;VALUES&gt;

              Perform benchmark runs for each value in the comma-separated  list  VALUES.  Replaces  the  string
              '{VAR}' in each command by the current parameter value.

       Example:
              hyperfine <b>-L</b> compiler gcc,clang '{compiler} <b>-O2</b> main.cpp'

              This performs benchmarks for 'gcc <b>-O2</b> main.cpp' and 'clang <b>-O2</b> main.cpp'.

              The  option  can  be  specified  multiple  times  to  run  benchmarks  for  all possible parameter
              combinations.

       <b>-S</b>, <b>--shell</b> &lt;SHELL&gt;

              Set the shell to use for executing benchmarked commands. This can be the name or the path  to  the
              shell  executable,  or  a full command line like "bash <b>--norc</b>". It can also be set to "default" to
              explicitly select the default shell on this platform. Finally, this can also be set to  "none"  to
              disable  the  shell.  In  this  case,  commands  will  be  executed  directly. They can still have
              arguments, but more complex things like "sleep 0.1; sleep 0.2" are not possible without a shell.

       <b>-N</b>

              An alias for '--shell=none'.

       <b>-i</b>, <b>--ignore-failure</b>

              Ignore non-zero exit codes of the benchmarked programs.

       <b>--style</b> &lt;TYPE&gt;

              Set output style type (default: auto).  Set  this  to  'basic'  to  disable  output  coloring  and
              interactive  elements.  Set it to 'full' to enable all effects even if no interactive terminal was
              detected. Set this to 'nocolor' to keep the interactive output without any  colors.  Set  this  to
              'color'  to  keep the colors without any interactive output. Set this to 'none' to disable all the
              output of the tool.

       <b>--sort</b> &lt;METHOD&gt;

              Specify the sort order of the speed comparison summary and the exported tables for markup  formats
              (Markdown, AsciiDoc, org-mode):

              * 'auto' (default): the speed comparison will be ordered by time and

              the markup tables will be ordered by command (input order).

              *  'command':  order  benchmarks in the way they were specified * 'mean-time': order benchmarks by
              mean runtime

       <b>-u</b>, <b>--time-unit</b> &lt;UNIT&gt;

              Set the time unit to be used. Possible values: microsecond, millisecond, second. If the option  is
              not  given,  the time unit is determined automatically. This option affects the standard output as
              well as all export formats except for CSV and JSON.

       <b>--export-asciidoc</b> &lt;FILE&gt;

              Export the timing summary statistics as an AsciiDoc table to the given FILE. The output time  unit
              can be changed using the <b>--time-unit</b> option.

       <b>--export-csv</b> &lt;FILE&gt;

              Export  the timing summary statistics as CSV to the given FILE. If you need the timing results for
              each individual run, use the JSON export format. The output time unit is always seconds.

       <b>--export-json</b> &lt;FILE&gt;

              Export the timing summary statistics and timings of individual runs as JSON to the given FILE. The
              output time unit is always seconds

       <b>--export-markdown</b> &lt;FILE&gt;

              Export the timing summary statistics as a Markdown table to the given FILE. The output  time  unit
              can be changed using the <b>--time-unit</b> option.

       <b>--export-orgmode</b> &lt;FILE&gt;

              Export the timing summary statistics as an Emacs org-mode table to the given FILE. The output time
              unit can be changed using the <b>--time-unit</b> option.

       <b>--show-output</b>

              Print  the  stdout  and  stderr of the benchmark instead of suppressing it. This will increase the
              time it takes for benchmarks to run, so it should only be used  for  debugging  purposes  or  when
              trying to benchmark output speed.

       <b>--output</b> &lt;WHERE&gt;

              Control  where  the  output  of  the  benchmark is redirected. Note that some programs like 'grep'
              detect when standard output is <u>/dev/null</u> and apply certain optimizations. To avoid that,  consider
              using '--output=pipe'.

              &lt;WHERE&gt; can be:

       null:  Redirect output to <u>/dev/null</u> (the default).

       pipe:  Feed the output through a pipe before discarding it.

       inherit:
              Don't redirect the output at all (same as

              '--show-output').

       &lt;FILE&gt;:
              Write the output to the given file.

              This option can be specified once for all commands or multiple times, once for each command. Note:
              If you want to log the output of each and every iteration, you can use a shell redirection and the
              '$HYPERFINE_ITERATION' environment variable:

              hyperfine 'my-command &gt; output-${HYPERFINE_ITERATION}.log'

       <b>--input</b> &lt;WHERE&gt;

              Control where the input of the benchmark comes from.

              &lt;WHERE&gt; can be:

       null:  Read from <u>/dev/null</u> (the default).

       &lt;FILE&gt;:
              Read the input from the given file.

       <b>-n</b>, <b>--command-name</b> &lt;NAME&gt;

              Give  a meaningful name to a command. This can be specified multiple times if several commands are
              benchmarked.

       <b>-h</b>, <b>--help</b>

              Print help

       <b>-V</b>, <b>--version</b>

              Print version

hyperfine 1.19.0                                  November 2024                                     <u><a href="../man1/HYPERFINE.1.html">HYPERFINE</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>