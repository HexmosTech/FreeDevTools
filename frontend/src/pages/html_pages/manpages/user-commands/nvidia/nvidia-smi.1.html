<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>nvidia-smi - NVIDIA System Management Interface program</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/nvidia-utils-575-server">nvidia-utils-575-server_575.57.08-0ubuntu0.25.04.2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       nvidia-smi - NVIDIA System Management Interface program

</pre><h4><b>SYNOPSIS</b></h4><pre>
       nvidia-smi [OPTION1 [ARG1]] [OPTION2 [ARG2]] ...

</pre><h4><b>DESCRIPTION</b></h4><pre>
       nvidia-smi (also NVSMI) provides monitoring and management capabilities for each of NVIDIA's Tesla,
       Quadro, GRID and GeForce devices from Fermi and higher architecture families. GeForce Titan series
       devices are supported for most functions with very limited information provided for the remainder of the
       Geforce brand. NVSMI is a cross platform tool that supports all standard NVIDIA driver-supported Linux
       distros, as well as 64bit versions of Windows starting with Windows Server 2008 R2. Metrics can be
       consumed directly by users via stdout, or provided by file via CSV and XML formats for scripting
       purposes.

       Note that much of the functionality of NVSMI is provided by the underlying NVML C-based library. See the
       NVIDIA developer website link below for more information about NVML. NVML-based python bindings are also
       available.

       The output of NVSMI is not guaranteed to be backwards compatible. However, both NVML and the Python
       bindings are backwards compatible, and should be the first choice when writing any tools that must be
       maintained across NVIDIA driver releases.

       <b>NVML</b> <b>SDK:</b> <u>https://docs.nvidia.com/deploy/nvml-api/index.html</u>

       <b>Python</b> <b>bindings:</b> <u><a href="http://pypi.python.org/pypi/nvidia-ml-py/">http://pypi.python.org/pypi/nvidia-ml-py/</a></u>

</pre><h4><b>OPTIONS</b></h4><pre>
   <b>GENERAL</b> <b>OPTIONS</b>
   <b>-h,</b> <b>--help</b>
       Print usage information and exit.

   <b>--version</b>
       Print version information and exit.

   <b>LIST</b> <b>OPTIONS</b>
   <b>-L,</b> <b>--list-gpus</b>
       List each of the NVIDIA GPUs in the system, along with their UUIDs.

   <b>-B,</b> <b>--list-excluded-gpus</b>
       List each of the excluded NVIDIA GPUs in the system, along with their UUIDs.

   <b>SUMMARY</b> <b>OPTIONS</b>
   <b>Show</b> <b>a</b> <b>summary</b> <b>of</b> <b>GPUs</b> <b>connected</b> <b>to</b> <b>the</b> <b>system.</b>
   <b>[any</b> <b>one</b> <b>of]</b>
   <b>-i,</b> <b>--id=ID</b>
       Target a specific GPU.

   <b>-f</b> <b>FILE,</b> <b>--filename=FILE</b>
       Log to the specified file, rather than to stdout.

   <b>-l</b> <b>SEC,</b> <b>--loop=SEC</b>
       Probe until Ctrl+C at specified second interval.

   <b>QUERY</b> <b>OPTIONS</b>
   <b>-q,</b> <b>--query</b>
       Display GPU or Unit info. Displayed info includes all data listed in the (<u>GPU</u> <u>ATTRIBUTES</u>) or (<u>UNIT</u>
       <u>ATTRIBUTES</u>) sections of this document. Some devices and/or environments don't support all possible
       information. Any unsupported data is indicated by a "N/A" in the output. By default information for all
       available GPUs or Units is displayed. Use the <b>-i</b> option to restrict the output to a single GPU or Unit.

   <b>[plus</b> <b>optionally]</b>
   <b>-u,</b> <b>--unit</b>
       Display Unit data instead of GPU data. Unit data is only available for NVIDIA S-class Tesla enclosures.

   <b>-i,</b> <b>--id=ID</b>
       Display data for a single specified GPU or Unit. The specified id may be the GPU/Unit's 0-based index in
       the natural enumeration returned by the driver, the GPU's board serial number, the GPU's UUID, or the
       GPU's PCI bus ID (as domain:bus:device.function in hex). It is recommended that users desiring
       consistency use either UUID or PCI bus ID, since device enumeration ordering is not guaranteed to be
       consistent between reboots and board serial number might be shared between multiple GPUs on the same
       board.

   <b>-f</b> <b>FILE,</b> <b>--filename=FILE</b>
       Redirect query output to the specified file in place of the default stdout. The specified file will be
       overwritten.

   <b>-x,</b> <b>--xml-format</b>
       Produce XML output in place of the default human-readable format. Both GPU and Unit query outputs conform
       to corresponding DTDs. These are available via the <b>--dtd</b> flag.

   <b>--dtd</b>
       Use with <b>-x</b>. Embed the DTD in the XML output.

   <b>--debug=FILE</b>
       Produces an encrypted debug log for use in submission of bugs back to NVIDIA.

   <b>-d</b> <b>TYPE,</b> <b>--display=TYPE</b>
       Display only selected information: MEMORY, UTILIZATION, ECC, TEMPERATURE, POWER, CLOCK, COMPUTE, PIDS,
       PERFORMANCE, SUPPORTED_CLOCKS, PAGE_RETIREMENT, ACCOUNTING, ENCODER_STATS, SUPPORTED_GPU_TARGET_TEMP,
       VOLTAGE, FBC_STATS, ROW_REMAPPER, RESET_STATUS, GSP_FIRMWARE_VERSION, POWER_SMOOTHING, POWER_PROFILES.
       Flags can be combined with comma e.g. "MEMORY,ECC". Sampling data with max, min and avg is also returned
       for POWER, UTILIZATION and CLOCK display types. Doesn't work with -u/--unit or -x/--xml-format flags.

   <b>-l</b> <b>SEC,</b> <b>--loop=SEC</b>
       Continuously report query data at the specified interval, rather than the default of just once. The
       application will sleep in-between queries. Note that on Linux ECC error or Xid error events will print
       out during the sleep period if the -x flag was not specified. Pressing Ctrl+C at any time will abort the
       loop, which will otherwise run indefinitely. If no argument is specified for the <b>-l</b> form a default
       interval of 5 seconds is used.

   <b>-lms</b> <b>ms,</b> <b>--loop-ms=ms</b>
       Same as -l,--loop but in milliseconds.

   <b>SELECTIVE</b> <b>QUERY</b> <b>OPTIONS</b>
       Allows the caller to pass an explicit list of properties to query.

   <b>[one</b> <b>of]</b>
    <b>--query-gpu="</b>
       Information about GPU. Pass comma separated list of properties you want to query. e.g. --query-
       gpu=pci.bus_id,persistence_mode. Call --help-query-gpu for more info.

   <b>--query-supported-clocks=</b>
       List of supported clocks. Call --help-query-supported-clocks for more info.

   <b>--query-compute-apps=</b>
       List of currently active compute processes. Call --help-query-compute-apps for more info.

   <b>--query-accounted-apps=</b>
       List of accounted compute processes. Call --help-query-accounted-apps for more info. This query is not
       supported on vGPU host.

   <b>--query-retired-pages=</b>
       List of GPU device memory pages that have been retired. Call --help-query-retired-pages for more info.

   <b>--query-remapped-rows=</b>
       Information about remapped rows. Call --help-query-remapped-rows for more info.

   <b>[mandatory]</b>
   <b>--format=</b>
       Comma separated list of format options:

       • csv - comma separated values (MANDATORY)

       • noheader - skip first line with column headers

       • nounits - don't print units for numerical values

   <b>[plus</b> <b>any</b> <b>of]</b>
   <b>-i,</b> <b>--id=ID</b>
       Display  data  for a single specified GPU. The specified id may be the GPU's 0-based index in the natural
       enumeration returned by the driver, the GPU's board serial number, the GPU's UUID, or the GPU's  PCI  bus
       ID  (as  domain:bus:device.function in hex). It is recommended that users desiring consistency use either
       UUID or PCI bus ID, since device enumeration ordering is not guaranteed to be consistent between  reboots
       and board serial number might be shared between multiple GPUs on the same board.

   <b>-f</b> <b>FILE,</b> <b>--filename=FILE</b>
       Redirect  query  output  to the specified file in place of the default stdout. The specified file will be
       overwritten.

   <b>-l</b> <b>SEC,</b> <b>--loop=SEC</b>
       Continuously report query data at the specified interval, rather than  the  default  of  just  once.  The
       application  will  sleep  in-between queries. Note that on Linux ECC error or Xid error events will print
       out during the sleep period if the -x flag was not specified. Pressing Ctrl+C at any time will abort  the
       loop,  which  will  otherwise  run  indefinitely.  If  no argument is specified for the <b>-l</b> form a default
       interval of 5 seconds is used.

   <b>-lms</b> <b>ms,</b> <b>--loop-ms=ms</b>
       Same as -l,--loop but in milliseconds.

   <b>DEVICE</b> <b>MODIFICATION</b> <b>OPTIONS</b>
   <b>[any</b> <b>one</b> <b>of]</b>
   <b>-pm,</b> <b>--persistence-mode=MODE</b>
       Set the persistence mode for the target GPUs. See the (<u>GPU</u>  <u>ATTRIBUTES</u>)  section  for  a  description  of
       persistence  mode.  Requires  root.  Will  impact  all GPUs unless a single GPU is specified using the -i
       argument. The effect of this operation is immediate. However, it does not persist across  reboots.  After
       each reboot persistence mode will default to "Disabled". Available on Linux only.

   <b>-e,</b> <b>--ecc-config=CONFIG</b>
       Set  the  ECC  mode  for the target GPUs. See the (<u>GPU</u> <u>ATTRIBUTES</u>) section for a description of ECC mode.
       Requires root. Will impact all GPUs unless a single GPU is specified using the -i argument. This  setting
       takes effect after the next reboot and is persistent.

   <b>-p,</b> <b>--reset-ecc-errors=TYPE</b>
       Reset  the  ECC error counters for the target GPUs. See the (<u>GPU</u> <u>ATTRIBUTES</u>) section for a description of
       ECC error counter types. Available arguments are 0\|VOLATILE or 1\|AGGREGATE. Requires root. Will  impact
       all  GPUs  unless  a  single  GPU  is  specified  using  the -i argument. The effect of this operation is
       immediate. Clearing aggregate counts is not supported on Ampere+

   <b>-c,</b> <b>--compute-mode=MODE</b>
       Set the compute mode for the target GPUs. See the (<u>GPU</u> <u>ATTRIBUTES</u>) section for a description  of  compute
       mode.  Requires  root.  Will  impact all GPUs unless a single GPU is specified using the -i argument. The
       effect of this operation is immediate. However, it does not persist across  reboots.  After  each  reboot
       compute mode will reset to "DEFAULT".

   <b>-dm</b> <b>TYPE,</b> <b>--driver-model=TYPE</b>
   <b>-fdm</b> <b>TYPE,</b> <b>--force-driver-model=TYPE</b>
       Enable or disable TCC driver model. For Windows only. Requires administrator privileges. -dm will fail if
       a  display  is  attached,  but  -fdm will force the driver model to change. Will impact all GPUs unless a
       single GPU is specified using the -i argument. A reboot is required for the change  to  take  place.  See
       <b>Driver</b>  <b>Model</b>  for  more information on Windows driver models. An error message indicates that retrieving
       the field failed.

   <b>--gom=MODE</b>
       Set GPU Operation Mode: 0/ALL_ON, 1/COMPUTE, 2/LOW_DP  Supported  on  GK110  M-class  and  X-class  Tesla
       products  from  the  Kepler family. Not supported on Quadro and Tesla C-class products. LOW_DP and ALL_ON
       are the only modes supported on  GeForce  Titan  devices.  Requires  administrator  privileges.  See  <u>GPU</u>
       <u>Operation</u>  <u>Mode</u>  for  more  information  about  GOM.  GOM  changes  take  effect after reboot. The reboot
       requirement might be removed in the future. Compute only GOMs don't support WDDM (Windows Display  Driver
       Model)

   <b>-r,</b> <b>--gpu-reset</b>
       Trigger  a  reset  of one or more GPUs. Can be used to clear GPU HW and SW state in situations that would
       otherwise require a machine reboot. Typically useful if a double bit ECC error has occurred. Optional  -i
       switch  can  be  used  to  target  one or more specific devices. Without this option, all GPUs are reset.
       Requires root. There can't be any applications using  these  devices  (e.g.  CUDA  application,  graphics
       application like X server, monitoring application like other instance of nvidia-smi). There also can't be
       any compute applications running on any other GPU in the system if individual GPU reset is not feasible.

       Starting  with the NVIDIA Ampere architecture, GPUs with NVLink connections can be individually reset. On
       Ampere NVSwitch systems, Fabric Manager is required to facilitate reset. On  Hopper  and  later  NVSwitch
       systems, the dependency on Fabric Manager to facilitate reset is removed.

       If  Fabric  Manager  is  not  running,  or  if  any  of the GPUs being reset are based on an architecture
       preceding the NVIDIA Ampere architecture, any GPUs with NVLink connections to a GPU being reset must also
       be reset in the same command. This can be done either by omitting the -i switch, or using the  -i  switch
       to  specify  the  GPUs  to  be  reset. If the -i option does not specify a complete set of NVLink GPUs to
       reset, this command will issue an error identifying the additional GPUs that  must  be  included  in  the
       reset command.

       GPU  reset  is  not guaranteed to work in all cases. It is not recommended for production environments at
       this time. In some situations there may be HW components on the board that fail  to  revert  back  to  an
       initial  state  following  the reset request. This is more likely to be seen on Fermi-generation products
       vs. Kepler, and more likely to be seen if the reset is being performed on a hung GPU.

       Following a reset, it is recommended that the health of each reset GPU be verified before further use. If
       any GPU is not healthy a complete reset should be instigated by power cycling the node.

       GPU reset operation will not be supported on MIG enabled vGPU guests.

       Visit <u><a href="http://developer.nvidia.com/gpu-deployment-kit">http://developer.nvidia.com/gpu-deployment-kit</a></u> to download the GDK.

   <b>-vm,</b> <b>--virt-mode=MODE</b>
       Switch GPU Virtualization Mode. Sets GPU virtualization mode to 3/VGPU or 4/VSGA. Virtualization mode  of
       a GPU can only be set when it is running on a hypervisor.

   <b>-lgc,</b> <b>--lock-gpu-clocks=MIN_GPU_CLOCK,MAX_GPU_CLOCK</b>
       Specifies &lt;minGpuClock,maxGpuClock&gt; clocks as a pair (e.g. 1500,1500) that defines closest desired locked
       GPU  clock  speed  in  MHz.  Input can also use be a singular desired clock value (e.g. &lt;GpuClockValue&gt;).
       Optionally, --mode can be supplied to specify the clock locking modes. Supported on Volta+. Requires root

       <b>--mode=0</b> <b>(Default)</b>
                      This mode is the default clock locking mode and provides the  highest  possible  frequency
                      accuracies supported by the hardware.

       <b>--mode=1</b>       The  clock  locking  algorithm  leverages  close  loop  controllers  to  achieve frequency
                      accuracies with improved  perf  per  watt  for  certain  class  of  applications.  Due  to
                      convergence  latency  of  close loop controllers, the frequency accuracies may be slightly
                      lower than default mode 0.

   <b>-lmc,</b> <b>--lock-memory-clocks=MIN_MEMORY_CLOCK,MAX_MEMORY_CLOCK</b>
       Specifies &lt;minMemClock,maxMemClock&gt; clocks as a pair (e.g. 5100,5100) that defines the range  of  desired
       locked   Memory   clock  speed  in  MHz.  Input  can  also  be  a  singular  desired  clock  value  (e.g.
       &lt;MemClockValue&gt;).

   <b>-rgc,</b> <b>--reset-gpu-clocks</b>
       Resets the GPU clocks to the default value. Supported on Volta+. Requires root.

   <b>-rmc,</b> <b>--reset-memory-clocks</b>
       Resets the memory clocks to the default value. Supported on Volta+. Requires root.

   <b>-ac,</b> <b>--applications-clocks=MEM_CLOCK,GRAPHICS_CLOCK</b>
       Specifies maximum &lt;memory,graphics&gt; clocks as a pair (e.g.  2000,800)  that  defines  GPU's  speed  while
       running  applications  on  a  GPU.  Supported  on  Maxwell-based  GeForce  and from the Kepler+ family in
       Tesla/Quadro/Titan devices. Requires root.

   <b>-rac,</b> <b>--reset-applications-clocks</b>
       Resets the applications clocks to the default value. Supported on  Maxwell-based  GeForce  and  from  the
       Kepler+ family in Tesla/Quadro/Titan devices. Requires root.

   <b>-lmcd,</b> <b>--lock-memory-clocks-deferred</b>
       Specifies  the  memory clock that defines the closest desired Memory Clock in MHz. The memory clock takes
       effect the next time the GPU is initialized. This can be guaranteed by unloading and reloading the kernel
       module. Requires root.

   <b>-rmcd,</b> <b>--reset-memory-clocks-deferred</b>
       Resets the memory clock to default value. Driver unload and reload is required for this to  take  effect.
       This can be done by unloading and reloading the kernel module. Requires root.

   <b>-pl,</b> <b>--power-limit=POWER_LIMIT</b>
       Specifies  maximum power limit in watts. Accepts integer and floating point numbers. it takes an optional
       argument --scope. Only on supported devices from Kepler family. Requires administrator privileges.  Value
       needs to be between Min and Max Power Limit as reported by nvidia-smi.

   <b>-sc,</b> <b>--scope=0/GPU,</b> <b>1/TOTAL_MODULE</b>
       Specifies  the scope of the power limit. Following are the options: 0/GPU: This only changes power limits
       for the GPU 1/Module: This changes the power for the module containing multiple components. E.g. GPU  and
       CPU.

   <b>-cc,</b> <b>--cuda-clocks=MODE</b>
       Overrides or restores default CUDA clocks Available arguments are 0\|RESTORE_DEFAULT or 1\|OVERRIDE.

   <b>-am,</b> <b>--accounting-mode=MODE</b>
       Enables  or  disables  GPU  Accounting.  With  GPU  Accounting  one  can keep track of usage of resources
       throughout lifespan of a  single  process.  Only  on  supported  devices  from  Kepler  family.  Requires
       administrator privileges. Available arguments are 0\|DISABLED or 1\|ENABLED.

   <b>-caa,</b> <b>--clear-accounted-apps</b>
       Clears  all  processes  accounted  so  far.  Only  on  supported  devices  from  Kepler  family. Requires
       administrator privileges.

   <b>--auto-boost-default=MODE</b>
       Set the default auto boost policy to 0/DISABLED or 1/ENABLED, enforcing the change only  after  the  last
       boost  client has exited. Only on certain Tesla devices from the Kepler+ family and Maxwell-based GeForce
       devices. Requires root.

   <b>--auto-boost-permission=MODE</b>
       Allow  non-admin/root  control  over  auto  boost  mode.   Available   arguments   are   0\|UNRESTRICTED,
       1\|RESTRICTED.  Only  on certain Tesla devices from the Kepler+ family and Maxwell-based GeForce devices.
       Requires root.

   <b>-mig,</b> <b>--multi-instance-gpu=MODE</b>
       Enables or disables Multi Instance GPU mode. Only  supported  on  devices  based  on  the  NVIDIA  Ampere
       architecture. Requires root. Available arguments are 0\|DISABLED or 1\|ENABLED.

   <b>-gtt,</b> <b>--gpu-target-temp=MODE</b>
       Set  GPU  Target  Temperature  for  a  GPU  in  degree celsius. Requires administrator privileges. Target
       temperature should be within limits supported by GPU. These limits can be retrieved by using query option
       with SUPPORTED_GPU_TARGET_TEMP.

   <b>[plus</b> <b>optionally]</b>
   <b>-i,</b> <b>--id=ID</b>
       Modify a single specified GPU. The specified id may be  the  GPU/Unit's  0-based  index  in  the  natural
       enumeration  returned  by the driver, the GPU's board serial number, the GPU's UUID, or the GPU's PCI bus
       ID (as domain:bus:device.function in hex). It is recommended that users desiring consistency  use  either
       UUID  or PCI bus ID, since device enumeration ordering is not guaranteed to be consistent between reboots
       and board serial number might be shared between multiple GPUs on the same board.

   <b>-eom,</b> <b>--error-on-warning</b>
       Return a non-zero error for warnings.

   <b>UNIT</b> <b>MODIFICATION</b> <b>OPTIONS</b>
   <b>-t,</b> <b>--toggle-led=STATE</b>
       Set the LED indicator state on the front and back of the unit to  the  specified  color.  See  the  (<u>UNIT</u>
       <u>ATTRIBUTES</u>)  section  for  a  description  of  the  LED states. Allowed colors are 0\|GREEN and 1\|AMBER.
       Requires root.

   <b>[plus</b> <b>optionally]</b>
   <b>-i,</b> <b>--id=ID</b>
       Modify a single specified Unit. The specified id is the Unit's 0-based index in the  natural  enumeration
       returned by the driver.

   <b>SHOW</b> <b>DTD</b> <b>OPTIONS</b>
   <b>--dtd</b>
       Display Device or Unit DTD.

   <b>[plus</b> <b>optionally]</b>
   <b>-f</b> <b>FILE,</b> <b>--filename=FILE</b>
       Redirect  query  output  to the specified file in place of the default stdout. The specified file will be
       overwritten.

   <b>-u,</b> <b>--unit</b>
       Display Unit DTD instead of device DTD.

   <b>topo</b>
       Display topology information about the system. Use "nvidia-smi topo -h" for more information. Linux only.
       Shows all GPUs NVML is able to detect but CPU and NUMA node affinity information will only be  shown  for
       GPUs with Kepler or newer architectures. Note: GPU enumeration is the same as NVML.

   <b>drain</b>
       Display and modify the GPU drain states. A drain state is one in which the GPU is no longer accepting new
       clients,  and  is  used  while  preparing  to  power  down  the  GPU.  Use "nvidia-smi drain -h" for more
       information. Linux only.

   <b>nvlink</b>
       Display nvlink information. Use "nvidia-smi nvlink -h" for more information.

   <b>clocks</b>
       Query and control clocking behavior. Use "nvidia-smi clocks --help" for more information.

   <b>vgpu</b>
       Display information on GRID virtual GPUs. Use "nvidia-smi vgpu -h" for more information.

   <b>mig</b>
       Provides controls for MIG management. "nvidia-smi mig -h" for more information.

   <b>boost-slider</b>
       Provides controls for boost sliders management. "nvidia-smi boost-slider -h" for more information.

   <b>power-hint</b>
       Provides queries for power hint. "nvidia-smi power-hint -h" for more information.

   <b>conf-compute</b>
       Provides control and queries for confidential compute. "nvidia-smi conf-compute -h" for more information.

   <b>power-smoothing</b>
       Provides controls  and  information  for  power  smoothing.  "nvidia-smi  power-smoothing  -h"  for  more
       information.

   <b>power-profiles</b>
       Profiles  controls  and  information for workload power profiles. "nvidia-smi power-profiles -h" for more
       information.

   <b>encodersessions</b>
       Display Encoder Sessions information. "nvidia-smi encodersessions -h" for more information.

</pre><h4><b>RETURN</b> <b>VALUE</b></h4><pre>
       Return code reflects whether the operation succeeded or failed and what was the reason of failure.

       • Return code 0 - Success

       • Return code 2 - A supplied argument or flag is invalid

       • Return code 3 - The requested operation is not available on target device

       • Return code 4 - The current user does not have  permission  to  access  this  device  or  perform  this
         operation

       • Return code 6 - A query to find an object was unsuccessful

       • Return code 8 - A device's external power cables are not properly attached

       • Return code 9 - NVIDIA driver is not loaded

       • Return code 10 - NVIDIA Kernel detected an interrupt issue with a GPU

       • Return code 12 - NVML Shared Library couldn't be found or loaded

       • Return code 13 - Local version of NVML doesn't implement this function

       • Return code 14 - infoROM is corrupted

       • Return code 15 - The GPU has fallen off the bus or has otherwise become inaccessible

       • Return code 255 - Other error or internal driver error occurred

</pre><h4><b>GPU</b> <b>ATTRIBUTES</b></h4><pre>
       The  following  list describes all possible data returned by the <b>-q</b> device query option. Unless otherwise
       noted all numerical results are base 10 and unitless.

   <b>Timestamp</b>
       The current system timestamp at the time  nvidia-smi  was  invoked.  Format  is  "Day-of-week  Month  Day
       HH:MM:SS Year".

   <b>Driver</b> <b>Version</b>
       The version of the installed NVIDIA display driver. This is an alphanumeric string.

   <b>CUDA</b> <b>Version</b>
       The version of the CUDA toolkit installed on the system. This is an alphanumeric string.

   <b>Attached</b> <b>GPUs</b>
       The number of NVIDIA GPUs in the system.

   <b>Product</b> <b>Name</b>
       The official product name of the GPU. This is an alphanumeric string. For all products.

   <b>Product</b> <b>Brand</b>
       The official brand of the GPU. This is an alphanumeric string. For all products.

   <b>Product</b> <b>Architecture</b>
       The official architecture name of the GPU. This is an alphanumeric string. For all products.

   <b>Display</b> <b>Mode</b>
       This field is deprecated, and will be removed in a future release.

   <b>Display</b> <b>Attached</b>
       A  flag  that  indicates  whether  a physical display (e.g. monitor) is currently connected to any of the
       GPU's connectors. "Yes" indicates an attached display. "No" indicates otherwise.

   <b>Display</b> <b>Active</b>
       A flag that indicates whether a display is initialized on the GPU's (e.g.  memory  is  allocated  on  the
       device  for  display).  Display  can  be  active  even  when no monitor is physically attached. "Enabled"
       indicates an active display. "Disabled" indicates otherwise.

   <b>Persistence</b> <b>Mode</b>
       A flag that indicates whether persistence mode is enabled for the  GPU.  Value  is  either  "Enabled"  or
       "Disabled".  When  persistence  mode  is  enabled  the  NVIDIA  driver remains loaded even when no active
       clients, such as X11 or nvidia-smi, exist. This minimizes the driver load latency associated with running
       dependent apps, such as CUDA programs. For all CUDA-capable products. Linux only.

   <b>Addressing</b> <b>Mode</b>
       A field that indicates which addressing mode is currently active. The value is "ATS" or "HMM" or  "None".
       When  the  mode  is  "ATS",  system  allocated memory like malloc is addressable from the GPU via Address
       Translation Services. This means there is effectively a single set of page tables used by  both  the  CPU
       and  the GPU. When the mode is "HMM", system allocated memory like malloc is addressable from the GPU via
       software-based mirroring of the CPU's page tables, on the GPU. When the mode is "None", neither  ATS  nor
       HMM is active. Linux only.

   <b>MIG</b> <b>Mode</b>
       MIG Mode configuration status

       <b>Current</b>        MIG mode currently in use - NA/Enabled/Disabled

       <b>Pending</b>        Pending configuration of MIG Mode - Enabled/Disabled

   <b>Accounting</b> <b>Mode</b>
       A  flag  that  indicates  whether  accounting  mode  is enabled for the GPU. Value is either "Enabled" or
       "Disabled". When accounting is enabled statistics are calculated for each compute process running on  the
       GPU.  Statistics  can  be  queried during the lifetime or after termination of the process. The execution
       time of process is reported as 0 while the process is in running state and updated  to  actual  execution
       time after the process has terminated. See --help-query-accounted-apps for more info.

   <b>Accounting</b> <b>Mode</b> <b>Buffer</b> <b>Size</b>
       Returns  the  size of the circular buffer that holds list of processes that can be queried for accounting
       stats. This is the maximum number of processes that accounting information  will  be  stored  for  before
       information about oldest processes will get overwritten by information about new processes.

   <b>Driver</b> <b>Model</b>
       On  Windows, the TCC and WDDM driver models are supported. The driver model can be changed with the (-dm)
       or (-fdm) flags. The TCC driver model is optimized for compute applications.  I.E.  kernel  launch  times
       will  be  quicker  with  TCC.  The  WDDM  driver  model  is designed for graphics applications and is not
       recommended for compute applications. Linux does not support multiple driver models, and will always have
       the value of "N/A".

       <b>Current</b>        The driver model currently in use. Always "N/A" on Linux.

       <b>Pending</b>        The driver model that will be used on the next reboot. Always "N/A" on Linux.

   <b>Serial</b> <b>Number</b>
       This number matches the serial number physically printed on each board. It is a globally unique immutable
       alphanumeric value.

   <b>GPU</b> <b>UUID</b>
       This value is the globally unique immutable alphanumeric identifier of the GPU. It does not correspond to
       any physical label on the board.

   <b>Minor</b> <b>Number</b>
       The minor number for the device is such that the Nvidia device node file for each GPU will have the  form
       /dev/nvidia[minor number]. Available only on Linux platform.

   <b>VBIOS</b> <b>Version</b>
       The BIOS of the GPU board.

   <b>MultiGPU</b> <b>Board</b>
       Whether or not this GPU is part of a multiGPU board.

   <b>Board</b> <b>ID</b>
       The  unique  board  ID  assigned  by the driver. If two or more GPUs have the same board ID and the above
       "MultiGPU" field is true then the GPUs are on the same board.

   <b>Board</b> <b>Part</b> <b>Number</b>
       The unique part number of the GPU's board

   <b>GPU</b> <b>Part</b> <b>Number</b>
       The unique part number of the GPU

   <b>FRU</b> <b>Part</b> <b>Number</b>
       Unique FRU part number of the GPU

   <b>Platform</b> <b>Info</b>
       Platform Information are compute tray platform specific information. They are GPU's positional index  and
       platform identifying information.

       <b>Chassis</b> <b>Serial</b> <b>Number</b>

       Serial Number of the chassis containing this GPU.

       <b>Slot</b> <b>Number</b>

       The slot number in the chassis containing this GPU (includes switches).

       <b>Tray</b> <b>Index</b>

       The tray index within the compute slots in the chassis containing this GPU (does not include switches).

       <b>Host</b> <b>ID</b>

       Index of the node within the slot containing this GPU.

       <b>Peer</b> <b>Type</b>

       Platform indicated NVLink-peer type (e.g. switch present or not).

       <b>Module</b> <b>Id</b>

       ID of this GPU within the node.

       <b>GPU</b> <b>Fabric</b> <b>GUID</b>

       Fabric ID for this GPU.

   <b>Inforom</b> <b>Version</b>
       Version  numbers  for  each object in the GPU board's inforom storage. The inforom is a small, persistent
       store of configuration and state data for the GPU. All inforom version fields are numerical.  It  can  be
       useful  to  know  these  version  numbers because some GPU features are only available with inforoms of a
       certain version or higher.

       If any of the fields below return Unknown Error additional Inforom verification check  is  performed  and
       appropriate warning message is displayed.

       <b>Image</b> <b>Version</b>  Global  version  of  the  infoROM  image.  Image  version just like VBIOS version uniquely
                      describes the exact version of the infoROM flashed on the board  in  contrast  to  infoROM
                      object version which is only an indicator of supported features.

       <b>OEM</b> <b>Object</b>     Version for the OEM configuration data.

       <b>ECC</b> <b>Object</b>     Version for the ECC recording data.

       <b>Power</b> <b>Management</b> <b>Object</b>
                      Version for the power management data.

       <b>Inforom</b> <b>checksum</b> <b>validation</b>
                      Inforom  checksum  validation  ("valid",  "invalid", "N/A") Only available via --query-gpu
                      inforom.checksum_validation

   <b>Inforom</b> <b>BBX</b> <b>Object</b> <b>Flush</b>
       Information about flushing of the blackbox data to the inforom storage.

       <b>Latest</b> <b>Timestamp</b>
                      The timestamp of the latest flush of the BBX Object during the current run.

       <b>Latest</b> <b>Duration</b>
                      The duration of the latest flush of the BBX Object during the current run.

   <b>GPU</b> <b>Operation</b> <b>Mode</b>
       GOM allows one to reduce power usage and optimize GPU throughput by disabling GPU features.

       Each GOM is designed to meet specific user needs.

       In "All On" mode everything is enabled and running at full speed.

       The "Compute" mode is designed for running only compute tasks. Graphics operations are not allowed.

       The "Low Double Precision" mode is designed for running graphics applications  that  don't  require  high
       bandwidth double precision.

       GOM can be changed with the (--gom) flag.

       Supported on GK110 M-class and X-class Tesla products from the Kepler family. Not supported on Quadro and
       Tesla  C-class products. Low Double Precision and All On modes are the only modes available for supported
       GeForce Titan products.

       <b>Current</b>        The GOM currently in use.

       <b>Pending</b>        The GOM that will be used on the next reboot.

   <b>GPU</b> <b>C2C</b> <b>Mode</b>
       The C2C mode of the GPU.

   <b>GPU</b> <b>Reset</b> <b>Status</b>
       Reset status of the GPU. This functionality is deprecated.

       <b>Reset</b> <b>Required</b> Requested functionality has been deprecated

       <b>Drain</b> <b>and</b> <b>Reset</b> <b>Recommended</b>
                      Requested functionality has been deprecated

   <b>GPU</b> <b>Recovery</b> <b>Action</b>
       Action to take to clear fault that previously happened. It is not intended for  determining  which  fault
       triggered recovery action.
       Possible values: None, Reset, Reboot, Drain P2P, Drain and Reset

       <b>None</b>

       No recovery action needed

       <b>Reset</b>

       Example scenario - Uncontained HBM/SRAM UCE
       The GPU has encountered a fault that requires a reset to recover.
       Terminate  all  GPU  processes,  reset  the  GPU  using 'nvidia-smi -r', and the GPU can be used again by
       starting new GPU processes.

       <b>Reboot</b>

       Example scenario - UVM fatal error
       The GPU has encountered a fault may have left the OS in an inconsistent state.
       Reboot the operating system to restore the OS back to a consistent state.
       Node reboot required.
       Application cannot restart without node reboot
       OS warm reboot is sufficient (no need for AC/DC cycle)

       <b>Drain</b> <b>P2P</b>

       Example scenario - N/A
       The GPU has encountered a fault that requires all peer-to-peer traffic to be quiesced.
       Terminate all GPU processes that conduct peer-to-peer traffic and disable UVM persistence mode.
       Disable job scheduling (no new jobs), stop all applications  when  convenient,  if  persistence  mode  is
       enabled, disable it
       Once  all  peer-to-peer  traffic are drained, query NVML_FI_DEV_GET_GPU_RECOVERY_ACTION again, which will
       return one of the other actions.
       If still DRAIN_P2P, then GPU reset.

       <b>Drain</b> <b>and</b> <b>Reset</b>

       Example scenario - Contained HBM UCE
       Reset Recommended.
       The GPU has encountered a fault that results the GPU to temporarily operate at a reduced  capacity,  such
       as part of its frame buffer memory being offlined, or some of its MIG partitions down.
       No  new  work  should  be  scheduled  on  the GPU, but existing work that didn’t get affected are safe to
       continue until they finish or reach a good checkpoint.
       Safe to restart application (memory capacity will be reduced due to dynamic page offlining), but need  to
       eventually reset (to get row remap).
       Asserted only for UCE row remaps.
       After all existing work have drained, reset the GPU to regain its full capacity.

   <b>GSP</b> <b>Firmware</b> <b>Version</b>
       Firmware version of GSP. This is an alphanumeric string.

   <b>PCI</b>
       Basic PCI info for the device. Some of this information may change whenever cards are added/removed/moved
       in a system. For all products.

       <b>Bus</b>            PCI bus number, in hex

       <b>Device</b>         PCI device number, in hex

       <b>Domain</b>         PCI domain number, in hex

       <b>Base</b> <b>Classcode</b> PCI Base classcode, in hex

       <b>Sub</b> <b>Classcode</b>  PCI Sub classcode, in hex

       <b>Device</b> <b>Id</b>      PCI vendor device id, in hex

       <b>Sub</b> <b>System</b> <b>Id</b>  PCI Sub System id, in hex

       <b>Bus</b> <b>Id</b>         PCI bus id as "domain:bus:device.function", in hex

   <b>GPU</b> <b>Link</b> <b>information</b>
       The PCIe link generation and bus width

       <b>Current</b>        The current link generation and width. These may be reduced when the GPU is not in use.

       <b>Max</b>            The maximum link generation and width possible with this GPU and system configuration. For
                      example,  if  the GPU supports a higher PCIe generation than the system supports then this
                      reports the system PCIe generation.

   <b>Bridge</b> <b>Chip</b>
       Information related to Bridge Chip on the device. The bridge chip firmware is  only  present  on  certain
       boards and may display "N/A" for some newer multiGPUs boards.

       <b>Type</b>           The type of bridge chip. Reported as N/A if doesn't exist.

       <b>Firmware</b> <b>Version</b>
                      The firmware version of the bridge chip. Reported as N/A if doesn't exist.

   <b>Replays</b> <b>Since</b> <b>Reset</b>
       The number of PCIe replays since reset.

   <b>Replay</b> <b>Number</b> <b>Rollovers</b>
       The  number  of  PCIe  replay  number  rollovers  since  reset.  A  replay number rollover occurs after 4
       consecutive replays and results in retraining the link.

   <b>Tx</b> <b>Throughput</b>
       The GPU-centric transmission throughput across the PCIe bus in MB/s over the past 20ms. Only supported on
       Maxwell architectures and newer.

   <b>Rx</b> <b>Throughput</b>
       The GPU-centric receive throughput across the PCIe bus in MB/s over the  past  20ms.  Only  supported  on
       Maxwell architectures and newer.

   <b>Atomic</b> <b>Caps</b>
       The PCIe atomic capabilities of outbound/inbound operations of the GPU.

   <b>Fan</b> <b>Speed</b>
       The  fan  speed value is the percent of the product's maximum noise tolerance fan speed that the device's
       fan is currently intended to run at. This value may exceed 100% in  certain  cases.  Note:  The  reported
       speed  is  the  intended fan speed. If the fan is physically blocked and unable to spin, this output will
       not match the actual fan speed. Many parts do not report fan speeds because they rely on cooling via fans
       in the surrounding enclosure. For all discrete products with dedicated fans.

   <b>Performance</b> <b>State</b>
       The current performance state for the GPU. States range from P0 (maximum  performance)  to  P12  (minimum
       performance).

   <b>Clocks</b> <b>Event</b> <b>Reasons</b>
       Retrieves information about factors that are reducing the frequency of clocks.

       If all event reasons are returned as "Not Active" it means that clocks are running as high as possible.

       <b>Idle</b>           Nothing  is running on the GPU and the clocks are dropping to Idle state. This limiter may
                      be removed in a later release.

       <b>Application</b> <b>Clocks</b> <b>Setting</b>
                      GPU clocks are limited by applications clocks setting. E.g. can be changed  using  nvidia-
                      smi --applications-clocks=

       <b>SW</b> <b>Power</b> <b>Cap</b>   SW  Power  Scaling algorithm is reducing the clocks below requested clocks because the GPU
                      is consuming too much power. E.g. SW power  cap  limit  can  be  changed  with  nvidia-smi
                      --power-limit=

       <b>HW</b> <b>Slowdown</b>    HW  Slowdown  (reducing  the  core clocks by a factor of 2 or more) is engaged. HW Thermal
                      Slowdown and HW Power Brake will be displayed on Pascal+.

       This is an indicator of:\

       • Temperature being too high (HW Thermal Slowdown)\

       • External Power Brake Assertion is  triggered  (e.g.  by  the  system  power  supply)  (HW  Power  Brake
         Slowdown)\

       • Power draw is too high and Fast Trigger protection is reducing the clocks

       <b>SW</b> <b>Thermal</b> <b>Slowdown</b>
                      SW  Thermal  capping  algorithm  is  reducing  clocks  below  requested clocks because GPU
                      temperature is higher than Max Operating Temp

   <b>Clock</b> <b>Event</b> <b>Reasons</b> <b>Counters</b>
       Counters, in microseconds, for the amount of time factors have been reducing the frequency of clocks

       <b>SW</b> <b>Power</b> <b>Capping</b>
                      Amount of time SW Power Scaling algorithm has reduced the clocks  below  requested  clocks
                      because the GPU was consuming too much power.

       <b>Sync</b> <b>Boost</b> <b>Group</b>
                      Amount  of  time the clock frequency of this GPU was reduced to match the minimum possible
                      clock across the sync boost group.

       <b>SW</b> <b>Thermal</b> <b>Slowdown</b>
                      Amount of time SW Thermal capping algorithm has  reduced  clocks  below  requested  clocks
                      because GPU temperature was higher than Max Operating Temp.

       <b>HW</b> <b>Thermal</b> <b>Slowdown</b>
                      Amount  of time HW Thermal Slowdown was engaged, reducing the core clocks by a factor of 2
                      or more, due to temperature being too high.

       <b>HW</b> <b>Power</b> <b>Braking</b>
                      Amount of time External Power Brake Assertion was triggered  (e.g.  by  the  system  power
                      supply).

   <b>Sparse</b> <b>Operation</b> <b>Mode</b>
       A  flag that indicates whether sparse operation mode is enabled for the GPU. Value is either "Enabled" or
       "Disabled". Reported as "N/A" if not supported.

   <b>FB</b> <b>Memory</b> <b>Usage</b>
       On-board frame buffer memory information. Reported total memory can be affected by ECC state. If ECC does
       affect the total available memory, memory is decreased by several percent, due to  the  requisite  parity
       bits.  The driver may also reserve a small amount of memory for internal use, even without active work on
       the GPU. On systems where GPUs are NUMA nodes, the accuracy of FB memory utilization provided by  nvidia-
       smi depends on the memory accounting of the operating system. This is because FB memory is managed by the
       operating  system  instead  of  the  NVIDIA GPU driver. Typically, pages allocated from FB memory are not
       released even after the process terminates to enhance  performance.  In  scenarios  where  the  operating
       system  is  under  memory  pressure,  it  may  resort  to utilizing FB memory. Such actions can result in
       discrepancies in the accuracy of memory reporting. For all products.

       <b>Total</b>          Total size of FB memory.

       <b>Reserved</b>       Reserved size of FB memory.

       <b>Used</b>           Used size of FB memory.

       <b>Free</b>           Available size of FB memory.

   <b>BAR1</b> <b>Memory</b> <b>Usage</b>
       BAR1 is used to map the FB (device memory) so that it can be directly accessed by the CPU or by 3rd party
       devices (peer-to-peer on the PCIe bus).

       <b>Total</b>          Total size of BAR1 memory.

       <b>Used</b>           Used size of BAR1 memory.

       <b>Free</b>           Available size of BAR1 memory.

   <b>Compute</b> <b>Mode</b>
       The compute mode flag indicates whether individual or multiple compute applications may run on the GPU.

       "Default" means multiple contexts are allowed per device.

       "Exclusive Process" means only one context is allowed per device, usable from multiple threads at a time.

       "Prohibited" means no contexts are allowed per device (no compute apps).

       "EXCLUSIVE_PROCESS" was added in CUDA 4.0. Prior CUDA releases supported only one exclusive  mode,  which
       is equivalent to "EXCLUSIVE_THREAD" in CUDA 4.0 and beyond.

       For all CUDA-capable products.

   <b>Utilization</b>
       Utilization  rates  report  how  busy  each  GPU  is  over time, and can be used to determine how much an
       application is using the GPUs in the system. Note: On  MIG-enabled  GPUs,  querying  the  utilization  of
       encoder, decoder, jpeg, ofa, gpu, and memory is not currently supported.

       Note:  During  driver  initialization  when  ECC  is  enabled one can see high GPU and Memory Utilization
       readings.  This  is  caused  by  ECC  Memory  Scrubbing  mechanism  that  is  performed   during   driver
       initialization.

       <b>GPU</b>            Percent of time over the past sample period during which one or more kernels was executing
                      on  the  GPU.  The  sample  period may be between 1 second and 1/6 second depending on the
                      product.

       <b>Memory</b>         Percent of time over the past sample period during which global (device) memory was  being
                      read or written. The sample period may be between 1 second and 1/6 second depending on the
                      product.

       <b>Encoder</b>        Percent of time over the past sample period during which the GPU's video encoder was being
                      used.   The   sampling   rate   is   variable   and  can  be  obtained  directly  via  the
                      nvmlDeviceGetEncoderUtilization() API

       <b>Decoder</b>        Percent of time over the past sample period during which the GPU's video decoder was being
                      used.  The  sampling  rate  is  variable  and   can   be   obtained   directly   via   the
                      nvmlDeviceGetDecoderUtilization() API

       <b>JPEG</b>           Percent  of time over the past sample period during which the GPU's JPEG decoder was being
                      used.  The  sampling  rate  is  variable  and   can   be   obtained   directly   via   the
                      nvmlDeviceGetJpgUtilization() API

       <b>OFA</b>            Percent  of  time  over  the  past  sample period during which the GPU's OFA (Optical Flow
                      Accelerator) was being used. The sampling rate is variable and can  be  obtained  directly
                      via the nvmlDeviceGetOfaUtilization() API

   <b>Encoder</b> <b>Stats</b>
       Encoder Stats report the count of active encoder sessions, along with the average Frames Per Second (FPS)
       and average latency (in microseconds) for all these active sessions on this device.

       <b>Active</b> <b>Sessions</b>
                      The total number of active encoder sessions on this device.

       <b>Average</b> <b>FPS</b>    The average Frame Per Sencond (FSP) of all active encoder sessions on this device.

       <b>Average</b> <b>Latency</b>
                      The average latency in microseconds of all active encoder sessions on this device.

   <b>DRAM</b> <b>Encryption</b> <b>Mode</b>
       A  flag that indicates whether DRAM Encryption support is enabled. May be either "Enabled" or "Disabled".
       Changes to DRAM Encryption mode require a reboot. Requires Inforom ECC object.

       <b>Current</b>        The DRAM Encryption mode that the GPU is currently operating under.

       <b>Pending</b>        The DRAM Encryption mode that the GPU will operate under after the next reboot.

   <b>ECC</b> <b>Mode</b>
       A flag that indicates whether ECC support is enabled. May be either "Enabled" or "Disabled".  Changes  to
       ECC mode require a reboot. Requires Inforom ECC object version 1.0 or higher.

       <b>Current</b>        The ECC mode that the GPU is currently operating under.

       <b>Pending</b>        The ECC mode that the GPU will operate under after the next reboot.

   <b>ECC</b> <b>Errors</b>
       NVIDIA  GPUs  can provide error counts for various types of ECC errors. Some ECC errors are either single
       or double bit, where single bit errors are corrected and double bit  errors  are  uncorrectable.  Texture
       memory  errors  may  be  correctable  via  resend  or uncorrectable if the resend fails. These errors are
       available across two timescales (volatile  and  aggregate).  Single  bit  ECC  errors  are  automatically
       corrected  by  the  HW  and  do  not  result  in  data corruption. Double bit errors are detected but not
       corrected. Please see the ECC documents on the web for information on compute application  behavior  when
       double  bit  errors  occur.  Volatile  error  counters track the number of errors detected since the last
       driver load. Aggregate error counts persist indefinitely and thus act as a lifetime counter.

       A note about volatile counts: On Windows this is once per boot. On Linux this can be  more  frequent.  On
       Linux  the driver unloads when no active clients exist. Hence, if persistence mode is enabled or there is
       always a driver client active (e.g. X11), then Linux also sees per-boot behavior. If not, volatile counts
       are reset each time a compute app is run.

       Tesla and Quadro products pre-volta can display total ECC error counts, as well as a breakdown of  errors
       based on location on the chip. The locations are described below. Location-based data for aggregate error
       counts requires Inforom ECC object version 2.0. All other ECC counts require ECC object version 1.0.

       <b>Device</b> <b>Memory</b>  Errors detected in global device memory.

       <b>Register</b> <b>File</b>  Errors detected in register file memory.

       <b>L1</b> <b>Cache</b>       Errors detected in the L1 cache.

       <b>L2</b> <b>Cache</b>       Errors detected in the L2 cache.

       <b>Texture</b> <b>Memory</b> Parity errors detected in texture memory.

       <b>Total</b>          Total  errors  detected across entire chip. Sum of <b>Device</b> <b>Memory</b>, <b>Register</b> <b>File</b>, <b>L1</b> <b>Cache</b>,
                      <b>L2</b> <b>Cache</b> and <b>Texture</b> <b>Memory</b>.

       On Turing the output is such:

       <b>SRAM</b> <b>Correctable</b>
                      Number of correctable errors detected in any of the SRAMs

       <b>SRAM</b> <b>Uncorrectable</b>
                      Number of uncorrectable errors detected in any of the SRAMs

       <b>DRAM</b> <b>Correctable</b>
                      Number of correctable errors detected in the DRAM

       <b>DRAM</b> <b>Uncorrectable</b>
                      Number of uncorrectable errors detected in the DRAM

       On Ampere+ The categorization of SRAM errors has been expanded upon. SRAM errors are now  categorized  as
       either  parity  or SEC-DED (single error correctable/double error detectable) depending on which unit hit
       the error. A histogram has been added that categorizes what unit hit the SRAM error. Additionally a  flag
       has been added that indicates if the threshold for the specific SRAM has been exceeded.

       <b>SRAM</b> <b>Uncorrectable</b> <b>Parity</b>
                      Number of uncorrectable errors detected in SRAMs that are parity protected

       <b>SRAM</b> <b>Uncorrectable</b> <b>SEC-DED</b>
                      Number of uncorrectable errors detected in SRAMs that are SEC-DED protected

       <b>Aggregate</b> <b>Uncorrectable</b> <b>SRAM</b> <b>Sources</b>

       <b>SRAM</b> <b>L2</b>        Errors that occurred in the L2 cache

       <b>SRAM</b> <b>SM</b>        Errors that occurred in the SM

       <b>SRAM</b> <b>Microcontroller</b>
                      Errors that occurred in a microcontroller (PMU/GSP etc...)

       <b>SRAM</b> <b>PCIE</b>      Errors that occrred in any PCIE related unit

       <b>SRAM</b> <b>Other</b>     Errors occuring in anything else not covered above

   <b>Page</b> <b>Retirement</b>
       NVIDIA  GPUs  can  retire  pages  of  GPU device memory when they become unreliable. This can happen when
       multiple single bit ECC errors occur for the same page, or on a double bit ECC  error.  When  a  page  is
       retired, the NVIDIA driver will hide it such that no driver, or application memory allocations can access
       it.

       <b>Double</b>  <b>Bit</b>  <b>ECC</b>  The  number  of  GPU device memory pages that have been retired due to a double bit ECC
       error.

       <b>Single</b> <b>Bit</b> <b>ECC</b> The number of GPU device memory pages that have been retired due to  multiple  single  bit
       ECC errors.

       <b>Pending</b>  Checks  if  any GPU device memory pages are pending blacklist on the next reboot. Pages that are
       retired but not yet blacklisted can still be allocated, and may cause further reliability issues.

   <b>Row</b> <b>Remapper</b>
       NVIDIA GPUs can remap rows of GPU device memory when they become  unreliable.  This  can  happen  when  a
       single  uncorrectable  ECC  error or multiple correctable ECC errors occur on the same row. When a row is
       remapped, the NVIDIA driver will remap the faulty row to a reserved row. All future accesses to  the  row
       will access the reserved row instead of the faulty row. This feature is available on Ampere+

       <b>Correctable</b> <b>Error</b> The number of rows that have been remapped due to correctable ECC errors.

       <b>Uncorrectable</b> <b>Error</b> The number of rows that have been remapped due to uncorrectable ECC errors.

       <b>Pending</b>  Indicates  whether or not a row is pending remapping. The GPU must be reset for the remapping to
       go into effect.

       <b>Remapping</b> <b>Failure</b> <b>Occurred</b> Indicates whether or not a row remapping has failed in the past.

       <b>Bank</b> <b>Remap</b> <b>Availability</b> <b>Histogram</b> Each memory bank has a fixed number of reserved rows that can  be  used
       for  row  remapping.  The histogram will classify the remap availability of each bank into Maximum, High,
       Partial, Low and None. Maximum availability means that all reserved  rows  are  available  for  remapping
       while  None means that no reserved rows are available. Correctable row remappings don't count towards the
       availability histogram since row remappings due to correctable  row  remappings  can  be  evicted  by  an
       uncorrectable row remapping.

   <b>Temperature</b>
       Readings  from  temperature sensors on the board. All readings are in degrees C. Not all products support
       all reading types. In particular, products in module form factors that  rely  on  case  fans  or  passive
       cooling do not usually provide temperature readings. See below for restrictions.

       T.Limit:  The  T.Limit  sensor  measures  the  current  margin in degree Celsius to the maximum operating
       temperature. As such it is not an absolute temperature reading rather a relative measurement.

       Not all products support T.Limit sensor readings.

       When supported, nvidia-smi reports the current T.Limit temperature as a signed value that counts down.  A
       T.Limit  temperature  of  0  C  or  lower  indicates that the GPU may optimize its clock based on thermal
       conditions. Further, when the T.Limit sensor is supported,  available  temperature  thresholds  are  also
       reported relative to T.Limit (see below) instead of absolute measurements.

       <b>GPU</b>            Core GPU temperature. For all discrete and S-class products.

       <b>T.Limit</b> <b>Temp</b>   Current margin in degrees Celsius from the maximum GPU operating temperature.

       <b>Shutdown</b> <b>Temp</b>  The temperature at which a GPU will shutdown.

       <b>Shutdown</b> <b>T.Limit</b> <b>Temp</b>
                      The  T.Limit temperature below which a GPU may shutdown. Since shutdown can only triggered
                      by the maximum GPU temperature it is possible for the current T.Limit to be more  negative
                      than this threshold.

       <b>Slowdown</b> <b>Temp</b>  The  temperature at which a GPU HW will begin optimizing clocks due to thermal conditions,
                      in order to cool.

       <b>Slowdown</b> <b>T.Limit</b> <b>Temp</b>
                      The T.Limit temperature below  which  a  GPU  HW  may  optimize  its  clocks  for  thermal
                      conditions.  Since this clock adjustment can only triggered by the maximum GPU temperature
                      it is possible for the current T.Limit to be more negative than this threshold.

       <b>Max</b> <b>Operating</b> <b>Temp</b>
                      The temperature at which GPU SW will optimize its clock for thermal conditions.

       <b>Max</b> <b>Operating</b> <b>T.Limit</b> <b>Temp</b>
                      The T.Limit temperature below which GPU SW will optimize its clock for thermal conditions.

   <b>Power</b> <b>Readings</b>
       Power readings help to shed light on the current power usage of the GPU, and the factors that affect that
       usage. When power management is enabled the GPU limits power draw under load to fit within  a  predefined
       power  envelope  by  manipulating  the  current  performance state. See below for limits of availability.
       Please note that power readings are not applicable for Pascal and higher GPUs with BA sensor boards.

       <b>Power</b> <b>State</b>    Power State is deprecated and has been renamed to Performance State in 2.285. To  maintain
                      XML compatibility, in XML format Performance State is listed in both places.

       <b>Power</b> <b>Management</b>
                      A  flag  that  indicates whether power management is enabled. Either "Supported" or "N/A".
                      Requires Inforom PWR object version 3.0 or higher or Kepler device.

       <b>Instantaneous</b> <b>Power</b> <b>Draw</b>
                      The last measured power draw for the entire board,  in  watts.  Only  available  if  power
                      management is supported.

       <b>Average</b> <b>Power</b> <b>Draw</b>
                      The  average power draw for the entire board for the last second, in watts. Only supported
                      on Ampere (except  GA100)  or  newer  devices.  Only  available  if  power  management  is
                      supported.

       <b>Power</b> <b>Limit</b>    The  software power limit, in watts. Set by software such as nvidia-smi. Only available if
                      power management is supported. Requires Inforom PWR object version 3.0 or higher or Kepler
                      device. On Kepler devices Power Limit can be adjusted using -pl,--power-limit= switches.

       <b>Enforced</b> <b>Power</b> <b>Limit</b>
                      The power management algorithm's power ceiling,  in  watts.  Total  board  power  draw  is
                      manipulated  by  the  power management algorithm such that it stays under this value. This
                      limit is the minimum of various limits such as  the  software  limit  listed  above.  Only
                      available if power management is supported. Requires a Kepler device. Please note that for
                      boards without INA sensors, it is the GPU power draw that is being manipulated.

       <b>Default</b> <b>Power</b> <b>Limit</b>
                      The  default power management algorithm's power ceiling, in watts. Power Limit will be set
                      back to Default Power Limit after driver unload. Only on  supported  devices  from  Kepler
                      family.

       <b>Min</b> <b>Power</b> <b>Limit</b>
                      The  minimum value in watts that power limit can be set to. Only on supported devices from
                      Kepler family.

       <b>Max</b> <b>Power</b> <b>Limit</b>
                      The maximum value in watts that power limit can be set to. Only on supported devices  from
                      Kepler family.

   <b>Power</b> <b>Smoothing</b>
       Power  Smoothing  related  definitions  and currently set values. This feature allows users to tune power
       parameters to minimize power fluctuations in large datacenter environments.

       <b>Enabled</b>        Value is "Yes" if the feature is enabled and "No" if the feature is not enabled.

       <b>Privilege</b> <b>Level</b>
                      The current privilege for the user. Value is 0, 1 or 2. Note that the higher the privilege
                      level, the more information the user will have access to.

       <b>Immediate</b> <b>Ramp</b> <b>Down</b>
                      Values are "Enabled" or "Disabled". Indicates  if  ramp  down  hysteresis  value  will  be
                      honored (when enabled) or ignored (when disabled).

       <b>Current</b> <b>TMP</b>    The last read value of the Total Module Power, in watts.

       <b>Current</b> <b>TMP</b> <b>FLoor</b>
                      The last read value of the Total Module Power floor, in watts. This value is calculated by
                      doing TMP Ceiling * (% TMP FLoor value)

       <b>Max</b> <b>%</b> <b>TMP</b> <b>Floor</b>
                      The highest percentage value for which the Percent TMP Floor can be set.

       <b>Min</b> <b>%</b> <b>TMP</b> <b>Floor</b>
                      The lowest percentage value for which the Percent TMP Floor can be set.

       <b>HW</b> <b>Lifetime</b> <b>%</b> <b>Remaining</b>
                      As  this  feature  is  used, the circuitry which drives the feature wears down. This value
                      gives the percentage of the remaining lifetime of this hardware.

       <b>Number</b> <b>of</b> <b>Preset</b> <b>Profiles</b>
                      This value is the total number of Preset Profiles supported.

   <b>Current</b> <b>Profile</b>
       Values for the currently acvive power smoothing preset profile.

       **% TMP Floor**
                      The percentage of the TMP Ceiling, which is used to set the TMP floor, for  the  currently
                      active preset profile. For example, if max TMP is 1000 W, and the % TMP floor is 50%, then
                      the  min  TMP  value will be 500 W. This value is in the range [Min % TMP Floor, Max % TMP
                      Floor].

       <b>Ramp</b> <b>Up</b> <b>Rate</b>   The ramp up rate, measured in mW/s, for the currently active preset profile.

       <b>Ramp</b> <b>Down</b> <b>Rate</b> The ramp down rate, measured in mW/s, for the currently active preset profile.

       <b>Ramp</b> <b>Down</b> <b>Hysteresis</b>
                      The ramp down hysteresis value, in ms, for the currently active preset profile.

       <b>Active</b> <b>Preset</b> <b>Profile</b> <b>Number</b>
                      The number of the active preset profile.

   <b>Admin</b> <b>Overrides</b>
       Admin overrides allow users with sufficient permissions to preempt the values  of  the  currently  active
       preset  profile.  If an admin override is set for one of the fields, then this value will be used instead
       of any other configured value.

       **% TMP Floor**
                      The admin override value for % TMP Floor. This value is in the range [Min % TMP Floor, Max
                      % TMP Floor].

       <b>Ramp</b> <b>Up</b> <b>Rate</b>   The admin override value for ramp up rate, measured in mW/s.

       <b>Ramp</b> <b>Down</b> <b>Rate</b> The admin override value for ramp down rate, measured in mW/s.

       <b>Ramp</b> <b>Down</b> <b>Hysteresis</b>
                      The admin override value for ramp down hysteresis value, in ms.

   <b>Workload</b> <b>Power</b> <b>Profiles</b>
       Pre-tuned GPU profiles help to provide immediate, optimized configurations for Datacenter use cases. This
       sections includes information about the currently requested on enfornced power profiles.

       <b>Requested</b> <b>Profiles</b>
                      The list of user requested profiles.

       <b>Enforced</b> <b>Profiles</b>
                      Since many of the profiles  have  conflicting  goals,  some  configurations  of  requested
                      profiles  are incompatible. This is the list of the requested profiles which are currently
                      enforced.

   <b>Clocks</b>
       Current frequency at which parts of the GPU are running. All  readings  are  in  MHz.  Note  that  it  is
       possible  for clocks to report a lower freqency than the lowest frequency that can be set by SW due to HW
       optimizations in certain scenarios.

       <b>Graphics</b>       Current frequency of graphics (shader) clock.

       <b>SM</b>             Current frequency of SM (Streaming Multiprocessor) clock.

       <b>Memory</b>         Current frequency of memory clock.

       <b>Video</b>          Current frequency of video (encoder + decoder) clocks.

   <b>Applications</b> <b>Clocks</b>
       User specified frequency at which  applications  will  be  running  at.  Can  be  changed  with  [-ac  \|
       --applications-clocks] switches.

       <b>Graphics</b>       User specified frequency of graphics (shader) clock.

       <b>Memory</b>         User specified frequency of memory clock.

   <b>Default</b> <b>Applications</b> <b>Clocks</b>
       Default  frequency  at which applications will be running at. Application clocks can be changed with [-ac
       \| --applications-clocks] switches. Application clocks can be set to  default  using  [-rac  \|  --reset-
       applications-clocks] switches.

       <b>Graphics</b>       Default frequency of applications graphics (shader) clock.

       <b>Memory</b>         Default frequency of applications memory clock.

   <b>Max</b> <b>Clocks</b>
       Maximum frequency at which parts of the GPU are design to run. All readings are in MHz.

       On  GPUs  from  Fermi family current P0 clocks (reported in Clocks section) can differ from max clocks by
       few MHz.

       <b>Graphics</b>       Maximum frequency of graphics (shader) clock.

       <b>SM</b>             Maximum frequency of SM (Streaming Multiprocessor) clock.

       <b>Memory</b>         Maximum frequency of memory clock.

       <b>Video</b>          Maximum frequency of video (encoder + decoder) clock.

   <b>Clock</b> <b>Policy</b>
       User-specified settings for automated clocking changes such as auto boost.

       <b>Auto</b> <b>Boost</b>     Indicates whether auto boost mode is currently enabled for this GPU (On) or  disabled  for
                      this  GPU  (Off).  Shows  (N/A)  if  boost is not supported. Auto boost allows dynamic GPU
                      clocking based on power, thermal and utilization. When auto boost is disabled the GPU will
                      attempt to maintain clocks at precisely the Current Application Clocks settings  (whenever
                      a  CUDA context is active). With auto boost enabled the GPU will still attempt to maintain
                      this floor, but will opportunistically boost to higher  clocks  when  power,  thermal  and
                      utilization  headroom  allow.  This  setting persists for the life of the CUDA context for
                      which it was requested. Apps can request a particular mode either via an  NVML  call  (see
                      NVML SDK) or by setting the CUDA environment variable CUDA_AUTO_BOOST.

       <b>Auto</b> <b>Boost</b> <b>Default</b>
                      Indicates  the default setting for auto boost mode, either enabled (On) or disabled (Off).
                      Shows (N/A) if boost is not supported. Apps will run in the default mode if they have  not
                      explicitly  requested a particular mode. Note: Auto Boost settings can only be modified if
                      "Persistence Mode" is enabled, which is NOT by default.

   <b>Supported</b> <b>clocks</b>
       List of possible memory and graphics clocks combinations that the GPU can operate  on  (not  taking  into
       account  HW  brake  reduced  clocks).  These  are  the  only  clock  combinations  that  can be passed to
       --applications-clocks flag. Supported Clocks are listed only when -q  -d  SUPPORTED_CLOCKS  switches  are
       provided or in XML format.

   <b>Voltage</b>
       Current voltage reported by the GPU. All units are in mV.

       <b>Graphics</b>       Current  voltage of the graphics unit. This field is deprecated and always displays "N/A".
                      Voltage will be removed in a future release.

   <b>Fabric</b>
       GPU Fabric information

       <b>State</b>

       Indicates the state of the GPU's handshake with the nvidia-fabricmanager (a.k.a. GPU fabric probe)
       Possible values: Completed, In Progress, Not Started, Not supported

       <b>Status</b>

       Status of the GPU fabric probe response from the nvidia-fabricmanager.
       Possible values: NVML_SUCCESS or one of the failure codes.

       <b>Clique</b> <b>ID</b>

       A clique is a set of GPUs that can communicate to each other over NVLink.
       The GPUs belonging to the same clique share the same clique ID.
       Clique ID will only be valid for NVLink multi-node systems.

       <b>Cluster</b> <b>UUID</b>

       UUID of an NVLink multi-node cluster to which this GPU belongs.
       Cluster UUID will be zero for NVLink single-node systems.

       <b>Health</b>

       Bandwidth - is the GPU NVLink bandwidth degraded or not &lt;True/False&gt;
       Route Recovery in progress - is NVLink route recovery in progress &lt;True/False&gt;
       Route Unhealthy - is NVLink route recovery failed or aborted &lt;True/False&gt;
       Access Timeout Recovery - is NVLink access timeout recovery in progress &lt;True/False&gt;

   <b>Processes</b>
       List of processes having Compute or Graphics Context on the device. Compute processes are reported on all
       the fully supported products. Reporting for Graphics processes  is  limited  to  the  supported  products
       starting with Kepler architecture.

       Each Entry is of format "&lt;GPU Index&gt; &lt;PID&gt; &lt;Type&gt; &lt;Process Name&gt; &lt;GPU Memory Usage&gt;"

       <b>GPU</b> <b>Index</b>      Represents NVML Index of the device.

       <b>GPU</b> <b>Instance</b> <b>Index</b>
                      Represents GPU Instance Index of the MIG device (if enabled).

       <b>Compute</b> <b>Instance</b> <b>Index</b>
                      Represents Compute Instance Index of the MIG device (if enabled).

       <b>PID</b>            Represents Process ID corresponding to the active Compute or Graphics context.

       <b>Type</b>           Displayed  as  "C"  for  Compute  Process,  "G" for Graphics Process, "M" for MPS ("Multi-
                      Process Service") Compute Process, and "C+G" or "M+C" for the process having both  Compute
                      and Graphics or MPS Compute and Compute contexts.

       <b>Process</b> <b>Name</b>   Represents process name for the Compute or Graphics process.

       <b>GPU</b> <b>Memory</b> <b>Usage</b>
                      Amount  of memory used on the device by the context. Not available on Windows when running
                      in WDDM mode because Windows KMD manages all the memory not NVIDIA driver.

   <b>Device</b> <b>Monitoring</b>
       The "nvidia-smi dmon" command-line is used to monitor one or more GPUs (up to 16  devices)  plugged  into
       the system. This tool allows the user to see one line of monitoring data per monitoring cycle. The output
       is  in  concise  format and easy to interpret in interactive mode. The output data per line is limited by
       the terminal size. It is supported on Tesla, GRID, Quadro and limited  GeForce  products  for  Kepler  or
       newer  GPUs  under  bare  metal  64  bits  Linux.  By  default, the monitoring data includes Power Usage,
       Temperature, SM clocks, Memory clocks and Utilization values for SM, Memory, Encoder, Decoder,  JPEG  and
       OFA.  It  can  also  be configured to report other metrics such as frame buffer memory usage, bar1 memory
       usage, power/thermal violations and aggregate single/double bit ecc errors. If any of the metric  is  not
       supported  on the device or any other error in fetching the metric is reported as "-" in the output data.
       The user can also configure monitoring frequency and the number of monitoring iterations  for  each  run.
       There  is  also  an option to include date and time at each line. All the supported options are exclusive
       and can be used together in any order. Note: On MIG-enabled GPUs, querying the  utilization  of  encoder,
       decoder, jpeg, ofa, gpu, and memory is not currently supported.

       <b>Usage:</b>

       <b>1)</b> <b>Default</b> <b>with</b> <b>no</b> <b>arguments</b>

       <u>nvidia-smi</u> <u>dmon</u>

       Monitors default metrics for up to 16 supported devices under natural enumeration (starting with GPU
       index 0) at a frequency of 1 sec. Runs until terminated with ^C.

       <b>2)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>devices</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-i</u> <u>&lt;device1,device2,</u> <u>..</u> <u>,</u> <u>deviceN&gt;</u>

       Reports default metrics for the devices selected by comma separated device list. The tool picks up to 16
       supported devices from the list under natural enumeration (starting with GPU index 0).

       <b>3)</b> <b>Select</b> <b>metrics</b> <b>to</b> <b>be</b> <b>displayed</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-s</u> <u>&lt;metric_group&gt;</u>

       &lt;metric_group&gt; can be one or more from the following:

       p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if supported

       u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA Utilization in %)

       c - Proc and Mem Clocks (in MHz)

       v - Power Violations (in %) and Thermal Violations (as a boolean flag)

       m - Frame Buffer, Bar1 and Confidential Compute protected memory usage (in MB)

       e - ECC (Number of aggregated single bit, double bit ecc errors) and PCIe Replay errors

       t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)

       <b>4)</b> <b>Configure</b> <b>monitoring</b> <b>iterations</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-c</u> <u>&lt;number</u> <u>of</u> <u>samples&gt;</u>

       Displays data for specified number of samples and exit.

       <b>5)</b> <b>Configure</b> <b>monitoring</b> <b>frequency</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-d</u> <u>&lt;time</u> <u>in</u> <u>secs&gt;</u>

       Collects and displays data at every specified monitoring interval until terminated with ^C.

       <b>6)</b> <b>Display</b> <b>date</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-o</u> <u>D</u>

       Prepends monitoring data with date in YYYYMMDD format.

       <b>7)</b> <b>Display</b> <b>time</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-o</u> <u>T</u>

       Prepends monitoring data with time in HH:MM:SS format.

       <b>8)</b> <b>Select</b> <b>GPM</b> <b>metrics</b> <b>to</b> <b>be</b> <b>displayed</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>--gpm-metrics</u> <u>&lt;gpmMetric1,gpmMetric2,...,gpmMetricN&gt;</u>

       &lt;gpmMetricX&gt; Refer to the documentation for nvmlGpmMetricId_t in the NVML header file

       <b>9)</b> <b>Select</b> <b>which</b> <b>level</b> <b>of</b> <b>GPM</b> <b>metrics</b> <b>to</b> <b>be</b> <b>displayed</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>--gpm-options</u> <u>&lt;gpmMode&gt;</u>

       &lt;gpmMode&gt; can be one of the following:

       d - Display Device Level GPM metrics

       m - Display MIG Level GPM metrics

       dm - Display Device and MIG Level GPM metrics

       md - Display Device and MIG Level GPM metrics, same as 'dm'

       <b>10)</b> <b>Modify</b> <b>output</b> <b>format</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>--format</u> <u>&lt;formatSpecifier&gt;</u>

       &lt;formatSpecifier&gt; can be any comma separated combination of the following:

       csv - Format dmon output as CSV

       nounit - Remove unit line from dmon output

       noheader - Remove header line from dmon output

       <b>11)</b> <b>Help</b> <b>Information</b>

       <u>nvidia-smi</u> <u>dmon</u> <u>-h</u>

       Displays help information for using the command line.

   <b>Daemon</b> <b>(EXPERIMENTAL)</b>
       The "nvidia-smi daemon" starts a background process to monitor one or more GPUs plugged in to the system.
       It  monitors the requested GPUs every monitoring cycle and logs the file in compressed format at the user
       provided path or the default location at /var/log/nvstats/. The log file is created  with  system's  date
       appended  to  it  and  of  the format nvstats-YYYYMMDD. The flush operation to the log file is done every
       alternate monitoring cycle. Daemon also  logs  it's  own  PID  at  /var/run/nvsmi.pid.  By  default,  the
       monitoring  data  to  persist includes Power Usage, Temperature, SM clocks, Memory clocks and Utilization
       values for SM, Memory, Encoder, Decoder, JPEG and OFA. The daemon tools can also be configured to  record
       other  metrics  such  as  frame  buffer  memory  usage,  bar1  memory usage, power/thermal violations and
       aggregate single/double bit ecc errors.The default monitoring  cycle  is  set  to  10  secs  and  can  be
       configured  via  command-line.  It is supported on Tesla, GRID, Quadro and GeForce products for Kepler or
       newer GPUs under bare metal 64 bits Linux. The daemon requires root privileges to run, and only  supports
       running  a  single  instance  on  the  system. All of the supported options are exclusive and can be used
       together in any order. Note: On MIG-enabled GPUs, querying the utilization  of  encoder,  decoder,  jpeg,
       ofa, gpu, and memory is not currently supported. <b>Usage:</b>

       <b>1)</b> <b>Default</b> <b>with</b> <b>no</b> <b>arguments</b>

       <u>nvidia-smi</u> <u>daemon</u>

       Runs in the background to monitor default metrics for up to 16 supported devices under natural
       enumeration (starting with GPU index 0) at a frequency of 10 sec. The date stamped log file is created at
       /var/log/nvstats/.

       <b>2)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>devices</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-i</u> <u>&lt;device1,device2,</u> <u>..</u> <u>,</u> <u>deviceN&gt;</u>

       Runs in the background to monitor default metrics for the devices selected by comma separated device
       list. The tool picks up to 16 supported devices from the list under natural enumeration (starting with
       GPU index 0).

       <b>3)</b> <b>Select</b> <b>metrics</b> <b>to</b> <b>be</b> <b>monitored</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-s</u> <u>&lt;metric_group&gt;</u>

       &lt;metric_group&gt; can be one or more from the following:

       p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if supported

       u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA Utilization in %)

       c - Proc and Mem Clocks (in MHz)

       v - Power Violations (in %) and Thermal Violations (as a boolean flag)

       m - Frame Buffer, Bar1 and Confidential Compute protected memory usage (in MB)

       e - ECC (Number of aggregated single bit, double bit ecc errors) and PCIe Replay errors

       t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)

       <b>4)</b> <b>Configure</b> <b>monitoring</b> <b>frequency</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-d</u> <u>&lt;time</u> <u>in</u> <u>secs&gt;</u>

       Collects data at every specified monitoring interval until terminated.

       <b>5)</b> <b>Configure</b> <b>log</b> <b>directory</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-p</u> <u>&lt;path</u> <u>of</u> <u>directory&gt;</u>

       The log files are created at the specified directory.

       <b>6)</b> <b>Configure</b> <b>log</b> <b>file</b> <b>name</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-j</u> <u>&lt;string</u> <u>to</u> <u>append</u> <u>log</u> <u>file</u> <u>name&gt;</u>

       The command-line is used to append the log file name with the user provided string.

       <b>7)</b> <b>Terminate</b> <b>the</b> <b>daemon</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-t</u>

       This command-line uses the stored PID (at /var/run/nvsmi.pid) to terminate the daemon. It makes the best
       effort to stop the daemon and offers no guarantees for it's termination. In case the daemon is not
       terminated, then the user can manually terminate by sending kill signal to the daemon. Performing a GPU
       reset operation (via nvidia-smi) requires all GPU processes to be exited, including the daemon. Users who
       have the daemon open will see an error to the effect that the GPU is busy.

       <b>8)</b> <b>Help</b> <b>Information</b>

       <u>nvidia-smi</u> <u>daemon</u> <u>-h</u>

       Displays help information for using the command line.

   <b>Replay</b> <b>Mode</b> <b>(EXPERIMENTAL)</b>
       The  "nvidia-smi replay" command-line is used to extract/replay all or parts of log file generated by the
       daemon. By default, the tool tries to pull the metrics such  as  Power  Usage,  Temperature,  SM  clocks,
       Memory  clocks and Utilization values for SM, Memory, Encoder, Decoder, JPEG and OFA. The replay tool can
       also fetch other metrics such as frame buffer memory usage, bar1 memory usage,  power/thermal  violations
       and  aggregate  single/double bit ecc errors. There is an option to select a set of metrics to replay, If
       any of the requested metric is not maintained or logged as not-supported then it's shown as  "-"  in  the
       output.  The  format of data produced by this mode is such that the user is running the device monitoring
       utility interactively. The command line requires mandatory option "-f" to specify complete  path  of  the
       log  filename, all the other supported options are exclusive and can be used together in any order. Note:
       On MIG-enabled GPUs, querying the utilization of encoder, decoder, jpeg, ofa,  gpu,  and  memory  is  not
       currently supported. <b>Usage:</b>

       <b>1)</b> <b>Specify</b> <b>log</b> <b>file</b> <b>to</b> <b>be</b> <b>replayed</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-f</u> <u>&lt;log</u> <u>file</u> <u>name&gt;</u>

       Fetches monitoring data from the compressed log file and allows the user to see one line of monitoring
       data (default metrics with time-stamp) for each monitoring iteration stored in the log file. A new line
       of monitoring data is replayed every other second irrespective of the actual monitoring frequency
       maintained at the time of collection. It is displayed till the end of file or until terminated by ^C.

       <b>2)</b> <b>Filter</b> <b>metrics</b> <b>to</b> <b>be</b> <b>replayed</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-f</u> <u>&lt;path</u> <u>to</u> <u>log</u> <u>file&gt;</u> <u>-s</u> <u>&lt;metric_group&gt;</u>

       &lt;metric_group&gt; can be one or more from the following:

       p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if supported

       u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA Utilization in %)

       c - Proc and Mem Clocks (in MHz)

       v - Power Violations (in %) and Thermal Violations (as a boolean flag)

       m - Frame Buffer, Bar1 and Confidential Compute protected memory usage (in MB)

       e - ECC (Number of aggregated single bit, double bit ecc errors) and PCIe Replay errors

       t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)

       <b>3)</b> <b>Limit</b> <b>replay</b> <b>to</b> <b>one</b> <b>or</b> <b>more</b> <b>devices</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-f</u> <u>&lt;log</u> <u>file&gt;</u> <u>-i</u> <u>&lt;device1,device2,</u> <u>..</u> <u>,</u> <u>deviceN&gt;</u>

       Limits reporting of the metrics to the set of devices selected by comma separated device list. The tool
       skips any of the devices not maintained in the log file.

       <b>4)</b> <b>Restrict</b> <b>the</b> <b>time</b> <b>frame</b> <b>between</b> <b>which</b> <b>data</b> <b>is</b> <b>reported</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-f</u> <u>&lt;log</u> <u>file&gt;</u> <u>-b</u> <u>&lt;start</u> <u>time</u> <u>in</u> <u>HH:MM:SS</u> <u>format&gt;</u> <u>-e</u> <u>&lt;end</u> <u>time</u> <u>in</u> <u>HH:MM:SS</u> <u>format&gt;</u>

       This option allows the data to be limited between the specified time range. Specifying time as 0 with -b
       or -e option implies start or end file respectively.

       <b>5)</b> <b>Redirect</b> <b>replay</b> <b>information</b> <b>to</b> <b>a</b> <b>log</b> <b>file</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-f</u> <u>&lt;log</u> <u>file&gt;</u> <u>-r</u> <u>&lt;output</u> <u>file</u> <u>name&gt;</u>

       This option takes log file as an input and extracts the information related to default metrics in the
       specified output file.

       <b>6)</b> <b>Help</b> <b>Information</b>

       <u>nvidia-smi</u> <u>replay</u> <u>-h</u>

       Displays help information for using the command line.

   <b>Process</b> <b>Monitoring</b>
       The  "nvidia-smi  pmon"  command-line is used to monitor compute and graphics processes running on one or
       more GPUs (up to 16 devices) plugged into the system. This tool allows the user to see the statistics for
       all the running processes on each device at every monitoring cycle. The output is in concise  format  and
       easy  to  interpret  in interactive mode. The output data per line is limited by the terminal size. It is
       supported on Tesla, GRID, Quadro and limited GeForce products for Kepler or newer GPUs under  bare  metal
       64  bits  Linux.  By  default,  the  monitoring  data for each process includes the pid, command name and
       average utilization values for SM, Memory, Encoder and Decoder since the last monitoring  cycle.  It  can
       also  be  configured to report frame buffer memory usage for each process. If there is no process running
       for the device, then all the metrics are reported as "-" for the device. If any  of  the  metric  is  not
       supported  on  the device or any other error in fetching the metric is also reported as "-" in the output
       data. The user can also configure monitoring frequency and the number of monitoring iterations  for  each
       run.  There  is  also  an  option  to  include  date and time at each line. All the supported options are
       exclusive and can be used together in any order. Note: On MIG-enabled GPUs, querying the  utilization  of
       encoder, decoder, jpeg, ofa, gpu, and memory is not currently supported.

       <b>Usage:</b>

       <b>1)</b> <b>Default</b> <b>with</b> <b>no</b> <b>arguments</b>

       <u>nvidia-smi</u> <u>pmon</u>

       Monitors all the processes running on each device for up to 16 supported devices under natural
       enumeration (starting with GPU index 0) at a frequency of 1 sec. Runs until terminated with ^C.

       <b>2)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>devices</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-i</u> <u>&lt;device1,device2,</u> <u>..</u> <u>,</u> <u>deviceN&gt;</u>

       Reports statistics for all the processes running on the devices selected by comma separated device list.
       The tool picks up to 16 supported devices from the list under natural enumeration (starting with GPU
       index 0).

       <b>3)</b> <b>Select</b> <b>metrics</b> <b>to</b> <b>be</b> <b>displayed</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-s</u> <u>&lt;metric_group&gt;</u>

       &lt;metric_group&gt; can be one or more from the following:

       u - Utilization (SM, Memory, Encoder, Decoder, JPEG, and OFA Utilization for the process in %). Reports
       average utilization since last monitoring cycle.

       m - Frame Buffer and Confidential Compute protected memory usage (in MB). Reports instantaneous value for
       memory usage.

       <b>4)</b> <b>Configure</b> <b>monitoring</b> <b>iterations</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-c</u> <u>&lt;number</u> <u>of</u> <u>samples&gt;</u>

       Displays data for specified number of samples and exit.

       <b>5)</b> <b>Configure</b> <b>monitoring</b> <b>frequency</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-d</u> <u>&lt;time</u> <u>in</u> <u>secs&gt;</u>

       Collects and displays data at every specified monitoring interval until terminated with ^C. The
       monitoring frequency must be between 1 to 10 secs.

       <b>6)</b> <b>Display</b> <b>date</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-o</u> <u>D</u>

       Prepends monitoring data with date in YYYYMMDD format.

       <b>7)</b> <b>Display</b> <b>time</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-o</u> <u>T</u>

       Prepends monitoring data with time in HH:MM:SS format.

       <b>8)</b> <b>Help</b> <b>Information</b>

       <u>nvidia-smi</u> <u>pmon</u> <u>-h</u>

       Displays help information for using the command line.

   <b>Topology</b>
       List  topology  information about the system's GPUs, how they connect to each other, their CPU and memory
       affinities as well as qualified NICs capable of RDMA.

       Note: On some systems, a NIC is used as a PCI bridge for the NVLINK switches and is  not  useful  from  a
       networking  or  RDMA  point  of  view.  The nvidia-smi topo command will filter the NIC's ports/PCIe sub-
       functions out of the topology matrix by examining the NIC's  sysfs  entries.  On  some  kernel  versions,
       nvidia-smi requires root privileges to read these sysfs entries.

       <b>Usage:</b>

       <b>Topology</b> <b>connections</b> <b>and</b> <b>affinities</b> <b>matrix</b> <b>between</b> <b>the</b> <b>GPUs</b> <b>and</b> <b>NICs</b> <b>in</b> <b>the</b> <b>system</b>

       <u>nvidia-smi</u> <u>topo</u> <u>-m</u>

       Displays a matrix of connections between all GPUs and NICs in the system along with CPU/memory affinities
       for the GPUs with the following legend:

       Legend:
        X = Self
        SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
        NODE  =  Connection  traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA
       node
        PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
        PXB = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
        PIX = Connection traversing a single PCIe switch NV# = Connection traversing a bonded set of # NVLinks

       Note: This command may also display bonded NICs which may not be RDMA capable.

       <u>nvidia-smi</u> <u>topo</u> <u>-mp</u>

       Displays a matrix of PCI-only connections between all GPUs and NICs in the system along with CPU/memory
       affinities for the GPUs with the same legend as the 'nvidia-smi topo -m' command. This command excludes
       NVLINK connections and shows PCI connections between GPUs.

       <u>nvidia-smi</u> <u>topo</u> <u>-c</u> <u>&lt;CPU</u> <u>number&gt;</u>

       Shows all the GPUs with an affinity to the specified CPU number.

       <u>nvidia-smi</u> <u>topo</u> <u>-n</u> <u>&lt;traversal_path&gt;</u> <u>-i</u> <u>&lt;deviceID&gt;</u>

       Shows all the GPUs connected with the given GPU using the specified traversal path.  The  traversal  path
       values are:
        0 = A single PCIe switch on a dual GPU board
        1 = A single PCIe switch
        2 = Multiple PCIe switches
        3 = A PCIe host bridge
        4 = An on-CPU interconnect link between PCIe host bridges
        5 = An SMP interconnect link between NUMA nodes

       <u>nvidia-smi</u> <u>topo</u> <u>-p</u> <u>-i</u> <u>&lt;deviceID1&gt;,&lt;deviceID2&gt;</u>

       Shows the most direct PCIe path traversal for a given pair of GPUs.

       <u>nvidia-smi</u> <u>topo</u> <u>-p2p</u> <u>&lt;capability&gt;</u>

       Shows the P2P status between all GPUs, given a capability. Capability values are:
        r - p2p read capability
        w - p2p write capability
        n - p2p nvlink capability
        a - p2p atomics capability
        p - p2p pcie capability

       <u>nvidia-smi</u> <u>topo</u> <u>-C</u> <u>-i</u> <u>&lt;deviceID&gt;</u>

       Shows the NUMA ID of the nearest CPU for a GPU represented by the device ID.

       <u>nvidia-smi</u> <u>topo</u> <u>-M</u> <u>-i</u> <u>&lt;deviceID&gt;</u>

       Shows the NUMA ID of the nearest memory for a GPU represented by the device ID.

       <u>nvidia-smi</u> <u>topo</u> <u>-gnid</u> <u>-i</u> <u>&lt;deviceID&gt;</u>

       Shows the NUMA ID of the GPU represented by the device ID, if applicable. Displays N/A otherwise.

       <u>nvidia-smi</u> <u>topo</u> <u>-nvme</u>

       Displays  a  matrix of PCI connections between all GPUs and NVME devices in the system with the following
       legend:

       Legend:
        X = Self
        SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
        NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges  within  a  NUMA
       node
        PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
        PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
        PIX = Connection traversing at most a single PCIe bridge

   <b>Nvlink</b>
       The  "nvidia-smi nvlink" command-line is used to manage the GPU's Nvlinks. It provides options to set and
       query Nvlink information.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>Select</b> <b>a</b> <b>specific</b> <b>NvLink</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-l</u> <u>&lt;GPU</u> <u>Nvlink</u> <u>Id&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--list</u> <u>&lt;GPU</u> <u>Nvlink</u> <u>Id&gt;</u>

       Selects a specific Nvlink of the GPU for the given command, if valid. If not used, the given command-line
       option allies to all of the GPU's Nvlinks.

       <b>4)</b> <b>Query</b> <b>Nvlink</b> <b>Status</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-s</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--status</u>

       Get the status of the GPU's Nvlinks.

       If Active, the Bandwidth of the links will be displayed.

       If the link is present but Not Active, it will show the link as Inactive.

       If the link is in Sleep state, it will show as Sleep.

       <b>5)</b> <b>Query</b> <b>Nvlink</b> <b>capabilities</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-c</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--capabilities</u>

       Get the GPU's Nvlink capabilities.

       <b>6)</b> <b>Query</b> <b>the</b> <b>Nvlink's</b> <b>remote</b> <b>node</b> <b>PCI</b> <b>bus</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-p</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-pcibusid</u>

       Get the Nvlink's remote node PCI bus ID.

       <b>7)</b> <b>Query</b> <b>the</b> <b>Nvlink's</b> <b>remote</b> <b>link</b> <b>info</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-R</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-remotelinkinfo</u>

       Get the remote device PCI bus ID and NvLink ID for a link.

       <b>8)</b> <b>Set</b> <b>Nvlink</b> <b>Counter</b> <b>Control</b> <b>is</b> <b>DEPRECATED</b>

       <b>9)</b> <b>Get</b> <b>Nvlink</b> <b>Counter</b> <b>Control</b> <b>is</b> <b>DEPRECATED</b>

       <b>10)</b> <b>Get</b> <b>Nvlink</b> <b>Counters</b> <b>is</b> <b>DEPRECATED,</b> <b>-gt/--getthroughput</b> <b>should</b> <b>be</b> <b>used</b> <b>instead</b>

       <b>11)</b> <b>Reset</b> <b>Nvlink</b> <b>counters</b> <b>is</b> <b>DEPRECATED</b>

       <b>12)</b> <b>Query</b> <b>Nvlink</b> <b>Error</b> <b>Counters</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-e</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--errorcounters</u>

       Get the Nvlink error counters.

       For NVLink 4

       Replay Errors - count the number of replay 'events' that occurred

       Recovery Errors - count the number of link recovery events

       CRC Errors - count the number of CRC errors in received packets

       For NVLink 5

       Tx packets - Total Tx packets on the link

       Tx bytes - Total Tx bytes on the link

       Rx packets - Total Rx packets on the link

       Rx bytes - Total Rx bytes on the link

       Malformed packet Errors - Number of packets Rx on a link where packets are malformed

       Buffer overrun Errors - Number of packets that were discarded on Rx due to buffer overrun

       Rx Errors - Total number of packets with errors Rx on a link

       Rx remote Errors - Total number of packets Rx - stomp/EBP marker

       Rx General Errors - Total number of packets Rx with header mismatch

       Local link integrity Errors - Total number of times that the count of local errors exceeded a threshold

       Tx discards - Total number of tx error packets that were discarded

       Link recovery successful events - Number of times link went from Up to recovery, succeeded and link came
       back up

       Link recovery failed events - Number of times link went from Up to recovery, failed and link was declared
       down

       Total link recovery events - Number of times link went from Up to recovery, irrespective of the result

       Effective Errors - Sum of the number of errors in each Nvlink packet

       Effective BER - BER for symbol errors

       Symbol Errors - Number of errors in rx symbols

       Symbol BER - BER for symbol errors

       FEC Errors - [0-15] - count of symbol errors that are corrected

       <b>13)</b> <b>Query</b> <b>Nvlink</b> <b>CRC</b> <b>error</b> <b>counters</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-ec</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--crcerrorcounters</u>

       Get the Nvlink per-lane CRC/ECC error counters.

       CRC - NVLink 4 and before - Total Rx CRC errors on an NVLink Lane

       ECC - NVLink 4 - Total Rx ECC errors on an NVLink Lane

       Deprecated NVLink 5 onwards

       <b>14)</b> <b>Reset</b> <b>Nvlink</b> <b>Error</b> <b>Counters</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-re</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--reseterrorcounters</u>

       Reset all Nvlink error counters to zero.

       NvLink 5 NOT SUPPORTED

       <b>15)</b> <b>Query</b> <b>Nvlink</b> <b>throughput</b> <b>counters</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-gt</u> <u>&lt;Data</u> <u>Type&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--getthroughput</u> <u>&lt;Data</u> <u>Type&gt;</u>

       &lt;Data Type&gt; can be one of the following:

       d - Tx and Rx data payload in KiB.

       r - Tx and Rx raw payload and protocol overhead in KiB.

       <b>16)</b> <b>Set</b> <b>Nvlink</b> <b>Low</b> <b>Power</b> <b>thresholds</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-sLowPwrThres</u> <u>&lt;Threshold&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--setLowPowerThreshold</u> <u>&lt;Threshold&gt;</u>

       Set the Nvlink Low Power Threshold, before the links go into Low Power Mode.

       Threshold ranges and units can be found using -gLowPwrInfo.

       <b>17)</b> <b>Get</b> <b>Nvlink</b> <b>Low</b> <b>Power</b> <b>Info</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-gLowPwrInfo</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--getLowPowerInfo</u>

       Query the Nvlink's Low Power Info.

       <b>18)</b> <b>Set</b> <b>Nvlink</b> <b>Bandwidth</b> <b>mode</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-sBwMode</u> <u>&lt;Bandwidth</u> <u>Mode&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--setBandwidthMode</u> <u>&lt;Bandwidth</u> <u>Mode&gt;</u>

       Set the Nvlink Bandwidth mode for all GPUs. This is DEPRECATED for Blackwell+.

       The options are:

       FULL - All links are at max Bandwidth.

       OFF - Bandwidth is not used. P2P is via PCIe bus.

       MIN - Bandwidth is at minimum speed.

       HALF - Bandwidth is at around half of FULL speed.

       3QUARTER - Bandwidth is at around 75% of FULL speed.

       <b>19)</b> <b>Get</b> <b>Nvlink</b> <b>Bandwidth</b> <b>mode</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-gBwMode</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--getBandwidthMode</u>

       Get the Nvlink Bandwidth mode for all GPUs. THis is DEPRECATED for Blackwell+.

       <b>20)</b> <b>Query</b> <b>for</b> <b>Nvlink</b> <b>Bridge</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-cBridge</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--checkBridge</u>

       Query for Nvlink Bridge presence.

       <b>21)</b> <b>Set</b> <b>the</b> <b>GPU's</b> <b>Nvlink</b> <b>Width</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-sLWidth</u> <u>&lt;Link</u> <u>Width&gt;</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--setLinkWidth</u> <u>&lt;Link</u> <u>Width&gt;</u>

       Set the GPU's Nvlink width, which will be keep those number of links Active, and the rest to sleep.

       &lt;Link Width&gt; can be one of the following:

       values - List possible Link Widths to be set.

       The numerical value from the above option.

       <b>22)</b> <b>Get</b> <b>the</b> <b>GPU's</b> <b>Nvlink</b> <b>Width</b>

       <u>nvidia-smi</u> <u>nvlink</u> <u>-gLWidth</u>

       <u>nvidia-smi</u> <u>nvlink</u> <u>--getLinkWidth</u>

       Query the GPU's Nvlink Width.

   <b>C2C</b>
       The "nvidia-smi c2c" command-line is used to manage the GPU's C2C Links. It provides options to query C2C
       Link information.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>c2c</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>Select</b> <b>a</b> <b>specific</b> <b>C2C</b> <b>Link</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-l</u> <u>&lt;GPU</u> <u>C2C</u> <u>Id&gt;</u>

       <u>nvidia-smi</u> <u>c2c</u> <u>--list</u> <u>&lt;GPU</u> <u>C2C</u> <u>Id&gt;</u>

       Selects a specific C2C Link of the GPU for the given command, if valid. If not used, the given command-
       line option allies to all of the GPU's C2C Links.

       <b>4)</b> <b>Query</b> <b>C2C</b> <b>Link</b> <b>Status</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-s</u>

       <u>nvidia-smi</u> <u>c2c</u> <u>--status</u>

       Get the status of the GPU's C2C Links. If active, the Bandwidth of the links will be displayed.

       <b>5)</b> <b>Query</b> <b>C2C</b> <b>Link</b> <b>Error</b> <b>Counters</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-e</u>

       <u>nvidia-smi</u> <u>c2c</u> <u>-errorCounters</u>

       Display the C2C Link error counters.

       <b>6)</b> <b>Query</b> <b>C2C</b> <b>Link</b> <b>Power</b> <b>Info</b>

       <u>nvidia-smi</u> <u>c2c</u> <u>-gLowPwrInfo</u>

       <u>nvidia-smi</u> <u>c2c</u> <u>-getLowPowerInfo</u>

       Display the C2C Link Power state.
                      &gt;

   <b>vGPU</b> <b>Management</b>
       The "nvidia-smi vgpu" command reports on GRID vGPUs executing on supported GPUs and hypervisors (refer to
       driver release notes for supported platforms). Summary reporting provides basic information  about  vGPUs
       currently executing on the system. Additional options provide detailed reporting of vGPU properties, per-
       vGPU  reporting  of  SM,  Memory,  Encoder,  Decoder, Jpeg, and OFA utilization, and per-GPU reporting of
       supported and  creatable  vGPUs.  Periodic  reports  can  be  automatically  generated  by  specifying  a
       configurable  loop  frequency  to  any  command.  Note:  On MIG-enabled GPUs, querying the utilization of
       encoder, decoder, jpeg, ofa, gpu, and memory is not currently supported.

       <b>Usage:</b>

       <b>1)</b> <b>Help</b> <b>Information</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-h</u>

       Displays help information for using the command line.

       <b>2)</b> <b>Default</b> <b>with</b> <b>no</b> <b>arguments</b>

       <u>nvidia-smi</u> <u>vgpu</u>

       Reports summary of all the vGPUs currently active on each device.

       <b>3)</b> <b>Display</b> <b>detailed</b> <b>info</b> <b>on</b> <b>currently</b> <b>active</b> <b>vGPUs</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-q</u>

       Collects and displays information on currently active vGPUs on each device, including driver version,
       utilization, and other information.

       <b>4)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>devices</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-i</u> <u>&lt;device1,device2,</u> <u>..</u> <u>,</u> <u>deviceN&gt;</u>

       Reports summary for all the vGPUs currently active on the devices selected by comma-separated device
       list.

       <b>5)</b> <b>Display</b> <b>supported</b> <b>vGPUs</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-s</u>

       Displays vGPU types supported on each device. Use the -v / --verbose option to show detailed info on each
       vGPU type.

       <b>6)</b> <b>Display</b> <b>creatable</b> <b>vGPUs</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-c</u>

       Displays vGPU types creatable on each device. This varies dynamically, depending on the vGPUs already
       active on the device. Use the -v / --verbose option to show detailed info on each vGPU type.

       <b>7)</b> <b>Report</b> <b>utilization</b> <b>for</b> <b>currently</b> <b>active</b> <b>vGPUs.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-u</u>

       Reports average utilization (SM, Memory, Encoder, Decoder, Jpeg, and OFA) for each active vGPU since last
       monitoring cycle. The default cycle time is 1 second, and the command runs until terminated with ^C. If a
       device has no active vGPUs, its metrics are reported as "-".

       <b>8)</b> <b>Configure</b> <b>loop</b> <b>frequency</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>[-s</u> <u>-c</u> <u>-q</u> <u>-u]</u> <u>-l</u> <u>&lt;time</u> <u>in</u> <u>secs&gt;</u>

       Collects and displays data at a specified loop interval until terminated with ^C. The loop frequency must
       be between 1 and 10 secs. When no time is specified, the loop frequency defaults to 5 secs.

       <b>9)</b> <b>Display</b> <b>GPU</b> <b>engine</b> <b>usage</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-p</u>

       Display GPU engine usage of currently active processes running in the vGPU VMs.

       <b>10)</b> <b>Display</b> <b>migration</b> <b>capabitlities.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-m</u>

       Display pGPU's migration/suspend/resume capability.

       <b>11)</b> <b>Display</b> <b>the</b> <b>vGPU</b> <b>Software</b> <b>scheduler</b> <b>state.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-ss</u>

       Display the information about vGPU Software scheduler state.

       <b>12)</b> <b>Display</b> <b>the</b> <b>vGPU</b> <b>Software</b> <b>scheduler</b> <b>capabilities.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-sc</u>

       Display the list of supported vGPU scheduler policies returned along with the other capabilities values,
       if the engine is Graphics type. For other engine types, it is BEST EFFORT policy and other capabilities
       will be zero. If ARR is supported and enabled, scheduling frequency and averaging factor are applicable
       else timeSlice is applicable.

       <b>13)</b> <b>Display</b> <b>the</b> <b>vGPU</b> <b>Software</b> <b>scheduler</b> <b>logs.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-sl</u>

       Display the vGPU Software scheduler runlist logs.

       <u>nvidia-smi</u> <u>--query-vgpu-scheduler-logs=[input</u> <u>parameters]</u>

       Display the vGPU Software scheduler runlist logs in CSV format.

       <b>14)</b> <b>Set</b> <b>the</b> <b>vGPU</b> <b>Software</b> <b>scheduler</b> <b>state.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>--set-vgpu-scheduler-state</u> <u>[options]</u>

       Set the vGPU Software scheduler policy and states.

       <b>15)</b> <b>Display</b> <b>Nvidia</b> <b>Encoder</b> <b>session</b> <b>info.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-es</u>

       Display the information about encoder sessions for currently running vGPUs.

       <b>16)</b> <b>Display</b> <b>accounting</b> <b>statistics.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>--query-accounted-apps=[input</u> <u>parameters]</u>

       Display accounting stats for compute/graphics processes.

       To find the list of properties which can be queried, run - 'nvidia-smi --help-query-accounted-apps'.

       <b>17)</b> <b>Display</b> <b>Nvidia</b> <b>Frame</b> <b>Buffer</b> <b>Capture</b> <b>session</b> <b>info.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-fs</u>

       Display the information about FBC sessions for currently running vGPUs.

       <u>Note</u> <u>:</u> <u>Horizontal</u> <u>resolution,</u> <u>vertical</u> <u>resolution,</u> <u>average</u> <u>FPS</u> <u>and</u> <u>average</u> <u>latency</u> <u>data</u> <u>for</u> <u>a</u> <u>FBC</u> <u>session</u>
       <u>may</u> <u>be</u> <u>zero</u> <u>if</u> <u>there</u> <u>are</u> <u>no</u> <u>new</u> <u>frames</u> <u>captured</u> <u>since</u> <u>the</u> <u>session</u> <u>started.</u>

       <b>18)</b> <b>Set</b> <b>vGPU</b> <b>heterogeneous</b> <b>mode.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-shm</u>

       Set vGPU heterogeneous mode of the device for timesliced vGPUs with different framebuffer sizes.
                      &gt;

       <b>19)</b> <b>Set</b> <b>vGPU</b> <b>MIG</b> <b>timeslice</b> <b>mode.</b>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-smts</u>

       Set vGPU MIG timeslice mode of the device.
                      &gt;

       <b>20)</b> <b>Display</b> <b>the</b> <b>currently</b> <b>creatable</b> <b>vGPU</b> <b>types</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-c</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-c</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

       <b>21)</b> <b>Display</b> <b>detailed</b> <b>information</b> <b>of</b> <b>the</b> <b>currently</b> <b>active</b> <b>vGPU</b> <b>instances</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-q</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-q</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

       <b>22)</b> <b>Display</b> <b>the</b> <b>vGPU</b> <b>scheduler</b> <b>state</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-ss</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-ss</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

       <b>23)</b> <b>Get</b> <b>the</b> <b>vGPU</b> <b>heterogeneous</b> <b>mode</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-ghm</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-ghm</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance. If not used, the given command-line option applies to all of the GPU instances.

       <b>24)</b> <b>Set</b> <b>the</b> <b>vGPU</b> <b>heterogeneous</b> <b>mode</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-shm</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-shm</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

       <b>25)</b> <b>Set</b> <b>the</b> <b>vGPU</b> <b>Software</b> <b>scheduler</b> <b>state</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance.</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>set-vgpu-scheduler-state</u> <u>[options]</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>vgpu</u> <u>set-vgpu-scheduler-state</u> <u>[options]</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

       <b>26)</b> <b>Display</b> <b>the</b> <b>vGPU</b> <b>scheduler</b> <b>logs</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-sl</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-sl</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>--query-gpu-instance-vgpu-scheduler-logs=[input</u> <u>parameters]</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u>
       <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Display the vGPU Software scheduler logs in CSV format on the user provided GPU Instance.

       <b>27)</b> <b>Display</b> <b>detailed</b> <b>information</b> <b>of</b> <b>the</b> <b>currently</b> <b>creatable</b> <b>vGPU</b> <b>types</b> <b>on</b> <b>the</b> <b>user</b> <b>provided</b> <b>GPU</b> <b>Instance</b>

       <u>nvidia-smi</u> <u>vgpu</u> <u>-c</u> <u>-v</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       <u>nvidia-smi</u> <u>vgpu</u> <u>-c</u> <u>-v</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>
                      &gt;

       Provide comma separated values for more than one GPU instance. The target GPU index (MANDATORY) for the
       given GPU instance.

   <b>MIG</b> <b>Management</b>
       The privileged "nvidia-smi mig" command-line is used to manage MIG-enabled GPUs. It provides  options  to
       create, list and destroy GPU instances and compute instances.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>GPU</b> <b>instances</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u>

       Selects one or more GPU instances using the given comma-separated GPU instance IDs. If not used, the
       given command-line option applies to all of the GPU instances.

       <b>4)</b> <b>Select</b> <b>one</b> <b>or</b> <b>more</b> <b>compute</b> <b>instances</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-ci</u> <u>&lt;compute</u> <u>instance</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--compute-instance-id</u> <u>&lt;compute</u> <u>instance</u> <u>IDs&gt;</u>

       Selects one or more compute instances using the given comma-separated compute instance IDs. If not used,
       the given command-line option applies to all of the compute instances.

       <b>5)</b> <b>List</b> <b>GPU</b> <b>instance</b> <b>profiles</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lgip</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-gpu-instance-profiles</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Lists GPU instance profiles, their availability and IDs. Profiles describe the supported types of GPU
       instances, including all of the GPU resources they exclusively control.

       <b>6)</b> <b>List</b> <b>GPU</b> <b>instance</b> <b>possible</b> <b>placements</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lgipp</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-gpu-instance-possible-placements</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Lists GPU instance possible placements. Possible placements describe the locations of the supported types
       of GPU instances within the GPU.

       <b>7)</b> <b>Create</b> <b>GPU</b> <b>instance</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-cgi</u> <u>&lt;GPU</u> <u>instance</u> <u>specifiers&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--create-gpu-instance</u> <u>&lt;GPU</u> <u>instance</u> <u>specifiers&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Creates GPU instances for the given GPU instance specifiers. A GPU instance specifier comprises a GPU
       instance profile name or ID and an optional placement specifier consisting of a colon and a placement
       start index. The command fails if the GPU resources required to allocate the requested GPU instances are
       not available, or if the placement index is not valid for the given profile.

       <b>8)</b> <b>Create</b> <b>a</b> <b>GPU</b> <b>instance</b> <b>along</b> <b>with</b> <b>the</b> <b>default</b> <b>compute</b> <b>instance</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-cgi</u> <u>&lt;GPU</u> <u>instance</u> <u>profile</u> <u>IDs</u> <u>or</u> <u>names&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u> <u>-C</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--create-gpu-instance</u> <u>&lt;GPU</u> <u>instance</u> <u>profile</u> <u>IDs</u> <u>or</u> <u>names&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u> <u>--default-</u>
       <u>compute-instance</u>

       <b>9)</b> <b>List</b> <b>GPU</b> <b>instances</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lgi</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-gpu-instances</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Lists GPU instances and their IDs.

       <b>10)</b> <b>Destroy</b> <b>GPU</b> <b>instance</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-dgi</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--destroy-gpu-instances</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Destroys GPU instances. The command fails if the requested GPU instance is in use by an application.

       <b>11)</b> <b>List</b> <b>compute</b> <b>instance</b> <b>profiles</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lcip</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-compute-instance-profiles</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Lists compute instance profiles, their availability and IDs. Profiles describe the supported types of
       compute instances, including all of the GPU resources they share or exclusively control.

       <b>12)</b> <b>List</b> <b>compute</b> <b>instance</b> <b>possible</b> <b>placements</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lcipp</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-compute-instance-possible-placements</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u>
       <u>IDs&gt;</u>

       Lists compute instance possible placements. Possible placements describe the locations of the supported
       types of compute instances within the GPU instance.

       <b>13)</b> <b>Create</b> <b>compute</b> <b>instance</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-cci</u> <u>&lt;compute</u> <u>instance</u> <u>profile</u> <u>IDs</u> <u>or</u> <u>names&gt;</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--create-compute-instance</u> <u>&lt;compute</u> <u>instance</u> <u>profile</u> <u>IDs</u> <u>or</u> <u>names&gt;</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u>
       <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Creates compute instances for the given compute instance spcifiers. A compute instance specifier
       comprises a compute instance profile name or ID and an optional placement specifier consisting of a colon
       and a placement start index. The command fails if the GPU resources required to allocate the requested
       compute instances are not available, or if the placement index is not valid for the given profile.

       <b>14)</b> <b>List</b> <b>compute</b> <b>instances</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-lci</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--list-compute-instances</u> <u>--gpu-instance-id</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Lists compute instances and their IDs.

       <b>15)</b> <b>Destroy</b> <b>compute</b> <b>instance</b>

       <u>nvidia-smi</u> <u>mig</u> <u>-dci</u> <u>-ci</u> <u>&lt;compute</u> <u>instance</u> <u>IDs&gt;</u> <u>-gi</u> <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>mig</u> <u>--destroy-compute-instance</u> <u>--compute-instance-id</u> <u>&lt;compute</u> <u>instance</u> <u>IDs&gt;</u> <u>--gpu-instance-id</u>
       <u>&lt;GPU</u> <u>instance</u> <u>IDs&gt;</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Destroys compute instances. The command fails if the requested compute instance is in use by an
       application.

   <b>Boost</b> <b>Slider</b>
       The privileged "nvidia-smi boost-slider" command-line is used to manage boost slider on GPUs. It provides
       options to list and control boost sliders.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>List</b> <b>boost</b> <b>sliders</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-l</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--list</u>

       List all boost sliders for the selected devices.

       <b>4)</b> <b>Set</b> <b>video</b> <b>boost</b> <b>slider</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--vboost</u> <u>&lt;value&gt;</u>

       Set the video boost slider for the selected devices.

   <b>Power</b> <b>Hint</b>
       The privileged "nvidia-smi power-hint" command-line is used to query power hint on GPUs.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>List</b> <b>power</b> <b>hint</b> <b>info</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-l</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--list-info</u>

       List all boost sliders for the selected devices.

       <b>4)</b> <b>Query</b> <b>power</b> <b>hint</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-gc</u> <u>&lt;value&gt;</u> <u>-t</u> <u>&lt;value&gt;</u> <u>-p</u> <u>&lt;profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--graphics-clock</u> <u>&lt;value&gt;</u> <u>--temperature</u> <u>&lt;value&gt;</u> <u>--profile</u> <u>&lt;profile</u> <u>ID&gt;</u>

       Query power hint with graphics clock, temperature and profile id.

       <b>5)</b> <b>Query</b> <b>power</b> <b>hint</b>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>-gc</u> <u>&lt;value&gt;</u> <u>-mc</u> <u>&lt;value&gt;</u> <u>-t</u> <u>&lt;value&gt;</u> <u>-p</u> <u>&lt;profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>boost-slider</u> <u>--graphics-clock</u> <u>&lt;value&gt;</u> <u>--memory-clock</u> <u>&lt;value&gt;</u> <u>--temperature</u> <u>&lt;value&gt;</u> <u>--profile</u>
       <u>&lt;profile</u> <u>ID&gt;</u>

       Query power hint with graphics clock, memory clock, temperature and profile id.

   <b>Confidential</b> <b>Compute</b>
       The "nvidia-smi conf-compute" command-line is used to manage confidential compute. It provides options to
       set and query confidential compute.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>CPU</b> <b>capability</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-gc</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-cpu-caps</u>

       Get confidential compute CPU capability.

       <b>4)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>GPUs</b> <b>capability</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-gg</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-gpus-caps</u>

       Get confidential compute GPUs capability.

       <b>5)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>devtools</b> <b>mode</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-d</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-devtools-mode</u>

       Get confidential compute DevTools mode.

       <b>6)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>environment</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-e</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-environment</u>

       Get confidential compute environment.

       <b>7)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>feature</b> <b>status</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-f</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-cc-feature</u>

       Get confidential compute CC feature status.

       <b>8)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>GPU</b> <b>protected/unprotected</b> <b>memory</b> <b>sizes</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-gm</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-mem-size-info</u>

       Get confidential compute GPU protected/unprotected memory sizes.

       <b>9)</b> <b>Set</b> <b>confidential</b> <b>compute</b> <b>GPU</b> <b>unprotected</b> <b>memory</b> <b>size</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-sm</u> <u>&lt;value&gt;</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--set-unprotected-mem-size</u> <u>&lt;value&gt;</u>

       Set confidential compute GPU unprotected memory size in KiB. Requires root.

       <b>10)</b> <b>Set</b> <b>confidential</b> <b>compute</b> <b>GPUs</b> <b>ready</b> <b>state</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-srs</u> <u>&lt;value&gt;</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--set-gpus-ready-state</u> <u>&lt;value&gt;</u>

       Set confidential compute GPUs ready state. The value must be 1 to set the ready state and 0 to unset it.
       Requires root.

       <b>11)</b> <b>Query</b> <b>confidential</b> <b>compute</b> <b>GPUs</b> <b>ready</b> <b>state</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-grs</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-gpus-ready-state</u>

       Get confidential compute GPUs ready state.

       <b>12)</b> <b>Set</b> <b>Confidential</b> <b>Compute</b> <b>Key</b> <b>Rotation</b> <b>Max</b> <b>Attacker</b> <b>Advantage</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-skr</u> <u>&lt;value&gt;</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--set-key-rotation-max-attacker-advantage</u>

       Set Confidential Compute Key Rotation Max Attacker Advantage.

       <b>13)</b> <b>Display</b> <b>Confidential</b> <b>Compute</b> <b>Key</b> <b>Rotation</b> <b>Threshold</b> <b>Info</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-gkr</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-key-rotation-threshold-info</u>

       Display Confidential Compute Key Rotation Threshold Info.

       <b>14)</b> <b>Display</b> <b>Confidential</b> <b>Compute</b> <b>Multi-GPU</b> <b>Mode</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-mgm</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--get-multigpu-mode</u>

       Display Confidential Compute Multi-GPU Mode.
                      &gt;

       <b>15)</b> <b>Display</b> <b>Confidential</b> <b>Compute</b> <b>Detailed</b> <b>Info</b>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>-q</u>

       <u>nvidia-smi</u> <u>conf-compute</u> <u>--query-conf-compute</u>

       Display Confidential Compute Detailed Info.
                      &gt;

   <b>GPU</b> <b>Performance</b> <b>Monitoring(GPM)</b> <b>Stream</b> <b>State</b>
       The  "nvidia-smi gpm" command-line is used to manage GPU performance monitoring unit. It provides options
       to query and set the stream state.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>gpm</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>gpm</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>gpm</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>Query</b> <b>GPU</b> <b>performance</b> <b>monitoring</b> <b>stream</b> <b>state</b>

       <u>nvidia-smi</u> <u>gpm</u> <u>-g</u>

       <u>nvidia-smi</u> <u>gpm</u> <u>--get-stream-state</u>

       Get gpm stream state for the selected devices.

       <b>4)</b> <b>Set</b> <b>GPU</b> <b>performance</b> <b>monitoring</b> <b>stream</b> <b>state</b>

       <u>nvidia-smi</u> <u>gpm</u> <u>-s</u> <u>&lt;value&gt;</u>

       <u>nvidia-smi</u> <u>gpm</u> <u>--set-stream-state</u> <u>&lt;value&gt;</u>

       Set gpm stream state for the selected devices.

   <b>GPU</b> <b>PCI</b> <b>section</b>
       The "nvidia-smi pci" command-line is used to manage GPU PCI counters. It provides options  to  query  and
       clear PCI counters.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>pci</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>Query</b> <b>PCI</b> <b>error</b> <b>counters</b>

       <u>nvidia-smi</u> <u>pci</u> <u>-i</u> <u>&lt;GPU</u> <u>index&gt;</u> <u>-gErrCnt</u>

       Query PCI error counters of a GPU

       <b>3)</b> <b>Clear</b> <b>PCI</b> <b>error</b> <b>counters</b>

       <u>nvidia-smi</u> <u>pci</u> <u>-i</u> <u>&lt;GPU</u> <u>index&gt;</u> <u>-cErrCnt</u>

       Clear PCI error counters of a GPU

       <b>4)</b> <b>Query</b> <b>PCI</b> <b>counters</b>

       <u>nvidia-smi</u> <u>pci</u> <u>-i</u> <u>&lt;GPU</u> <u>index&gt;</u> <u>-gCnt</u>

       Query PCI RX and TX counters of a GPU

   <b>Power</b> <b>Smoothing</b>
       The  "nvidia-smi power-smoothing" command-line is used to manage Power Smoothing related data on the GPU.
       It provides options to set Power Smoothing related data and query the preset profile definitions.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>2)</b> <b>List</b> <b>one</b> <b>Preset</b> <b>Profile</b> <b>ID</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-p</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Selects a Preset Profile ID for which to update a value. This is required when updating a Preset Profile
       parameter and prohibited in all other cases.

       <b>2)</b> <b>Set</b> <b>Active</b> <b>Preset</b> <b>Profile</b> <b>ID</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-spp</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--set-preset-profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Activate the deisred Preset Profile ID. Requires root.

       <b>2)</b> <b>Update</b> <b>percentage</b> <b>Total</b> <b>Module</b> <b>Power</b> <b>(TMP)</b> <b>floor</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-ptf</u> <u>&lt;Percentage&gt;</u> <u>-p</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--percent-tmp-floor</u> <u>&lt;Percentage&gt;</u> <u>--profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Sets the percentage TMP floor to inputted value for a given Preset Profile ID. The desired percentage
       should be from 0 - 100, given in the form of "AB.CD", with a maximum of two decimal places of precision.
       For example, to set value to 34.56%, user will input 34.56. Input can also contain zero or one decimal
       places of precision. This option requires a profile ID as an argument. Requires root.

       <b>2)</b> <b>Update</b> <b>Ramp-Up</b> <b>Rate</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-rur</u> <u>&lt;value&gt;</u> <u>-p</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--ramp-up-rate</u> <u>&lt;value&gt;</u> <u>--profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Sets the Ramp-Up Rate to the desired value for a given Preset Profile ID. The rate given must be in the
       units of mW/s. This option requires a profile ID as an argument. Requires root.

       <b>2)</b> <b>Update</b> <b>Ramp-Down</b> <b>Rate</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-rdr</u> <u>&lt;value&gt;</u> <u>-p</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--ramp-down-rate</u> <u>&lt;value&gt;</u> <u>--profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Sets the Ramp-Down Rate to the desired value for a given Preset Profile ID. The rate given must be in the
       units of mW/s. This option requires a profile ID as an argument. Requires root.

       <b>2)</b> <b>Update</b> <b>Ramp-Down</b> <b>Hysteresis</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-rdh</u> <u>&lt;value&gt;</u> <u>-p</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--ramp-down-hysteresis</u> <u>&lt;value&gt;</u> <u>--profile</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       Sets the Ramp-Down Hysteresis to the desired value for a given Preset Profile ID. The rate given must be
       in the units of ms. This option requires a profile ID as an argument. Requires root.

       <b>2)</b> <b>Displays</b> <b>the</b> <b>Preset</b> <b>Profile</b> <b>definitions</b> <b>for</b> <b>all</b> <b>Profile</b> <b>IDs</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-ppd</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--print-profile-definitions</u>

       Displays all values for each Preset Profile IDs.

       <b>2)</b> <b>Set</b> <b>Feature</b> <b>State</b>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>-s</u> <u>&lt;state&gt;</u>

       <u>nvidia-smi</u> <u>power-smoothing</u> <u>--state</u> <u>&lt;state&gt;</u>

       Sets the state of the feature to either 0/DISABLED or 1/ENABLED. Requires root.

    <b>Power</b> <b>Profiles"</b>
       The "nvidia-smi power-profiles" command-line is used to manage Workload Power Profiles  related  data  on
       the GPU. It provides options to update Power Profiles data and query the supported Power Profiles.

       <b>Usage:</b>

       <b>1)</b> <b>Display</b> <b>help</b> <b>menu</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-h</u>

       Displays help menu for using the command-line.

       <b>2)</b> <b>List</b> <b>one</b> <b>or</b> <b>more</b> <b>GPUs</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-i</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--id</u> <u>&lt;GPU</u> <u>IDs&gt;</u>

       Selects one or more GPUs using the given comma-separated GPU indexes, PCI bus IDs or UUIDs. If not used,
       the given command-line option applies to all of the supported GPUs.

       <b>3)</b> <b>List</b> <b>Power</b> <b>Profiles</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-l</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--list</u>

       List all Workload Power Profiles supported by the device.

       <b>4)</b> <b>List</b> <b>Detailed</b> <b>Power</b> <b>Profiles</b> <b>info</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-ld</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--list-detailed</u>

       List all Workload Power Profiles supported by the device along with their metadata. This includes the
       Profile ID, the Priority (where a lower number indicates a higher priority), and Profiles that conflict
       with the given profile. If two or more conflicting profiles are requested, not all my be enforced.

       <b>5)</b> <b>Get</b> <b>Requested</b> <b>Profiles</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-gr</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--get-requested</u>

       Get a list of all currently requested Power Profiles. Note that if any of the profiles conflict, then not
       all may be enforced.

       <b>6)</b> <b>Set</b> <b>Requested</b> <b>Profiles</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-sr</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--set-requested</u> <u>&lt;Profile</u> <u>ID(s)&gt;</u>

       Adds the input profile(s) to the list of requested Power Profiles. The input is a comma separated list of
       profile IDs with no spaces. Requires root.

       <b>7)</b> <b>Clear</b> <b>Requested</b> <b>Profiles</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-cr</u> <u>&lt;Profile</u> <u>ID&gt;</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--clear-requested</u> <u>&lt;Profile</u> <u>ID(s)&gt;</u>

       Removes the input profile(s) to the list of requested Power Profiles. The input is a comma separated list
       of profile IDs with no spaces. Requires root.

       <b>8)</b> <b>Get</b> <b>Enforced</b> <b>Profiles</b>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>-ge</u>

       <u>nvidia-smi</u> <u>power-profiles</u> <u>--get-enforced</u>

       Get a list of all currently enforced Power Profiles. Note that this list may differ from the requested
       Profiles list if multiple conflicting profiles are selected.

</pre><h4><b>UNIT</b> <b>ATTRIBUTES</b></h4><pre>
       The  following list describes all possible data returned by the <b>-q</b> <b>-u</b> unit query option. Unless otherwise
       noted all numerical results are base 10 and unitless.

   <b>Timestamp</b>
       The current system timestamp at the time  nvidia-smi  was  invoked.  Format  is  "Day-of-week  Month  Day
       HH:MM:SS Year".

   <b>Driver</b> <b>Version</b>
       The version of the installed NVIDIA display driver. Format is "Major-Number.Minor-Number".

   <b>HIC</b> <b>Info</b>
       Information about any Host Interface Cards (HIC) that are installed in the system.

       <b>Firmware</b> <b>Version</b>
                      The version of the firmware running on the HIC.

   <b>Attached</b> <b>Units</b>
       The number of attached Units in the system.

   <b>Product</b> <b>Name</b>
       The official product name of the unit. This is an alphanumeric value. For all S-class products.

   <b>Product</b> <b>Id</b>
       The  product  identifier for the unit. This is an alphanumeric value of the form "part1-part2-part3". For
       all S-class products.

   <b>Product</b> <b>Serial</b>
       The immutable globally unique identifier for the unit. This is an alphanumeric  value.  For  all  S-class
       products.

   <b>Firmware</b> <b>Version</b>
       The  version  of the firmware running on the unit. Format is "Major-Number.Minor-Number". For all S-class
       products.

   <b>LED</b> <b>State</b>
       The LED indicator is used to flag systems with potential problems. An LED color  of  AMBER  indicates  an
       issue. For all S-class products.

       <b>Color</b>          The color of the LED indicator. Either "GREEN" or "AMBER".

       <b>Cause</b>          The  reason  for  the  current  LED  color.  The cause may be listed as any combination of
                      "Unknown", "Set to AMBER by host system", "Thermal  sensor  failure",  "Fan  failure"  and
                      "Temperature exceeds critical limit".

   <b>Temperature</b>
       Temperature  readings  for  important  components  of  the  Unit.  All readings are in degrees C. Not all
       readings may be available. For all S-class products.

       <b>Intake</b>         Air temperature at the unit intake.

       <b>Exhaust</b>        Air temperature at the unit exhaust point.

       <b>Board</b>          Air temperature across the unit board.

   <b>PSU</b>
       Readings for the unit power supply. For all S-class products.

       <b>State</b>          Operating state of the PSU. The power supply state can be any of the following:  "Normal",
                      "Abnormal",  "High  voltage",  "Fan  failure",  "Heatsink  temperature",  "Current limit",
                      "Voltage below UV alarm threshold", "Low-voltage", "I2C remote off command",  "MOD_DISABLE
                      input" or "Short pin transition".

       <b>Voltage</b>        PSU voltage setting, in volts.

       <b>Current</b>        PSU current draw, in amps.

   <b>Fan</b> <b>Info</b>
       Fan  readings  for  the  unit. A reading is provided for each fan, of which there can be many. For all S-
       class products.

       <b>State</b>          The state of the fan, either "NORMAL" or "FAILED".

       <b>Speed</b>          For a healthy fan, the fan's speed in RPM.

   <b>Attached</b> <b>GPUs</b>
       A list of PCI bus ids that correspond to each of the GPUs attached to the unit. The bus ids have the form
       "domain:bus:device.function", in hex. For all S-class products.

</pre><h4><b>NOTES</b></h4><pre>
       On Linux, NVIDIA device files may be modified by nvidia-smi if run  as  root.  Please  see  the  relevant
       section of the driver README file.

       The  <b>-a</b>  and  <b>-g</b>  arguments  are  now  deprecated  in  favor of <b>-q</b> and <b>-i</b>, respectively. However, the old
       arguments still work for this release.

</pre><h4><b>EXAMPLES</b></h4><pre>
   <b>nvidia-smi</b> <b>-q</b>
       Query attributes for all GPUs once, and display in plain text to stdout.

   <b>nvidia-smi</b> <b>--format=csv,noheader</b> <b>--query-gpu=uuid,persistence_mode</b>
       Query UUID and persistence mode of all GPUs in the system.

   <b>nvidia-smi</b> <b>-q</b> <b>-d</b> <b>ECC,POWER</b> <b>-i</b> <b>0</b> <b>-l</b> <b>10</b> <b>-f</b> <b>out.log</b>
       Query ECC errors and power consumption for GPU 0 at a frequency of 10 seconds, indefinitely,  and  record
       to the file out.log.

    <b>nvidia-smi</b> <b>-c</b> <b>1</b> <b>-i</b> <b>GPU-b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8""</b>
       Set      the      compute      mode      to      "PROHIBITED"     for     GPU     with     UUID     "GPU-
       b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8".

   <b>nvidia-smi</b> <b>-q</b> <b>-u</b> <b>-x</b> <b>--dtd</b>
       Query attributes for all Units once, and display in XML format with embedded DTD to stdout.

   <b>nvidia-smi</b> <b>--dtd</b> <b>-u</b> <b>-f</b> <b>nvsmi_unit.dtd</b>
       Write the Unit DTD to nvsmi_unit.dtd.

   <b>nvidia-smi</b> <b>-q</b> <b>-d</b> <b>SUPPORTED_CLOCKS</b>
       Display supported clocks of all GPUs.

   <b>nvidia-smi</b> <b>-i</b> <b>0</b> <b>--applications-clocks</b> <b>2500,745</b>
       Set applications clocks to 2500 MHz memory, and 745 MHz graphics.

   <b>nvidia-smi</b> <b>mig</b> <b>-cgi</b> <b>19</b>
       Create a MIG GPU instance on profile ID 19.

   <b>nvidia-smi</b> <b>mig</b> <b>-cgi</b> <b>19:2</b>
       Create a MIG GPU instance on profile ID 19 at placement start index 2.

   <b>nvidia-smi</b> <b>boost-slider</b> <b>-l</b>
       List all boost sliders for all GPUs.

   <b>nvidia-smi</b> <b>boost-slider</b> <b>--vboost</b> <b>1</b>
       Set vboost to value 1 for all GPUs.

   <b>nvidia-smi</b> <b>power-hint</b> <b>-l</b>
       List clock range, temperature range and supported profiles of power hint.

   <b>nvidia-smi</b> <b>boost-slider</b> <b>-gc</b> <b>1350</b> <b>-t</b> <b>60</b> <b>-p</b> <b>0</b>
       Query power hint with graphics clock at 1350MHz, temperature at 60C and profile ID at 0.

   <b>nvidia-smi</b> <b>boost-slider</b> <b>-gc</b> <b>1350</b> <b>-mc</b> <b>1215</b> <b>-t</b> <b>n5</b> <b>-p</b> <b>1</b>
       Query power hint with graphics clock at 1350MHz, memory clock at 1216MHz, temperature at -5C and  profile
       ID at 1.

</pre><h4><b>CHANGE</b> <b>LOG</b></h4><pre>
   <b>Known</b> <b>Issues</b>

       • On  systems  where  GPUs  are  NUMA nodes, the accuracy of FB memory utilization provided by nvidia-smi
         depends on the memory accounting of the operating system. This is because FB memory is managed  by  the
         operating  system  instead  of the NVIDIA GPU driver. Typically, pages allocated from FB memory are not
         released even after the process terminates to enhance performance. In  scenarios  where  the  operating
         system  is  under  memory  pressure,  it  may resort to utilizing FB memory. Such actions can result in
         discrepancies in the accuracy of memory reporting.

       • On Linux GPU Reset can't be triggered when there is pending GOM change.

       • On Linux GPU Reset may not successfully change pending ECC mode. A  full  reboot  may  be  required  to
         enable the mode change.

       • On  Linux  platforms  that  configure NVIDIA GPUs as NUMA nodes, enabling persistence mode or resetting
         GPUs may print 'Warning: persistence mode is disabled on device' if nvidia-persistenced is not running,
         or if nvidia-persistenced cannot access files in the NVIDIA driver's procfs directory  for  the  device
         (/proc/driver/nvidia/gpus/&lt;PCI  config=''  address&gt;=''&gt;/).  During  GPU  reset  and driver reload, this
         directory will be deleted and recreated, and outstanding references to the deleted directory,  such  as
         mounts or shells, can prevent processes from accessing files in the new directory.

       • There  might be a slight discrepency between volatile/aggregate ECC counters if recovery action was not
         taken

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v575</b> <b>Update</b> <b>and</b> <b>v570</b>

       • Added new --query-gpu option inforom.checksum_validation  to  check  the  inforom  checksum  validation
         (nvidia-smi --query-gpu inforom.checksum_validation)

       • Updated  'nvidia-smi -q' to print both 'Instantaneous Power Draw' and 'Average Power Draw' in all cases
         where 'Power Draw' used to be printed.

       • Added support to nvidia-smi c2c -e to display C2C Link Errors

       • Added support to nvidia-smi c2c -gLowPwrInfo to display C2C Link Power state

       • Added new fields for Clock Event Reason Counters which can be queries with 'nvidia-smi -q' or with  the
         'nvidia-smi -q -d PERFORMANCE' display flag.

       • Added   new   query   GPU   options   for   Clock   Event   Reason   Counters:   'nvidia-smi   --query-
         gpu=clocks_event_reasons_counters.{sw_power_cap,sw_thermal_slowdown,sync_boost,hw_thermal_slowdown,hw_power_brake_slowdown}'

       • Added new fields for MIG timeslicing which can be queried with 'nvidia-smi -q'

       • Added a new cmdline option '-smts' to 'nvidia-smi vgpu' to set vGPU MIG timeslice mode

       • Added a new sub-option '-gi' to 'nvidia-smi vgpu -c' to query the currently creatable vGPU types on the
         user provided GPU Instance

       • Added a new sub-option '-gi' to 'nvidia-smi vgpu -q' to query detailed  information  of  the  currently
         active vGPU instances on the user provided GPU Instance

       • Added  a  new  cmdline  option  '-ghm'  to 'nvidia-smi vgpu' to get vGPU heterogeneous mode on the user
         provided GPU Instance

       • Added a new sub-option '-gi' to 'nvidia-smi vgpu -shm' to set the vGPU heterogeneous mode on  the  user
         provided GPU Instance

       • Added new field for max instances per GPU Instance which can be queried with 'nvidia-smi vgpu -s -v'

       • Added a new sub-option '-gi' to 'nvidia-smi vgpu -ss' to query the vGPU software scheduler state on the
         user provided GPU Instance

       • Added  a new sub-option '-gi' to 'nvidia-smi vgpu -sl' to query the vGPU software scheduler logs on the
         user provided GPU Instance

       • Added a new sub-option '-gi'  to  'nvidia-smi  vgpu  set-scheduler-state'  to  set  the  vGPU  software
         scheduler state on the user provided GPU Instance.

       • Added  a new sub-option '-gi' to 'nvidia-smi vgpu -c -v' to query detailed information of the creatable
         vGPU types on the user provided GPU Instance

       • Added a new cmdlin option '--query-gpu-instance-vgpu-scheduler-logs' to 'nvidia-smi vgpu'  to  get  the
         vGPU  software  scheduler  logs  on  the  user provided GPU Instance in CSV format. See nvidia-smi vgpu
         --help-gpu-instance-vgpu-query-scheduler-logs for details.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v570</b> <b>Update</b> <b>and</b> <b>v565</b>

       • Added new cmdline option '-\sLWidth' and '-\gLWidth' to 'nvidia-smi nvlink'

       • Added new ability to display Nvlink sleep state with 'nvidia-smi nvlink -\s for  Blackwell  and  onward
         generations'

       • Added   new   query   GPU   options   for  average/instant  module  power  draw:  'nvidia-smi  --query-
         gpu=module.power.draw.{average,instant}'

       • Added  new  query  GPU  options  for  default/max/min  module  power   limits:   'nvidia-smi   --query-
         gpu=module.power.{default_limit,max_limit,min_limit}'

       • Added new query GPU options for module power limits: 'nvidia-smi --query-gpu=module.power.limit'

       • Added   new   query   GPU   options   for   enforced   module   power   limits:   'nvidia-smi  --query-
         gpu=module.enforced.power.limit'

       • Added new query GPU aliases for GPU Power options

       • Added a new command to get confidential compute info: 'nvidia-smi conf-compute -q'

       • Added new Power Profiles section in nvidia-smi -q and corresponding -d display flag POWER_PROFILES

       • Added new  Power  Profiles  option  'nvidia-smi  power-profiles'  to  get/set  power  profiles  related
         information.

       • Added the platform information query to 'nvidia-smi -q'

       • Added the platform information query to 'nvidia-smi --query-gpu platform'

       • Added new Power Smoothing option 'nvidia-smi power-smoothing' to set power smoothing related values.

       • Added new Power Smoothing section in nvidia-smi -q and corresponding -d display flag POWER_SMOOTHING

       • Deprecated graphics voltage value from Voltage section of nvidia-smi -q. Voltage now always displays as
         'N/A' and will be removed in a future release.

       • Added new topo option nvidia-smi topo -nvme to display GPUs vs NVMes connecting path.

       • Changed  help string for the command 'nvidia-smi topo -p2p -p' from 'prop' to 'pcie' to better describe
         the p2p capability.

       • Added new command 'nvidia-smi pci -gCnt' to query PCIe RX/TX Bytes.

       • Added EGM capability display under new Capabilities section in nvidia-smi -q command.

       • Add multiGpuMode dipsplay via nvidia-smi via 'nvidia-smi conf-compute --get-multigpu-mode' or  'nvidia-
         smi conf-compute -mgm'

       • GPU  Reset  Status in nvidia-smi -q has been deprecated. GPU Recovery action provides all the necessary
         actions

       • nvidia-smi -q will now display Dram encryption state

       • nvidia-smi -den/--dram-encryption 0/1 to disable/enable dram encryption

       • Added new status to nvidia fabric health. nvidia-smi -q will display 3 new fields in  Fabric  Health  -
         Route Recovery in progress, Route Unhealthy and Access Timeout Recovery

       • In nvidia-smi -q Platform Info - RACK GUID is changed to Platform Info - RACK Serial Number

       • In nvidia-smi --query-gpu new option for gpu_recovery_action is added

       • Added new counters for Nvlink5 in nvidia-smi nvlink -e:

         • Effective Errors to get sum of the number of errors in each Nvlink packet

         • Effective BER to get Effective BER for effective errors

         • FEC Errors - 0 to 15 to get count of symbol errors that are corrected

       • Added a new output field called 'GPU Fabric GUID' to the 'nvidia-smi -q' output

       • Added a new property called 'platform.gpu_fabric_guid' to 'nvidia-smi --query-gpu'

       • Updated 'nvidia-smi nvlink -gLowPwrInfo' command to display the Power Threshold Range and Units

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v565</b> <b>Update</b> <b>and</b> <b>v560</b>

       • Added the reporting of vGPU homogeneous mode to 'nvidia-smi -q'.

       • Added  the  reporting  of  homogeneous  vGPU  placements  to 'nvidia-smi vgpu -s -v', complementing the
         existing reporting of heterogeneous vGPU placements.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v560</b> <b>Update</b> <b>and</b> <b>v555</b>

       • Added 'Atomic Caps Inbound' in the PCI section of 'nvidia-smi -q'.

       • Updated ECC and row remapper output for options '--query-gpu' and '--query-remapped-rows'.

       • Added support for events including  ECC  single-bit  error  storm,  DRAM  retirement,  DRAM  retirement
         failure, contained/nonfatal poison and uncontained/fatal poison.

       • Added support in 'nvidia-smi nvlink -e' to display NVLink5 error counters

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v550</b> <b>Update</b> <b>and</b> <b>v545</b>

       • Added a new cmdline option to print out version information: --version

       • Added  ability  to print out only the GSP firmware version with'nvidia-smi -q -d'. Example commandline:
         nvidia-smi -q -d GSP_FIRMWARE_VERSION

       • Added support to query pci.baseClass and pci.subClass. See nvidia-smi --help-query-gpu for details.

       • Added PCI base and sub classcodes to 'nvidia-smi -q' output.

       • Added new cmdline option '--format' to 'nvidia-smi dmon' to  support  'csv',  'nounit'  and  'noheader'
         format specifiers

       • Added  a  new  cmdline option '--gpm-options' to 'nvidia-smi dmon' to support GPM metrics report in MIG
         mode

       • Added the NVJPG and NVOFA utilization report to 'nvidia-smi pmon'

       • Added the NVJPG and NVOFA utilization report to 'nvidia-smi -q -d utilization'

       • Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu -q' to report NVJPG/NVOFA  utilization
         on active vgpus

       • Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu -u' to periodically report NVJPG/NVOFA
         utilization on active vgpus

       • Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu -p' to periodically report NVJPG/NVOFA
         utilization on running processs of active vgpus

       • Added a new cmdline option '-shm' to 'nvidia-smi vgpu' to set vGPU heterogeneous mode

       • Added the reporting of vGPU heterogeneous mode in 'nvidia-smi -q'

       • Added  ability to call 'nvidia-smi mig -lgip' and 'nvidia-smi mig -lgipp' to work without requiring MIG
         being enabled

       • Added support to query confidential compute key rotation threshold info.

       • Added support to set confidential compute key rotation max attacker advantage.

       • Added a new cmdline option '--sparse-operation-mode' to 'nvidia-smi clocks' to set the sparse operation
         mode

       • Added the reporting of sparse operation mode to 'nvidia-smi -q -d PERFORMANCE'

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v535</b> <b>Update</b> <b>and</b> <b>v545</b>

       • Added support to query the timestamp and duration of the latest flush of the BBX object to the  inforom
         storage.

       • Added support for reporting out GPU Memory power usage.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v535</b> <b>Update</b> <b>and</b> <b>v530</b>

       • Updated the SRAM error status reported in the ECC query 'nvidia-smi -q -d ECC'

       • Added support to query and report the GPU JPEG and OFA (Optical Flow Accelerator) utilizations.

       • Removed deprecated 'stats' command.

       • Added support to set the vGPU software scheduler state.

       • Renamed counter collection unit to gpu performance monitoring.

       • Added new C2C Mode reporting to device query.

       • Added back clock_throttle_reasons to --query-gpu to not break backwards compatibility

       • Added support to get confidential compute CPU capability and GPUs capability.

       • Added support to set confidential compute unprotected memory and GPU ready state.

       • Added support to get confidential compute memory info and GPU ready state.

       • Added support to display confidential compute devtools mode, environment and feature status.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v525</b> <b>Update</b> <b>and</b> <b>v530</b>

       • Added  support  to query power.draw.average and power.draw.instant. See nvidia-smi --help-query-gpu for
         details.

       • Added support to get the vGPU software scheduler state.

       • Added support to get the vGPU software scheduler logs.

       • Added support to get the vGPU software scheduler capabilities.

       • Renamed Clock Throttle Reasons to Clock Event Reasons.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v520</b> <b>Update</b> <b>and</b> <b>v525</b>

       • Added support to query and set counter collection unit stream state.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v470</b> <b>Update</b> <b>and</b> <b>v510</b>

       • Add new 'Reserved' memory reporting to the FB memory output

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v465</b> <b>Update</b> <b>and</b> <b>v470</b>

       • Added support to query power hint

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v460</b> <b>Update</b> <b>and</b> <b>v465</b>

       • Removed support for -acp,--application-clock-permissions option

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v450</b> <b>Update</b> <b>and</b> <b>v460</b>

       • Add option to specify placement when creating a MIG GPU instance.

       • Added support to query and control boost slider

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v445</b> <b>Update</b> <b>and</b> <b>v450</b>

       • Added --lock-memory-clock and --reset-memory-clock command to lock  to  closest  min/max  Memory  clock
         provided and ability to reset Memory clock

       • Allow fan speeds greater than 100% to be reported

       • Added topo support to display NUMA node affinity for GPU devices

       • Added support to create MIG instances using profile names

       • Added support to create the default compute instance while creating a GPU instance

       • Added support to query and disable MIG mode on Windows

       • Removed support of GPU reset(-r) command on MIG enabled vGPU guests

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v418</b> <b>Update</b> <b>and</b> <b>v445</b>

       • Added support for Multi Instance GPU (MIG)

       • Added support to individually reset NVLink-capable GPUs based on the NVIDIA Ampere architecture

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v361</b> <b>Update</b> <b>and</b> <b>v418</b>

       • Support for Volta and Turing architectures, bug fixes, performance improvements, and new features

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v352</b> <b>Update</b> <b>and</b> <b>v361</b>

       • Added nvlink support to expose the publicly available NVLINK NVML APIs

       • Added clocks sub-command with synchronized boost support

       • Updated nvidia-smi stats to report GPU temperature metric

       • Updated nvidia-smi dmon to support PCIe throughput

       • Updated nvidia-smi daemon/replay to support PCIe throughput

       • Updated nvidia-smi dmon, daemon and replay to support PCIe Replay Errors

       • Added GPU part numbers in nvidia-smi -q

       • Removed support for exclusive thread compute mode

       • Added Video (encoder/decode) clocks to the Clocks and Max Clocks display of nvidia-smi -q

       • Added memory temperature output to nvidia-smi dmon

       • Added  --lock-gpu-clock and --reset-gpu-clock command to lock to closest min/max GPU clock provided and
         reset clock

       • Added --cuda-clocks to override or restore default CUDA clocks

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v346</b> <b>Update</b> <b>and</b> <b>v352</b>

       • Added topo support to display affinities per GPU

       • Added topo support to display neighboring GPUs for a given level

       • Added topo support to show pathway between two given GPUs

       • Added 'nvidia-smi pmon' command-line for process monitoring in scrolling format

       • Added '--debug' option to produce an encrypted debug log for use in submission of bugs back to NVIDIA

       • Fixed reporting of Used/Free memory under Windows WDDM mode

       • The accounting stats is updated to include both running and terminated processes. The execution time of
         running process is reported as 0 and updated to actual value when the process is terminated.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v340</b> <b>Update</b> <b>and</b> <b>v346</b>

       • Added reporting of PCIe replay counters

       • Added support for reporting Graphics processes via nvidia-smi

       • Added reporting of PCIe utilization

       • Added dmon command-line for device monitoring in scrolling format

       • Added daemon command-line to run in background and monitor devices as a daemon process. Generates dated
         log files at /var/log/nvstats/

       • Added replay command-line to replay/extract the stat files generated by the daemon tool

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v331</b> <b>Update</b> <b>and</b> <b>v340</b>

       • Added reporting of temperature threshold information.

       • Added reporting of brand information (e.g. Tesla, Quadro, etc.)

       • Added support for K40d and K80.

       • Added reporting of  max,  min  and  avg  for  samples  (power,  utilization,  clock  changes).  Example
         commandline: nvidia-smi -q -d power,utilization, clock

       • Added  nvidia-smi  stats interface to collect statistics such as power, utilization, clock changes, xid
         events and perf capping counters with a notion of time attached to each  sample.  Example  commandline:
         nvidia-smi stats

       • Added  support  for collectively reporting metrics on more than one GPU. Used with comma separated with
         '-i' option. Example: nvidia-smi -i 0,1,2

       • Added support for displaying the GPU encoder and decoder utilizations

       • Added nvidia-smi topo interface to display the GPUDirect communication matrix (EXPERIMENTAL)

       • Added support for displayed the GPU board ID and whether or not it is a multiGPU board

       • Removed user-defined throttle reason from XML output

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v5.319</b> <b>Update</b> <b>and</b> <b>v331</b>

       • Added reporting of minor number.

       • Added reporting BAR1 memory size.

       • Added reporting of bridge chip firmware.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v4.319</b> <b>Production</b> <b>and</b> <b>v4.319</b> <b>Update</b>

       • Added new --applications-clocks-permission switch to change permission  requirements  for  setting  and
         resetting applications clocks.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v4.304</b> <b>and</b> <b>v4.319</b> <b>Production</b>

       • Added  reporting  of  Display  Active  state  and  updated documentation to clarify how it differs from
         Display Mode and Display Active state

       • For consistency on multi-GPU boards nvidia-smi -L always displays UUID instead of serial number

       • Added machine readable selective reporting. See SELECTIVE QUERY OPTIONS section of nvidia-smi -h

       • Added queries for page retirement information. See --help-query-retired-pages and -d PAGE_RETIREMENT

       • Renamed Clock Throttle Reason User Defined Clocks to Applications Clocks Setting

       • On error, return codes have distinct non zero values for each error class. See RETURN VALUE section

       • nvidia-smi -i can now query information from healthy GPU when there is a problem with other GPU in  the
         system

       • All messages that point to a problem with a GPU print pci bus id of a GPU at fault

       • New  flag  --loop-ms  for  querying  information  at higher rates than once a second (can have negative
         impact on system performance)

       • Added queries for accounting procsses. See --help-query-accounted-apps and -d ACCOUNTING

       • Added the enforced power limit to the query output

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v4.304</b> <b>RC</b> <b>and</b> <b>v4.304</b> <b>Production</b>

       • Added reporting of GPU Operation Mode (GOM)

       • Added new --gom switch to set GPU Operation Mode

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v3.295</b> <b>and</b> <b>v4.304</b> <b>RC</b>

       • Reformatted non-verbose output due to user feedback. Removed pending information from table.

       • Print out helpful message if initialization fails due to kernel module not receiving interrupts

       • Better error handling when NVML shared library is not present in the system

       • Added new --applications-clocks switch

       • Added new filter to --display switch. Run with -d SUPPORTED_CLOCKS to list possible clocks on a GPU

       • When reporting free memory, calculate it from the rounded total and used memory so that values add up

       • Added reporting of power management limit constraints and default limit

       • Added new --power-limit switch

       • Added reporting of texture memory ECC errors

       • Added reporting of Clock Throttle Reasons

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v2.285</b> <b>and</b> <b>v3.295</b>

       • Clearer error reporting for running commands (like changing compute mode)

       • When running commands on multiple GPUs at once N/A errors are treated as warnings.

       • nvidia-smi -i now also supports UUID

       • UUID format changed to match UUID standard and will report a different value.

   <b>Changes</b> <b>between</b> <b>nvidia-smi</b> <b>v2.0</b> <b>and</b> <b>v2.285</b>

       • Report VBIOS version.

       • Added -d/--display flag to filter parts of data

       • Added reporting of PCI Sub System ID

       • Updated docs to indicate we support M2075 and C2075

       • Report HIC HWBC firmware version with -u switch

       • Report max(P0) clocks next to current clocks

       • Added --dtd flag to print the device or unit DTD

       • Added message when NVIDIA driver is not running

       • Added reporting of PCIe link generation (max and current), and link width (max and current).

       • Getting pending driver model works on non-admin

       • Added support for running nvidia-smi on Windows Guest accounts

       • Running nvidia-smi without -q command will output non verbose version of -q instead of help

       • Fixed parsing of -l/--loop= argument (default value, 0, to big value)

       • Changed format of pciBusId (to XXXX:XX:XX.X - this change was visible in 280)

       • Parsing of busId for -i command is less restrictive. You can  pass  0:2:0.0  or  0000:02:00  and  other
         variations

       • Changed versioning scheme to also include 'driver version'

       • XML format always conforms to DTD, even when error conditions occur

       • Added  support  for  single  and  double bit ECC events and XID errors (enabled by default with -l flag
         disabled for -x flag)

       • Added device reset -r --gpu-reset flags

       • Added listing of compute running processes

       • Renamed power state to performance state. Deprecated support exists in XML output only.

       • Updated DTD version number to 2.0 to match the updated XML output

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       On Linux, the driver README is installed as /usr/share/doc/NVIDIA_GLX-1.0/README.txt

</pre><h4><b>AUTHOR</b></h4><pre>
       NVIDIA Corporation

</pre><h4><b>COPYRIGHT</b></h4><pre>
       Copyright 2011-2025 NVIDIA Corporation

Version nvidia-smi 575.57                        Sat May 24 2025                                   <u><a href="../man1/nvidia-smi.1.html">nvidia-smi</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>