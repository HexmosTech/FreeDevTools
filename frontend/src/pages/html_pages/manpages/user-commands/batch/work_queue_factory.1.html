<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>work_queue_factory - maintain a pool of Work Queue workers on a batch system.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/coop-computing-tools">coop-computing-tools_7.14.5-1build1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       <b>work_queue_factory</b> - maintain a pool of Work Queue workers on a batch system.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       work_queue_factory -M <u>&lt;project-name&gt;</u> -T <u>&lt;batch-type&gt;</u> [options]

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>work_queue_factory</b>  submits  and  maintains  a  number of <b><a href="../man1/work_queue_worker.1.html">work_queue_worker</a>(1)</b> processes on various batch
       systems, such as Condor and UGE.  All the  workers  managed  by  a  <b>work_queue_factory</b>  process  will  be
       directed  to  work  for  a  specific  manager,  or  any  set  of  managers matching a given project name.
       <b>work_queue_factory</b> will automatically determine the correct number of workers to have running,  based  on
       criteria  set  on  the  command  line.   The decision on how many workers to run is reconsidered once per
       minute.

       By default, <b>work_queue_factory</b> will run as many workers as the indicated managers  have  tasks  ready  to
       run.   If  there  are  multiple managers, then enough workers will be started to satisfy their collective
       needs.  For example, if there are two managers with the same project name, each with  10  tasks  to  run,
       then <b>work_queue_factory</b> will start a total of 20 workers.

       If  the  number  of  needed  workers  increases,  <b>work_queue_factory</b> will submit more workers to meet the
       desired need.  However, it will not run more than a fixed maximum number of  workers,  given  by  the  -W
       option.

       If  the need for workers drops, <b>work_queue_factory</b> does not remove them immediately, but waits to them to
       exit on their own.  (This happens when the worker has been idle for a certain time.)  A minimum number of
       workers will be maintained, given by the -w option.

       If given the -c option, then <b>work_queue_factory</b> will consider the capacity reported by each manager.  The
       capacity is the estimated number of workers that the manager thinks it can  handle,  based  on  the  task
       execution  and  data  transfer  times  currently  observed  at  the  manager.   With  the  -c  option on,
       <b>work_queue_factory</b> will consider the manager's capacity to be the maximum number of workers to run.

       If <b>work_queue_factory</b> receives a terminating signal, it will attempt to remove all running workers before
       exiting.

</pre><h4><b>OPTIONS</b></h4><pre>
       General options:

        <b>-T,--batch-type=</b><u>&lt;type&gt;</u>
               Batch system type (required). One of: local, wq, vine,  condor,  uge,  pbs,  lsf,  torque,  moab,
              slurm, amazon, k8s, dryrun

        <b>-C,--config-file=</b><u>&lt;file&gt;</u>
               Use configuration file <u>&lt;file&gt;</u>.

        <b>-M,--manager-name=</b><u>&lt;project&gt;</u>
               Project name of managers to server, can be regex

        <b>-F,--foremen-name=</b><u>&lt;project&gt;</u>
               Foremen to serve, can be a regular expression.

        <b>--catalog=</b><u>&lt;host:port&gt;</u>
               Catalog server to query for managers.

        <b>-P,--password=</b><u>&lt;pwdfile&gt;</u>
               Password file for workers to authenticate.

        <b>-S,--scratch-dir=</b><u>&lt;dir&gt;</u>
               Use  this  scratch  dir  for factory. Default is /tmp/wq-factory-$UID.  Also configurable through
              environment variables <b>CCTOOLS_TEMP</b> or <b>TMPDIR</b>

        <b>--run-factory-as-manager</b>
               Force factory to run itself as a manager.

        <b>--parent-death</b>
               Exit if parent process dies.

        <b>-d,--debug=</b><u>&lt;subsystem&gt;</u>
               Enable debugging for this subsystem.

        <b>-o,--debug-file=</b><u>&lt;file&gt;</u>
               Send debugging to this file.

        <b>-O,--debug-file-size=</b><u>&lt;mb&gt;</u>
               Specify the size of the debug file.

        <b>-v,--version</b>
               Show the version string.

        <b>-h,--help</b>
               Show this screen.

              Concurrent control options:

        <b>-w,--min-workers=</b><u>&lt;n&gt;</u>
               Minimum workers running (default=5).

        <b>-W,--max-workers=</b><u>&lt;n&gt;</u>
               Maximum workers running (default=100).

        <b>--workers-per-cycle=</b><u>&lt;n&gt;</u>
               Max number of new workers per 30s (default=5)

        <b>-t,--timeout=</b><u>&lt;time&gt;</u>
               Workers abort after idle time (default=300).

        <b>--factory-timeout=</b><u>&lt;n&gt;</u>
               Exit after no manager seen in <u>&lt;n&gt;</u> seconds.

        <b>--tasks-per-worker=</b><u>&lt;n&gt;</u>
               Average tasks per worker (default=one per core).

        <b>-c,--capacity=</b><u>&lt;cap&gt;</u>
               Use worker capacity reported by managers.

              Resource management options:

        <b>--cores=</b><u>&lt;n&gt;</u>

               Set the number of cores requested per worker.

        <b>--gpus=</b><u>&lt;n&gt;</u>

               Set the number of GPUs requested per worker.

        <b>--memory=</b><u>&lt;mb&gt;</u>

               Set the amount of memory (in MB) per worker.

        <b>--disk=</b><u>&lt;mb&gt;</u>

               Set the amount of disk (in MB) per worker.

        <b>--autosize</b>

               Autosize worker to slot (Condor, Mesos, K8S).

              Worker environment options:

        <b>--env=</b><u>&lt;variable=value&gt;</u>

               Environment variable to add to worker.

        <b>-E,--extra-options=</b><u>&lt;options&gt;</u>

               Extra options to give to worker.

        <b>--worker-binary=</b><u>&lt;file&gt;</u>

               Alternate binary instead of work_queue_worker.

        <b>--wrapper=</b><u>&lt;cmd&gt;</u>

               Wrap factory with this command prefix.

        <b>--wrapper-input=</b><u>&lt;file&gt;</u>
               Add this input file needed by the wrapper.

        <b>--python-env=</b><u>&lt;file.tar.gz&gt;</u>
               Run each worker inside this python environment.

              Options  specific to batch systems:

        <b>-B,--batch-options=</b><u>&lt;options&gt;</u>
               Generic batch system options.

        <b>--amazon-config=</b><u>&lt;cfg&gt;</u>
               Specify Amazon config file.

        <b>--condor-requirements=</b><u>&lt;reqs&gt;</u>
               Set requirements for the workers as Condor jobs.

</pre><h4><b>EXIT</b> <b>STATUS</b></h4><pre>
       On success, returns zero. On failure, returns non-zero.

</pre><h4><b>EXAMPLES</b></h4><pre>
       Suppose you have a Work Queue manager with a project name of "barney".  To maintain workers  for  barney,
       do this:

               work_queue_factory -T condor -M barney

       To maintain a maximum of 100 workers on an UGE batch system, do this:

               work_queue_factory -T uge -M barney -W 100

       To start workers such that the workers exit after 5 minutes (300s) of idleness:

               work_queue_factory -T condor -M barney -t 300

       If you want to start workers that match any project that begins with barney, use a regular expression:

               work_queue_factory -T condor -M "barney.*" -t 300

       If running on condor, you may manually specify condor requirements:

               work_queue_factory -T condor -M barney --condor-requirements 'MachineGroup == "disc"' --condor-requirements 'has_matlab == true'

       Repeated  uses  of condor-requirements are and-ed together. The previous example will produce a statement
       equivalent to:

       requirements = ((MachineGroup == "disc") &amp;&amp; (has_matlab == true))

       Use the configuration file <b>my_conf</b>:

               work_queue_factory -Cmy_conf

       <b>my_conf</b> should be a proper JSON document, as:

               {
                       "manager-name": "my_manager.*",
                       "max-workers": 100,
                       "min-workers": 0
               }

       Valid configuration fields are:

               manager-name
               foremen-name
               min-workers
               max-workers
               workers-per-cycle
               task-per-worker
               timeout
               worker-extra-options
               condor-requirements
               cores
               memory
               disk

</pre><h4><b>KNOWN</b> <b>BUGS</b></h4><pre>
       The capacity measurement currently assumes single-core tasks running on single-core workers, and  behaves
       unexpectedly with multi-core tasks or multi-core workers.

</pre><h4><b>COPYRIGHT</b></h4><pre>
       The  Cooperative  Computing  Tools are Copyright (C) 2022 The University of Notre Dame.  This software is
       distributed under the GNU General Public License.  See the file COPYING for details.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       •   <b>Cooperative</b> <b>Computing</b> <b>Tools</b> <b>Documentation</b>

       •   <b>Work</b> <b>Queue</b> <b>User</b> <b>Manual</b>

       •   <b><a href="../man1/work_queue_worker.1.html">work_queue_worker</a>(1)</b>  <b><a href="../man1/work_queue_status.1.html">work_queue_status</a>(1)</b>  <b><a href="../man1/work_queue_factory.1.html">work_queue_factory</a>(1)</b>  <b><a href="../man1/condor_submit_workers.1.html">condor_submit_workers</a>(1)</b>  <b>uge_sub‐</b>
           <b><a href="../man1/mit_workers.1.html">mit_workers</a>(1)</b> <b><a href="../man1/torque_submit_workers.1.html">torque_submit_workers</a>(1)</b>

CCTools 7.14.5 FINAL                                                                       <u><a href="../man1/work_queue_factory.1.html">work_queue_factory</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>