<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>makeflow - workflow engine for executing distributed workflows</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/coop-computing-tools">coop-computing-tools_7.14.5-1build1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       <b>makeflow</b> - workflow engine for executing distributed workflows

</pre><h4><b>SYNOPSIS</b></h4><pre>
       makeflow [options] <u>&lt;dagfile&gt;</u>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>Makeflow</b>  is a workflow engine for distributed computing. It accepts a specification of a large amount of
       work to be performed, and runs it on remote machines in parallel where possible. In addition, <b>Makeflow</b> is
       fault-tolerant, so you can use it to coordinate very large tasks that may run for days or  weeks  in  the
       face  of  failures.  <b>Makeflow</b> is designed to be similar to Make, so if you can write a Makefile, then you
       can write a <b>Makeflow</b>.

       You can run a <b>Makeflow</b> on your local machine to test it out. If you have a multi-core machine,  then  you
       can  run multiple tasks simultaneously. If you have a Condor pool or a Sun Grid Engine batch system, then
       you can send your jobs there to run. If you don't already have a batch  system,  <b>Makeflow</b>  comes  with  a
       system  called  Work Queue that will let you distribute the load across any collection of machines, large
       or small.  <b>Makeflow</b> also supports execution in a Docker container, regardless of the batch system used.

</pre><h4><b>OPTIONS</b></h4><pre>
       When makeflow is ran without arguments, it will attempt to execute the workflow specified by the <b>Makeflow</b>
       dagfile using the local execution engine.

   <b>Commands</b>
        <b>-c,--clean=</b><u>&lt;option&gt;</u>
              Clean up: remove logfile and all targets. If option is one  of  [intermediates,  outputs,  cache],
              only indicated files are removed.

        <b>-f,--summary-log=</b><u>&lt;file&gt;</u>
              Write summary of workflow to file.

        <b>-h,--help</b>
              Show this help screen.

        <b>-v,--version</b>
              Show version string.

        <b>-X,--chdir=</b><u>&lt;directory&gt;</u>
              Chdir to enable executing the Makefile in other directory.

        <b>--argv=</b><u>&lt;file&gt;</u>
              Use command line arguments from JSON file.

   <b>Workflow</b> <b>Handling</b>
        <b>-a,--advertise</b>
              Advertise the manager information to a catalog server.

        <b>-l,--makeflow-log=</b><u>&lt;logfile&gt;</u>
              Use this file for the makeflow log. (default is X.makeflowlog)

        <b>-L,--batch-log=</b><u>&lt;logfile&gt;</u>
              Use this file for the batch system log. (default is X.<u>&lt;type&gt;</u>log)

        <b>-m,--email=</b><u>&lt;email&gt;</u>
              Email summary of workflow to address.

        <b>-j,--max-local=</b><u>&lt;#&gt;</u>
              Max number of local jobs to run at once. (default is # of cores)

        <b>-J,--max-remote=</b><u>&lt;#&gt;</u>
              Max number of remote jobs to run at once. (default is 1000 for -Twq, 100 otherwise)

        <b>-R,--retry</b>
              Automatically retry failed batch jobs up to 100 times.

        <b>-r,--retry-count=</b><u>&lt;n&gt;</u>
              Automatically retry failed batch jobs up to n times.

        <b>--local-cores=</b><u>&lt;#&gt;</u>
              Max number of cores used for local execution.

        <b>--local-memory=</b><u>&lt;#&gt;</u>
              Max amount of memory used for local execution.

        <b>--local-disk=</b><u>&lt;#&gt;</u>
              Max amount of disk used for local execution.

              OPTION_END

   <b>Batch</b> <b>Options</b>
        <b>-B,--batch-options=</b><u>&lt;options&gt;</u>
              Add these options to all batch submit files.

        <b>--send-environment</b>
              Send all local environment variables in remote execution.

        <b>--wait-for-files-upto=</b><u>&lt;#&gt;</u>
              Wait for output files to be created upto this many seconds (e.g., to deal with NFS semantics).

        <b>-S,--submission-timeout=</b><u>&lt;timeout&gt;</u>
              Time to retry failed batch job submission. (default is 3600s)

        <b>-T,--batch-type=</b><u>&lt;type&gt;</u>
              Batch  system  type:  local,  dryrun,  condor,  wq,  vine, uge, pbs, torque, slurm, moab, cluster,
              amazon. (default is local)

        <b>--safe-submit-mode</b>
              Excludes resources at submission. (SLURM, TORQUE, and PBS)

        <b>--ignore-memory-spec</b>
              Excludes memory at submission. (SLURM)

        <b>--batch-mem-type=</b><u>&lt;type&gt;</u>
              Specify memory resource type. (SGE)

        <b>--working-dir=</b><u>&lt;dir|url&gt;</u>
              Working directory for batch system.

        <b>--sandbox</b>
              Run task in sandbox using bash script and task directory.

        <b>--verbose-jobnames</b>
              Set the job name based on the command.

        <b>--keep-wrapper-stdout</b>
              Do not redirect to /dev/null the stdout file from the batch system.

   <b>JSON/JX</b> <b>Options</b>
        <b>--json</b>
              Interpret <u>&lt;dagfile&gt;</u> as a JSON format Makeflow.

        <b>--jx</b>  Evaluate JX expressions in <u>&lt;dagfile&gt;</u>. Implies --json.

        <b>--jx-args=</b><u>&lt;args&gt;</u>
              Read variable definitions from the JX file <u>&lt;args&gt;</u>.

        <b>--jx-define=</b><u>&lt;VAL=EXPR&gt;</u>
              Set the variable <u>&lt;VAL&gt;</u> to the JX expression <u>&lt;EXPR&gt;</u>.

        <b>--jx-context=</b><u>&lt;ctx&gt;</u>
              Deprecated. See '--jx-args'.

   <b>Debugging</b> <b>Options</b>
        <b>-d,--debug=</b><u>&lt;subsystem&gt;</u>
              Enable debugging for this subsystem.

        <b>-o,--debug-file=</b><u>&lt;file&gt;</u>
              Write debugging output to this file. By default, debugging is sent to stderr (":stderr"). You  may
              specify logs to be sent to stdout (":stdout") instead.

        <b>--debug-rotate-max=</b><u>&lt;byte&gt;</u>
              Rotate debug file once it reaches this size.

        <b>--verbose</b>
              Display runtime progress on stdout.

   <b>TaskVine</b> <b>and</b> <b>Work</b> <b>Queue</b> <b>Options</b>
        <b>-C,--catalog-server=</b><u>&lt;catalog&gt;</u>
              Set catalog server to <u>&lt;catalog&gt;</u>. Format: HOSTNAME:PORT

        <b>-F,--wq-fast-abort=</b><u>&lt;#&gt;</u>
              WorkQueue fast abort multiplier. (default is deactivated)

        <b>-M,--project-name=</b><u>&lt;project&gt;</u>
              Set the project name to <u>&lt;project&gt;</u>.

        <b>-p,--port=</b><u>&lt;port&gt;</u>
              Port number to use with WorkQueue. (default is 9123, 0=arbitrary)

        <b>-Z,--port-file=</b><u>&lt;file&gt;</u>
              Select port at random and write it to this file.  (default is disabled)

        <b>-P,--priority=</b><u>&lt;integer&gt;</u>
              Priority. Higher the value, higher the priority.

        <b>-t,--keepalive-timeout=</b><u>&lt;#&gt;</u>
              Work Queue keepalive timeout (default: 30s)

        <b>-u,--keepalive-interval=</b><u>&lt;#&gt;</u>
              Work Queue keepalive interval (default: 120s)

        <b>-W,--schedule=</b><u>&lt;mode&gt;</u>
              WorkQueue scheduling algorithm. (time|files|fcfs)

        <b>--password=</b><u>&lt;pwfile&gt;</u>
              Password file for authenticating workers.

        <b>--ssl-cert=</b><u>&lt;&gt;</u>
               Set the SSL certificate file for encrypting connection.

        <b>--ssl-key=</b><u>&lt;&gt;</u>
               Set the SSL certificate file for encrypting connection.

        <b>--cache-mode</b>
               Control worker caching mode. (never|workflow|forever)

        <b>--preferred-connection=</b><u>&lt;connection&gt;</u>
              Indicate preferred connection. Chose one of by_ip or by_hostname. (default is by_ip)

   <b>Monitor</b> <b>Options</b>
        <b>--monitor=</b><u>&lt;dir&gt;</u>
              Enable the resource monitor, and write the monitor logs to <u>&lt;dir&gt;</u>

        <b>--monitor=</b><u>&lt;dir&gt;</u>
              Enable the resource monitor, and write the monitor logs to <u>&lt;dir&gt;</u>

        <b>--monitor-exe=</b><u>&lt;file&gt;</u>
              Specify resource monitor executable.

        <b>--monitor-with-time-series</b>
              Enable monitor time series.                 (default is disabled)

        <b>--monitor-with-opened-files</b>
              Enable monitoring of openened files.        (default is disabled)

        <b>--monitor-interval=</b><u>&lt;n&gt;</u>
              Set monitor interval to <u>&lt;n&gt;</u> seconds. (default 1 second)

        <b>--monitor-log-fmt=</b><u>&lt;fmt&gt;</u>
              Format for monitor logs. (default resource-rule-%06.6d, %d -&gt; rule number)

        <b>--monitor-measure-dir</b>
              Monitor measures the task's current execution directory size.

        <b>--allocation=</b><u>&lt;waste&gt;</u>
              When  monitoring is enabled, automatically assign resource allocations to tasks. Makeflow will try
              to minimize waste or maximize throughput.

   <b>Umbrella</b> <b>Options</b>
        <b>--umbrella-binary=</b><u>&lt;filepath&gt;</u>
              Umbrella binary for running every rule in a makeflow.

        <b>--umbrella-log-prefix=</b><u>&lt;filepath&gt;</u>
              Umbrella   log   file   prefix   for   running   every   rule   in   a   makeflow.   (default   is
              <u>&lt;makefilename&gt;</u>.umbrella.log)

        <b>--umbrella-log-prefix=</b><u>&lt;filepath&gt;</u>
              Umbrella   log   file   prefix   for   running   every   rule   in   a   makeflow.   (default   is
              <u>&lt;makefilename&gt;</u>.umbrella.log)

        <b>--umbrella-mode=</b><u>&lt;mode&gt;</u>
              Umbrella execution mode for running every rule in a makeflow. (default is local)

        <b>--umbrella-spec=</b><u>&lt;filepath&gt;</u>
              Umbrella spec for running every rule in a makeflow.

   <b>Docker</b> <b>Support</b>
        <b>--docker=</b><u>&lt;image&gt;</u>
               Run each task in the Docker container with this name.  The image will  be  obtained  via  "docker
              pull" if it is not already available.

        <b>--docker-tar=</b><u>&lt;tar&gt;</u>
               Run  each  task  in  the Docker container given by this tar file.  The image will be uploaded via
              "docker load" on each execution site.

        <b>--docker-opt=</b><u>&lt;string&gt;</u>
              Specify options to be used in DSingularityocker execution.

   <b>Singularity</b> <b>Support</b>
        <b>--singularity=</b><u>&lt;image&gt;</u>
               Run each task in the Singularity container with this name.  The container will  be  created  from
              the passed in image.

        <b>--singularity-opt=</b><u>&lt;string&gt;</u>
              Specify options to be used in Singularity execution.

   <b>Amazon</b> <b>Options</b>
        <b>--amazon-config=</b><u>&lt;path&gt;</u>
               Path to Amazon EC2 configuration file generated by makeflow_ec2_setup.

   <b>Kubernetes</b> <b>Options</b>
        <b>--k8s-image=</b><u>&lt;docker_image&gt;</u>
               Indicate the Docker image for running pods on Kubernetes cluster.

   <b>Mountfile</b> <b>Support</b>
        <b>--mounts=</b><u>&lt;mountfile&gt;</u>
              Use  this  file  as  a  mountlist. Every line of a mountfile can be used to specify the source and
              target of each input dependency in the format of <b>target</b> <b>source</b>  (Note  there  should  be  a  space
              between target and source.).

        <b>--cache=</b><u>&lt;cache_dir&gt;</u>
              Use this dir as the cache for file dependencies.

   <b>Archiving</b> <b>Options</b>
        <b>--archive=</b><u>&lt;&gt;</u>
              Archive  results of workflow at the specified path (by default /tmp/makeflow.archive.$UID) and use
              outputs of any archived jobs instead of re-executing job

        <b>--archive-dir=</b><u>&lt;path&gt;</u>
              Specify archive base directory.

        <b>--archive-read=</b><u>&lt;path&gt;</u>
              Only check to see if jobs have been cached and use outputs if it has been

        <b>--archive-s3=</b><u>&lt;s3_bucket&gt;</u>
              Base S3 Bucket name

        <b>--archive-s3-no-check=</b><u>&lt;s3_bucket&gt;</u>
              Blind upload files to S3 bucket (No existence check in bucket).

        <b>--s3-hostname=</b><u>&lt;s3_hostname&gt;</u>
              Base S3 hostname. Used for AWS S3.

        <b>--s3-keyid=</b><u>&lt;s3_key_id&gt;</u>
              Access Key for cloud server. Used for AWS S3.

        <b>--s3-secretkey=</b><u>&lt;secret_key&gt;</u>
              Secret Key for cloud server. Used for AWS S3.

   <b>VC3</b> <b>Builder</b> <b>Options</b>
        <b>--vc3-builder</b>
              Enable VC3 Builder

        <b>--vc3-exe=</b><u>&lt;file&gt;</u>
              VC3 Builder executable location

        <b>--vc3-log=</b><u>&lt;file&gt;</u>
              VC3 Builder log name

        <b>--vc3-options=</b><u>&lt;string&gt;</u>
              VC3 Builder option string

   <b>Other</b> <b>Options</b>
        <b>-A,--disable-afs-check</b>
              Disable the check for AFS. (experts only)

        <b>-z,--zero-length-error</b>
              Force failure on zero-length output files.

        <b>-g,--gc=</b><u>&lt;type&gt;</u>
              Enable garbage collection. (ref_cnt|on_demand|all)

        <b>--gc-size=</b><u>&lt;int&gt;</u>
              Set disk size to trigger GC. (on_demand only)

        <b>-G,--gc-count=</b><u>&lt;int&gt;</u>
              Set number of files to trigger GC. (ref_cnt only)

        <b>--wrapper=</b><u>&lt;script&gt;</u>
               Wrap all commands with this <b>script</b>. Each rule's original recipe is appended to <b>script</b> or replaces
              the first occurrence of <b>{}</b> in <b>script</b>.

        <b>--wrapper-input=</b><u>&lt;file&gt;</u>
               Wrapper command requires this input file. This option may be specified more than  once,  defining
              an array of inputs. Additionally, each job executing a recipe has a unique integer identifier that
              replaces occurrences <b>%%</b> in <b>file</b>.

        <b>--wrapper-output=</b><u>&lt;file&gt;</u>
               Wrapper  command requires this output file. This option may be specified more than once, defining
              an array of outputs. Additionally, each job executing a recipe has  a  unique  integer  identifier
              that replaces occurrences <b>%%</b> in <b>file</b>.

        <b>--enforcement</b>
              Use Parrot to restrict access to the given inputs/outputs.

        <b>--parrot-path=</b><u>&lt;path&gt;</u>
              Path to parrot_run executable on the host system.

        <b>--env-replace-path=</b><u>&lt;path&gt;</u>
              Path to env_replace executable on the host system.

        <b>--skip-file-check</b>
              Do not check for file existence before running.

        <b>--do-not-save-failed-output</b>
              Disable saving failed nodes to directory for later analysis.

        <b>--shared-fs=</b><u>&lt;dir&gt;</u>
              Assume the given directory is a shared filesystem accessible at all execution sites.

        <b>-X,--change-directory=</b><u>&lt;dir&gt;</u>
              Change to <u>&lt;dir&gt;</u> prior to executing the workflow.

        <b>-X,--change-directory=</b><u>&lt;dir&gt;</u>
              Change to <u>&lt;dir&gt;</u> prior to executing the workflow.

</pre><h4><b>DRYRUN</b> <b>MODE</b></h4><pre>
       When the batch system is set to <b>-T</b> <u>&lt;dryrun&gt;</u>, Makeflow runs as usual but does not actually execute jobs or
       modify  the  system.  This is useful to check that wrappers and substitutions are applied as expected. In
       addition, Makeflow will write an equivalent shell  script  to  the  batch  system  log  specified  by  <b>-L</b>
       <u>&lt;logfile&gt;</u>.  This  script  will run the commands in serial that Makeflow would have run. This shell script
       format may be useful for archival purposes, since it does not depend on Makeflow.

</pre><h4><b>ENVIRONMENT</b> <b>VARIABLES</b></h4><pre>
       The following environment variables will affect the execution of your <b>Makeflow</b>:

   <b>BATCH_OPTIONS</b>
       This corresponds to the <b>-B</b> <u>&lt;options&gt;</u> parameter and will  pass  extra  batch  options  to  the  underlying
       execution engine.

   <b>MAKEFLOW_MAX_LOCAL_JOBS</b>
       This  corresponds  to  the <b>-j</b> <u>&lt;#&gt;</u> parameter and will set the maximum number of local batch jobs.  If a <b>-j</b>
       <u>&lt;#&gt;</u> parameter is specified, the minimum of the argument and the environment variable is used.

   <b>MAKEFLOW_MAX_REMOTE_JOBS</b>
       This corresponds to the <b>-J</b> <u>&lt;#&gt;</u> parameter and will set the maximum number of local batch jobs.   If  a  <b>-J</b>
       <u>&lt;#&gt;</u> parameter is specified, the minimum of the argument and the environment variable is used.

       Note that variables defined in your <b>Makeflow</b> are exported to the environment.

   <b>TCP_LOW_PORT</b>
       Inclusive low port in range used with -p 0.

   <b>TCP_HIGH_PORT</b>
       Inclusive high port in range used with -p 0.

</pre><h4><b>EXIT</b> <b>STATUS</b></h4><pre>
       On success, returns zero.  On failure, returns non-zero.

</pre><h4><b>EXAMPLES</b></h4><pre>
       Run makeflow locally with debugging:

               makeflow -d all Makeflow

       Run makeflow on Condor will special requirements:

               makeflow -T condor -B "requirements = MachineGroup == 'ccl'" Makeflow

       Run makeflow with WorkQueue using named workers:

               makeflow -T wq -a -M project.name Makeflow

       Create a directory containing all of the dependencies required to run the specified makeflow

               makeflow -b bundle Makeflow

</pre><h4><b>COPYRIGHT</b></h4><pre>
       The  Cooperative  Computing  Tools are Copyright (C) 2022 The University of Notre Dame.  This software is
       distributed under the GNU General Public License.  See the file COPYING for details.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       •   <b>Cooperative</b> <b>Computing</b> <b>Tools</b> <b>Documentation</b>

       •   <b>Makeflow</b> <b>User</b> <b>Manual</b>

       •   <b><a href="../man1/makeflow.1.html">makeflow</a>(1)</b> <b><a href="../man1/makeflow_monitor.1.html">makeflow_monitor</a>(1)</b> <b><a href="../man1/makeflow_analyze.1.html">makeflow_analyze</a>(1)</b> <b><a href="../man1/makeflow_viz.1.html">makeflow_viz</a>(1)</b>  <b><a href="../man1/makeflow_graph_log.1.html">makeflow_graph_log</a>(1)</b>  <b><a href="../man1/starch.1.html">starch</a>(1)</b>
           <b><a href="../man1/makeflow_ec2_setup.1.html">makeflow_ec2_setup</a>(1)</b> <b><a href="../man1/makeflow_ec2_cleanup.1.html">makeflow_ec2_cleanup</a>(1)</b>

CCTools 7.14.5 FINAL                                                                                 <u><a href="../man1/makeflow.1.html">makeflow</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>