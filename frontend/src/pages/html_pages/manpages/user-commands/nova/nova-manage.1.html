<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>nova-manage - Management tool for the OpenStack Compute services.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/nova-common">nova-common_31.0.0-0ubuntu1_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       nova-manage - Management tool for the OpenStack Compute services.

</pre><h4><b>SYNOPSIS</b></h4><pre>
          nova-manage &lt;category&gt; [&lt;action&gt; [&lt;options&gt;...]]

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>nova-manage</b> controls cloud computing instances by managing various admin-only aspects of Nova.

       The standard pattern for executing a <b>nova-manage</b> command is:

          nova-manage &lt;category&gt; &lt;command&gt; [&lt;args&gt;]

       Run without arguments to see a list of available command categories:

          nova-manage

       You can also run with a category argument such as <b>db</b> to see a list of all commands in that category:

          nova-manage db

</pre><h4><b>OPTIONS</b></h4><pre>
       These  options  apply to all commands and may be given in any order, before or after commands. Individual
       commands may provide additional options. Options without an argument can be combined after a single dash.

       <b>-h,</b> <b>--help</b>
              Show a help message and exit

       <b>--config-dir</b> <b>&lt;dir&gt;</b>
              Path to a config directory to pull <b>*.conf</b> files from. This file set is sorted, so as to provide  a
              predictable parse order if individual options are over-ridden. The set is parsed after the file(s)
              specified  via  previous  <u>--config-file</u>, arguments hence over-ridden options in the directory take
              precedence.  This option must be set from the command-line.

       <b>--config-file</b> <b>&lt;path&gt;</b>
              Path to a config file to use. Multiple config files can be specified, with values in  later  files
              taking precedence. Defaults to None. This option must be set from the command-line.

       <b>--log-config-append</b> <b>&lt;path&gt;,</b> <b>--log-config</b> <b>&lt;path&gt;,</b> <b>--log_config</b> <b>&lt;path&gt;</b>
              The  name  of  a  logging  configuration  file.  This  file  is  appended  to any existing logging
              configuration files.  For details about logging configuration files, see the Python logging module
              documentation. Note that when logging configuration files are used then all logging  configuration
              is set in the configuration file and other logging configuration options are ignored (for example,
              <u>--log-date-format</u>).

       <b>--log-date-format</b> <b>&lt;format&gt;</b>
              Defines the format string for <b>%(asctime)s</b> in log records. Default: None. This option is ignored if
              <u>--log-config-append</u> is set.

       <b>--log-dir</b> <b>&lt;dir&gt;,</b> <b>--logdir</b> <b>&lt;dir&gt;</b>
              The   base   directory   used   for   relative   log_file   paths.   This  option  is  ignored  if
              <u>--log-config-append</u> is set.

       <b>--log-file</b> <b>PATH,</b> <b>--logfile</b> <b>&lt;path&gt;</b>
              Name of log file to send logging output to.  If no default is set, logging will go  to  stderr  as
              defined by use_stderr.  This option is ignored if <u>--log-config-append</u> is set.

       <b>--syslog-log-facility</b> <b>SYSLOG_LOG_FACILITY</b>
              Syslog facility to receive log lines.  This option is ignored if <u>--log-config-append</u> is set.

       <b>--use-journal</b>
              Enable  journald  for  logging. If running in a systemd environment you may wish to enable journal
              support.  Doing so will use the journal native protocol  which  includes  structured  metadata  in
              addition to log messages. This option is ignored if <u>--log-config-append</u> is set.

       <b>--nouse-journal</b>
              The inverse of <u>--use-journal</u>.

       <b>--use-json</b>
              Use JSON formatting for logging. This option is ignored if <u>--log-config-append</u> is set.

       <b>--nouse-json</b>
              The inverse of <u>--use-json</u>.

       <b>--use-syslog</b>
              Use  syslog  for  logging. Existing syslog format is DEPRECATED and will be changed later to honor
              RFC5424.  This option is ignored if <u>--log-config-append</u> is set.

       <b>--nouse-syslog</b>
              The inverse of <u>--use-syslog</u>.

       <b>--watch-log-file</b>
              Uses logging handler designed to watch file system.  When  log  file  is  moved  or  removed  this
              handler  will  open  a  new  log  file with specified path instantaneously. It makes sense only if
              <u>--log-file</u>  option  is  specified  and  Linux  platform  is  used.  This  option  is  ignored   if
              <u>--log-config-append</u> is set.

       <b>--nowatch-log-file</b>
              The inverse of <u>--watch-log-file</u>.

       <b>--debug,</b> <b>-d</b>
              If enabled, the logging level will be set to <b>DEBUG</b> instead of the default <b>INFO</b> level.

       <b>--nodebug</b>
              The inverse of <u>--debug</u>.

       <b>--post-mortem</b>
              Allow post-mortem debugging.

       <b>--nopost-mortem</b>
              The inverse of <u>--post-mortem</u>.

       <b>--version</b>
              Show program's version number and exit

</pre><h4><b>DATABASE</b> <b>COMMANDS</b></h4><pre>
   <b>db</b> <b>version</b>
          nova-manage db version

       Print the current main database version.

   <b>db</b> <b>sync</b>
          nova-manage db sync [--local_cell] [VERSION]

       Upgrade  the main database schema up to the most recent version or <b>VERSION</b> if specified. By default, this
       command will also attempt to upgrade the schema for the cell0 database if it is mapped.  If  <u>--local_cell</u>
       is  specified, then only the main database in the current cell is upgraded. The local database connection
       is determined by  <u>database.connection</u>  in  the  configuration  file,  passed  to  nova-manage  using  the
       <b>--config-file</b> option(s).

       Refer  to  the <b>nova-manage</b> <b>cells_v2</b> <b>map_cell0</b> or <b>nova-manage</b> <b>cells_v2</b> <b>simple_cell_setup</b> commands for more
       details on mapping the cell0 database.

       This command should be run <b>after</b> <b>nova-manage</b> <b>api_db</b> <b>sync</b>.

       Options

       <b>--local_cell</b>
              Only sync db in the local cell: do not attempt to fan-out to all cells.

       Return codes
                                ┌─────────────┬──────────────────────────────────────┐
                                │ Return code │ Description                          │
                                ├─────────────┼──────────────────────────────────────┤
                                │ 0           │ Successfully synced database schema. │
                                ├─────────────┼──────────────────────────────────────┤
                                │ 1           │ Failed to access cell0.              │
                                └─────────────┴──────────────────────────────────────┘

       Changed in version 20.0.0: (Train)

       Removed support for the legacy <b>--version</b> <b>&lt;version&gt;</b> argument.

       Changed in version 24.0.0: (Xena)

       Migrated versioning engine  to  alembic.  The  optional  <b>VERSION</b>  argument  is  now  expected  to  be  an
       alembic-based version. sqlalchemy-migrate-based versions will be rejected.

   <b>db</b> <b>archive_deleted_rows</b>
          nova-manage db archive_deleted_rows [--max_rows &lt;rows&gt;] [--verbose]
            [--until-complete] [--before &lt;date&gt;] [--purge] [--all-cells] [--task-log]
            [--sleep]

       Move  deleted  rows  from  production  tables  to  shadow tables. Note that the corresponding rows in the
       <b>instance_mappings</b>, <b>request_specs</b> and <b>instance_group_member</b> tables of the API  database  are  purged  when
       instance records are archived and thus, <u>api_database.connection</u> is required in the config file.

       If  automating,  this  should  be  run  continuously  while  the  result  is 1, stopping at 0, or use the
       <u>--until-complete</u> option.

       Changed in version 24.0.0: (Xena)

       Added <u>--task-log</u>, <u>--sleep</u> options.

       Options

       <b>--max_rows</b> <b>&lt;rows&gt;</b>
              Maximum number of deleted rows to archive. Defaults to  1000.  Note  that  this  number  does  not
              include  the  corresponding  rows,  if  any,  that  are  removed from the API database for deleted
              instances.

       <b>--before</b> <b>&lt;date&gt;</b>
              Archive rows that have been deleted before <b>&lt;date&gt;</b>. Accepts date  strings  in  the  default  format
              output by the <b>date</b> command, as well as <b>YYYY-MM-DD[HH:mm:ss]</b>. For example:

                 # Purge shadow table rows older than a specific date
                 nova-manage db archive_deleted_rows --before 2015-10-21
                 # or
                 nova-manage db archive_deleted_rows --before "Oct 21 2015"
                 # Times are also accepted
                 nova-manage db archive_deleted_rows --before "2015-10-21 12:00"

              Note  that  relative dates (such as <b>yesterday</b>) are not supported natively. The <b>date</b> command can be
              helpful here:

                 # Archive deleted rows more than one month old
                 nova-manage db archive_deleted_rows --before "$(date -d 'now - 1 month')"

       <b>--verbose</b>
              Print how many rows were archived per table.

       <b>--until-complete</b>
              Run continuously until all deleted rows are archived.  Use <u>--max_rows</u> as a  batch  size  for  each
              iteration.

       <b>--purge</b>
              Purge all data from shadow tables after archive completes.

       <b>--all-cells</b>
              Run command across all cells.

       <b>--task-log</b>
              Also  archive  <b>task_log</b>  table records. Note that <b>task_log</b> records are never deleted, so archiving
              them will move all of the <b>task_log</b> records up to now into the shadow tables. It is recommended  to
              also specify the <u>--before</u> option to avoid races for those consuming <b>task_log</b> record data via the ‐
              <u>/os-instance_usage_audit_log</u> API (example: Telemetry).

       <b>--sleep</b>
              The  amount of time in seconds to sleep between batches when <u>--until-complete</u> is used. Defaults to
              0.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Nothing was archived.                 │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ Some number of rows were archived.    │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Invalid value for <u>--max_rows</u>.         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ No connection  to  the  API  database │
                               │             │ could     be     established    using │
                               │             │ <u>api_database.connection</u>.              │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ Invalid value for <u>--before</u>.           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 255         │ An unexpected error occurred.         │
                               └─────────────┴───────────────────────────────────────┘

   <b>db</b> <b>purge</b>
          nova-manage db purge [--all] [--before &lt;date&gt;] [--verbose] [--all-cells]

       Delete rows from shadow tables. For <u>--all-cells</u> to work, the API database connection information must  be
       configured.

       Added in version 18.0.0: (Rocky)

       Options

       <b>--all</b>  Purge all rows in the shadow tables.

       <b>--before</b> <b>&lt;date&gt;</b>
              Delete  archived  rows  that  were  deleted  from Nova before <b>&lt;date&gt;</b>.  Accepts date strings in the
              default format output by the <b>date</b> command, as well as <b>YYYY-MM-DD[HH:mm:ss]</b>. For example:

                 # Purge shadow table rows deleted before specified date
                 nova-manage db purge --before 2015-10-21
                 # or
                 nova-manage db purge --before "Oct 21 2015"
                 # Times are also accepted
                 nova-manage db purge --before "2015-10-21 12:00"

              Note that relative dates (such as <b>yesterday</b>) are not supported natively. The <b>date</b> command  can  be
              helpful here:

                 # Archive deleted rows more than one month old
                 nova-manage db purge --before "$(date -d 'now - 1 month')"

       <b>--verbose</b>
              Print information about purged records.

       <b>--all-cells</b>
              Run against all cell databases.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Rows were deleted.                    │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ Required arguments were not provided. │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Invalid value for <u>--before</u>.           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Nothing was purged.                   │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ No  connection  to  the  API database │
                               │             │ could    be     established     using │
                               │             │ <u>api_database.connection</u>.              │
                               └─────────────┴───────────────────────────────────────┘

   <b>db</b> <b>online_data_migrations</b>
          nova-manage db online_data_migrations [--max-count &lt;count&gt;]

       Perform data migration to update all live data.

       This  command should be called after upgrading database schema and nova services on all controller nodes.
       If it exits with partial updates (exit status 1)  it  should  be  called  again,  even  if  some  updates
       initially  generated errors, because some updates may depend on others having completed. If it exits with
       status 2, intervention is required to resolve the issue causing remaining updates to fail. It  should  be
       considered successfully completed only when the exit status is 0.

       For example:

          $ nova-manage db online_data_migrations
          Running batches of 50 until complete
          2 rows matched query migrate_instances_add_request_spec, 0 migrated
          2 rows matched query populate_queued_for_delete, 2 migrated
          +---------------------------------------------+--------------+-----------+
          |                  Migration                  | Total Needed | Completed |
          +---------------------------------------------+--------------+-----------+
          |         create_incomplete_consumers         |      0       |     0     |
          |      migrate_instances_add_request_spec     |      2       |     0     |
          |       migrate_quota_classes_to_api_db       |      0       |     0     |
          |        migrate_quota_limits_to_api_db       |      0       |     0     |
          |          migration_migrate_to_uuid          |      0       |     0     |
          |     populate_missing_availability_zones     |      0       |     0     |
          |          populate_queued_for_delete         |      2       |     2     |
          |                populate_uuids               |      0       |     0     |
          +---------------------------------------------+--------------+-----------+

       In  the  above  example, the <b>migrate_instances_add_request_spec</b> migration found two candidate records but
       did not need  to  perform  any  kind  of  data  migration  for  either  of  them.  In  the  case  of  the
       <b>populate_queued_for_delete</b>  migration,  two  candidate  records  were  found  which  did  require  a data
       migration. Since <u>--max-count</u> defaults to 50 and only two records were migrated with  no  more  candidates
       remaining, the command completed successfully with exit code 0.

       Added in version 13.0.0: (Mitaka)

       Options

       <b>--max-count</b> <b>&lt;count&gt;</b>
              Controls  the  maximum  number  of objects to migrate in a given call. If not specified, migration
              will occur in batches of 50 until fully complete.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ No (further) updates are possible.    │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ Some    updates    were     completed │
                               │             │ successfully.   Note   that  not  all │
                               │             │ updates may have succeeded.           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Some updates generated errors and  no │
                               │             │ other  migrations  were  able to take │
                               │             │ effect in the last batch attempted.   │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 127         │ Invalid input was provided.           │
                               └─────────────┴───────────────────────────────────────┘

   <b>db</b> <b>ironic_compute_node_move</b>
          nova-manage db ironic_compute_node_move --ironic-node-uuid &lt;uuid&gt; --destination-host &lt;host&gt;

       Move ironic nodes, along with any associated instances, between nova-compute services.

       This is useful when migrating away from using peer_list and  multiple  hash  ring  balanced  nova-compute
       servers to the new ironic shard system.

       First  you must turn off the nova-compute service that currently manages the Ironic host. Second you mark
       that nova-compute service as forced down via the Nova API. Third, you ensure the new nova-compute service
       is correctly configured to target the appropriate shard (and optionally also a conductor group). Finally,
       most Ironic nodes should now move to the new service, but any Ironic nodes with instances  on  them  will
       need to be manually moved to their new Ironic service by using this nova-manage command.

       Added in version 28.0.0: (2023.2 Bobcat)

       Options

       <b>--ironic-node-uuid</b> <b>&lt;uuid&gt;</b>
              Ironic  node  uuid  to  be  moved  (which  is  also  the  Nova  compute  node uuid and the uuid of
              corresponding resource provider in Placement).

              The Nova compute service that currently manages this Ironic node must first be  marked  a  "forced
              down"  via  the  Nova  API,  in  a  similar way to a down hypervisor that is about to have its VMs
              evacuated to a replacement hypervisor.

       <b>--destination-host</b> <b>&lt;host&gt;</b>
              Destination ironic nova-compute service CONF.host.

</pre><h4><b>API</b> <b>DATABASE</b> <b>COMMANDS</b></h4><pre>
   <b>api_db</b> <b>version</b>
          nova-manage api_db version

       Print the current API database version.

       Added in version 2015.1.0: (Kilo)

   <b>api_db</b> <b>sync</b>
          nova-manage api_db sync [VERSION]

       Upgrade the API database schema up to the most recent version or <b>VERSION</b> if specified. This command  does
       not  create the API database, it runs schema migration scripts. The API database connection is determined
       by <u>api_database.connection</u> in the configuration file passed to nova-manage.

       This command should be run before <b>nova-manage</b> <b>db</b> <b>sync</b>.

       Added in version 2015.1.0: (Kilo)

       Changed in version 18.0.0: (Rocky)

       Added support for  upgrading  the  optional  placement  database  if  <b>[placement_database]/connection</b>  is
       configured.

       Changed in version 20.0.0: (Train)

       Removed support for upgrading the optional placement database as placement is now a separate project.

       Removed support for the legacy <b>--version</b> <b>&lt;version&gt;</b> argument.

       Changed in version 24.0.0: (Xena)

       Migrated  versioning  engine  to  alembic.  The  optional  <b>VERSION</b>  argument  is  now  expected  to be an
       alembic-based version. sqlalchemy-migrate-based versions will be rejected.

</pre><h4><b>CELLS</b> <b>V2</b> <b>COMMANDS</b></h4><pre>
   <b>cell_v2</b> <b>simple_cell_setup</b>
          nova-manage cell_v2 simple_cell_setup [--transport-url &lt;transport_url&gt;]

       Setup a fresh cells v2 environment. If <u>--transport-url</u> is not specified, it will use the one  defined  by
       <u>transport_url</u> in the configuration file.

       Added in version 14.0.0: (Newton)

       Options

       <b>--transport-url</b> <b>&lt;transport_url&gt;</b>
              The transport url for the cell message queue.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Setup is completed.                   │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ No  hosts are reporting, meaning none │
                               │             │ can be mapped, or  if  the  transport │
                               │             │ URL is missing or invalid.            │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>map_cell0</b>
          nova-manage cell_v2 map_cell0 [--database_connection &lt;database_connection&gt;]

       Create a cell mapping to the database connection for the cell0 database.  If a database_connection is not
       specified,  it  will  use  the  one  defined  by  <u>database.connection</u> in the configuration file passed to
       nova-manage. The cell0 database is used for instances that have not been  scheduled  to  any  cell.  This
       generally applies to instances that have encountered an error before they have been scheduled.

       Added in version 14.0.0: (Newton)

       Options

       <b>--database_connection</b> <b>&lt;database_connection&gt;</b>
              The  database  connection  URL  for  <b>cell0</b>. This is optional. If not provided, a standard database
              connection will be used based on the main database connection from nova configuration.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ <b>cell0</b> is created successfully or  has │
                               │             │ already been set up.                  │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>map_instances</b>
          nova-manage cell_v2 map_instances --cell_uuid &lt;cell_uuid&gt;
            [--max-count &lt;max_count&gt;] [--reset]

       Map  instances to the provided cell. Instances in the nova database will be queried from oldest to newest
       and mapped to the provided cell.  A <u>--max-count</u> can be set on the number of instance to map in  a  single
       run.  Repeated  runs of the command will start from where the last run finished so it is not necessary to
       increase <u>--max-count</u> to finish.  A <u>--reset</u> option can be passed which will reset the marker, thus  making
       the  command  start from the beginning as opposed to the default behavior of starting from where the last
       run finished.

       If <u>--max-count</u> is not specified, all instances in the cell will be mapped in batches of 50. If you have a
       large number of instances, consider specifying a custom value and run the command until it exits with 0.

       Added in version 12.0.0: (Liberty)

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              Unmigrated instances will be mapped to the cell with the UUID provided.

       <b>--max-count</b> <b>&lt;max_count&gt;</b>
              Maximum number of instances to map. If not set, all instances  in  the  cell  will  be  mapped  in
              batches of 50. If you have a large number of instances, consider specifying a custom value and run
              the command until it exits with 0.

       <b>--reset</b>
              The  command  will  start  from  the beginning as opposed to the default behavior of starting from
              where the last run finished.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ All instances have been mapped.       │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ There  are  still  instances  to   be │
                               │             │ mapped.                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 127         │ Invalid value for <u>--max-count</u>.        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 255         │ An unexpected error occurred.         │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>map_cell_and_hosts</b>
          nova-manage cell_v2 map_cell_and_hosts [--name &lt;cell_name&gt;]
            [--transport-url &lt;transport_url&gt;] [--verbose]

       Create  a  cell mapping to the database connection and message queue transport URL, and map hosts to that
       cell. The database connection comes from the <u>database.connection</u> defined in the configuration file passed
       to nova-manage. If <u>--transport-url</u> is not specified, it will use the one defined by <u>transport_url</u> in  the
       configuration  file.  This command is idempotent (can be run multiple times), and the verbose option will
       print out the resulting cell mapping UUID.

       Added in version 13.0.0: (Mitaka)

       Options

       <b>--transport-url</b> <b>&lt;transport_url&gt;</b>
              The transport url for the cell message queue.

       <b>--name</b> <b>&lt;cell_name&gt;</b>
              The name of the cell.

       <b>--verbose</b>
              Output the cell mapping uuid for any newly mapped hosts.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Successful completion.                │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ The  transport  url  is  missing   or │
                               │             │ invalid                               │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>verify_instance</b>
          nova-manage cell_v2 verify_instance --uuid &lt;instance_uuid&gt; [--quiet]

       Verify  instance  mapping  to  a cell. This command is useful to determine if the cells v2 environment is
       properly setup, specifically in terms of the cell, host, and instance mapping records required.

       Added in version 14.0.0: (Newton)

       Options

       <b>--uuid</b> <b>&lt;instance_uuid&gt;</b>
              The instance UUID to verify.

       <b>--quiet</b>
              Do not print anything.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ The instance was successfully  mapped │
                               │             │ to a cell.                            │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ The instance is not mapped to a cell. │
                               │             │ See the <b>map_instances</b> command.        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ The  cell mapping is missing. See the │
                               │             │ <b>map_cell_and_hots</b> command if you  are │
                               │             │ upgrading    from    a    cells    v1 │
                               │             │ environment,         and          the │
                               │             │ <b>simple_cell_setup</b>  command if you are │
                               │             │ upgrading   from   a   non-cells   v1 │
                               │             │ environment.                          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ The  instance  is  a deleted instance │
                               │             │ that still has an instance mapping.   │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ The instance is an archived  instance │
                               │             │ that still has an instance mapping.   │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>create_cell</b>
          nova-manage cell_v2 create_cell [--name &lt;cell_name&gt;]
            [--transport-url &lt;transport_url&gt;]
            [--database_connection &lt;database_connection&gt;] [--verbose] [--disabled]

       Create   a   cell   mapping   to   the  database  connection  and  message  queue  transport  URL.  If  a
       database_connection is not specified,  it  will  use  the  one  defined  by  <u>database.connection</u>  in  the
       configuration  file  passed  to  nova-manage.  If  <u>--transport-url</u>  is not specified, it will use the one
       defined by <u>transport_url</u> in the configuration file. The verbose option will print out the resulting  cell
       mapping  UUID.  All  the  cells created are by default enabled. However passing the <u>--disabled</u> option can
       create a pre-disabled cell, meaning no scheduling will happen to this cell.

       Added in version 15.0.0: (Ocata)

       Changed in version 18.0.0: (Rocky)

       Added <u>--disabled</u> option.

       Options

       <b>--name</b> <b>&lt;cell_name&gt;</b>
              The name of the cell.

       <b>--database_connection</b> <b>&lt;database_connection&gt;</b>
              The database URL for the cell database.

       <b>--transport-url</b> <b>&lt;transport_url&gt;</b>
              The transport url for the cell message queue.

       <b>--verbose</b>
              Output the UUID of the created cell.

       <b>--disabled</b>
              Create a pre-disabled cell.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ The  cell  mapping  was  successfully │
                               │             │ created.                              │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ The   transport   URL   or   database │
                               │             │ connection was missing or invalid.    │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Another cell  is  already  using  the │
                               │             │ provided    transport    URL   and/or │
                               │             │ database connection combination.      │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>discover_hosts</b>
          nova-manage cell_v2 discover_hosts [--cell_uuid &lt;cell_uuid&gt;] [--verbose]
            [--strict] [--by-service]

       Searches cells, or a single cell, and maps found hosts. This command will check  the  database  for  each
       cell  (or  a  single  one  if  passed  in) and map any hosts which are not currently mapped. If a host is
       already mapped, nothing will be done. You need to re-run this command  each  time  you  add  a  batch  of
       compute  hosts  to  a cell (otherwise the scheduler will never place instances there and the API will not
       list the new hosts). If <u>--strict</u> is specified, the command will only return 0 if  an  unmapped  host  was
       discovered  and  mapped  successfully.  If  <u>--by-service</u>  is  specified,  this  command  will look in the
       appropriate cell(s) for any nova-compute services and ensure there are host mappings for  them.  This  is
       less  efficient  and  is  only  necessary  when using compute drivers that may manage zero or more actual
       compute nodes at any given time (currently only ironic).

       This command should be run once after all compute hosts have been deployed  and  should  not  be  run  in
       parallel. When run in parallel, the commands will collide with each other trying to map the same hosts in
       the database at the same time.

       Added in version 14.0.0: (Newton)

       Changed in version 16.0.0: (Pike)

       Added <u>--strict</u> option.

       Changed in version 18.0.0: (Rocky)

       Added <u>--by-service</u> option.

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              If provided only this cell will be searched for new hosts to map.

       <b>--verbose</b>
              Provide detailed output when discovering hosts.

       <b>--strict</b>
              Considered  successful  (exit  code 0) only when an unmapped host is discovered. Any other outcome
              will be considered a failure (non-zero exit code).

       <b>--by-service</b>
              Discover hosts by service instead of compute node.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Hosts were successfully mapped or  no │
                               │             │ hosts   needed   to   be  mapped.  If │
                               │             │ <u>--strict</u> is specified, returns 0 only │
                               │             │ if an unmapped  host  was  discovered │
                               │             │ and mapped.                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ If   <u>--strict</u>  is  specified  and  no │
                               │             │ unmapped  hosts  were  found.    Also │
                               │             │ returns  1 if an exception was raised │
                               │             │ while running.                        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ The command was aborted because of  a │
                               │             │ duplicate  host  mapping  found. This │
                               │             │ means  the  command   collided   with │
                               │             │ another     running    <b>discover_hosts</b> │
                               │             │ command or  scheduler  periodic  task │
                               │             │ and is safe to retry.                 │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>list_cells</b>
          nova-manage cell_v2 list_cells [--verbose]

       By  default the cell name, UUID, disabled state, masked transport URL and database connection details are
       shown. Use the <u>--verbose</u> option to see  transport  URL  and  database  connection  with  their  sensitive
       details.

       Added in version 15.0.0: (Ocata)

       Changed in version 18.0.0: (Rocky)

       Added the <b>disabled</b> column to output.

       Options

       <b>--verbose</b>
              Show sensitive details, such as passwords.

       Return codes
                                            ┌─────────────┬─────────────┐
                                            │ Return code │ Description │
                                            ├─────────────┼─────────────┤
                                            │ 0           │ Success.    │
                                            └─────────────┴─────────────┘

   <b>cell_v2</b> <b>delete_cell</b>
          nova-manage cell_v2 delete_cell [--force] --cell_uuid &lt;cell_uuid&gt;

       Delete a cell by the given UUID.

       Added in version 15.0.0: (Ocata)

       Options

       <b>--force</b>
              Delete hosts and instance_mappings that belong to the cell as well.

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              The UUID of the cell to delete.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ An  empty  cell was found and deleted │
                               │             │ successfully or a cell that has hosts │
                               │             │ was found and the cell, hosts and the │
                               │             │ instance_mappings    were     deleted │
                               │             │ successfully   with   <u>--force</u>  option │
                               │             │ (this happens if there are no  living │
                               │             │ instances).                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ A  cell  with the provided UUID could │
                               │             │ not be found.                         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Host  mappings  were  found  for  the │
                               │             │ cell,  meaning the cell is not empty, │
                               │             │ and  the  <u>--force</u>  option   was   not │
                               │             │ provided.                             │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ There  are active instances mapped to │
                               │             │ the cell (cell not empty).            │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ There are (inactive) instances mapped │
                               │             │ to the cell and  the  <u>--force</u>  option │
                               │             │ was not provided.                     │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>list_hosts</b>
          nova-manage cell_v2 list_hosts [--cell_uuid &lt;cell_uuid&gt;]

       Lists  the hosts in one or all v2 cells. By default hosts in all v2 cells are listed. Use the <u>--cell_uuid</u>
       option to list hosts in a specific cell.

       Added in version 17.0.0: (Queens)

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              The UUID of the cell.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Success.                              │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ The cell indicated by <u>--cell_uuid</u> was │
                               │             │ not found.                            │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>update_cell</b>
          nova-manage cell_v2 update_cell --cell_uuid &lt;cell_uuid&gt;
            [--name &lt;cell_name&gt;] [--transport-url &lt;transport_url&gt;]
            [--database_connection &lt;database_connection&gt;] [--disable] [--enable]

       Updates the properties of a cell by the given uuid. If a database_connection is not  specified,  it  will
       attempt  to  use  the one defined by <u>database.connection</u> in the configuration file. If a transport_url is
       not specified, it will attempt to use the one defined by <u>transport_url</u> in the configuration file.

       <b>NOTE:</b>
          Updating the <b>transport_url</b> or <b>database_connection</b> fields on a running system will NOT  result  in  all
          nodes immediately using the new values.  Use caution when changing these values.

          The  scheduler will not notice that a cell has been enabled/disabled until it is restarted or sent the
          SIGHUP signal.

       Added in version 16.0.0: (Pike)

       Changed in version 18.0.0: (Rocky)

       Added <u>--enable</u>, <u>--disable</u> options.

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              The UUID of the cell to update.

       <b>--name</b> <b>&lt;cell_name&gt;</b>
              Set the cell name.

       <b>--transport-url</b> <b>&lt;transport_url&gt;</b>
              Set the cell <b>transport_url</b>. Note that running nodes will not see the change until restarted or the
              <b>SIGHUP</b> signal is sent.

       <b>--database_connection</b> <b>&lt;database_connection&gt;</b>
              Set the cell <b>database_connection</b>. Note that running nodes will not see the change until  restarted
              or the <b>SIGHUP</b> signal is sent.

       <b>--disable</b>
              Disables  the  cell. Note that the scheduling will be blocked to this cell until it is enabled and
              the <b>nova-scheduler</b> service is restarted or the <b>SIGHUP</b> signal is sent.

       <b>--enable</b>
              Enables the cell. Note that the <b>nova-scheduler</b> service  will  not  see  the  change  until  it  is
              restarted or the <b>SIGHUP</b> signal is sent.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Success.                              │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ The   cell   was  not  found  by  the │
                               │             │ provided UUID.                        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ The specified properties could not be │
                               │             │ set.                                  │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ The provided  <u>--transport-url</u>  or/and │
                               │             │ <u>--database_connection</u> parameters were │
                               │             │ same as another cell.                 │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ An  attempt  was  made to disable and │
                               │             │ enable a cell at the same time.       │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 5           │ An attempt was  made  to  disable  or │
                               │             │ enable cell0.                         │
                               └─────────────┴───────────────────────────────────────┘

   <b>cell_v2</b> <b>delete_host</b>
          nova-manage cell_v2 delete_host --cell_uuid &lt;cell_uuid&gt; --host &lt;host&gt;

       Delete a host by the given host name and the given cell UUID.

       Added in version 17.0.0: (Queens)

       <b>NOTE:</b>
          The  scheduler  caches host-to-cell mapping information so when deleting a host the scheduler may need
          to be restarted or sent the SIGHUP signal.

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              The UUID of the cell.

       <b>--host</b> <b>&lt;host&gt;</b>
              The host to delete.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ The empty host was found and  deleted │
                               │             │ successfully                          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ A  cell with the specified UUID could │
                               │             │ not be found.                         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ A host with the specified name  could │
                               │             │ not be found                          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ The  host  with the specified name is │
                               │             │ not in  a  cell  with  the  specified │
                               │             │ UUID.                                 │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ The  host with the specified name has │
                               │             │ instances (host not empty).           │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>PLACEMENT</b> <b>COMMANDS</b></h4><pre>
   <b>placement</b> <b>heal_allocations</b>
          nova-manage placement heal_allocations [--max-count &lt;max_count&gt;]
            [--verbose] [--skip-port-allocations] [--dry-run]
            [--instance &lt;instance_uuid&gt;] [--cell &lt;cell_uuid] [--force]

       Iterates over non-cell0 cells looking for instances which  do  not  have  allocations  in  the  Placement
       service  and  which  are not undergoing a task state transition. For each instance found, allocations are
       created against the compute node resource provider for that instance based on the flavor associated  with
       the instance.

       <b>NOTE:</b>
          Nested  allocations  are  only partially supported. Nested allocations due to Neutron ports having QoS
          policies are supported since 20.0.0 (Train) release. But nested allocations  due  to  vGPU  or  Cyborg
          device  profile requests in the flavor are not supported. Also if you are using provider.yaml files on
          compute hosts to define additional resources,  if  those  resources  are  defined  on  child  resource
          providers then instances using such resources are not supported.

       Also  if  the  instance  has  any port attached that has resource request (e.g. <u>Quality</u> <u>of</u> <u>Service</u> <u>(QoS):</u>
       <u>Guaranteed</u> <u>Bandwidth</u>) but the corresponding allocation is  not  found  then  the  allocation  is  created
       against  the  network  device  resource  providers  according to the resource request of that port. It is
       possible that the missing allocation cannot be created either due to not having enough resource inventory
       on the host the instance resides on or because more than one resource provider could fulfill the request.
       In this case the instance needs to be manually deleted or the port needs to  be  detached.  When  nova  ‐
       <u>supports</u> <u>migrating</u> <u>instances</u> <u>with</u> <u>guaranteed</u> <u>bandwidth</u> <u>ports</u>, migration will heal missing allocations for
       these instances.

       Before  the allocations for the ports are persisted in placement nova-manage tries to update each port in
       neutron to refer to the resource provider UUID which provides the requested resources. If any of the port
       updates fail in neutron or the allocation update fails in placement the command tries to  roll  back  the
       partial  updates  to  the  ports.  If the roll back fails then the process stops with exit code <b>7</b> and the
       admin needs to do the rollback in neutron manually according to the description in the exit code section.

       There is also a special case handled for instances that <u>do</u> have allocations created before Placement  API
       microversion  1.8  where project_id and user_id values were required. For those types of allocations, the
       project_id and user_id are updated using the values from the instance.

       This command requires that the <u>api_database.connection</u>  and  <u>placement</u>  configuration  options  are  set.
       Placement API &gt;= 1.28 is required.

       Added in version 18.0.0: (Rocky)

       Changed in version 20.0.0: (Train)

       Added <u>--dry-run</u>, <u>--instance</u>, and <u>--skip-port-allocations</u> options.

       Changed in version 21.0.0: (Ussuri)

       Added <u>--cell</u> option.

       Changed in version 22.0.0: (Victoria)

       Added <u>--force</u> option.

       Changed in version 25.0.0: (Yoga)

       Added  support  for  healing  port  allocations  if port-resource-request-groups neutron API extension is
       enabled and therefore ports can request multiple group of resources e.g. by using both guaranteed minimum
       bandwidth and guaranteed minimum packet rate QoS policy rules.

       Options

       <b>--max-count</b> <b>&lt;max_count&gt;</b>
              Maximum number of instances to process. If not specified, all  instances  in  each  cell  will  be
              mapped  in  batches  of  50. If you have a large number of instances, consider specifying a custom
              value and run the command until it exits with 0 or 4.

       <b>--verbose</b>
              Provide verbose output during execution.

       <b>--dry-run</b>
              Runs the command and prints output but does not commit any changes. The return code should be 4.

       <b>--instance</b> <b>&lt;instance_uuid&gt;</b>
              UUID of a specific instance to process. If specified <u>--max-count</u> has no effect. Mutually exclusive
              with <u>--cell</u>.

       <b>--skip-port-allocations</b>
              Skip the healing of the resource allocations of  bound  ports.  E.g.  healing  bandwidth  resource
              allocation  for  ports  having  minimum QoS policy rules attached. If your deployment does not use
              such a feature then the performance impact of querying neutron ports  for  each  instance  can  be
              avoided with this flag.

       <b>--cell</b> <b>&lt;cell_uuid&gt;</b>
              Heal allocations within a specific cell. Mutually exclusive with <u>--instance</u>.

       <b>--force</b>
              Force heal allocations. Requires the <u>--instance</u> argument.

       Return codes
             ┌─────────────┬───────────────────────────────────────────────────────────────────────────┐
             │ Return code │ Description                                                               │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 0           │ Command  completed  successfully  and                                     │
             │             │ allocations were created.                                                 │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 1           │ <u>--max-count</u> was reached and there are                                     │
             │             │ more instances to process.                                                │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 2           │ Unable to find a compute node  record                                     │
             │             │ for a given instance.                                                     │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 3           │ Unable    to   create   (or   update)                                     │
             │             │ allocations for an  instance  against                                     │
             │             │ its compute node resource provider.                                       │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 4           │ Command completed successfully but no                                     │
             │             │ allocations were created.                                                 │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 5           │ Unable to query ports from neutron                                        │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 6           │ Unable to update ports in neutron                                         │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 7           │ Cannot   roll   back   neutron   port                                     │
             │             │ updates.  Manual  steps  needed.  The                                     │
             │             │ error  message  will  indicate  which                                     │
             │             │ neutron ports need to be  changed  to                                     │
             │             │ clean up <b>binding:profile</b> of the port:                                     │
             │             │                                                                           │
             │             │           $ openstack port unset &lt;port_uuid&gt; --binding-profile allocation │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 127         │ Invalid input.                                                            │
             ├─────────────┼───────────────────────────────────────────────────────────────────────────┤
             │ 255         │ An unexpected error occurred.                                             │
             └─────────────┴───────────────────────────────────────────────────────────────────────────┘

   <b>placement</b> <b>sync_aggregates</b>
          nova-manage placement sync_aggregates [--verbose]

       Mirrors  compute  host  aggregates to resource provider aggregates in the Placement service. Requires the
       <u>api_database</u> and <u>placement</u> sections of the nova configuration file to be populated.

       Specify <u>--verbose</u> to get detailed progress output during execution.

       <b>NOTE:</b>
          Depending on the size of your deployment and the number of compute hosts in aggregates,  this  command
          could  cause  a non-negligible amount of traffic to the placement service and therefore is recommended
          to be run during maintenance windows.

       Added in version 18.0.0: (Rocky)

       Options

       <b>--verbose</b>
              Provide verbose output during execution.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Successful run                        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ A host was found with more  than  one │
                               │             │ matching compute node record          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ An  unexpected  error  occurred while │
                               │             │ working with the placement API        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Failed updating  provider  aggregates │
                               │             │ in placement                          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ Host  mappings  not  found for one or │
                               │             │ more host aggregate members           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 5           │ Compute node records  not  found  for │
                               │             │ one or more hosts                     │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 6           │ Resource  provider  not found by uuid │
                               │             │ for a given host                      │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 255         │ An unexpected error occurred.         │
                               └─────────────┴───────────────────────────────────────┘

   <b>placement</b> <b>audit</b>
          nova-manage placement audit [--verbose] [--delete]
            [--resource_provider &lt;uuid&gt;]

       Iterates over all the Resource Providers (or just one if you provide the UUID) and then verifies  if  the
       compute  allocations are either related to an existing instance or a migration UUID. If not, it will tell
       which allocations are orphaned.

       This command requires that the <u>api_database.connection</u>  and  <u>placement</u>  configuration  options  are  set.
       Placement API &gt;= 1.14 is required.

       Added in version 21.0.0: (Ussuri)

       Options

       <b>--verbose</b>
              Provide verbose output during execution.

       <b>--resource_provider</b> <b>&lt;provider_uuid&gt;</b>
              UUID of a specific resource provider to verify.

       <b>--delete</b>
              Deletes orphaned allocations that were found.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ No orphaned allocations were found    │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Orphaned allocations were found       │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ All  found  orphaned allocations were │
                               │             │ deleted                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 127         │ Invalid input                         │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>VOLUME</b> <b>ATTACHMENT</b> <b>COMMANDS</b></h4><pre>
   <b>volume_attachment</b> <b>get_connector</b>
          nova-manage volume_attachment get_connector

       Show the host connector for this compute host.

       When called with the <b>--json</b> switch this dumps a JSON string containing the connector information for  the
       current  host,  which  can  be  saved  to  a file and used as input for the <b>nova-manage</b> <b>volume_attachment</b>
       <b>refresh</b> command.

       Added in version 24.0.0: (Xena)

       Return codes
                                    ┌─────────────┬──────────────────────────────┐
                                    │ Return code │ Description                  │
                                    ├─────────────┼──────────────────────────────┤
                                    │ 0           │ Success                      │
                                    ├─────────────┼──────────────────────────────┤
                                    │ 1           │ An unexpected error occurred │
                                    └─────────────┴──────────────────────────────┘

   <b>volume_attachment</b> <b>show</b>
          nova-manage volume_attachment show [INSTANCE_UUID] [VOLUME_ID]

       Show the details of a the volume attachment between <b>VOLUME_ID</b> and <b>INSTANCE_UUID</b>.

       Added in version 24.0.0: (Xena)

       Return codes
                                 ┌─────────────┬────────────────────────────────────┐
                                 │ Return code │ Description                        │
                                 ├─────────────┼────────────────────────────────────┤
                                 │ 0           │ Success                            │
                                 ├─────────────┼────────────────────────────────────┤
                                 │ 1           │ An unexpected error occurred       │
                                 ├─────────────┼────────────────────────────────────┤
                                 │ 2           │ Instance not found                 │
                                 ├─────────────┼────────────────────────────────────┤
                                 │ 3           │ Instance is not attached to volume │
                                 └─────────────┴────────────────────────────────────┘

   <b>volume_attachment</b> <b>refresh</b>
          nova-manage volume_attachment refresh [INSTANCE_UUID] [VOLUME_ID] [CONNECTOR_PATH]

       Refresh the connection info associated with a given volume attachment.

       The instance must be attached to the volume, have a <b>vm_state</b> of <b>stopped</b> and not be <b>locked</b>.

       <b>CONNECTOR_PATH</b> should be the path to a JSON-formatted file containing up to  date  connector  information
       for  the  compute  currently  hosting  the  instance as generated using the <b>nova-manage</b> <b>volume_attachment</b>
       <b>get_connector</b> command.

       Added in version 24.0.0: (Xena)

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Success                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Connector path does not exist         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Failed to open connector path         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ Instance does not exist               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 5           │ Instance  state  invalid   (must   be │
                               │             │ stopped and unlocked)                 │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 6           │ Volume   is   not   attached  to  the │
                               │             │ instance                              │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 7           │ Connector host is not correct         │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>LIBVIRT</b> <b>COMMANDS</b></h4><pre>
   <b>libvirt</b> <b>get_machine_type</b>
          nova-manage libvirt get_machine_type [INSTANCE_UUID]

       Fetch and display the recorded machine type of a libvirt instance identified by <b>INSTANCE_UUID</b>.

       Added in version 23.0.0: (Wallaby)

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Successfully completed                │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Unable to find instance  or  instance │
                               │             │ mapping                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ No machine type found for instance    │
                               └─────────────┴───────────────────────────────────────┘

   <b>libvirt</b> <b>update_machine_type</b>
          nova-manage libvirt update_machine_type \
              [INSTANCE_UUID] [MACHINE_TYPE] [--force]

       Set or update the recorded machine type of instance <b>INSTANCE_UUID</b> to machine type <b>MACHINE_TYPE</b>.

       The following criteria must be met when using this command:

       • The instance must have a <b>vm_state</b> of <b>STOPPED</b>, <b>SHELVED</b> or <b>SHELVED_OFFLOADED</b>.

       • The  machine  type  must  be  supported.  The  supported list includes alias and versioned types of <b>pc</b>,
         <b>pc-i440fx</b>, <b>pc-q35</b>, <b>q35</b>, <b>virt</b> or <b>s390-ccw-virtio</b>.

       • The update will not move the instance between underlying machine types.  For example, <b>pc</b> to <b>q35</b>.

       • The update will not move the instance between an alias and versioned machine type or  vice  versa.  For
         example, <b>pc</b> to <b>pc-1.2.3</b> or <b>pc-1.2.3</b> to <b>pc</b>.

       A <b>--force</b> flag is provided to skip the above checks but caution should be taken as this could easily lead
       to the underlying ABI of the instance changing when moving between machine types.

       Added in version 23.0.0: (Wallaby)

       Options

       <b>--force</b>
              Skip machine type compatibility checks and force machine type update.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Update completed successfully         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Unable  to  find instance or instance │
                               │             │ mapping                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ The instance has an invalid <b>vm_state</b>  │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ The proposed update  of  the  machine │
                               │             │ type is invalid                       │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 5           │ The    provided   machine   type   is │
                               │             │ unsupported                           │
                               └─────────────┴───────────────────────────────────────┘

   <b>libvirt</b> <b>list_unset_machine_type</b>
          nova-manage libvirt list_unset_machine_type [--cell-uuid &lt;cell-uuid&gt;]

       List the UUID of any instance without <b>hw_machine_type</b> set.

       This  command  is  useful  for  operators  attempting  to  determine  when  it  is  safe  to  change  the
       <u>libvirt.hw_machine_type</u> option within an environment.

       Added in version 23.0.0: (Wallaby)

       Options

       <b>--cell_uuid</b> <b>&lt;cell_uuid&gt;</b>
              The UUID of the cell to list instances from.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Completed  successfully, no instances │
                               │             │ found without <b>hw_machine_type</b> set     │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Unable to find cell mapping           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Instances        found        without │
                               │             │ <b>hw_machine_type</b> set                   │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>IMAGE</b> <b>PROPERTY</b> <b>COMMANDS</b></h4><pre>
   <b>image_property</b> <b>show</b>
          nova-manage image_property show [INSTANCE_UUID] [IMAGE_PROPERTY]

       Fetch and display the recorded image property <b>IMAGE_PROPERTY</b> of an instance identified by <b>INSTANCE_UUID</b>.

       Added in version 25.0.0: (Yoga)

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Successfully completed                │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Unable  to  find instance or instance │
                               │             │ mapping                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ No image property found for instance  │
                               └─────────────┴───────────────────────────────────────┘

   <b>image_property</b> <b>set</b>
          nova-manage image_property set \
              [INSTANCE_UUID] [--property] [IMAGE_PROPERTY]=[VALUE]

       Set or update the recorded image property <b>IMAGE_PROPERTY</b> of instance <b>INSTANCE_UUID</b> to value <b>VALUE</b>.

       The following criteria must be met when using this command:

       • The instance must have a <b>vm_state</b> of <b>STOPPED</b>, <b>SHELVED</b> or <b>SHELVED_OFFLOADED</b>.

       This command is useful for operators who need to update stored instance image properties that have become
       invalidated by a change of instance machine type, for example.

       Added in version 25.0.0: (Yoga)

       Options

       <b>--property</b>
              Image property to set using the format  name=value.  For  example:  <b>--property</b>  <b>hw_disk_bus=virtio</b>
              <b>--property</b> <b>hw_cdrom_bus=sata</b>.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Update completed successfully         │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Unable  to  find instance or instance │
                               │             │ mapping                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ The instance has an invalid <b>vm_state</b>  │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 4           │ The provided image property  name  is │
                               │             │ invalid                               │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 5           │ The  provided image property value is │
                               │             │ invalid                               │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>LIMITS</b> <b>COMMANDS</b></h4><pre>
   <b>limits</b> <b>migrate_to_unified_limits</b>
          nova-manage limits migrate_to_unified_limits [--project-id &lt;project-id&gt;]
          [--region-id &lt;region-id&gt;] [--verbose] [--dry-run] [--quiet]
          [--no-embedded-flavor-scan]

       Migrate quota limits from the Nova database to unified limits in Keystone.

       This command is useful for operators to migrate from legacy quotas to unified limits. Limits are migrated
       by copying them from the Nova database to Keystone by creating them using the Keystone API.

       The Nova configuration file used by <b>nova-manage</b> must have  a  <u>keystone_authtoken</u>  section  that  contains
       authentication settings in order for the Keystone API calls to succeed. As an example:

          [keystone_authtoken]
          project_domain_name = Default
          project_name = service
          user_domain_name = Default
          username = nova
          auth_url = <a href="http://127.0.0.1/identity">http://127.0.0.1/identity</a>
          auth_type = password
          password = &lt;password&gt;

       By  default <u>Keystone</u> <u>policy</u> <u>configuration</u>, access to create, update, and delete in the <u>unified</u> <u>limits</u> <u>API</u>
       is restricted to callers with the <b>admin</b> role. You will need to ensure  that  the  user  configured  under
       <u>keystone_authtoken</u> has the necessary role and scope.

       <b>WARNING:</b>
          The  <b>limits</b>  <b>migrate_to_unified_limits</b> command will create limits only for resources that exist in the
          legacy quota system and any resource that does not have a unified limit in Keystone will use  a  quota
          limit of <b>0</b>.

          For  resource  classes  that are allocated by the placement service and have no default limit set, you
          will need to create default limits manually. The most common example is class:DISK_GB.  All  Nova  API
          requests that need to allocate DISK_GB will fail quota enforcement until a default limit for it is set
          in Keystone.

          See the <u>unified</u> <u>limits</u> <u>documentation</u> about creating limits using the OpenStackClient.

       Added in version 28.0.0: (2023.2 Bobcat)

       Changed in version 31.0.0: (2025.1 Epoxy)

       Added   flavor   scanning   for   resource   classes   missing   limits   along   with  the  --quiet  and
       --no-embedded-flavor-scan options.

       Options

       <b>--project-id</b> <b>&lt;project-id&gt;</b>
              The project ID for which to migrate quota limits.

       <b>--region-id</b> <b>&lt;region-id&gt;</b>
              The region ID for which to migrate quota limits.

       <b>--verbose</b>
              Provide verbose output during execution.

       <b>--dry-run</b>
              Show what limits would be created without actually creating them. Flavors will  still  be  scanned
              for resource classes missing limits.

       <b>--quiet</b>
              Do not output anything during execution.

       <b>--no-embedded-flavor-scan</b>
              Do not scan instances embedded flavors for resource classes missing limits.

       Return codes
                               ┌─────────────┬───────────────────────────────────────┐
                               │ Return code │ Description                           │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 0           │ Command completed successfully        │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 1           │ An unexpected error occurred          │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 2           │ Failed to connect to the database     │
                               ├─────────────┼───────────────────────────────────────┤
                               │ 3           │ Missing    registered   limits   were │
                               │             │ identified                            │
                               └─────────────┴───────────────────────────────────────┘

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <u><a href="../man1/nova-policy.1.html">nova-policy</a>(1)</u>, <u><a href="../man1/nova-status.1.html">nova-status</a>(1)</u>

</pre><h4><b>BUGS</b></h4><pre>
       • Nova bugs are managed at <u>Launchpad</u>

</pre><h4><b>AUTHOR</b></h4><pre>
       <a href="mailto:openstack@lists.openstack.org">openstack@lists.openstack.org</a>

</pre><h4><b>COPYRIGHT</b></h4><pre>
       2010-present, OpenStack Foundation

31.0.0                                            Apr 03, 2025                                    <u><a href="../man1/NOVA-MANAGE.1.html">NOVA-MANAGE</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>