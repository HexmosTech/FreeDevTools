<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>dupemap - Creates a database of file checksums and uses it to eliminate duplicates</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/magicrescue">magicrescue_1.1.10+dfsg-2build2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       dupemap - Creates a database of file checksums and uses it to eliminate duplicates

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>dupemap</b> [ <u>options</u> ] [ <b>-d</b> <u>database</u> ] <u>operation</u> <u>path...</u>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>dupemap</b> recursively scans each <u>path</u> to find checksums of file contents.  Directories are searched through
       in no particular order.  Its actions depend on whether the <b>-d</b> option is given, and on the <u>operation</u>
       parameter, which must be a comma-seperated list of <b>scan</b>, <b>report</b>, <b>delete</b>:

   <b>Without</b> <b>-d</b>
       <b>dupemap</b> will take action when it sees the same checksum repeated more than once, i.e. it simply finds
       duplicates recursively.  The action depends on <u>operation</u>:

       <b>report</b> Report what files are encountered more than once, printing their names to standard output.

       <b>delete</b>[<b>,report</b>]
              Delete files that are encountered more than once.  Print their names if <b>report</b> is also given.

              <u>WARNING:</u> use the <b>report</b> operation first to see what will be deleted.

              <u>WARNING:</u>  You are advised to make a backup of the target first, e.g. with "cp -al" (for GNU cp) to
              create hard links recursively.

   <b>With</b> <b>-d</b>
       The <u>database</u> argument to <b>-d</b> will denote a database file (see the "DATABASE" section in  this  manual  for
       details)  to read from or write to.  In this mode, the <b>scan</b> operation should be run on one <u>path</u>, followed
       by the <b>report</b> or <b>delete</b> operation on another (<u>not</u> <u>the</u> <u>same!</u>) <u>path</u>.

       <b>scan</b>   Add the checksum of each file to <u>database</u>.  This operation must be run  initially  to  create  the
              database.   To  start  over,  you  must  manually  delete the database file(s) (see the "DATABASE"
              section).

       <b>report</b> Print each file name if its checksum is found in <u>database</u>.

       <b>delete</b>[<b>,report</b>]
              Delete each file if its checksum is found in <u>database</u>.  If <b>report</b> is also present, print the  name
              of each deleted file.

              <u>WARNING:</u>  if  you run <b>dupemap</b> <b>delete</b> on the same <u>path</u> you just ran <b>dupemap</b> <b>scan</b> on, it will <u>delete</u>
              <u>every</u> <u>file!</u> The idea of these options is to scan one <u>path</u> and delete files in a second <u>path</u>.

              <u>WARNING:</u> use the <b>report</b> operation first to see what will be deleted.

              <u>WARNING:</u> You are advised to make a backup of the target first, e.g. with "cp -al" (for GNU cp)  to
              create hard links recursively.

</pre><h4><b>OPTIONS</b></h4><pre>
       <b>-d</b> <u>database</u>
              Use <u>database</u> as an on-disk database to read from or write to.  See the "DESCRIPTION" section above
              about how this influences the operation of <b>dupemap</b>.

       <b>-I</b> <u>file</u>
              Reads input files from <u>file</u> in addition to those listed on the command line.  If <u>file</u> is "-", read
              from standard input.  Each line will be interpreted as a file name.

              The  paths  given  here will NOT be scanned recursively.  Directories will be ignored and symlinks
              will be followed.

       <b>-m</b> <u>minsize</u>
              Ignore files below this size.

       <b>-M</b> <u>maxsize</u>
              Ignore files above this size.

</pre><h4><b>USAGE</b></h4><pre>
   <b>General</b> <b>usage</b>
       The easiest operations to understand is when the <b>-d</b> option is not given.  To delete all  duplicate  files
       in <u>/tmp/recovered-files</u>, do:

           $ dupemap delete /tmp/recovered-files

       Often, <b>dupemap</b> <b>scan</b> is run to produce a checksum database of all files in a directory tree.  Then <b>dupemap</b>
       <b>delete</b>  is run on another directory, possibly following <b>dupemap</b> <b>report</b>.  For example, to delete all files
       in <u>/tmp/recovered-files</u> that already exist in <u>$HOME</u>, do this:

           $ dupemap -d homedir.map scan $HOME
           $ dupemap -d homedir.map delete,report /tmp/recovered-files

   <b>Usage</b> <b>with</b> <b>magicrescue</b>
       The main application for <b>dupemap</b> is to  take  some  pain  out  of  performing  undelete  operations  with
       <b><a href="../man1/magicrescue.1.html">magicrescue</a></b>(1).   The  reason is that <b>magicrescue</b> will extract every single file of the specified type on
       the block device, so undeleting files requires you to find a few files out of hundreds, which can take  a
       long  time if done manually.  What we want to do is to only extract the documents that don't exist on the
       file system already.

       In the following scenario, you have accidentally deleted some important Word documents  in  Windows.   If
       this  were  a  real-world scenario, then by all means use The Sleuth Kit.  However, <b>magicrescue</b> will work
       even when the directory entries were overwritten, i.e. more files were stored in the same folder later.

       You boot into Linux and change to  a  directory  with  lots  of  space.   Mount  the  Windows  partition,
       preferably read-only (especially with NTFS), and create the directories we will use.

           $ mount -o ro /dev/hda1 /mnt/windows
           $ mkdir healthy_docs rescued_docs

       Extract  all the healthy Word documents with <b>magicrescue</b> and build a database of their checksums.  It may
       seem a little redundant to send all the documents through <b>magicrescue</b> first, but the reason is that  this
       process  may  modify them (e.g. stripping trailing garbage), and therefore their checksum will not be the
       same as the original documents.  Also, it will find  documents  embedded  inside  other  files,  such  as
       uncompressed zip archives or files with the wrong extension.

           $ find /mnt/windows -type f \
             |magicrescue -I- -r msoffice -d healthy_docs
           $ dupemap -d healthy_docs.map scan healthy_docs
           $ rm -rf healthy_docs

       Now rescue all "msoffice" documents from the block device and get rid of everything that's not a *.doc.

           $ magicrescue -Mo -r msoffice -d rescued_docs /dev/hda1 \
             |grep -v '\.doc$'|xargs rm -f

       Remove all the rescued documents that also appear on the file system, and remove duplicates.

           $ dupemap -d healthy_docs.map delete,report rescued_docs
           $ dupemap delete,report rescued_docs

       The  <u>rescued_docs</u>  folder should now contain only a few files.  This will be the undeleted files and some
       documents that were not stored in contiguous blocks (use that defragger ;-)).

   <b>Usage</b> <b>with</b> <b>fsck</b>
       In this scenario (based on a true story), you have a hard disk that's gone bad.  You have managed  to  <u>dd</u>
       about 80% of the contents into the file <u>diskimage</u>, and you have an old backup from a few months ago.  The
       disk is using reiserfs on Linux.

       First,  use  fsck to make the file system usable again.  It will find many nameless files and put them in
       <u>lost+found</u>.  You need to make sure there is some free space on the disk image, so fsck has  something  to
       work with.

           $ cp diskimage diskimage.bak
           $ dd if=/dev/zero bs=1M count=2048 &gt;&gt; diskimage
           $ reiserfsck --rebuild-tree diskimage
           $ mount -o loop diskimage <a href="file:/mnt">/mnt</a>
           $ ls /mnt/lost+found
           (tons of files)

       Our  strategy will be to restore the system with the old backup as a base and merge the two other sets of
       files (<u>/mnt/lost+found</u> and <u><a href="file:/mnt">/mnt</a></u>) into the backup after eliminating duplicates.   Therefore  we  create  a
       checksum database of the directory we have unpacked the backup in.

           $ dupemap -d backup.map scan <a href="file:~/backup">~/backup</a>

       Next, we eliminate all the files from the rescued image that are also present in the backup.

           $ dupemap -d backup.map delete,report <a href="file:/mnt">/mnt</a>

       We  also  want  to  remove  duplicates from <u>lost+found</u>, and we want to get rid of any files that are also
       present in the other directories in <u><a href="file:/mnt">/mnt</a></u>.

           $ dupemap delete,report /mnt/lost+found
           $ ls <a href="file:/mnt">/mnt</a>|grep -v lost+found|xargs dupemap -d mnt.map scan
           $ dupemap -d mnt.map delete,report /mnt/lost+found

       This should leave only the files in <u><a href="file:/mnt">/mnt</a></u> that have changed  since  the  last  backup  or  got  corrupted.
       Particularly,  the contents of <u>/mnt/lost+found</u> should now be reduced enough to manually sort through them
       (or perhaps use <b><a href="../man1/magicsort.1.html">magicsort</a></b>(1)).

   <b>Primitive</b> <b>intrusion</b> <b>detection</b>
       You can use <b>dupemap</b> to see what files change on your system.  This is one of the more  exotic  uses,  and
       it's only included for inspiration.

       First, you map the whole file system.

           $ dupemap -d old.map scan /

       Then  you come back a few days/weeks later and run <b>dupemap</b> <b>report</b>.  This will give you a view of what <u>has</u>
       <u>not</u> changed.  To see what <u>has</u> changed, you need a list of the whole file system.  You can get  this  list
       along with preparing a new map easily.  Both lists need to be sorted to be compared.

           $ dupemap -d old.map report /|sort &gt; unchanged_files
           $ dupemap -d current.map scan /|sort &gt; current_files

       All  that's  left  to do is comparing these files and preparing for next week.  This assumes that the dbm
       appends the ".db" extension to database files.

           $ diff unchanged_files current_files &gt; changed_files
           $ mv current.map.db old.map.db

</pre><h4><b>DATABASE</b></h4><pre>
       The actual database file(s) written by <b>dupecheck</b> will have some relation to the  <u>database</u>  argument,  but
       most  implementations  append  an extension.  For example, Berkeley DB names the files <u>database</u><b>.db</b>, while
       Solaris and GDBM creates both a <u>database</u><b>.dir</b> and <u>database</u><b>.pag</b> file.

       <b>dupecheck</b> depends on a database library for storing the checksums.   It  currently  requires  the  POSIX-
       standardized  <b>ndbm</b>  library,  which  must  be  present  on XSI-compliant UNIXes.  Implementations are not
       required to handle hash key collisions, and a failure to do that could make  <b>dupecheck</b>  delete  too  many
       files.  I haven't heard of such an implementation, though.

       The  current  checksum  algorithm  is the file's CRC32 combined with its size.  Both values are stored in
       native byte order, and because of varying type sizes the database is <u>not</u> portable  across  architectures,
       compilers and operating systems.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man1/magicrescue.1.html">magicrescue</a></b>(1), <b><a href="../man1/weeder.1.html">weeder</a></b>(1)

       This  tool  does  the same thing <b>weeder</b> does, except that <b>weeder</b> cannot seem to handle many files without
       crashing, and it has no largefile support.

</pre><h4><b>BUGS</b></h4><pre>
       There is a tiny chance that two different files can have the same checksum and size.  The probability  of
       this  happening  is around 1 to 10^14, and since <b>dupemap</b> is part of the Magic Rescue package, which deals
       with disaster recovery, that chance becomes an insignificant part of the game.  You should consider  this
       if you apply <b>dupemap</b> to other applications, especially if they are security-related (see next paragraph).

       It  is  possible to craft a file to have a known CRC32.  You need to keep this in mind if you use <b>dupemap</b>
       on untrusted data.  A solution to this could be to implement an option for using MD5 checksums instead.

</pre><h4><b>AUTHOR</b></h4><pre>
       Jonas Jensen &lt;<a href="mailto:jbj@knef.dk">jbj@knef.dk</a>&gt;

</pre><h4><b>LATEST</b> <b>VERSION</b></h4><pre>
       This   tool   is   part   of    Magic    Rescue.     You    can    find    the    latest    version    at
       &lt;https://github.com/jbj/magicrescue&gt;

1.1.10                                             2018-10-16                                         <u><a href="../man1/DUPEMAP.1.html">DUPEMAP</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>