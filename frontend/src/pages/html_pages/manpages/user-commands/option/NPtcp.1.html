<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NetPIPE - Network Protocol Independent Performance Evaluator</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/netpipe-tcp">netpipe-tcp_3.7.2-9.1build1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       NetPIPE - <u>Net</u><b>work</b> <u>P</u><b>rotocol</b> <u>I</u><b>ndependent</b> <u>P</u><b>erformance</b> <u>E</u><b>valuator</b>

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>NPtcp</b> [<b>-h</b> <u>receiver_hostname</u>] [<b>-b</b> <u>TCP_buffer_sizes</u>] [options]

       mpirun [<b>-machinefile</b> <u>hostlist</u>] -np 2 <b>NPmpi</b> [-a] [-S] [-z] [options]

       mpirun [<b>-machinefile</b> <u>hostlist</u>] -np 2 <b>NPmpi2</b> [-f] [-g] [options]

       <b>NPpvm</b> [options]

       See the TESTING sections below for a more complete description of how to run NetPIPE in each environment.
       The  OPTIONS  section  describes the general options available for all modules.  See the README file from
       the tar-ball at <a href="http://www.scl.ameslab.gov/Projects/NetPIPE/">http://www.scl.ameslab.gov/Projects/NetPIPE/</a> for documentation  on  the  InfiniBand,  GM,
       SHMEM, LAPI, and memcpy modules.

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>NetPIPE</b>  uses  a  simple  series  of  ping-pong tests over a range of message sizes to provide a complete
       measure of the performance of a network.  It bounces messages of increasing size between  two  processes,
       whether  across  a  network  or within an SMP system.  Message sizes are chosen at regular intervals, and
       with slight perturbations, to provide a complete evaluation of the communication system.  Each data point
       involves many ping-pong tests to provide an accurate timing.  Latencies are calculated  by  dividing  the
       round trip time in half for small messages ( less than 64 Bytes ).

       The  communication  time  for  small  messages  is dominated by the overhead in the communication layers,
       meaning that the transmission is latency bound.  For larger  messages,  the  communication  rate  becomes
       bandwidth  limited  by some component in the communication subsystem (PCI bus, network card link, network
       switch).

       These measurements can be done at the message-passing layer (MPI,  MPI-2,  and  PVM)  or  at  the  native
       communications  layers  that  that run upon (TCP/IP, GM for Myrinet cards, InfiniBand, SHMEM for the Cray
       T3E systems, and LAPI for IBM SP systems).  Recent work is being aimed at measuring some internal  system
       properties such as the memcpy module that measures the internal memory copy rates, or a disk module under
       development that measures the performance to various I/O devices.

       Some uses for NetPIPE include:

              Comparing the latency and maximum throughput of various network cards.

              Comparing the performance between different types of networks.

              Looking   for  inefficiencies  in  the  message-passing  layer  by  comparing  it  to  the  native
              communication layer.

              Optimizing the message-passing layer and tune OS and driver parameters for optimal performance  of
              the communication subsystem.

       <b>NetPIPE</b>  is  provided  with  many  modules  allowing it to interface with a wide variety of communication
       layers.  It is fairly easy to write new interfaces for other reliable protocols  by  using  the  existing
       modules as examples.

</pre><h4><b>TESTING</b> <b>TCP</b></h4><pre>
       NPtcp  can now be launched in two ways, by manually starting NPtcp on both systems or by using a nplaunch
       script.  To manually start NPtcp, the NetPIPE receiver must be started first on the remote  system  using
       the command:

       NPtcp [options]

       then the primary transmitter is started on the local system with the command

       NPtcp -h <u>receiver_hostname</u> [options]

       Any options used must be the same on both sides.

       The nplaunch script uses ssh to launch the remote receiver before starting the local transmitter.  To use
       rsh, simply change the nplaunch script.

       nplaunch NPtcp -h <u>receiver_hostname</u> [options]

       The  <b>-b</b> <u>TCP_buffer_sizes</u>  option sets the TCP socket buffer size, which can greatly influence the maximum
       throughput on some systems.  A throughput graph  that  flattens  out  suddenly  may  be  a  sign  of  the
       performance being limited by the socket buffer sizes.

</pre><h4><b>TESTING</b> <b>MPI</b> <b>and</b> <b>MPI-2</b></h4><pre>
       Use  of the MPI interface for NetPIPE depends on the MPI implementation being used.  All will require the
       number of processes to be specified, usually with a <u>-np</u> <u>2</u> argument.  Clusters environments may require  a
       list  of the hosts being used when each job is run.  Put the list of hosts in hostlist then, for OpenMPI,
       run NetPIPE using:

       mpirun --hostfile <u>hostlist</u> -np 2 NPmpi [NetPIPE options]

       For MPICH2 use instead:

       mpirun -machinefile <u>hostlist</u> -np 2 NPmpi [NetPIPE options]

       To test the 1-sided communications of the MPI-2 standard, compile using:

       <b>make</b> <b>mpi2</b>

       Running as described above and MPI will use  1-sided  MPI_Put()  calls  in  both  directions,  with  each
       receiver  blocking until the last byte has been overwritten before bouncing the message back.  Use the <u>-f</u>
       option to force usage of a fence to block rather than an overwrite of the last byte.  The <u>-g</u> option  will
       use MP_Get() functions to transfer the data rather than MP_Put().

</pre><h4><b>TESTING</b> <b>PVM</b></h4><pre>
       Start the pvm system using:

       pvm

       and adding a second machine with the PVM command

       add <u>receiver_hostname</u>

       Exit  the PVM command line interface using quit, then run the PVM NetPIPE receiver on one system with the
       command:

       NPpvm [options]

       and run the TCP NetPIPE transmitter on the other system with the command:

       NPpvm -h <u>receiver</u> <u>hostname</u> [options]

       Any options used must be the same on both sides.  The nplaunch script may also  be  used  with  NPpvm  as
       described above for NPtcp.

</pre><h4><b>TESTING</b> <b>METHODOLOGY</b></h4><pre>
       <b>NetPIPE</b>  tests  network performance by sending a number of messages at each block size, starting from the
       lower bound on the message sizes.

       The message size is incremented until the upper bound on the message size  is  reached  or  the  time  to
       transmit  a  block  exceeds  one  second,  which  ever occurs first.  Message sizes are chosen at regular
       intervals, and for slight  perturbations  from  them  to  provide  a  more  complete  evaluation  of  the
       communication subsystem.

       The  <b>NetPIPE</b>  output  file  may  be graphed using a program such as <b><a href="../man1/gnuplot.1.html">gnuplot</a>(1).</b>  The output file contains
       three columns: the number of bytes in the block, the transfer rate in bits per second, and  the  time  to
       transfer  the  block  (half  the  round-trip time).  The first two columns are normally used to graph the
       throughput vs block size, while the third column provides  the  latency.   For  example,  the  <b>throughput</b>
       <b>versus</b>  <b>block</b>  <b>size</b>  graph  can  be  created by graphing bytes versus bits per second.  Sample <b><a href="../man1/gnuplot.1.html">gnuplot</a>(1)</b>
       commands for such a graph would be

       set logscale x

       plot "np.out"

</pre><h4><b>OPTIONS</b></h4><pre>
       <b>-a</b>     asynchronous mode: prepost receives (MPI, IB modules)

       <b>-b</b> <u>TCP_buffer_sizes</u>
              Set the send and receive TCP buffer sizes (TCP module only).

       <b>-B</b>     Burst mode where all receives are preposted at once (MPI, IB modules).

       <b>-f</b>     Use a fence to block for completion (MPI2 module only).

       <b>-g</b>     Use MPI_Get() instead of MPI_Put() (MPI2 module only).

       <b>-h</b> <u>hostname</u>
              Specify the name of the receiver host to connect to (TCP, PVM, IB, GM).

       <b>-I</b>     Invalidate cache to measure performance without  cache  effects  (mostly  affects  IB  and  memcpy
              modules).

       <b>-i</b>     Do an integrity check instead of a performance evaluation.

       <b>-l</b> <u>starting_msg_size</u>
              Specify the lower bound for the size of messages to be tested.

       <b>-n</b> <u>nrepeats</u>
              Set the number of repeats for each test to a constant.  Otherwise, the number of repeats is chosen
              to  provide  an accurate timing for each test.  Be very careful if specifying a low number so that
              the time for the ping-pong test exceeds the timer accuracy.

       <b>-O</b> <u>source_offset,dest_offset</u>
              Specify the source and destination offsets of the buffers from perfect page alignment.

       <b>-o</b> <u>output_filename</u>
              Specify the output filename (default is np.out).

       <b>-p</b> <u>perturbation_size</u>
              NetPIPE chooses the message sizes at regular intervals, increasing  them  exponentially  from  the
              lower boundary to the upper boundary.  At each point, it also tests perturbations of 3 bytes above
              and  3  bytes below each test point to find idiosyncrasies in the system.  This perturbation value
              can be changed using the <u>-p</u> option, or turned off using <u>-p</u> <u>0</u> <b>.</b>

       <b>-r</b>     This option resets the TCP sockets after every test (TCP module only).  It is necessary  for  some
              streaming tests to get good measurements since the socket window size may otherwise collapse.

       <b>-s</b>     Set streaming mode where data is only transmitted in one direction.

       <b>-S</b>     Use synchronous sends (MPI module only).

       <b>-u</b> <u>upper_bound</u>
              Specify  the  upper  boundary  to the size of message being tested.  By default, NetPIPE will stop
              when the time to transmit a block exceeds one second.

       <b>-z</b>     Receive messages using MPI_ANY_SOURCE (MPI module only)

       <b>-2</b>     Set bi-directional mode where both sides send and receive at the  same  time  (supported  by  most
              modules).   You  may need to use <u>-a</u> to choose asynchronous communications for MPI to avoid freeze-
              ups.  For TCP, the maximum test size will be limited by the TCP buffer sizes.

</pre><h4><b>FILES</b></h4><pre>
       <u>np.out</u> Default output file for <b>NetPIPE</b>.  Overridden by the <b>-o</b> option.

</pre><h4><b>AUTHOR</b></h4><pre>
       The original NetPIPE core plus TCP and MPI modules were written by Quinn Snell, Armin Mikler, Guy Helmer,
       and  John  Gustafson.   NetPIPE  is  currently  being  developed  and  maintained  by  Dave  Turner  with
       contributions from many students (Bogdan Vasiliu, Adam Oline, Xuehua Chen, and Brian Smith).

       Send comments/bug-reports to: <u>&lt;<a href="mailto:netpipe@scl.ameslab.gov">netpipe@scl.ameslab.gov</a>&gt;.</u>

       Additional    information    about    <b>NetPIPE</b>    can    be    found    on   the   World   Wide   Web   at
       <u><a href="http://www.scl.ameslab.gov/Projects/NetPIPE/">http://www.scl.ameslab.gov/Projects/NetPIPE/</a></u>

</pre><h4><b>BUGS</b></h4><pre>
       As of version 3.6.1, there is a bug that causes NetPIPE to segfault on RedHat Enterprise systems. I  will
       debug this as soon as I get access to a few such systems.  -Dave Turner (<a href="mailto:turner@ameslab.gov">turner@ameslab.gov</a>)

NetPIPE                                           June 1, 2004                                        <u><a href="../man1/netpipe.1.html">netpipe</a></u>(1)
</pre>
 </div>
</div></section>
</div>
</body>
</html>