<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>C Syntax</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/openmpi-doc">openmpi-doc_5.0.7-1build1_all</a> <br><br><pre>
</pre><h4><b>SYNTAX</b></h4><pre>
   <b>C</b> <b>Syntax</b>
          #include &lt;mpi.h&gt;

          int MPI_Reduce_scatter(const void *sendbuf, void *recvbuf, const int recvcounts[],
               MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)

          int MPI_Ireduce_scatter(const void *sendbuf, void *recvbuf, const int recvcounts[],
               MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request *request)

          int MPI_Reduce_scatter_init(const void *sendbuf, void *recvbuf,
               const int recvcounts[], MPI_Datatype datatype, MPI_Op op,
               MPI_Comm comm, MPI_Info info, MPI_Request *request)

   <b>Fortran</b> <b>Syntax</b>
          USE MPI
          ! or the older form: INCLUDE 'mpif.h'
          MPI_REDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP,
                       COMM, IERROR)
               &lt;type&gt;  SENDBUF(*), RECVBUF(*)
               INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, IERROR

          MPI_IREDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP,
                       COMM, REQUEST, IERROR)
               &lt;type&gt;  SENDBUF(*), RECVBUF(*)
               INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, REQUEST, IERROR

          MPI_REDUCE_SCATTER_INIT(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP,
                       COMM, INFO, REQUEST, IERROR)
               &lt;type&gt;  SENDBUF(*), RECVBUF(*)
               INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, INFO, REQUEST, IERROR

   <b>Fortran</b> <b>2008</b> <b>Syntax</b>
          USE mpi_f08
          MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm,
                       ierror)
               TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
               TYPE(*), DIMENSION(..) :: recvbuf
               INTEGER, INTENT(IN) :: recvcounts(*)
               TYPE(MPI_Datatype), INTENT(IN) :: datatype
               TYPE(MPI_Op), INTENT(IN) :: op
               TYPE(MPI_Comm), INTENT(IN) :: comm
               INTEGER, OPTIONAL, INTENT(OUT) :: ierror

          MPI_Ireduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm,
                       request, ierror)
               TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: sendbuf
               TYPE(*), DIMENSION(..), ASYNCHRONOUS :: recvbuf
               INTEGER, INTENT(IN), ASYNCHRONOUS :: recvcounts(*)
               TYPE(MPI_Datatype), INTENT(IN) :: datatype
               TYPE(MPI_Op), INTENT(IN) :: op
               TYPE(MPI_Comm), INTENT(IN) :: comm
               TYPE(MPI_Request), INTENT(OUT) :: request
               INTEGER, OPTIONAL, INTENT(OUT) :: ierror

          MPI_Reduce_scatter_init(sendbuf, recvbuf, recvcounts, datatype, op, comm,
                       info, request, ierror)
               TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: sendbuf
               TYPE(*), DIMENSION(..), ASYNCHRONOUS :: recvbuf
               INTEGER, INTENT(IN), ASYNCHRONOUS :: recvcounts(*)
               TYPE(MPI_Datatype), INTENT(IN) :: datatype
               TYPE(MPI_Op), INTENT(IN) :: op
               TYPE(MPI_Comm), INTENT(IN) :: comm
               TYPE(MPI_Info), INTENT(IN) :: info
               TYPE(MPI_Request), INTENT(OUT) :: request
               INTEGER, OPTIONAL, INTENT(OUT) :: ierror

</pre><h4><b>INPUT</b> <b>PARAMETERS</b></h4><pre>
       • <b>sendbuf</b>: Starting address of send buffer (choice).

       • <b>recvcounts</b>: Integer array specifying the number of elements in  result  distributed  to  each  process.
         Array must be identical on all calling processes.

       • <b>datatype</b>: Datatype of elements of input buffer (handle).

       • <b>op</b>: Operation (handle).

       • <b>comm</b>: Communicator (handle).

       • <b>info</b>: Info (handle, persistent).

</pre><h4><b>OUTPUT</b> <b>PARAMETERS</b></h4><pre>
       • <b>recvbuf</b>: Starting address of receive buffer (choice).

       • <b>request</b>: Request (handle, non-blocking only).

       • <b>ierror</b>: Fortran only: Error status (integer).

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <u>MPI_Reduce_scatter</u>  first  does an element-wise reduction on vector of <u>count</u> = S(i)<u>recvcounts</u>[i] elements
       in the send buffer defined by <u>sendbuf</u>, <u>count</u>, and <u>datatype</u>. Next, the  resulting  vector  of  results  is
       split  into  n  disjoint  segments,  where  n is the number of processes in the group. Segment i contains
       <u>recvcounts</u>[i] elements.  The ith segment is sent to process i and stored in the receive buffer defined by
       <u>recvbuf</u>, <u>recvcounts</u>[i], and <u>datatype</u>.

</pre><h4><b>USE</b> <b>OF</b> <b>IN-PLACE</b> <b>OPTION</b></h4><pre>
       When the communicator is an intracommunicator, you can perform a reduce-scatter operation  in-place  (the
       output buffer is used as the input buffer). Use the variable MPI_IN_PLACE as the value of the <u>sendbuf</u>. In
       this  case,  the  input  data is taken from the top of the receive buffer. The area occupied by the input
       data may be either longer or shorter than the data filled by the output data.

</pre><h4><b>WHEN</b> <b>COMMUNICATOR</b> <b>IS</b> <b>AN</b> <b>INTER-COMMUNICATOR</b></h4><pre>
       When the communicator is an inter-communicator, the reduce-scatter operation occurs in two phases. First,
       the result of the reduction performed on the data provided  by  the  processes  in  the  first  group  is
       scattered  among  the  processes in the second group. Then the reverse occurs: the reduction performed on
       the data provided by the processes in the second group is scattered among  the  processes  in  the  first
       group.  For each group, all processes provide the same <u>recvcounts</u> argument, and the sum of the <u>recvcounts</u>
       values should be the same for both groups.

</pre><h4><b>NOTES</b> <b>ON</b> <b>COLLECTIVE</b> <b>OPERATIONS</b></h4><pre>
       The  reduction functions ( MPI_Op ) do not return an error value. As a result, if the functions detect an
       error, all they can do is either call <u>MPI_Abort</u> or silently skip the problem. Thus,  if  you  change  the
       error handler from MPI_ERRORS_ARE_FATAL to something else, for example, MPI_ERRORS_RETURN , then no error
       may be indicated.

       The  reason for this is the performance problems in ensuring that all collective routines return the same
       error value.

</pre><h4><b>ERRORS</b></h4><pre>
       Almost all MPI routines return an error value; C routines as  the  return  result  of  the  function  and
       Fortran routines in the last argument.

       Before  the  error  value  is  returned,  the current MPI error handler associated with the communication
       object (e.g., communicator, window, file) is called.  If no communication object is associated  with  the
       MPI  call,  then  the call is considered attached to MPI_COMM_SELF and will call the associated MPI error
       handler.  When  MPI_COMM_SELF  is  not  initialized   (i.e.,   before   <u>MPI_Init</u>/<u>MPI_Init_thread</u>,   after
       <u>MPI_Finalize</u>,  or  when using the Sessions Model exclusively) the error raises the initial error handler.
       The initial error handler can be changed by calling <u>MPI_Comm_set_errhandler</u> on MPI_COMM_SELF  when  using
       the  World  model,  or the mpi_initial_errhandler CLI argument to mpiexec or info key to <u>MPI_Comm_spawn</u>/‐
       <u>MPI_Comm_spawn_multiple</u>.  If no other appropriate error handler has been set, then the  MPI_ERRORS_RETURN
       error  handler  is  called for MPI I/O functions and the MPI_ERRORS_ABORT error handler is called for all
       other MPI functions.

       Open MPI includes three predefined error handlers that can be used:

       • <b>MPI_ERRORS_ARE_FATAL</b> Causes the program to abort all connected MPI processes.

       • <b>MPI_ERRORS_ABORT</b> An error handler that can be invoked on a communicator, window, file, or session. When
         called on a communicator, it acts as if <u>MPI_Abort</u> was called on  that  communicator.  If  called  on  a
         window  or file, acts as if <u>MPI_Abort</u> was called on a communicator containing the group of processes in
         the corresponding window or file. If called on a session, aborts only the local process.

       • <b>MPI_ERRORS_RETURN</b> Returns an error code to the application.

       MPI applications can also implement their own error handlers by calling:

       • <u>MPI_Comm_create_errhandler</u> then <u>MPI_Comm_set_errhandler</u>

       • <u>MPI_File_create_errhandler</u> then <u>MPI_File_set_errhandler</u>

       • <u>MPI_Session_create_errhandler</u> then <u>MPI_Session_set_errhandler</u> or at <u>MPI_Session_init</u>

       • <u>MPI_Win_create_errhandler</u> then <u>MPI_Win_set_errhandler</u>

       Note that MPI does not guarantee that an MPI program can continue past an error.

       See the <u>MPI</u> <u>man</u> <u>page</u> for a full list of <u>MPI</u> <u>error</u> <u>codes</u>.

       See the Error Handling section of the MPI-3.1 standard for more information.

</pre><h4><b>COPYRIGHT</b></h4><pre>
       2003-2025, The Open MPI Community

                                                  Jun 07, 2025                        <u><a href="../man3/MPI_REDUCE_SCATTER_INIT.3.html">MPI_REDUCE_SCATTER_INIT</a></u>(3)
</pre>
 </div>
</div></section>
</div>
</body>
</html>