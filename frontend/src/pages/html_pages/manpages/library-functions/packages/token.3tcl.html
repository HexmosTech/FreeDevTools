<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>string::token - Regex based iterative lexing</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/tcllib">tcllib_2.0+dfsg-4_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       string::token - Regex based iterative lexing

</pre><h4><b>SYNOPSIS</b></h4><pre>
       package require <b>Tcl</b> <b>8.5</b> <b>9</b>

       package require <b>string::token</b> <b>?1.1?</b>

       package require <b>fileutil</b>

       <b>::string</b> <b>token</b> <b>text</b> <u>lex</u> <u>string</u>

       <b>::string</b> <b>token</b> <b>file</b> <u>lex</u> <u>path</u>

       <b>::string</b> <b>token</b> <b>chomp</b> <u>lex</u> <u>startvar</u> <u>string</u> <u>resultvar</u>

________________________________________________________________________________________________________________

</pre><h4><b>DESCRIPTION</b></h4><pre>
       This package provides commands for regular expression based lexing (tokenization) of strings.

       The complete set of procedures is described below.

       <b>::string</b> <b>token</b> <b>text</b> <u>lex</u> <u>string</u>
              This  command takes an ordered dictionary <u>lex</u> mapping regular expressions to labels, and tokenizes
              the <u>string</u> according to this dictionary.

              The result of the command is a list of tokens, where each token is  a  3-element  list  of  label,
              start- and end-index in the <u>string</u>.

              The command will throw an error if it is not able to tokenize the whole string.

       <b>::string</b> <b>token</b> <b>file</b> <u>lex</u> <u>path</u>
              This  command  is  a  convenience  wrapper  around  <b>::string</b>  <b>token</b> <b>text</b> above, and <b>fileutil::cat</b>,
              enabling the easy tokenization of whole files.  <u>Note</u> that this command loads the file wholly  into
              memory before starting to process it.

              If  the  file  is  too large for this mode of operation a command directly based on <b>::string</b> <b>token</b>
              <b>chomp</b> below will be necessary.

       <b>::string</b> <b>token</b> <b>chomp</b> <u>lex</u> <u>startvar</u> <u>string</u> <u>resultvar</u>
              This command is the work horse underlying <b>::string</b> <b>token</b> <b>text</b> above. It is exposed to enable users
              to write their own lexers, which, for example may apply different lexing dictionaries according to
              some internal state, etc.

              The command takes an ordered dictionary <u>lex</u> mapping regular  expressions  to  labels,  a  variable
              <u>startvar</u>  which  indicates  where  to  start  lexing  in  the  input <u>string</u>, and a result variable
              <u>resultvar</u> to extend.

              The result of the command is a tri-state numeric code indicating one of

              <b>0</b>      No token found.

              <b>1</b>      Token found.

              <b>2</b>      End of string reached.

              Note that recognition of a token from <u>lex</u> is started at the character index in <u>startvar</u>.

              If a token was recognized (status <b>1</b>) the command will update the index in <u>startvar</u> to point to the
              first character of the <u>string</u> past the recognized token, and it will further extend the  <u>resultvar</u>
              with  a  3-element  list containing the label associated with the regular expression of the token,
              and the start- and end-character-indices of the token in <u>string</u>.

              Neither <u>startvar</u> nor <u>resultvar</u> will be updated if no token is recognized at all.

              Note that the regular expressions are applied (tested) in the order they are specified in <u>lex</u>, and
              the first matching pattern stops the process. Because of this it is  recommended  to  specify  the
              patterns to lex with from the most specific to the most general.

              Further  note  that  all  regex  patterns  are implicitly prefixed with the constraint escape <b>A</b> to
              ensure that a match starts exactly at the character index found in <u>startvar</u>.

</pre><h4><b>BUGS,</b> <b>IDEAS,</b> <b>FEEDBACK</b></h4><pre>
       This document, and the package it describes, will undoubtedly contain bugs and  other  problems.   Please
       report  such  in  the  category  <u>textutil</u>  of the <u>Tcllib</u> <u>Trackers</u> [<a href="http://core.tcl.tk/tcllib/reportlist">http://core.tcl.tk/tcllib/reportlist</a>].
       Please also report any ideas for enhancements you may have for either package and/or documentation.

       When proposing code changes, please provide <u>unified</u> <u>diffs</u>, i.e the output of <b>diff</b> <b>-u</b>.

       Note further that <u>attachments</u> are strongly preferred over inlined patches. Attachments  can  be  made  by
       going  to the <b>Edit</b> form of the ticket immediately after its creation, and then using the left-most button
       in the secondary navigation bar.

</pre><h4><b>KEYWORDS</b></h4><pre>
       lexing, regex, string, tokenization

</pre><h4><b>CATEGORY</b></h4><pre>
       Text processing

tcllib                                                 1.1                                   <u>string::<a href="../man3tcl/token.3tcl.html">token</a></u>(3tcl)
</pre>
 </div>
</div></section>
</div>
</body>
</html>