<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MPI_Comm_spawn_multiple -  Spawn a dynamic MPI process from multiple executables</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/lam4-dev">lam4-dev_7.1.4-7.2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       MPI_Comm_spawn_multiple -  Spawn a dynamic MPI process from multiple executables

</pre><h4><b>SYNOPSIS</b></h4><pre>
       #include &lt;mpi.h&gt;
       int
       MPI_Comm_spawn_multiple(int count, char **commands, char ***argvs,
                             int *maxprocs, MPI_Info *infos, int root,
                             MPI_Comm comm, MPI_Comm *intercomm,
                             int *errcodes)

</pre><h4><b>INPUT</b> <b>PARAMETERS</b></h4><pre>
       <b>count</b>  - number of commands (only significant at root)
       <b>commands</b>
              - commands to be executed (only significant at root)
       <b>argvs</b>  - arguments for commands (only significant at root)
       <b>maxprocs</b>
              - max number of processes for each command (only significant at root)
       <b>infos</b>  - startup hints for each command
       <b>root</b>   - rank of process to perform the spawn
       <b>comm</b>   - parent intracommunicator

</pre><h4><b>OUTPUT</b> <b>PARAMETERS</b></h4><pre>
       <b>intercomm</b>
              - child intercommunicator containing spawned processes
       <b>errcodes</b>
              - one code per process

</pre><h4><b>DESCRIPTION</b></h4><pre>
       A  group of processes can create another group of processes with <u>MPI_Comm_spawn_multiple</u> .  This function
       is a collective operation over the  parent  communicator.   The  child  group  starts  up  like  any  MPI
       application.   The  processes  must begin by calling <u>MPI_Init</u> , after which the pre-defined communicator,
       <u>MPI_COMM_WORLD</u> , may be used.  This world communicator contains only the child processes.  It is distinct
       from the <u>MPI_COMM_WORLD</u> of the parent processes.

       <u>MPI_Comm_spawn_multiple</u> is used to manually specify a group of different  executables  and  arguments  to
       spawn.   <u>MPI_Comm_spawn</u>  is  used  to  specify  one  executable  and set of arguments (although a LAM/MPI
       <a href="../man5/appschema.5.html">appschema</a>(5) can be provided to <u>MPI_Comm_spawn</u> via the "file" info key).

       Communication With Spawned Processes

       The natural communication mechanism between two groups is the intercommunicator.  The second communicator
       argument to <u>MPI_Comm_spawn_multiple</u> returns an intercommunicator whose local group  contains  the  parent
       processes  (same as the first communicator argument) and whose remote group contains child processes. The
       child processes can access the same intercommunicator by using the <u>MPI_Comm_get_parent</u> call.  The  remote
       group  size of the parent communicator is zero if the process was created by <u>mpirun</u> (1) instead of one of
       the spawn functions.  Both groups can decide to merge the  intercommunicator  into  an  intracommunicator
       (with  the  <u>MPI_Intercomm_merge</u> () function) and take advantage of other MPI collective operations.  They
       can then use the merged intracommunicator to create new communicators and reach other  processes  in  the
       MPI application.

       Resource Allocation

       Note that no MPI_Info keys are recognized by this implementation of <u>MPI_Comm_spawn_multiple</u> .  To use the
       "file"  info  key  to  specify  an  <a href="../man5/appschema.5.html">appschema</a>(5),  use  LAM's <u>MPI_Comm_spawn</u> .  This may be preferable to
       <u>MPI_Comm_spawn_multiple</u> because it allows the arbitrary specification of what nodes and/or CPUs should be
       used to launch jobs (either SPMD or MPMD).  See <a href="../man3/MPI_Comm_spawn.3.html">MPI_Comm_spawn</a>(3) for more details.

       The value of <u>MPI_INFO_NULL</u> should be given for each value in <u>infos</u> (the  <u>infos</u>  array  is  not  currently
       examined  by  LAM/MPI, so specifying non-NULL values for the array values is not harmful).  LAM schedules
       the given number of processes onto LAM nodes by starting with CPU 0 (or the  lowest  numbered  CPU),  and
       continuing  through higher CPU numbers, placing one process on each CPU.  If the process count is greater
       than the CPU count, the procedure repeats.

       Process Terminiation

       Note that the process[es] spawned by <u>MPI_COMM_SPAWN</u> (and  <u>MPI_COMM_SPAWN_MULTIPLE</u>  )  effectively  become
       orphans.   That  is,  the  spawnning MPI application does not wait for the spawned application to finish.
       Hence, there is  no  guarantee  the  spawned  application  has  finished  when  the  spawning  completes.
       Similarly, killing the spawning application will also have no effect on the spawned application.

       User  applications  can  effect  this  kind of behavior with <u>MPI_BARRIER</u> between the spawning and spawned
       processed before <u>MPI_FINALIZE</u> .

       Note that <u>lamclean</u> will kill *all* MPI processes.

       Process Count

       The <u>maxprocs</u> array parameter to <u>MPI_Comm_spawn_multiple</u> specifies the exact number  of  processes  to  be
       started.   If  it  is not possible to start the desired number of processes, <u>MPI_Comm_spawn_multiple</u> will
       return an error code.  Note that even though <u>maxprocs</u> is only relevant on the root, all ranks  must  have
       an  <u>errcodes</u> array long enough to handle an integer error code for every process that tries to launch, or
       give  MPI  constant  <u>MPI_ERRCODES_IGNORE</u>  for  the  <u>errcodes</u>  argument.   While  this  appears  to  be  a
       contradiction, it is per the MPI-2 standard.  :-\

       Frequently,  an application wishes to chooses a process count so as to fill all processors available to a
       job.  MPI indicates the maximum number of processes recommended for a job in the  pre-defined  attribute,
       <u>MPI_UNIVERSE_SIZE</u> , which is cached on <u>MPI_COMM_WORLD</u> .

       The typical usage is to subtract the value of <u>MPI_UNIVERSE_SIZE</u> from the number of processes currently in
       the  job  and  spawn  the difference.  LAM sets <u>MPI_UNIVERSE_SIZE</u> to the number of CPUs in the user's LAM
       session (as defined in the boot schema [<a href="../man5/bhost.5.html">bhost</a>(5)] via <u>lamboot</u> (1)).

       See <a href="../man3/MPI_Init.3.html">MPI_Init</a>(3) for other pre-defined attributes that are helpful when spawning.

       Locating an Executable Program

       The executable program file must be located on the node(s) where the process(es) will run.  On any  node,
       the directories specified by the user's PATH environment variable are searched to find the program.

       All MPI runtime options selected by <u>mpirun</u> (1) in the initial application launch remain in effect for all
       child processes created by the spawn functions.

       Command-line Arguments

       The  <u>argvs</u>  array  parameter  to  <u>MPI_Comm_spawn_multiple</u> should not contain the program name since it is
       given in the first parameter.  The command line that is passed to the newly launched program will be  the
       program name followed by the strings in corresponding entry in the <u>argvs</u> array.

</pre><h4><b>USAGE</b> <b>WITH</b> <b>IMPI</b> <b>EXTENSIONS</b></h4><pre>
       The  IMPI  standard  only  supports  MPI-1  functions.  Hence, this function is currently not designed to
       operate within an IMPI job.

</pre><h4><b>ERRORS</b></h4><pre>
       If an error occurs in an MPI function, the current MPI error handler is called to handle it.  By default,
       this error handler aborts the MPI job.  The error handler may be changed with  <u>MPI_Errhandler_set</u>  ;  the
       predefined  error  handler  <u>MPI_ERRORS_RETURN</u>  may be used to cause error values to be returned (in C and
       Fortran; this error handler is less useful in with the C++ MPI bindings.  The  predefined  error  handler
       <u>MPI::ERRORS_THROW_EXCEPTIONS</u>  should be used in C++ if the error value needs to be recovered).  Note that
       MPI does <u>not</u> guarantee that an MPI program can continue past an error.

       All MPI routines (except <u>MPI_Wtime</u> and <u>MPI_Wtick</u> ) return an error value; C routines as the value of  the
       function and Fortran routines in the last argument.  The C++ bindings for MPI do not return error values;
       instead,  error  values  are  communicated  by  throwing  exceptions  of  type <u>MPI::Exception</u> (but not by
       default).  Exceptions are only thrown if the error value is not <u>MPI::SUCCESS</u> .

       Note that if the <u>MPI::ERRORS_RETURN</u> handler is set in C++, while MPI functions will return upon an error,
       there will be no way to recover what the actual error value was.
       <b>MPI_SUCCESS</b>
              - No error; MPI routine completed successfully.
       <b>MPI_ERR_COMM</b>
              - Invalid communicator.  A common error is to use a null communicator in a call (not even  allowed
              in <u>MPI_Comm_rank</u> ).
       <b>MPI_ERR_SPAWN</b>
              -  Spawn  error;  one  or  more  of  the applications attempting to be launched failed.  Check the
              returned error code array.
       <b>MPI_ERR_ARG</b>
              - Invalid argument.  Some argument is invalid and is not identified by  a  specific  error  class.
              This is typically a NULL pointer or other such error.
       <b>MPI_ERR_ROOT</b>
              -  Invalid root.  The root must be specified as a rank in the communicator.  Ranks must be between
              zero and the size of the communicator minus one.
       <b>MPI_ERR_OTHER</b>
              - Other error; use <u>MPI_Error_string</u> to get more information about this error code.
       <b>MPI_ERR_INTERN</b>
              - An internal error has been detected.  This is fatal.  Please  send  a  bug  report  to  the  LAM
              mailing list (see <u><a href="http://www.lam-mpi.org/contact.php">http://www.lam-mpi.org/contact.php</a></u> ).

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man5/appschema.5.html">appschema</a>(5),       <a href="../man5/bhost.5.html">bhost</a>(5),       <a href="../man1/lamboot.1.html">lamboot</a>(1),      <a href="../man3/MPI_Comm_get_parent.3.html">MPI_Comm_get_parent</a>(3),      <a href="../man3/MPI_Intercomm_merge.3.html">MPI_Intercomm_merge</a>(3),
       <a href="../man3/MPI_Comm_spawn_multiple.3.html">MPI_Comm_spawn_multiple</a>(3), <a href="../man3/MPI_Info_create.3.html">MPI_Info_create</a>(3),  <a href="../man3/MPI_Info_set.3.html">MPI_Info_set</a>(3),  <a href="../man3/MPI_Info_delete.3.html">MPI_Info_delete</a>(3),  <a href="../man3/MPI_Info_free.3.html">MPI_Info_free</a>(3),
       <a href="../man3/MPI_Init.3.html">MPI_Init</a>(3), <a href="../man1/mpirun.1.html">mpirun</a>(1)

</pre><h4><b>MORE</b> <b>INFORMATION</b></h4><pre>
       For  more  information,  please  see the official MPI Forum web site, which contains the text of both the
       MPI-1 and MPI-2 standards.  These documents contain detailed information about each MPI function (most of
       which is not duplicated in these man pages).

       <u><a href="http://www.mpi-forum.org/">http://www.mpi-forum.org/</a></u>

</pre><h4><b>ACKNOWLEDGEMENTS</b></h4><pre>
       The LAM Team would like the thank the MPICH Team for the handy program to generate man  pages  ("doctext"
       from  <u><a href="ftp://ftp.mcs.anl.gov/pub/sowing/sowing.tar.gz">ftp://ftp.mcs.anl.gov/pub/sowing/sowing.tar.gz</a></u> ), the initial formatting, and some initial text for
       most of the MPI-1 man pages.

</pre><h4><b>LOCATION</b></h4><pre>
       spawnmult.c

LAM/MPI 7.1.4                                       6/24/2006                         <u><a href="../man3/MPI_Comm_spawn_multiple.3.html">MPI_Comm_spawn_multiple</a></u>(3)
</pre>
 </div>
</div></section>
</div>
</body>
</html>