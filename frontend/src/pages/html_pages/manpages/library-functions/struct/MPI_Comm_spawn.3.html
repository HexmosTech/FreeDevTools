<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MPI_Comm_spawn -  Spawn a dynamic MPI process</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/lam4-dev">lam4-dev_7.1.4-7.2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       MPI_Comm_spawn -  Spawn a dynamic MPI process

</pre><h4><b>SYNOPSIS</b></h4><pre>
       #include &lt;mpi.h&gt;
       int
       MPI_Comm_spawn(char* command, char** argv, int maxprocs, MPI_Info info,
                      int root, MPI_Comm comm, MPI_Comm *intercomm,
                      int *errcodes)

</pre><h4><b>INPUT</b> <b>PARAMETERS</b></h4><pre>
       <b>command</b>
              - Name of program to spawn (only significant at root)
       <b>argv</b>   - arguments to command (only significant at root)
       <b>maxprocs</b>
              - max number of processes to start (only significant at root)
       <b>info</b>   - startup hints
       <b>root</b>   - rank of process to perform the spawn
       <b>comm</b>   - parent intracommunicator

</pre><h4><b>OUTPUT</b> <b>PARAMETERS</b></h4><pre>
       <b>intercomm</b>
              - child intercommunicator containing spawned processes
       <b>errcodes</b>
              - one code per process

</pre><h4><b>DESCRIPTION</b></h4><pre>
       A  group  of  processes  can  create another group of processes with <u>MPI_Comm_spawn</u> .  This function is a
       collective operation over the parent communicator.  The child group starts up like any  MPI  application.
       The processes must begin by calling <u>MPI_Init</u> , after which the pre-defined communicator, <u>MPI_COMM_WORLD</u> ,
       may  be  used.   This  world  communicator  contains  only  the child processes.  It is distinct from the
       <u>MPI_COMM_WORLD</u> of the parent processes.

       <u>MPI_Comm_spawn_multiple</u> is used to manually specify a group of different  executables  and  arguments  to
       spawn.   <u>MPI_Comm_spawn</u>  is  used  to  specify  one  executable  and set of arguments (although a LAM/MPI
       <a href="../man5/appschema.5.html">appschema</a>(5) can be provided to <u>MPI_Comm_spawn</u> via the "lam_file" info key).

       Communication With Spawned Processes

       The natural communication mechanism between two groups is the intercommunicator.  The second communicator
       argument to <u>MPI_Comm_spawn</u> returns an intercommunicator whose local group contains the  parent  processes
       (same  as  the  first  communicator  argument) and whose remote group contains child processes. The child
       processes can access the same intercommunicator by using the <u>MPI_Comm_get_parent</u> call.  The remote  group
       size  of  the  parent communicator is zero if the process was created by <u>mpirun</u> (1) instead of one of the
       spawn functions.  Both groups can decide to merge the intercommunicator into an  intracommunicator  (with
       the  <u>MPI_Intercomm_merge</u>  function) and take advantage of other MPI collective operations.  They can then
       use the merged intracommunicator to create new  communicators  and  reach  other  processes  in  the  MPI
       application.

       Resource Allocation

       LAM/MPI  offers some MPI_Info keys for the placement of spawned applications.  Keys are looked for in the
       order listed below.  The first key that is found is used; any remaining keys are ignored.

       <u>lam_spawn_file</u>

       The value of this key can be the filename of an <a href="../man1/appschema.1.html">appschema</a>(1).  This allows the programmer to  specify  an
       arbitrary  set  of LAM CPUs or nodes to spawn MPI processes on.  In this case, only the appschema is used
       to spawn the application; <u>command</u> , <u>argv</u> , and <u>maxprocs</u> are all ignored (even at the  root).   Note  that
       even  though  <u>maxprocs</u>  is  ignored, <u>errcodes</u> must still be an array long enough to hold an integer error
       code for every process that tried to launch, or be the MPI constant <u>MPI_ERRCODES_IGNORE</u> .  Also note that
       <u>MPI_Comm_spawn_multiple</u> does <u>not</u> accept the "lam_spawn_file" info key.   As  such,  the  "lam_spawn_file"
       info  key  to  <u>MPI_Comm_spawn</u>  is  mainly intended to spawn MPMD applications and/or specify an arbitrary
       number of nodes to run on.

       Also note that this "lam_spawn_file" key is <u>not</u> portable to other MPI implementations; it is  a  LAM/MPI-
       specific  info  key.   If  specifying exact LAM nodes or CPUs is not necessary, users should probably use
       <u>MPI_Comm_spawn_multiple</u> to make their program more portable.

       <u>file</u>

       This key is a synonym for "lam_spawn_file".  Since "file" is  not  a  LAM-specific  name,  yet  this  key
       carries a LAM-specific meaning, its use is deprecated in favor of "lam_spawn_file".

       <u>lam_spawn_sched_round_robin</u>

       The  value of this key is a string representing a LAM CPU or node (using standard LAM nomenclature -- see
       <a href="../man1/mpirun.1.html">mpirun</a>(1)) to begin spawning on.  The use of this key allows the programmer to  indicate  which  node/CPU
       for LAM to start spawning on without having to write out a temporary app schema file.

       The  CPU  number  is  relative to the boot schema given to <a href="../man1/lamboot.1.html">lamboot</a>(1).  Only a single LAM node/CPU may be
       specified, such as "n3" or "c1".  If a node is specified, LAM will spawn one MPI process per node.  If  a
       CPU is specified, LAM will scedule one MPI process per CPU.  An error is returned if "N" or "C" is used.

       Note that LAM is not involved with run-time scheduling of the MPI process -- LAM only spawns processes on
       indicated  nodes.   The  operating  system  schedules these processes for executation just like any other
       process.  No attempt is made by LAM to bind processes to CPUs.  Hence, the "cX" nomenclature  is  just  a
       convenicence  mechanism  to inidicate how many MPI processes should be spawned on a given node; it is not
       indicative of operating system scheduling.

       For "nX" values, the first MPI process will be spawned on the indicated node.  The remaining (maxprocs  -
       1)  MPI  processes  will be spawned on successive nodes.  Specifically, if X is the starting node number,
       process i will be launched on "nK", where K = ((X + i) % total_nodes).  LAM will modulus the node  number
       with  the  total  number  of  nodes  in  the  current  LAM universe to prevent errors, thereby creating a
       "wraparound" effect.  Hence, this mechanism can be used for round-robin  scheduling,  regardless  of  how
       many nodes are in the LAM universe.

       For  "cX"  values, the algorithm is essentially the same, except that LAM will resolve "cX" to a specific
       node before spawning, and successive processes are spawned on the node where "cK" resides, where K =  ((X
       + i) % total_cpus).

       For  example,  if  there  are  8  nodes  and  16  CPUs  in  the current LAM universe (2 CPUs per node), a
       "lam_spawn_sched_round_robin" key is given with the value of "c14", and maxprocs is 4, LAM will spawn MPI

</pre><h4><b>PROCESSES</b> <b>ON</b></h4><pre>
       CPU  Node  MPI_COMM_WORLD rank
       ---  ----  -------------------
       c14  n7    0
       c15  n7    1
       c0   n0    2
       c1   n0    3

       <u>lam_no_root_node_schedule</u>

       This key is used to designate that the spawned processes must not be spawned or scheduled  on  the  "root
       node"  (the  node doing the spawn). There is no specific value associated with this key, but it should be
       given some non-null/non-empty dummy value.

       It is a node-specific key and not a CPU-specific one. Hence if the root node has multiple CPUs,  none  of
       the CPUs on this root node will take part in the scheduling of the spawned processes.

       No keys given

       If  none of the info keys listed above are used, the value of <u>MPI_INFO_NULL</u> should be given for <u>info</u> (all
       other keys are ignored, anyway - there is no harm in providing other keys).  In this case, LAM  schedules
       the  given  number  of  processes onto LAM nodes by starting with CPU 0 (or the lowest numbered CPU), and
       continuing through higher CPU numbers, placing one process on each CPU.  If the process count is  greater
       than the CPU count, the procedure repeats.

       Predefined Attributes

       The  pre-defined  attribute on <u>MPI_COMM_WORLD</u> , <u>MPI_UNIVERSE_SIZE</u> , can be useful in determining how many
       CPUs are currently unused.  For example, the value in <u>MPI_UNIVERSE_SIZE</u> is the number of  CPUs  that  LAM
       was  booted  with  (see <a href="../man1/MPI_Init.1.html">MPI_Init</a>(1)).  Subtracting the size of <u>MPI_COMM_WORLD</u> from this value returns the
       number of CPUs in the current LAM universe that the current application is not using (and  are  therefore
       likely not being used).

       Process Terminiation

       Note  that  the  process[es]  spawned by <u>MPI_COMM_SPAWN</u> (and <u>MPI_COMM_SPAWN_MULTIPLE</u> ) effectively become
       orphans.  That is, the spawnning MPI application does not wait for the  spawned  application  to  finish.
       Hence,  there  is  no  guarantee  the  spawned  application  has  finished  when  the spawning completes.
       Similarly, killing the spawning application will also have no effect on the spawned application.

       User applications can effect this kind of behavior with <u>MPI_BARRIER</u>  between  the  spawning  and  spawned
       processed before <u>MPI_FINALIZE</u> .

       Note that <u>lamclean</u> will kill *all* MPI processes.

       Process Count

       The <u>maxprocs</u> parameter to <u>MPI_Comm_spawn</u> specifies the exact number of processes to be started.  If it is
       not  possible  to  start the desired number of processes, <u>MPI_Comm_spawn</u> will return an error code.  Note
       that even though <u>maxprocs</u> is only relevant on the root, all ranks must have an <u>errcodes</u> array long enough
       to handle an integer  error  code  for  every  process  that  tries  to  launch,  or  give  MPI  constant
       <u>MPI_ERRCODES_IGNORE</u>  for  the <u>errcodes</u> argument.  While this appears to be a contradiction, it is per the
       MPI-2 standard.  :-\

       Frequently, an application wishes to chooses a process count so as to fill all processors available to  a
       job.   MPI  indicates the maximum number of processes recommended for a job in the pre-defined attribute,
       <u>MPI_UNIVERSE_SIZE</u> , which is cached on <u>MPI_COMM_WORLD</u> .

       The typical usage is to subtract the value of <u>MPI_UNIVERSE_SIZE</u> from the number of processes currently in
       the job and spawn the difference.  LAM sets <u>MPI_UNIVERSE_SIZE</u> to the number of CPUs  in  the  user's  LAM
       session (as defined in the boot schema [<a href="../man5/bhost.5.html">bhost</a>(5)] via <u>lamboot</u> (1)).

       See <a href="../man3/MPI_Init.3.html">MPI_Init</a>(3) for other pre-defined attributes that are helpful when spawning.

       Locating an Executable Program

       The  executable program file must be located on the node(s) where the process(es) will run.  On any node,
       the directories specified by the user's PATH environment variable are searched to find the program.

       All MPI runtime options selected by <u>mpirun</u> (1) in the initial application launch remain in effect for all
       child processes created by the spawn functions.

       Command-line Arguments

       The <u>argv</u> parameter to <u>MPI_Comm_spawn</u> should not contain the program name since it is given in  the  first
       parameter.   The  command  line  that  is  passed  to the newly launched program will be the program name
       followed by the strings in <u>argv</u> .

</pre><h4><b>USAGE</b> <b>WITH</b> <b>IMPI</b> <b>EXTENSIONS</b></h4><pre>
       The IMPI standard only supports MPI-1 functions.  Hence, this  function  is  currently  not  designed  to
       operate within an IMPI job.

</pre><h4><b>ERRORS</b></h4><pre>
       If an error occurs in an MPI function, the current MPI error handler is called to handle it.  By default,
       this  error  handler  aborts the MPI job.  The error handler may be changed with <u>MPI_Errhandler_set</u> ; the
       predefined error handler <u>MPI_ERRORS_RETURN</u> may be used to cause error values to be  returned  (in  C  and
       Fortran;  this  error  handler is less useful in with the C++ MPI bindings.  The predefined error handler
       <u>MPI::ERRORS_THROW_EXCEPTIONS</u> should be used in C++ if the error value needs to be recovered).  Note  that
       MPI does <u>not</u> guarantee that an MPI program can continue past an error.

       All  MPI routines (except <u>MPI_Wtime</u> and <u>MPI_Wtick</u> ) return an error value; C routines as the value of the
       function and Fortran routines in the last argument.  The C++ bindings for MPI do not return error values;
       instead, error values are communicated  by  throwing  exceptions  of  type  <u>MPI::Exception</u>  (but  not  by
       default).  Exceptions are only thrown if the error value is not <u>MPI::SUCCESS</u> .

       Note that if the <u>MPI::ERRORS_RETURN</u> handler is set in C++, while MPI functions will return upon an error,
       there will be no way to recover what the actual error value was.
       <b>MPI_SUCCESS</b>
              - No error; MPI routine completed successfully.
       <b>MPI_ERR_COMM</b>
              -  Invalid communicator.  A common error is to use a null communicator in a call (not even allowed
              in <u>MPI_Comm_rank</u> ).
       <b>MPI_ERR_SPAWN</b>
              - Spawn error; one or more of the applications  attempting  to  be  launched  failed.   Check  the
              returned error code array.
       <b>MPI_ERR_ARG</b>
              -  Invalid  argument.   Some  argument is invalid and is not identified by a specific error class.
              This is typically a NULL pointer or other such error.
       <b>MPI_ERR_ROOT</b>
              - Invalid root.  The root must be specified as a rank in the communicator.  Ranks must be  between
              zero and the size of the communicator minus one.
       <b>MPI_ERR_OTHER</b>
              - Other error; use <u>MPI_Error_string</u> to get more information about this error code.
       <b>MPI_ERR_INTERN</b>
              -  An  internal  error  has  been  detected.   This is fatal.  Please send a bug report to the LAM
              mailing list (see <u><a href="http://www.lam-mpi.org/contact.php">http://www.lam-mpi.org/contact.php</a></u> ).
       <b>MPI_ERR_NO_MEM</b>
              - This error class is associated with an error code that indicates that free space is exhausted.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man5/appschema.5.html">appschema</a>(5),      <a href="../man5/bhost.5.html">bhost</a>(5),      <a href="../man1/lamboot.1.html">lamboot</a>(1),       <a href="../man3/MPI_Comm_get_parent.3.html">MPI_Comm_get_parent</a>(3),       <a href="../man3/MPI_Intercomm_merge.3.html">MPI_Intercomm_merge</a>(3),
       <a href="../man3/MPI_Comm_spawn_multiple.3.html">MPI_Comm_spawn_multiple</a>(3),  <a href="../man3/MPI_Info_create.3.html">MPI_Info_create</a>(3),  <a href="../man3/MPI_Info_set.3.html">MPI_Info_set</a>(3),  <a href="../man3/MPI_Info_delete.3.html">MPI_Info_delete</a>(3), <a href="../man3/MPI_Info_free.3.html">MPI_Info_free</a>(3),
       <a href="../man3/MPI_Init.3.html">MPI_Init</a>(3), <a href="../man1/mpirun.1.html">mpirun</a>(1)

</pre><h4><b>MORE</b> <b>INFORMATION</b></h4><pre>
       For more information, please see the official MPI Forum web site, which contains the  text  of  both  the
       MPI-1 and MPI-2 standards.  These documents contain detailed information about each MPI function (most of
       which is not duplicated in these man pages).

       <u><a href="http://www.mpi-forum.org/">http://www.mpi-forum.org/</a></u>

</pre><h4><b>ACKNOWLEDGEMENTS</b></h4><pre>
       The  LAM  Team would like the thank the MPICH Team for the handy program to generate man pages ("doctext"
       from <u><a href="ftp://ftp.mcs.anl.gov/pub/sowing/sowing.tar.gz">ftp://ftp.mcs.anl.gov/pub/sowing/sowing.tar.gz</a></u> ), the initial formatting, and some initial text  for
       most of the MPI-1 man pages.

</pre><h4><b>LOCATION</b></h4><pre>
       spawn.c

LAM/MPI 7.1.4                                       6/24/2006                                  <u><a href="../man3/MPI_Comm_spawn.3.html">MPI_Comm_spawn</a></u>(3)
</pre>
 </div>
</div></section>
</div>
</body>
</html>