<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>nlopt_minimize_constrained - Minimize a multivariate nonlinear function subject to nonlinear constraints</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/libnlopt-dev">libnlopt-dev_2.7.1-6ubuntu3_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       nlopt_minimize_constrained - Minimize a multivariate nonlinear function subject to nonlinear constraints

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>#include</b> <b>&lt;nlopt.h&gt;</b>

       <b>nlopt_result</b> <b>nlopt_minimize_constrained(nlopt_algorithm</b> <u>algorithm</u><b>,</b>
                                   <b>int</b> <u>n</u><b>,</b>
                                   <b>nlopt_func</b> <u>f</u><b>,</b>
                                   <b>void*</b> <u>f_data</u><b>,</b>
                                   <b>int</b> <u>m</u><b>,</b>
                                   <b>nlopt_func</b> <u>fc</u><b>,</b>
                                   <b>void*</b> <u>fc_data</u><b>,</b>
                                   <b>ptrdiff_t</b> <u>fc_datum_size</u><b>,</b>
                                   <b>const</b> <b>double*</b> <u>lb</u><b>,</b>
                                   <b>const</b> <b>double*</b> <u>ub</u><b>,</b>
                                   <b>double*</b> <u>x</u><b>,</b>
                                   <b>double*</b> <u>minf</u><b>,</b>
                                   <b>double</b> <u>minf_max</u><b>,</b>
                                   <b>double</b> <u>ftol_rel</u><b>,</b>
                                   <b>double</b> <u>ftol_abs</u><b>,</b>
                                   <b>double</b> <u>xtol_rel</u><b>,</b>
                                   <b>const</b> <b>double*</b> <u>xtol_abs</u><b>,</b>
                                   <b>int</b> <u>maxeval</u><b>,</b>
                                   <b>double</b> <u>maxtime</u><b>);</b>

       You should link the resulting program with the linker flags
       -lnlopt -lm on Unix.

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>nlopt_minimize_constrained</b>()  attempts  to minimize a nonlinear function <u>f</u> of <u>n</u> design variables, subject
       to <u>m</u> nonlinear constraints described by the function <u>fc</u> (see below), using the specified <u>algorithm</u>.   The
       minimum  function value found is returned in <u>minf</u>, with the corresponding design variable values returned
       in the array <u>x</u> of length <u>n</u>.  The input values in <u>x</u> should be a  starting  guess  for  the  optimum.   The
       inputs  <u>lb</u>  and  <u>ub</u> are arrays of length <u>n</u> containing lower and upper bounds, respectively, on the design
       variables <u>x</u>.  The other parameters specify stopping criteria (tolerances, the maximum number of  function
       evaluations,  etcetera)  and  other information as described in more detail below.  The return value is a
       integer code indicating success (positive) or failure (negative), as described below.

       By changing the parameter <u>algorithm</u> among several predefined constants described below,  one  can  switch
       easily  between  a  variety  of  minimization  algorithms.  Some of these algorithms require the gradient
       (derivatives) of the function to be supplied via <u>f</u>, and other  algorithms  do  not  require  derivatives.
       Some  of  the algorithms attempt to find a global minimum within the given bounds, and others find only a
       local minimum.  Most of the algorithms only handle the case  where  <u>m</u>  is  zero  (no  explicit  nonlinear
       constraints); the only algorithms that currently support positive <u>m</u> are <b>NLOPT_LD_MMA</b> and <b>NLOPT_LN_COBYLA</b>.

       The  <b>nlopt_minimize_constrained</b>  function  is  a  wrapper  around  several  free/open-source minimization
       packages, as well as some new implementations  of  published  optimization  algorithms.   You  could,  of
       course,  compile  and  call  these  packages  separately,  and  in  some  cases this will provide greater
       flexibility than is available via the <b>nlopt_minimize_constrained</b> interface.  However, depending upon  the
       specific  function  being  minimized, the different algorithms will vary in effectiveness.  The intent of
       <b>nlopt_minimize_constrained</b> is to allow you to quickly switch between algorithms in  order  to  experiment
       with them for your problem, by providing a simple unified interface to these subroutines.

</pre><h4><b>OBJECTIVE</b> <b>FUNCTION</b></h4><pre>
       <b>nlopt_minimize_constrained</b>() minimizes an objective function <u>f</u> of the form:

             <b>double</b> <b>f(int</b> <u>n</u><b>,</b>
                      <b>const</b> <b>double*</b> <u>x</u><b>,</b>
                      <b>double*</b> <u>grad</u><b>,</b>
                      <b>void*</b> <u>f_data</u><b>);</b>

       The return value should be the value of the function at the point <u>x</u>, where <u>x</u> points to an array of length
       <u>n</u>   of   the   design   variables.    The   dimension   <u>n</u>   is   identical   to   the   one   passed   to
       <b>nlopt_minimize_constrained</b>().

       In addition, if the argument <u>grad</u> is not NULL, then <u>grad</u> points to an array  of  length  <u>n</u>  which  should
       (upon return) be set to the gradient of the function with respect to the design variables at <u>x</u>.  That is,
       <u>grad[i]</u>  should upon return contain the partial derivative df/dx[i], for 0 &lt;= i &lt; n, if <u>grad</u> is non-NULL.
       Not all of the optimization algorithms (below) use the gradient information:  for  algorithms  listed  as
       "derivative-free,"  the  <u>grad</u>  argument  will always be NULL and need never be computed.  (For algorithms
       that do use gradient information, however, <u>grad</u> may still be NULL for some calls.)

       The <u>f_data</u> argument is the same as the one passed to <b>nlopt_minimize_constrained</b>(), and  may  be  used  to
       pass  any  additional data through to the function.  (That is, it may be a pointer to some caller-defined
       data structure/type containing information your function  needs,  which  you  convert  from  void*  by  a
       typecast.)

</pre><h4><b>BOUND</b> <b>CONSTRAINTS</b></h4><pre>
       Most  of the algorithms in NLopt are designed for minimization of functions with simple bound constraints
       on the inputs.  That is, the input vectors x[i] are constrainted to lie in a hyperrectangle lb[i] &lt;= x[i]
       &lt;= ub[i] for 0 &lt;= i &lt; n, where <u>lb</u> and <u>ub</u> are the two arrays passed to <b>nlopt_minimize_constrained</b>().

       However, a few of the algorithms support partially or totally unconstrained optimization, as noted below,
       where a (totally or partially) unconstrained design variable is indicated by a lower bound equal to  -Inf
       and/or  an  upper bound equal to +Inf.  Here, Inf is the IEEE-754 floating-point infinity, which (in ANSI
       C99) is represented by the macro INFINITY in math.h.  Alternatively, for older C versions  you  may  also
       use the macro HUGE_VAL (also in math.h).

       With  some  of the algorithms, especially those that do not require derivative information, a simple (but
       not especially efficient) way to implement arbitrary nonlinear constraints is to return Inf  (see  above)
       whenever  the  constraints  are  violated  by a given input <u>x</u>.  More generally, there are various ways to
       implement constraints by adding "penalty terms" to your objective function, which are  described  in  the
       optimization  literature.  A much more efficient way to specify nonlinear constraints is described below,
       but is only supported by a small subset of the algorithms.

</pre><h4><b>NONLINEAR</b> <b>CONSTRAINTS</b></h4><pre>
       The <b>nlopt_minimize_constrained</b> function also allows you  to  specify  <u>m</u>  nonlinear  constraints  via  the
       function  <u>fc</u>,  where <u>m</u> is any nonnegative integer.  However, nonzero <u>m</u> is currently only supported by the
       <b>NLOPT_LD_MMA</b> and <b>NLOPT_LN_COBYLA</b> algorithms below.

       In particular, the nonlinear constraints are of the form <u>fc</u>(<u>x</u>) &lt;= 0, where the function <u>fc</u> is of the same
       form as the objective function described above:

             <b>double</b> <b>fc(int</b> <u>n</u><b>,</b>
                       <b>const</b> <b>double*</b> <u>x</u><b>,</b>
                       <b>double*</b> <u>grad</u><b>,</b>
                       <b>void*</b> <u>fc_datum</u><b>);</b>

       The return value should be the value of the constraint at the point <u>x</u>, where the dimension <u>n</u> is identical
       to the one passed to <b>nlopt_minimize_constrained</b>().  As for the objective function, if the  argument  <u>grad</u>
       is  not  NULL, then <u>grad</u> points to an array of length <u>n</u> which should (upon return) be set to the gradient
       of the function with respect to <u>x</u>.  (For any  algorithm  listed  as  "derivative-free"  below,  the  <u>grad</u>
       argument will always be NULL and need never be computed.)

       The <u>fc_datum</u> argument is based on the <u>fc_data</u> argument passed to <b>nlopt_minimize_constrained</b>(), and may be
       used  to  pass  any additional data through to the function, and is used to distinguish between different
       constraints.

       In particular, the constraint function <u>fc</u> will be called (at most) <u>m</u> times  for  each  <u>x</u>,  and  the  i-th
       constraint (0 &lt;= i &lt; <u>m</u>) will be passed an <u>fc_datum</u> argument equal to <u>fc_data</u> offset by i * <u>fc_datum_size</u>.
       For  example, suppose that you have a data structure of type "foo" that describes the data needed by each
       constraint, and you store the information for the constraints in an array "foo data[m]".  In  this  case,
       you  would  pass  "data" as the <u>fc_data</u> parameter to <b>nlopt_minimize_constrained</b>, and "sizeof(foo)" as the
       <u>fc_datum_size</u> parameter.  Then, your <u>fc</u> function would be called <u>m</u> times for each point,  and  be  passed
       &amp;data[0] through &amp;data[m-1] in sequence.

</pre><h4><b>ALGORITHMS</b></h4><pre>
       The  <u>algorithm</u>  parameter  specifies the optimization algorithm (for more detail on these, see the README
       files in the source-code subdirectories),  and  can  take  on  any  of  the  following  constant  values.
       Constants  with  <b>_G{N,D}_</b> in their names refer to global optimization methods, whereas <b>_L{N,D}_</b> refers to
       local optimization methods (that try to find a  local  minimum  starting  from  the  starting  guess  <u>x</u>).
       Constants  with  <b>_{G,L}N_</b>  refer  to  non-gradient  (derivative-free)  algorithms that do not require the
       objective function to supply a gradient, whereas <b>_{G,L}D_</b>  refers  to  derivative-based  algorithms  that
       require  the  objective  function  to supply a gradient.  (Especially for local optimization, derivative-
       based algorithms are generally superior to derivative-free ones: the gradient is good to have <u>if</u> you  can
       compute it cheaply, e.g. via an adjoint method.)

       <b>NLOPT_GN_DIRECT_L</b>
              Perform a global (G) derivative-free (N) optimization using the DIRECT-L search algorithm by Jones
              et al. as modified by Gablonsky et al. to be more weighted towards local search.  Does not support
              unconstrainted  optimization.   There are also several other variants of the DIRECT algorithm that
              are supported: <b>NLOPT_GN_DIRECT</b>, which is the original DIRECT algorithm; <b>NLOPT_GN_DIRECT_L_RAND</b>,  a
              slightly  randomized  version  of  DIRECT-L  that may be better in high-dimensional search spaces;
              <b>NLOPT_GN_DIRECT_NOSCAL</b>, <b>NLOPT_GN_DIRECT_L_NOSCAL</b>,  and  <b>NLOPT_GN_DIRECT_L_RAND_NOSCAL</b>,  which  are
              versions  of  DIRECT  where  the dimensions are not rescaled to a unit hypercube (which means that
              dimensions with larger bounds are given more weight).

       <b>NLOPT_GN_ORIG_DIRECT_L</b>
              A global (G) derivative-free optimization using  the  DIRECT-L  algorithm  as  above,  along  with
              <b>NLOPT_GN_ORIG_DIRECT</b>  which  is  the  original  DIRECT algorithm.  Unlike <b>NLOPT_GN_DIRECT_L</b> above,
              these two algorithms refer to code based on the original Fortran code of Gablonsky et  al.,  which
              has some hard-coded limitations on the number of subdivisions etc. and does not support all of the
              NLopt  stopping  criteria,  but  on  the  other  hand  supports arbitrary nonlinear constraints as
              described above.

       <b>NLOPT_GD_STOGO</b>
              Global (G) optimization using the StoGO algorithm  by  Madsen  et  al.   StoGO  exploits  gradient
              information (D) (which must be supplied by the objective) for its local searches, and performs the
              global  search by a branch-and-bound technique.  Only bound-constrained optimization is supported.
              There is also another variant of  this  algorithm,  <b>NLOPT_GD_STOGO_RAND</b>,  which  is  a  randomized
              version  of the StoGO search scheme.  The StoGO algorithms are only available if NLopt is compiled
              with C++ enabled, and should be linked via -lnlopt_cxx (via a C++ compiler, in order to  link  the
              C++ standard libraries).

       <b>NLOPT_LN_NELDERMEAD</b>
              Perform a local (L) derivative-free (N) optimization, starting at <u>x</u>, using the Nelder-Mead simplex
              algorithm,  modified  to  support  bound  constraints.   Nelder-Mead,  while  popular, is known to
              occasionally fail to converge for some objective functions, so it should  be  used  with  caution.
              Anecdotal  evidence,  on  the  other  hand,  suggests  that it works fairly well for discontinuous
              objectives.  See also <b>NLOPT_LN_SBPLX</b> below.

       <b>NLOPT_LN_SBPLX</b>
              Perform a local (L) derivative-free (N) optimization, starting at <u>x</u>, using an algorithm  based  on
              the  Subplex  algorithm of Rowan et al., which is an improved variant of Nelder-Mead (above).  Our
              implementation does not use Rowan's original code,  and  has  some  minor  modifications  such  as
              explicit  support for bound constraints.  (Like Nelder-Mead, Subplex often works well in practice,
              even for discontinuous objectives, but there is no rigorous  guarantee  that  it  will  converge.)
              Nonlinear  constraints  can  be  crudely  supported  by  returning  +Inf  when the constraints are
              violated, as explained above.

       <b>NLOPT_LN_PRAXIS</b>
              Local (L) derivative-free (N) optimization using the  principal-axis  method,  based  on  code  by
              Richard  Brent.  Designed for unconstrained optimization, although bound constraints are supported
              too (via the inefficient method of returning +Inf when the constraints are violated).

       <b>NLOPT_LD_LBFGS</b>
              Local (L) gradient-based (D) optimization using the limited-memory BFGS (L-BFGS) algorithm.   (The
              objective function must supply the gradient.)  Unconstrained optimization is supported in addition
              to simple bound constraints (see above).  Based on an implementation by Luksan et al.

       <b>NLOPT_LD_VAR2</b>
              Local  (L)  gradient-based  (D) optimization using a shifted limited-memory variable-metric method
              based on code by Luksan et al., supporting both unconstrained and bound-constrained  optimization.
              <b>NLOPT_LD_VAR2</b>  uses  a  rank-2  method,  while  <b>.B</b> <b>NLOPT_LD_VAR1</b> is another variant using a rank-1
              method.

       <b>NLOPT_LD_TNEWTON_PRECOND_RESTART</b>
              Local (L) gradient-based (D) optimization using an LBFGS-preconditioned  truncated  Newton  method
              with  steepest-descent  restarting,  based on code by Luksan et al., supporting both unconstrained
              and  bound-constrained  optimization.   There  are  several  other  variants  of  this  algorithm:
              <b>NLOPT_LD_TNEWTON_PRECOND</b>   (same   without  restarting),  <b>NLOPT_LD_TNEWTON_RESTART</b>  (same  without
              preconditioning), and <b>NLOPT_LD_TNEWTON</b> (same without restarting or preconditioning).

       <b>NLOPT_GN_CRS2_LM</b>
              Global (G) derivative-free (N) optimization using the controlled random search (CRS2) algorithm of
              Price, with the "local mutation" (LM) modification suggested by Kaelo and Ali.

       <b>NLOPT_GD_MLSL_LDS</b>, <b>NLOPT_GN_MLSL_LDS</b>
              Global (G) derivative-based (D) or derivative-free (N) optimization using the multi-level  single-
              linkage  (MLSL) algorithm with a low-discrepancy sequence (LDS).  This algorithm executes a quasi-
              random (LDS) sequence of local searches, with a  clustering  heuristic  to  avoid  multiple  local
              searches for the same local minimum.  The local search uses the derivative/nonderivative algorithm
              set  by <u>nlopt_set_local_search_algorithm</u> (currently defaulting to <u>NLOPT_LD_MMA</u> and <u>NLOPT_LN_COBYLA</u>
              for  derivative/nonderivative  searches,  respectively).   There  are  also  two  other  variants,
              <b>NLOPT_GD_MLSL</b>  and  <b>NLOPT_GN_MLSL</b>,  which  use pseudo-random numbers (instead of an LDS) as in the
              original MLSL algorithm.

       <b>NLOPT_LD_MMA</b>
              Local (L) gradient-based (D) optimization using the method of moving asymptotes (MMA), or rather a
              refined version of the algorithm as published by Svanberg  (2002).   (NLopt  uses  an  independent
              free-software/open-source  implementation  of  Svanberg's  algorithm.)  The <b>NLOPT_LD_MMA</b> algorithm
              supports both bound-constrained and unconstrained optimization, and  also  supports  an  arbitrary
              number (<u>m</u>) of nonlinear constraints as described above.

       <b>NLOPT_LN_COBYLA</b>
              Local  (L)  derivative-free  (N)  optimization  using  the COBYLA algorithm of Powell (Constrained
              Optimization BY Linear  Approximations).   The  <b>NLOPT_LN_COBYLA</b>  algorithm  supports  both  bound-
              constrained and unconstrained optimization, and also supports an arbitrary number (<u>m</u>) of nonlinear
              constraints as described above.

       <b>NLOPT_LN_NEWUOA</b>
              Local  (L) derivative-free (N) optimization using a variant of the the NEWUOA algorithm of Powell,
              based on successive quadratic approximations of the  objective  function.  We  have  modified  the
              algorithm  to  support  bound  constraints.   The  original NEWUOA algorithm is also available, as
              <b>NLOPT_LN_NEWUOA</b>, but this algorithm ignores the bound constraints <u>lb</u> and <u>ub</u>, and so it should only
              be used for unconstrained problems.

</pre><h4><b>STOPPING</b> <b>CRITERIA</b></h4><pre>
       Multiple stopping criteria for the optimization are supported, as specified by the following arguments to
       <b>nlopt_minimize_constrained</b>().  The optimization halts whenever any one of these  criteria  is  satisfied.
       In some cases, the precise interpretation of the stopping criterion depends on the optimization algorithm
       above  (although we have tried to make them as consistent as reasonably possible), and some algorithms do
       not support all of the stopping criteria.

       Important: you do not need to use all of the stopping criteria!  In most cases, you only need one or two,
       and can set the remainder to values where they do nothing (as described below).

       <b>minf_max</b>
              Stop when a function value less than or equal to <u>minf_max</u> is found.   Set  to  -Inf  or  NaN  (see
              constraints section above) to disable.

       <b>ftol_rel</b>
              Relative  tolerance  on  function  value:  stop  when  an optimization step (or an estimate of the
              minimum) changes the function value by less than <u>ftol_rel</u> multiplied by the absolute value of  the
              function  value.   (If  there is any chance that your minimum function value is close to zero, you
              might want to set an absolute tolerance with <u>ftol_abs</u> as well.)  Disabled if non-positive.

       <b>ftol_abs</b>
              Absolute tolerance on function value: stop when an  optimization  step  (or  an  estimate  of  the
              minimum) changes the function value by less than <u>ftol_abs</u>.  Disabled if non-positive.

       <b>xtol_rel</b>
              Relative  tolerance  on  design  variables:  stop when an optimization step (or an estimate of the
              minimum) changes every design variable by less than <u>xtol_rel</u> multiplied by the absolute  value  of
              the  design  variable.   (If there is any chance that an optimal design variable is close to zero,
              you might want to set an absolute tolerance with <u>xtol_abs</u> as well.)  Disabled if non-positive.

       <b>xtol_abs</b>
              Pointer to an array of length <u>n</u> <u>giving</u> <u>absolute</u> <u>tolerances</u>  <u>on</u>  <u>design</u>  <u>variables:</u>  <u>stop</u>  <u>when</u>  <u>an</u>
              optimization  step (or an estimate of the minimum) changes every design variable <u>x</u>[i] by less than
              <u>xtol_abs</u>[i].  Disabled if non-positive, or if <u>xtol_abs</u> is NULL.

       <b>maxeval</b>
              Stop when the number of function evaluations exceeds <u>maxeval</u>.  (This is not a strict maximum:  the
              number  of  function  evaluations  may  exceed  <u>maxeval</u>  slightly,  depending upon the algorithm.)
              Disabled if non-positive.

       <b>maxtime</b>
              Stop when the optimization time (in seconds) exceeds <u>maxtime</u>.  (This is not a strict maximum:  the
              time  may  exceed  <u>maxtime</u>  slightly,  depending  upon the algorithm and on how slow your function
              evaluation is.)  Disabled if non-positive.

</pre><h4><b>RETURN</b> <b>VALUE</b></h4><pre>
       The value returned is one of the following enumerated constants.

   <b>Successful</b> <b>termination</b> <b>(positive</b> <b>return</b> <b>values):</b>
       <b>NLOPT_SUCCESS</b>
              Generic success return value.

       <b>NLOPT_MINF_MAX_REACHED</b>
              Optimization stopped because <u>minf_max</u> (above) was reached.

       <b>NLOPT_FTOL_REACHED</b>
              Optimization stopped because <u>ftol_rel</u> or <u>ftol_abs</u> (above) was reached.

       <b>NLOPT_XTOL_REACHED</b>
              Optimization stopped because <u>xtol_rel</u> or <u>xtol_abs</u> (above) was reached.

       <b>NLOPT_MAXEVAL_REACHED</b>
              Optimization stopped because <u>maxeval</u> (above) was reached.

       <b>NLOPT_MAXTIME_REACHED</b>
              Optimization stopped because <u>maxtime</u> (above) was reached.

   <b>Error</b> <b>codes</b> <b>(negative</b> <b>return</b> <b>values):</b>
       <b>NLOPT_FAILURE</b>
              Generic failure code.

       <b>NLOPT_INVALID_ARGS</b>
              Invalid arguments (e.g. lower bounds are bigger  than  upper  bounds,  an  unknown  algorithm  was
              specified, etcetera).

       <b>NLOPT_OUT_OF_MEMORY</b>
              Ran out of memory.

</pre><h4><b>PSEUDORANDOM</b> <b>NUMBERS</b></h4><pre>
       For  stochastic  optimization  algorithms,  we use pseudorandom numbers generated by the Mersenne Twister
       algorithm, based on code from Makoto Matsumoto.  By default, the seed for the random numbers is generated
       from the system time, so that they will be different each time you run the program.  If you want  to  use
       deterministic random numbers, you can set the seed by calling:

                   <b>void</b> <b>nlopt_srand(unsigned</b> <b>long</b> <u>seed</u><b>);</b>

       Some  of  the  algorithms  also  support using low-discrepancy sequences (LDS), sometimes known as quasi-
       random numbers.  NLopt uses the Sobol LDS, which is implemented for up to 1111 dimensions.

</pre><h4><b>AUTHORS</b></h4><pre>
       Written by Steven G. Johnson.

       Copyright (c) 2007-2014 Massachusetts Institute of Technology.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man3/nlopt_minimize.3.html">nlopt_minimize</a>(3)

MIT                                                2007-08-23                      <u><a href="../man3/NLOPT_MINIMIZE_CONSTRAINED.3.html">NLOPT_MINIMIZE_CONSTRAINED</a></u>(3)
</pre>
 </div>
</div></section>
</div>
</body>
</html>