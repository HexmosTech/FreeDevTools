<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WWW::Mechanize::FAQ - Frequently Asked Questions about WWW::Mechanize</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/libwww-mechanize-perl">libwww-mechanize-perl_2.19-1ubuntu1_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       WWW::Mechanize::FAQ - Frequently Asked Questions about WWW::Mechanize

</pre><h4><b>VERSION</b></h4><pre>
       version 2.19

</pre><h4><b>How</b> <b>to</b> <b>get</b> <b>help</b> <b>with</b> <b>WWW::Mechanize</b></h4><pre>
       If your question isn't answered here in the FAQ, please turn to the communities at:

       •   StackOverflow &lt;https://stackoverflow.com/questions/tagged/www-mechanize&gt;

       •   #lwp on irc.perl.org

       •   &lt;<a href="http://perlmonks.org">http://perlmonks.org</a>&gt;

       •   The libwww-perl mailing list at &lt;<a href="http://lists.perl.org">http://lists.perl.org</a>&gt;

</pre><h4><b>JavaScript</b></h4><pre>
   <b>I</b> <b>have</b> <b>this</b> <b>web</b> <b>page</b> <b>that</b> <b>has</b> <b>JavaScript</b> <b>on</b> <b>it,</b> <b>and</b> <b>my</b> <b>Mech</b> <b>program</b> <b>doesn't</b> <b>work.</b>
       That's  because  WWW::Mechanize doesn't operate on the JavaScript.  It only understands the HTML parts of
       the page.

   <b>I</b> <b>thought</b> <b>Mech</b> <b>was</b> <b>supposed</b> <b>to</b> <b>work</b> <b>like</b> <b>a</b> <b>web</b> <b>browser.</b>
       It does pretty much, but it doesn't support JavaScript.

       I  added  some  basic  attempts  at  picking  up  URLs  in  "window.open()"  calls  and  return  them  in
       "$mech-&gt;links".  They work sometimes.

       Since  Javascript  is  completely  visible  to  the  client,  it cannot be used to prevent a scraper from
       following links. But it can make life difficult. If you want to scrape specific pages, then a solution is
       always possible.

       One typical use of Javascript is to perform argument checking before posting to the server. The  URL  you
       want   is   probably  just  buried  in  the  Javascript  function.  Do  a  regular  expression  match  on
       "$mech-&gt;content()" to find the link that you want and "$mech-&gt;get" it directly  (this  assumes  that  you
       know what you are looking for in advance).

       In more difficult cases, the Javascript is used for URL mangling to satisfy the needs of some middleware.
       In  this  case  you  need  to  figure  out what the Javascript is doing (why are these URLs always really
       long?). There is probably some function with one or more arguments which calculates  the  new  URL.  Step
       one:  using  your favorite browser, get the before and after URLs and save them to files. Edit each file,
       converting the argument separators ('?', '&amp;' or ';') into newlines. Now it is easy to use diff or comm to
       find out what Javascript did to the URL.  Step 2 - find the function call which created  the  URL  -  you
       will  need to parse and interpret its argument list. The Javascript Debugger in the Firebug extension for
       Firefox helps with the analysis. At this point, it is fairly trivial to write  your  own  function  which
       emulates the Javascript for the pages you want to process.

       Here's  another  approach that answers the question, "It works in Firefox, but why not Mech?"  Everything
       the web server knows about the client is present in the HTTP request. If two requests are identical,  the
       results  should be identical. So the real question is "What is different between the mech request and the
       Firefox request?"

       The Firefox extension "Tamper Data" is an effective tool for examining the headers of the requests to the
       server. Compare that with what LWP is sending. Once the two are  identical,  the  action  of  the  server
       should be the same as well.

       I  say  "should",  because  this  is  an  oversimplification  -  some values are naturally unique, e.g. a
       SessionID, but if a SessionID is present, that is probably sufficient, even  though  the  value  will  be
       different  between  the  LWP  request  and the Firefox request. The server could use the session to store
       information which is troublesome, but that's not the first place to  look  (and  highly  unlikely  to  be
       relevant when you are requesting the login page of your site).

       Generally  the  problem  is to be found in missing or incorrect POSTDATA arguments, Cookies, User-Agents,
       Accepts, etc. If you are using mech, then redirects and cookies should not be a problem, but  are  listed
       here  for  completeness.  If  you are missing headers, "$mech-&gt;add_header" can be used to add the headers
       that you need.

   <b>Which</b> <b>modules</b> <b>work</b> <b>like</b> <b>Mechanize</b> <b>and</b> <b>have</b> <b>JavaScript</b> <b>support?</b>
       In  no  particular  order:  Gtk2::WebKit::Mechanize,   WWW::Mechanize::Firefox,   WWW::Mechanize::Chrome,
       WWW::Scripter, WWW::Selenium

</pre><h4><b>How</b> <b>do</b> <b>I</b> <b>do</b> <b>X?</b></h4><pre>
   <b>Can</b> <b>I</b> <b>do</b> <b>[such-and-such]</b> <b>with</b> <b>WWW::Mechanize?</b>
       If  it's  possible with LWP::UserAgent, then yes.  WWW::Mechanize is a subclass of LWP::UserAgent, so all
       the wondrous magic of that class is inherited.

   <b>How</b> <b>do</b> <b>I</b> <b>use</b> <b>WWW::Mechanize</b> <b>through</b> <b>a</b> <b>proxy</b> <b>server?</b>
       See the docs in LWP::UserAgent on how to use the proxy.  Short version:

           $mech-&gt;proxy(['http', 'ftp'], '<a href="http://proxy.example.com">http://proxy.example.com</a>:8000/');

       or get the specs from the environment:

           $mech-&gt;env_proxy();

           # Environment set like so:
           gopher_proxy=<a href="http://proxy.my.place/">http://proxy.my.place/</a>
           wais_proxy=<a href="http://proxy.my.place/">http://proxy.my.place/</a>
           no_proxy="localhost,my.domain"
           export gopher_proxy wais_proxy no_proxy

   <b>How</b> <b>can</b> <b>I</b> <b>see</b> <b>what</b> <b>fields</b> <b>are</b> <b>on</b> <b>the</b> <b>forms?</b>
       Use the mech-dump utility, optionally installed with Mechanize.

           $ mech-dump --forms <a href="http://search.cpan.org">http://search.cpan.org</a>
           Dumping forms
           GET <a href="http://search.cpan.org/search">http://search.cpan.org/search</a>
             query=
             mode=all                        (option)  [*all|module|dist|author]
             &lt;NONAME&gt;=CPAN Search            (submit)

   <b>How</b> <b>do</b> <b>I</b> <b>get</b> <b>Mech</b> <b>to</b> <b>handle</b> <b>authentication?</b>
           use MIME::Base64;

           my $agent = WWW::Mechanize-&gt;new();
           my @args = (
               Authorization =&gt; "Basic " .
                   MIME::Base64::encode( USER . ':' . PASS )
           );

           $agent-&gt;credentials( ADDRESS, REALM, USER, PASS );
           $agent-&gt;get( URL, @args );

       If you want to use the credentials  for  all  future  requests,  you  can  also  use  the  LWP::UserAgent
       default_header() method instead of the extra arguments to get()

           $mech-&gt;default_header(
               Authorization =&gt; 'Basic ' . encode_base64( USER . ':' . PASSWORD ) );

   <b>How</b> <b>can</b> <b>I</b> <b>get</b> <b>WWW::Mechanize</b> <b>to</b> <b>execute</b> <b>this</b> <b>JavaScript?</b>
       You  can't.   JavaScript is entirely client-based, and WWW::Mechanize is a client that doesn't understand
       JavaScript.  See the top part of this FAQ.

   <b>How</b> <b>do</b> <b>I</b> <b>check</b> <b>a</b> <b>checkbox</b> <b>that</b> <b>doesn't</b> <b>have</b> <b>a</b> <b>value</b> <b>defined?</b>
       Set it to the value of "on".

           $mech-&gt;field( my_checkbox =&gt; 'on' );

   <b>How</b> <b>do</b> <b>I</b> <b>handle</b> <b>frames?</b>
       You don't deal with them as frames, per se, but as links.  Extract them with

           my @frame_links = $mech-&gt;find_link( tag =&gt; "frame" );

   <b>How</b> <b>do</b> <b>I</b> <b>get</b> <b>a</b> <b>list</b> <b>of</b> <b>HTTP</b> <b>headers</b> <b>and</b> <b>their</b> <b>values?</b>
       All HTTP::Headers methods work on a HTTP::Response object which  is  returned  by  the  get(),  reload(),
       "response()/res()", click(), submit_form(), and request() methods.

           my $mech = WWW::Mechanize-&gt;new( autocheck =&gt; 1 );
           $mech-&gt;get( '<a href="http://my.site.com">http://my.site.com</a>' );
           my $response = $mech-&gt;response();
           for my $key ( $response-&gt;header_field_names() ) {
               print $key, " : ", $response-&gt;header( $key ), "\n";
           }

   <b>How</b> <b>do</b> <b>I</b> <b>enable</b> <b>keep-alive?</b>
       Since  WWW::Mechanize  is  a  subclass  of LWP::UserAgent, you can use the same mechanism to enable keep-
       alive:

           use LWP::ConnCache;
           ...
           $mech-&gt;conn_cache(LWP::ConnCache-&gt;new);

   <b>How</b> <b>can</b> <b>I</b> <b>change/specify</b> <b>the</b> <b>action</b> <b>parameter</b> <b>of</b> <b>an</b> <b>HTML</b> <b>form?</b>
       You can access the action of the form by utilizing  the  HTML::Form  object  returned  from  one  of  the
       specifying form methods.

       Using "$mech-&gt;form_number($number)":

           my $mech = WWW::mechanize-&gt;new;
           $mech-&gt;get('<a href="http://someurlhere.com">http://someurlhere.com</a>');
           # Access the form using its Zero-Based Index by DOM order
           $mech-&gt;<a href="../man0/form_number.0.html">form_number</a>(0)-&gt;action('<a href="http://newAction">http://newAction</a>'); #ABS URL

       Using "$mech-&gt;form_name($number)":

           my $mech = WWW::mechanize-&gt;new;
           $mech-&gt;get('<a href="http://someurlhere.com">http://someurlhere.com</a>');
           #Access the form using its Zero-Based Index by DOM order
           $mech-&gt;form_name('trgForm')-&gt;action('<a href="http://newAction">http://newAction</a>'); #ABS URL

   <b>How</b> <b>do</b> <b>I</b> <b>save</b> <b>an</b> <b>image?</b>  <b>How</b> <b>do</b> <b>I</b> <b>save</b> <b>a</b> <b>large</b> <b>tarball?</b>
       An image is just content.  You get the image and save it.

           $mech-&gt;get( 'photo.jpg' );
           $mech-&gt;save_content( '/path/to/my/directory/photo.jpg' );

       You  can also save any content directly to disk using the ":content_file" flag to get(), which is part of
       LWP::UserAgent.

           $mech-&gt;get( '<a href="http://www.cpan.org/src/stable.tar.gz">http://www.cpan.org/src/stable.tar.gz</a>',
                       ':content_file' =&gt; 'stable.tar.gz' );

   <b>How</b> <b>do</b> <b>I</b> <b>pick</b> <b>a</b> <b>specific</b> <b>value</b> <b>from</b> <b>a</b> <b>"&lt;select&gt;"</b> <b>list?</b>
       Find the "HTML::Form::ListInput" in the page.

           my ($listbox) = $mech-&gt;find_all_inputs( name =&gt; 'listbox' );

       Then create a hash for the lookup:

           my %name_lookup;
           @name_lookup{ $listbox-&gt;value_names } = $listbox-&gt;possible_values;
           my $value = $name_lookup{ 'Name I want' };

       If you have duplicate names, this method won't work, and you'll have to loop over "$listbox-&gt;value_names"
       and "$listbox-&gt;possible_values" in parallel until you find a matching name.

   <b>How</b> <b>do</b> <b>I</b> <b>get</b> <b>Mech</b> <b>to</b> <b>not</b> <b>follow</b> <b>redirects?</b>
       You use functionality in LWP::UserAgent, not Mech itself.

           $mech-&gt;requests_redirectable( [] );

       Or you can set "max_redirect":

           $mech-&gt;max_redirect( 0 );

       Both these options can also be set in the constructor.  Mech doesn't understand them, so will  pass  them
       through to the LWP::UserAgent constructor.

</pre><h4><b>Why</b> <b>doesn't</b> <b>this</b> <b>work:</b> <b>Debugging</b> <b>your</b> <b>Mechanize</b> <b>program</b></h4><pre>
   <b>My</b> <b>Mech</b> <b>program</b> <b>doesn't</b> <b>work,</b> <b>but</b> <b>it</b> <b>works</b> <b>in</b> <b>the</b> <b>browser.</b>
       Mechanize  acts  like  a  browser,  but  apparently  something you're doing is not matching the browser's
       behavior.  Maybe it's expecting a certain web client, or maybe you've not handling a field properly.  For
       some reason, your Mech problem isn't doing exactly what the browser is doing, and  when  you  find  that,
       you'll have the answer.

   <b>My</b> <b>Mech</b> <b>program</b> <b>gets</b> <b>these</b> <b>500</b> <b>errors.</b>
       A  500  error from the web server says that the program on the server side died.  Probably the web server
       program was expecting certain inputs that you didn't supply, and  instead  of  handling  it  nicely,  the
       program died.

       Whatever the cause of the 500 error, if it works in the browser, but not in your Mech program, you're not
       acting like the browser.  See the previous question.

   <b>Why</b> <b>doesn't</b> <b>my</b> <b>program</b> <b>handle</b> <b>this</b> <b>form</b> <b>correctly?</b>
       Run <u>mech-dump</u> on your page and see what it says.

       <u>mech-dump</u>  is  a  marvelous  diagnostic tool for figuring out what forms and fields are on the page.  Say
       you're scraping CNN.com, you'd get this:

           $ mech-dump <a href="http://www.cnn.com/">http://www.cnn.com/</a>
           GET <a href="http://search.cnn.com/cnn/search">http://search.cnn.com/cnn/search</a>
             source=cnn                     (hidden readonly)
             invocationType=search/top      (hidden readonly)
             sites=web                      (radio)    [*web/The Web ??|cnn/CNN.com ??]
             query=                         (text)
             &lt;NONAME&gt;=Search                (submit)

           POST <a href="http://cgi.money.cnn.com/servlets/quote_redirect">http://cgi.money.cnn.com/servlets/quote_redirect</a>
             query=                         (text)
             &lt;NONAME&gt;=GET                   (submit)

           POST <a href="http://polls.cnn.com/poll">http://polls.cnn.com/poll</a>
             poll_id=2112                   (hidden readonly)
             question_1=&lt;UNDEF&gt;             (radio)    [1/Simplistic option|2/VIEW RESULTS]
             &lt;NONAME&gt;=VOTE                  (submit)

           GET <a href="http://search.cnn.com/cnn/search">http://search.cnn.com/cnn/search</a>
             source=cnn                     (hidden readonly)
             invocationType=search/bottom   (hidden readonly)
             sites=web                      (radio)    [*web/??CNN.com|cnn/??]
             query=                         (text)
             &lt;NONAME&gt;=Search                (submit)

       Four forms, including the first one duplicated at the end.  All the fields, all their defaults,  lovingly
       generated by HTML::Form's "dump" method.

       If  you  want  to  run <u>mech-dump</u> on something that doesn't lend itself to a quick URL fetch, then use the
       save_content() method to write the HTML to a file, and run <u>mech-dump</u> on the file.

   <b>Why</b> <b>don't</b> <b>https://</b> <b>URLs</b> <b>work?</b>
       You need either IO::Socket::SSL or Crypt::SSLeay installed.

   <b>Why</b> <b>don't</b> <b>file://</b> <b>URLs</b> <b>to</b> <b>files</b> <b>with</b> <b>a</b> <b>question</b> <b>mark</b> <b>in</b> <b>the</b> <b>name</b> <b>work?</b>
       If you have a local file named "how-are-you?", the URL for that  file  is  "file:how-are-you%3f".  That's
       because  URI::file is required to be url-encoded, just like any URL pointing to somewhere on the internet
       has to be if it contains reserved characters such as "?", "/" or "@". This is specified in RFC 3986.  See
       URI::Escape for a full list of reserved characters.

   <b>Why</b> <b>do</b> <b>I</b> <b>get</b> <b>"Input</b> <b>'fieldname'</b> <b>is</b> <b>readonly"?</b>
       You're trying to change the value of a hidden field and you have warnings on.

       First, make sure that you actually mean to change the field that you're changing, and that you don't have
       a typo.  Usually, hidden variables are set by the site you're working on for a reason.  If you change the
       value, you might be breaking some functionality by faking it out.

       If you really do want to change a hidden value, make the changes in a scope that has warnings turned off:

           {
           local $^W = 0;
           $agent-&gt;field( name =&gt; $value );
           }

   <b>I</b> <b>tried</b> <b>to</b> <b>[such-and-such]</b> <b>and</b> <b>I</b> <b>got</b> <b>this</b> <b>weird</b> <b>error.</b>
       Are you checking your errors?

       Are you sure?

       Are you checking that your action succeeded after every action?

       Are you sure?

       For example, if you try this:

           $mech-&gt;get( "<a href="http://my.site.com">http://my.site.com</a>" );
           $mech-&gt;follow_link( "foo" );

       and  the "get" call fails for some reason, then the Mech internals will be unusable for the "follow_link"
       and you'll get a weird error.  You <b>must</b>, after every action that GETs or POSTs a page,  check  that  Mech
       succeeded, or all bets are off.

           $mech-&gt;get( "<a href="http://my.site.com">http://my.site.com</a>" );
           die "Can't even get the home page: ", $mech-&gt;response-&gt;status_line
               unless $mech-&gt;success;

           $mech-&gt;follow_link( "foo" );
           die "Foo link failed: ", $mech-&gt;response-&gt;status_line
               unless $mech-&gt;success;

   <b>How</b> <b>do</b> <b>I</b> <b>figure</b> <b>out</b> <b>why</b> <b>"$mech-&gt;get($url)"</b> <b>doesn't</b> <b>work?</b>
       There  are  many reasons why a get() can fail. The server can take you to someplace you didn't expect. It
       can generate redirects which are not properly handled. You can get time-outs. Servers are down more often
       than you think! etc, etc, etc. A couple of places to start:

       1 Check "$mech-&gt;status()" after each call
       2 Check the URL with "$mech-&gt;uri()" to see where you ended up
       3 Try debugging with "LWP::ConsoleLogger".

       If things are really strange, turn on debugging with "use LWP::ConsoleLogger::Everywhere;" Just put  this
       in  the  main program. This causes LWP to print out a trace of the HTTP traffic between client and server
       and can be used to figure out what is happening at the protocol level.

       It is also useful to set many traps to verify that processing is proceeding as expected. A  Mech  program
       should  always have an "I didn't expect to get here" or "I don't recognize the page that I am processing"
       case and bail out.

       Since errors can be transient, by the time you notice that the  error  has  occurred,  it  might  not  be
       possible  to  reproduce  it  manually.  So  for  automated  processing it is useful to email yourself the
       following information:

       •   where processing is taking place

       •   An Error Message

       •   $mech-&gt;uri

       •   $mech-&gt;content

       You can also save the content of the page with "$mech-&gt;save_content( 'filename.html' );"

   <b>I</b> <b>submitted</b> <b>a</b> <b>form,</b> <b>but</b> <b>the</b> <b>server</b> <b>ignored</b> <b>everything!</b>  <b>I</b> <b>got</b> <b>an</b> <b>empty</b> <b>form</b> <b>back!</b>
       The post is handled by application software. It is common for PHP programmers to use the same  file  both
       to  display a form and to process the arguments returned. So the first task of the application programmer
       is to decide whether there are arguments to  processes.  The  program  can  check  whether  a  particular
       parameter  has  been  set, whether a hidden parameter has been set, or whether the submit button has been
       clicked.  (There are probably other ways that I haven't thought of).

       In any case, if your form is not setting the parameter (e.g. the submit button) which the web application
       is keying on (and as an outsider there is no way to know what it is keying on), it will not  notice  that
       the form has been submitted. Try using "$mech-&gt;click()" instead of "$mech-&gt;submit()" or vice-versa.

   <b>I've</b> <b>logged</b> <b>in</b> <b>to</b> <b>the</b> <b>server,</b> <b>but</b> <b>I</b> <b>get</b> <b>500</b> <b>errors</b> <b>when</b> <b>I</b> <b>try</b> <b>to</b> <b>get</b> <b>to</b> <b>protected</b> <b>content.</b>
       Some  web  sites  use  distributed  databases  for  their  processing.  It can take a few seconds for the
       login/session information to percolate through to all the  servers.  For  human  users  with  their  slow
       reaction times, this is not a problem, but a Perl script can outrun the server.  So try adding a <a href="../man5/sleep.5.html">sleep</a>(5)
       between logging in and actually doing anything (the optimal delay must be determined experimentally).

   <b>Mech</b> <b>is</b> <b>a</b> <b>big</b> <b>memory</b> <b>pig!</b>  <b>I'm</b> <b>running</b> <b>out</b> <b>of</b> <b>RAM!</b>
       Mech  keeps a history of every page, and the state it was in.  It actually keeps a clone of the full Mech
       object at every step along the way.

       You can limit this stack size with the  "stack_depth"  param  in  the  new()  constructor.   If  you  set
       stack_size to 0, Mech will not keep any history.

</pre><h4><b>AUTHOR</b></h4><pre>
       Andy Lester &lt;andy at petdance.com&gt;

</pre><h4><b>COPYRIGHT</b> <b>AND</b> <b>LICENSE</b></h4><pre>
       This software is copyright (c) 2004 by Andy Lester.

       This  is  free  software;  you  can  redistribute  it and/or modify it under the same terms as the Perl 5
       programming language system itself.

perl v5.40.1                                       2025-02-18                           <u>WWW::Mechanize::<a href="../man3pm/FAQ.3pm.html">FAQ</a></u>(3pm)
</pre>
 </div>
</div></section>
</div>
</body>
</html>