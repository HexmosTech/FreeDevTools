<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>C Syntax</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/openmpi-doc">openmpi-doc_5.0.7-1build1_all</a> <br><br><pre>
</pre><h4><b>SYNTAX</b></h4><pre>
   <b>C</b> <b>Syntax</b>
          #include &lt;mpi.h&gt;

          int MPI_Type_create_darray(int size, int rank, int ndims,
               const int array_of_gsizes[], const int array_of_distribs[],
               const int array_of_dargs[], const int array_of_psizes[],
               int order, MPI_Datatype oldtype, MPI_Datatype *newtype)

   <b>Fortran</b> <b>Syntax</b>
          USE MPI
          ! or the older form: INCLUDE 'mpif.h'
          MPI_TYPE_CREATE_DARRAY(SIZE, RANK, NDIMS, ARRAY_OF_GSIZES,
               ARRAY_OF_DISTRIBS, ARRAY_OF_DARGS, ARRAY_OF_PSIZES, ORDER,
               OLDTYPE, NEWTYPE, IERROR)

               INTEGER SIZE, RANK, NDIMS, ARRAY_OF_GSIZES(*), ARRAY_OF_DISTRIBS(*),
                       ARRAY_OF_DARGS(*), ARRAY_OF_PSIZES(*), ORDER, OLDTYPE,
                       NEWTYPE, IERROR

   <b>Fortran</b> <b>2008</b> <b>Syntax</b>
          USE mpi_f08
          MPI_Type_create_darray(size, rank, ndims, array_of_gsizes,
               array_of_distribs, array_of_dargs, array_of_psizes, order,
                       oldtype, newtype, ierror)
               INTEGER, INTENT(IN) :: size, rank, ndims, array_of_gsizes(ndims),
               array_of_distribs(ndims), array_of_dargs(ndims),
               array_of_psizes(ndims), order
               TYPE(MPI_Datatype), INTENT(IN) :: oldtype
               TYPE(MPI_Datatype), INTENT(OUT) :: newtype
               INTEGER, OPTIONAL, INTENT(OUT) :: ierror

</pre><h4><b>INPUT</b> <b>PARAMETERS</b></h4><pre>
       • <b>size</b>: Size of process group (positive integer).

       • <b>rank</b>: Rank in process group (nonnegative integer).

       • <b>ndims</b>: Number of array dimensions as well as process grid dimensions (positive integer).

       • <b>array_of_gsizes</b>:  Number  of  elements  of  type  <u>oldtype</u>  in  each dimension of global array (array of
         positive integers).

       • <b>array_of_distribs</b>: Distribution of array in each dimension (array of state).

       • <b>array_of_dargs</b>: Distribution argument in each dimension (array of positive integers).

       • <b>array_of_psizes</b>: Size of process grid in each dimension (array of positive integers).

       • <b>order</b>: Array storage order flag (state).

       • <b>oldtype</b>: Old data type (handle).

</pre><h4><b>OUTPUT</b> <b>PARAMETERS</b></h4><pre>
       • <b>newtype</b>: New data type (handle).

       • <b>ierror</b>: Fortran only: Error status (integer).

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <u>MPI_Type_create_darray</u> can be used to generate the data types corresponding to  the  distribution  of  an
       ndims-dimensional  array  of <u>oldtype</u> elements onto an <u>ndims</u>-dimensional grid of logical processes. Unused
       dimensions of <u>array_of_psizes</u> should be set to 1.  For a call to <u>MPI_Type_create_darray</u>  to  be  correct,
       the equation

            ndims-1
          pi              array_of_psizes[i] = size
            i=0

       must  be  satisfied.  The ordering of processes in the process grid is assumed to be row-major, as in the
       case of virtual Cartesian process topologies in the <u>MPI</u> <u>Standard</u>.

       Each dimension of the array can be distributed in one of three ways:

       • <b>MPI_DISTRIBUTE_BLOCK</b> - Block distribution

       • <b>MPI_DISTRIBUTE_CYCLIC</b> - Cyclic distribution

       • <b>MPI_DISTRIBUTE_NONE</b> - Dimension not distributed.

       The constant  MPI_DISTRIBUTE_DFLT_DARG  specifies  a  default  distribution  argument.  The  distribution
       argument  for  a  dimension  that  is  not  distributed  is  ignored.  For  any  dimension <u>i</u> in which the
       distribution is MPI_DISTRIBUTE_BLOCK, it erroneous to specify <u>array_of_dargs[i]</u>  <u>*</u>  <u>array_of_psizes[i]</u>  &lt;
       <u>array_of_gsizes[i]</u>.

       For  example,  the  HPF layout ARRAY(<a href="../man15/CYCLIC.15.html">CYCLIC</a>(15)) corresponds to MPI_DISTRIBUTE_CYCLIC with a distribution
       argument of 15, and the HPF layout ARRAY(BLOCK) corresponds to MPI_DISTRIBUTE_BLOCK with  a  distribution
       argument of MPI_DISTRIBUTE_DFLT_DARG.

       The <u>order</u> argument is used as in <u>MPI_Type_create_subarray</u> to specify the storage order. Therefore, arrays
       described  by this type constructor may be stored in Fortran (column-major) or C (row-major) order. Valid
       values for order are MPI_ORDER_FORTRAN and MPI_ORDER_C.

       This routine creates a new MPI data type with a typemap defined in terms of a function called  “cyclic()”
       (see below).

       Without  loss  of  generality, it suffices to define the typemap for the MPI_DISTRIBUTE_CYCLIC case where
       <b>MPI_DISTRIBUTE_DFLT_DARG</b> is not used.

       MPI_DISTRIBUTE_BLOCK and MPI_DISTRIBUTE_NONE  can  be  reduced  to  the  MPI_DISTRIBUTE_CYCLIC  case  for
       dimension <u>i</u> as follows.

       MPI_DISTRIBUTE_BLOCK   with   <u>array_of_dargs[i]</u>   equal  to  MPI_DISTRIBUTE_DFLT_DARG  is  equivalent  to
       MPI_DISTRIBUTE_CYCLIC with <u>array_of_dargs[i]</u> set to

          (array_of_gsizes[i] + array_of_psizes[i] - 1)/array_of_psizes[i]

       If <u>array_of_dargs[i]</u> is not MPI_DISTRIBUTE_DFLT_DARG, then MPI_DISTRIBUTE_BLOCK and DISTRIBUTE_CYCLIC are
       equivalent.

       MPI_DISTRIBUTE_NONE   is   equivalent   to   MPI_DISTRIBUTE_CYCLIC   with   <u>array_of_dargs[i]</u>   set    to
       <u>array_of_gsizes[i]</u>.

       Finally,  MPI_DISTRIBUTE_CYCLIC with <u>array_of_dargs[i]</u> equal to MPI_DISTRIBUTE_DFLT_DARG is equivalent to
       MPI_DISTRIBUTE_CYCLIC with <u>array_of_dargs[i]</u> set to 1.

</pre><h4><b>NOTES</b></h4><pre>
       For both Fortran and C arrays, the ordering of processes in the process grid is assumed to be  row-major.
       This  is consistent with the ordering used in virtual Cartesian process topologies in MPI. To create such
       virtual process topologies, or to find the coordinates of a process in the process grid, etc., users  may
       use the corresponding functions provided in MPI.

</pre><h4><b>ERRORS</b></h4><pre>
       Almost  all  MPI  routines  return  an  error  value; C routines as the return result of the function and
       Fortran routines in the last argument.

       Before the error value is returned, the current MPI  error  handler  associated  with  the  communication
       object  (e.g.,  communicator, window, file) is called.  If no communication object is associated with the
       MPI call, then the call is considered attached to MPI_COMM_SELF and will call the  associated  MPI  error
       handler.   When   MPI_COMM_SELF   is   not  initialized  (i.e.,  before  <u>MPI_Init</u>/<u>MPI_Init_thread</u>,  after
       <u>MPI_Finalize</u>, or when using the Sessions Model exclusively) the error raises the initial  error  handler.
       The  initial  error handler can be changed by calling <u>MPI_Comm_set_errhandler</u> on MPI_COMM_SELF when using
       the World model, or the mpi_initial_errhandler CLI argument to mpiexec or info  key  to  <u>MPI_Comm_spawn</u>/‐
       <u>MPI_Comm_spawn_multiple</u>.   If no other appropriate error handler has been set, then the MPI_ERRORS_RETURN
       error handler is called for MPI I/O functions and the MPI_ERRORS_ABORT error handler is  called  for  all
       other MPI functions.

       Open MPI includes three predefined error handlers that can be used:

       • <b>MPI_ERRORS_ARE_FATAL</b> Causes the program to abort all connected MPI processes.

       • <b>MPI_ERRORS_ABORT</b> An error handler that can be invoked on a communicator, window, file, or session. When
         called  on  a  communicator,  it  acts  as if <u>MPI_Abort</u> was called on that communicator. If called on a
         window or file, acts as if <u>MPI_Abort</u> was called on a communicator containing the group of processes  in
         the corresponding window or file. If called on a session, aborts only the local process.

       • <b>MPI_ERRORS_RETURN</b> Returns an error code to the application.

       MPI applications can also implement their own error handlers by calling:

       • <u>MPI_Comm_create_errhandler</u> then <u>MPI_Comm_set_errhandler</u>

       • <u>MPI_File_create_errhandler</u> then <u>MPI_File_set_errhandler</u>

       • <u>MPI_Session_create_errhandler</u> then <u>MPI_Session_set_errhandler</u> or at <u>MPI_Session_init</u>

       • <u>MPI_Win_create_errhandler</u> then <u>MPI_Win_set_errhandler</u>

       Note that MPI does not guarantee that an MPI program can continue past an error.

       See the <u>MPI</u> <u>man</u> <u>page</u> for a full list of <u>MPI</u> <u>error</u> <u>codes</u>.

       See the Error Handling section of the MPI-3.1 standard for more information.

</pre><h4><b>COPYRIGHT</b></h4><pre>
       2003-2025, The Open MPI Community

                                                  Jun 07, 2025                         <u><a href="../man3/MPI_TYPE_CREATE_DARRAY.3.html">MPI_TYPE_CREATE_DARRAY</a></u>(3)
</pre>
 </div>
</div></section>
</div>
</body>
</html>