<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WWW::RobotRules - database of robots.txt-derived permissions</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/libwww-robotrules-perl">libwww-robotrules-perl_6.02-1_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       WWW::RobotRules - database of robots.txt-derived permissions

</pre><h4><b>SYNOPSIS</b></h4><pre>
        use WWW::RobotRules;
        my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');

        use LWP::Simple qw(get);

        {
          my $url = "<a href="http://some.place/robots.txt">http://some.place/robots.txt</a>";
          my $robots_txt = get $url;
          $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
        }

        {
          my $url = "<a href="http://some.other.place/robots.txt">http://some.other.place/robots.txt</a>";
          my $robots_txt = get $url;
          $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
        }

        # Now we can check if a URL is valid for those servers
        # whose "robots.txt" files we've gotten and parsed:
        if($rules-&gt;allowed($url)) {
            $c = get $url;
            ...
        }

</pre><h4><b>DESCRIPTION</b></h4><pre>
       This module parses <u>/robots.txt</u> files as specified in "A Standard for Robot Exclusion", at
       &lt;<a href="http://www.robotstxt.org/wc/norobots.html">http://www.robotstxt.org/wc/norobots.html</a>&gt; Webmasters can use the <u>/robots.txt</u> file to forbid conforming
       robots from accessing parts of their web site.

       The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check if
       access to a given URL is prohibited.  The same WWW::RobotRules object can be used for one or more parsed
       <u>/robots.txt</u> files on any number of hosts.

       The following methods are provided:

       $rules = WWW::RobotRules-&gt;new($robot_name)
           This  is  the constructor for WWW::RobotRules objects.  The first argument given to <u>new()</u> is the name
           of the robot.

       $rules-&gt;parse($robot_txt_url, $content, $fresh_until)
           The <u>parse()</u> method takes as arguments the URL that was used to retrieve the <u>/robots.txt</u> file, and the
           contents of the file.

       $rules-&gt;allowed($uri)
           Returns TRUE if this robot is allowed to retrieve this URL.

       $rules-&gt;agent([$name])
           Get/set the agent name. NOTE: Changing the agent name will clear  the  robots.txt  rules  and  expire
           times out of the cache.

</pre><h4><b>ROBOTS.TXT</b></h4><pre>
       The  format  and  semantics  of  the  "/robots.txt"  file  are  as follows (this is an edited abstract of
       &lt;<a href="http://www.robotstxt.org/wc/norobots.html">http://www.robotstxt.org/wc/norobots.html</a>&gt;):

       The file consists of one or more records separated by one or more blank lines. Each record contains lines
       of the form

         &lt;field-name&gt;: &lt;value&gt;

       The field name is case insensitive.  Text after the '#' character on a line is  ignored  during  parsing.
       This is used for comments.  The following &lt;field-names&gt; can be used:

       User-Agent
          The  value of this field is the name of the robot the record is describing access policy for.  If more
          than one <u>User-Agent</u> field is present the record describes an identical access policy for more than one
          robot. At least one field needs to be present per record.  If the value is '*', the  record  describes
          the default access policy for any robot that has not not matched any of the other records.

          The  <u>User-Agent</u> fields must occur before the <u>Disallow</u> fields.  If a record contains a <u>User-Agent</u> field
          after a <u>Disallow</u> field, that constitutes a malformed record.  This parser will  assume  that  a  blank
          line  should  have  been placed before that <u>User-Agent</u> field, and will break the record into two.  All
          the fields before the <u>User-Agent</u> field will constitute a record, and the <u>User-Agent</u> field will be  the
          first field in a new record.

       Disallow
          The value of this field specifies a partial URL that is not to be visited. This can be a full path, or
          a partial path; any URL that starts with this value will not be retrieved

       Unrecognized records are ignored.

</pre><h4><b>ROBOTS.TXT</b> <b>EXAMPLES</b></h4><pre>
       The  following  example  "/robots.txt"  file  specifies that no robots should visit any URL starting with
       "/cyberworld/map/" or "<a href="file:/tmp/">/tmp/</a>":

         User-agent: *
         Disallow: /cyberworld/map/ # This is an infinite virtual URL space
         Disallow: <a href="file:/tmp/">/tmp/</a> # these will soon disappear

       This  example  "/robots.txt"  file  specifies  that  no  robots  should  visit  any  URL  starting   with
       "/cyberworld/map/", except the robot called "cybermapper":

         User-agent: *
         Disallow: /cyberworld/map/ # This is an infinite virtual URL space

         # Cybermapper knows where to go.
         User-agent: cybermapper
         Disallow:

       This example indicates that no robots should visit this site further:

         # go away
         User-agent: *
         Disallow: /

       This is an example of a malformed robots.txt file.

         # robots.txt for ancientcastle.example.com
         # I've locked myself away.
         User-agent: *
         Disallow: /
         # The castle is your home now, so you can go anywhere you like.
         User-agent: Belle
         Disallow: /west-wing/ # except the west wing!
         # It's good to be the Prince...
         User-agent: Beast
         Disallow:

       This file is missing the required blank lines between records.  However, the intention is clear.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       LWP::RobotUA, WWW::RobotRules::AnyDBM_File

</pre><h4><b>COPYRIGHT</b></h4><pre>
         Copyright 1995-2009, Gisle Aas
         Copyright 1995, Martijn Koster

       This  library  is  free  software;  you can redistribute it and/or modify it under the same terms as Perl
       itself.

perl v5.26.1                                       2018-04-14                               <u>WWW::<a href="../man3pm/RobotRules.3pm.html">RobotRules</a></u>(3pm)
</pre>
 </div>
</div></section>
</div>
</body>
</html>