<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>gluster - Gluster Console Manager (command line utility)</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/glusterfs-cli">glusterfs-cli_11.1-6_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       gluster - Gluster Console Manager (command line utility)

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>gluster</b>

       To run the program and display gluster prompt:

       <b>gluster</b> <b>[--remote-host=&lt;gluster_node&gt;]</b> <b>[--mode=script]</b> <b>[--xml]</b>

       (or)

       To specify a command directly:

       <b>gluster</b> <u>[commands]</u> <u>[options]</u> <u>[--remote-host=&lt;gluster_node&gt;]</u> <u>[--mode=script]</u> <u>[--xml]</u>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       The  Gluster  Console  Manager  is  a command line utility for elastic volume management. You can run the
       gluster command on any export server. The command enables administrators  to  perform  cloud  operations,
       such  as  creating,  expanding, shrinking, rebalancing, and migrating volumes without needing to schedule
       server downtime.

</pre><h4><b>COMMANDS</b></h4><pre>
   <b>Volume</b> <b>Commands</b>
        <b>volume</b> <b>info</b> <b>[all|&lt;VOLNAME&gt;]</b>
              Display information about all volumes, or the specified volume.

        <b>volume</b> <b>list</b>
              List all volumes in cluster

        <b>volume</b> <b>status</b> <b>[all</b> <b>|</b> <b>&lt;VOLNAME&gt;</b> <b>[nfs|shd|&lt;BRICK&gt;|quotad]]</b>
       <b>[detail|clients|mem|inode|fd|callpool|tasks|client-list]</b>
              Display status of all or specified volume(s)/brick

        <b>volume</b> <b>create</b> <b>&lt;NEW-VOLNAME&gt;</b> <b>[stripe</b> <b>&lt;COUNT&gt;]</b> <b>[[replica</b> <b>&lt;COUNT&gt;</b> <b>[arbiter</b> <b>&lt;COUNT&gt;]]|[replica</b> <b>2</b> <b>thin-</b>
       <b>arbiter</b> <b>1]]</b> <b>[disperse</b> <b>[&lt;COUNT&gt;]]</b> <b>[disperse-data</b> <b>&lt;COUNT&gt;]</b> <b>[redundancy</b> <b>&lt;COUNT&gt;]</b> <b>[transport</b>
       <b>&lt;tcp|rdma|tcp,rdma&gt;]</b> <b>&lt;NEW-BRICK&gt;</b> <b>...</b> <b>&lt;TA-BRICK&gt;</b>
              Create a new volume of the specified type using the  specified  bricks  and  transport  type  (the
              default  transport  type  is  tcp).   To create a volume with both transports (tcp and rdma), give
              'transport tcp,rdma' as an option.

        <b>volume</b> <b>delete</b> <b>&lt;VOLNAME&gt;</b>
              Delete the specified volume.

        <b>volume</b> <b>start</b> <b>&lt;VOLNAME&gt;</b>
              Start the specified volume.

        <b>volume</b> <b>stop</b> <b>&lt;VOLNAME&gt;</b> <b>[force]</b>
              Stop the specified volume.

        <b>volume</b> <b>set</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;OPTION&gt;</b> <b>&lt;PARAMETER&gt;</b> <b>[&lt;OPTION&gt;</b> <b>&lt;PARAMETER&gt;]</b> <b>...</b>
              Set the volume options.

        <b>volume</b> <b>get</b> <b>&lt;VOLNAME/all&gt;</b> <b>&lt;OPTION/all&gt;</b>
              Get the value of the all options or given option for  volume  &lt;VOLNAME&gt;  or  all  option.  gluster
              volume get all all is to get all global options

        <b>volume</b> <b>reset</b> <b>&lt;VOLNAME&gt;</b> <b>[option]</b> <b>[force]</b>
              Reset all the reconfigured options

        <b>volume</b> <b>barrier</b> <b>&lt;VOLNAME&gt;</b> <b>{enable|disable}</b>
              Barrier/unbarrier file operations on a volume

        <b>volume</b> <b>clear-locks</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;path&gt;</b> <b>kind</b> <b>{blocked|granted|all}{inode</b> <b>[range]|entry</b> <b>[basename]|posix</b>
       <b>[range]}</b>
              Clear locks held on path

        <b>volume</b> <b>help</b>
              Display help for the volume command.

   <b>Brick</b> <b>Commands</b>
        <b>volume</b> <b>add-brick</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;NEW-BRICK&gt;</b> <b>...</b>
              Add the specified brick to the specified volume.

        <b>volume</b> <b>remove-brick</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;BRICK&gt;</b> <b>...</b>
              Remove the specified brick from the specified volume.

              <b>Note:</b>  If  you  remove  the  brick,  the  data stored in that brick will not be available. You can
              migrate data from one brick to another using <b>replace-brick</b> option.

        <b>volume</b> <b>reset-brick</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;SOURCE-BRICK&gt;</b> <b>{{start}</b> <b>|</b> <b>{&lt;NEW-BRICK&gt;</b> <b>commit}}</b>
              Brings down or replaces the specified source brick with the new brick.

        <b>volume</b> <b>replace-brick</b> <b>&lt;VOLNAME&gt;</b> <b>&lt;SOURCE-BRICK&gt;</b> <b>&lt;NEW-BRICK&gt;</b> <b>commit</b> <b>force</b>
              Replace the specified source brick with a new brick.

        <b>volume</b> <b>rebalance</b> <b>&lt;VOLNAME&gt;</b> <b>start</b>
              Start rebalancing the specified volume.

        <b>volume</b> <b>rebalance</b> <b>&lt;VOLNAME&gt;</b> <b>stop</b>
              Stop rebalancing the specified volume.

        <b>volume</b> <b>rebalance</b> <b>&lt;VOLNAME&gt;</b> <b>status</b>
              Display the rebalance status of the specified volume.

   <b>Log</b> <b>Commands</b>
        <b>volume</b> <b>log</b> <b>&lt;VOLNAME&gt;</b> <b>rotate</b> <b>[BRICK]</b>
              Rotate the log file for corresponding volume/brick.

        <b>volume</b> <b>profile</b> <b>&lt;VOLNAME&gt;</b> <b>{start|info</b> <b>[peek|incremental</b> <b>[peek]|cumulative|clear]|stop}</b> <b>[nfs]</b>
              Profile operations on the volume. Once started, volume profile &lt;volname&gt; info provides  cumulative
              statistics of the FOPs performed.

        <b>volume</b> <b>top</b> <b>&lt;VOLNAME&gt;</b> <b>{open|read|write|opendir|readdir|clear}</b> <b>[nfs|brick</b> <b>&lt;brick&gt;]</b> <b>[list-cnt</b> <b>&lt;value&gt;]</b> <b>|</b>
       <b>{read-perf|write-perf}</b> <b>[bs</b> <b>&lt;size&gt;</b> <b>count</b> <b>&lt;count&gt;]</b> <b>[brick</b> <b>&lt;brick&gt;]</b> <b>[list-cnt</b> <b>&lt;value&gt;]</b>
              Generates  a  profile  of  a  volume representing the performance and bottlenecks/hotspots of each
              brick.

        <b>volume</b> <b>statedump</b> <b>&lt;VOLNAME&gt;</b> <b>[[nfs|quotad]</b> <b>[all|mem|iobuf|callpool|priv|fd|inode|history]...</b> <b>|</b> <b>[client</b>
       <b>&lt;hostname:process-id&gt;]]</b>
              Dumps the in memory state of the specified process or the bricks of the volume.

        <b>volume</b> <b>sync</b> <b>&lt;HOSTNAME&gt;</b> <b>[all|&lt;VOLNAME&gt;]</b>
              Sync the volume information from a peer

   <b>Peer</b> <b>Commands</b>
        <b>peer</b> <b>probe</b> <b>&lt;HOSTNAME&gt;</b>
              Probe the specified peer. In case the &lt;HOSTNAME&gt; given belongs to an already probed peer, the peer
              probe command will add the hostname to the peer if required.

        <b>peer</b> <b>detach</b> <b>&lt;HOSTNAME&gt;</b>
              Detach the specified peer.

        <b>peer</b> <b>status</b>
              Display the status of peers.

        <b>pool</b> <b>list</b>
              List all the nodes in the pool (including localhost)

        <b>peer</b> <b>help</b>
              Display help for the peer command.

   <b>Quota</b> <b>Commands</b>
        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>enable</b>
              Enable quota on the specified volume. This will  cause  all  the  directories  in  the  filesystem
              hierarchy  to be accounted and updated thereafter on each operation in the the filesystem. To kick
              start this accounting, a crawl is done over the hierarchy with an auxiliary client.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>disable</b>
              Disable quota on the volume. This will disable enforcement and accounting in the  filesystem.  Any
              configured limits will be lost.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>limit-usage</b> <b>&lt;PATH&gt;</b> <b>&lt;SIZE&gt;</b> <b>[&lt;PERCENT&gt;]</b>
              Set a usage  limit on the given path. Any previously set limit is overridden to the new value. The
              soft  limit  can optionally be specified (as a percentage of hard limit). If soft limit percentage
              is not provided the default soft limit value for the volume is used to decide the soft limit.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>limit-objects</b> <b>&lt;PATH&gt;</b> <b>&lt;SIZE&gt;</b> <b>[&lt;PERCENT&gt;]</b>
              Set an inode limit on the given path. Any previously set limit is overridden to the new value. The
              soft limit can optionally be specified (as a percentage of hard limit). If soft  limit  percentage
              is not provided the default soft limit value for the volume is used to decide the soft limit.

       NOTE: valid units of SIZE are : B, KB, MB, GB, TB, PB. If no unit is specified, the unit defaults to
       bytes.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>remove</b> <b>&lt;PATH&gt;</b>
              Remove any usage limit configured on the specified directory. Note that if any limit is configured
              on  the  ancestors  of  this  directory  (previous directories along the path), they will still be
              honored and enforced.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>remove-objects</b> <b>&lt;PATH&gt;</b>
              Remove any inode limit configured on the specified directory. Note that if any limit is configured
              on the ancestors of this directory (previous directories along  the  path),  they  will  still  be
              honored and enforced.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>list</b> <b>&lt;PATH&gt;</b>
              Lists the  usage and limits configured on directory(s). If a path is given only the limit that has
              been configured on the directory(if any) is displayed along with the directory's usage. If no path
              is given, usage and limits are displayed for all directories that has limits configured.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>list-objects</b> <b>&lt;PATH&gt;</b>
              Lists  the  inode  usage  and inode limits configured on directory(s). If a path is given only the
              limit that has been configured on the directory(if any) is displayed along  with  the  directory's
              inode  usage.  If  no  path  is given, usage and limits are displayed for all directories that has
              limits configured.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>default-soft-limit</b> <b>&lt;PERCENT&gt;</b>
              Set the percentage value for default soft limit for the volume.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>soft-timeout</b> <b>&lt;TIME&gt;</b>
              Set the soft timeout for the volume. The interval in which limits are  retested  before  the  soft
              limit is breached.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>hard-timeout</b> <b>&lt;TIME&gt;</b>
              Set  the  hard  timeout  for  the volume. The interval in which limits are retested after the soft
              limit is breached.

        <b>volume</b> <b>quota</b> <b>&lt;VOLNAME&gt;</b> <b>alert-time</b> <b>&lt;TIME&gt;</b>
              Set the frequency in which warning messages need to be logged (in the brick logs) once soft  limit
              is breached.

        <b>volume</b> <b>inode-quota</b> <b>&lt;VOLNAME&gt;</b> <b>enable/disable</b>
              Enable/disable inode-quota for &lt;VOLNAME&gt;

        <b>volume</b> <b>quota</b> <b>help</b>
              Display help for volume quota commands

       NOTE: valid units of time and their symbols are : hours(h/hr), minutes(m/min), seconds(s/sec),
       weeks(w/wk), Days(d/days).

   <b>Geo-replication</b> <b>Commands</b>
        <u>Note</u>: password-less ssh, from the primary node (where these commands are executed) to the secondary node
       &lt;SECONDARY_HOST&gt;, is a prerequisite for the geo-replication commands.

        <b>system::</b> <b>execute</b> <b>gsec_create</b>
              Generates pem keys which are required for push-pem

        <b>volume</b> <b>geo-replication</b> <b>&lt;PRIMARY_VOL&gt;</b> <b>&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;</b> <b>create</b> <b>[[ssh-port</b> <b>n][[no-</b>
       <b>verify]|[push-pem]]]</b> <b>[force]</b>
              Create  a  new  geo-replication session from &lt;PRIMARY_VOL&gt; to &lt;SECONDARY_HOST&gt; host machine having
              &lt;SECONDARY_VOL&gt;.  Use ssh-port n if custom SSH port is configured in  secondary  nodes.   Use  no-
              verify  if  the  rsa-keys  of nodes in primary volume is distributed to secondary nodes through an
              external agent.  Use push-pem to push the keys automatically.

        <b>volume</b> <b>geo-replication</b> <b>&lt;PRIMARY_VOL&gt;</b> <b>&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;</b> <b>{start|stop}</b> <b>[force]</b>
              Start/stop the geo-replication session from &lt;PRIMARY_VOL&gt; to &lt;SECONDARY_HOST&gt; host machine  having
              &lt;SECONDARY_VOL&gt;.

        <b>volume</b> <b>geo-replication</b> <b>[&lt;PRIMARY_VOL&gt;</b> <b>[&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;]]</b> <b>status</b> <b>[detail]</b>
              Query  status  of  the geo-replication session from &lt;PRIMARY_VOL&gt; to &lt;SECONDARY_HOST&gt; host machine
              having &lt;SECONDARY_VOL&gt;.

        <b>volume</b> <b>geo-replication</b> <b>&lt;PRIMARY_VOL&gt;</b> <b>&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;</b> <b>{pause|resume}</b> <b>[force]</b>
              Pause/resume the geo-replication session  from  &lt;PRIMARY_VOL&gt;  to  &lt;SECONDARY_HOST&gt;  host  machine
              having &lt;SECONDARY_VOL&gt;.

        <b>volume</b> <b>geo-replication</b> <b>&lt;PRIMARY_VOL&gt;</b> <b>&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;</b> <b>delete</b> <b>[reset-sync-time]</b>
              Delete  the  geo-replication  session  from  &lt;PRIMARY_VOL&gt; to &lt;SECONDARY_HOST&gt; host machine having
              &lt;SECONDARY_VOL&gt;.  Optionally you can also reset the sync time in  case  you  need  to  resync  the
              entire volume on session recreate.

        <b>volume</b> <b>geo-replication</b> <b>&lt;PRIMARY_VOL&gt;</b> <b>&lt;SECONDARY_HOST&gt;::&lt;SECONDARY_VOL&gt;</b> <b>config</b> <b>[[!]&lt;options&gt;</b> <b>[&lt;value&gt;]]</b>
              View  (when  no  option  provided)  or  set  configuration  for this geo-replication session.  Use
              "!&lt;OPTION&gt;" to reset option &lt;OPTION&gt; to default value.

   <b>Bitrot</b> <b>Commands</b>
        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>{enable|disable}</b>
              Enable/disable bitrot for volume &lt;VOLNAME&gt;

        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>signing-time</b> <b>&lt;time-in-secs&gt;</b>
              Waiting time for an object after last fd is closed to start signing process.

        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>signer-threads</b> <b>&lt;count&gt;</b>
              Number of signing process threads. Usually set to number of available cores.

        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>scrub-throttle</b> <b>{lazy|normal|aggressive}</b>
              Scrub-throttle value is a measure of how fast or slow  the  scrubber  scrubs  the  filesystem  for
              volume &lt;VOLNAME&gt;

        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>scrub-frequency</b> <b>{hourly|daily|weekly|biweekly|monthly}</b>
              Scrub frequency for volume &lt;VOLNAME&gt;

        <b>volume</b> <b>bitrot</b> <b>&lt;VOLNAME&gt;</b> <b>scrub</b> <b>{pause|resume|status|ondemand}</b>
              Pause/Resume  scrub.  Upon  resume,  scrubber continues where it left off. status option shows the
              statistics of scrubber. ondemand option starts the scrubbing immediately if the  scrubber  is  not
              paused or already running.

        <b>volume</b> <b>bitrot</b> <b>help</b>
              Display help for volume bitrot commands

          <b>Snapshot</b> <b>Commands</b>

        <b>snapshot</b> <b>create</b> <b>&lt;snapname&gt;</b> <b>&lt;volname&gt;</b> <b>[no-timestamp]</b> <b>[description</b> <b>&lt;description&gt;]</b> <b>[force]</b>
              Creates  a  snapshot  of  a  GlusterFS  volume.  User can provide a snap-name and a description to
              identify the snap. Snap will be created by appending timestamp in  GMT.  User  can  override  this
              behaviour  using "no-timestamp" option. The description cannot be more than 1024 characters. To be
              able to take a snapshot, volume should be present and it should be in started state.

        <b>snapshot</b> <b>restore</b> <b>&lt;snapname&gt;</b>
              Restores an already taken snapshot of a GlusterFS volume. Snapshot restore is an offline  activity
              therefore  if  the  volume is online (in started state) then the restore operation will fail. Once
              the snapshot is restored it will not be available in the list of snapshots.

        <b>snapshot</b> <b>clone</b> <b>&lt;clonename&gt;</b> <b>&lt;snapname&gt;</b>
              Create a clone of a snapshot volume, the resulting volume  will  be  GlusterFS  volume.  User  can
              provide  a  clone-name. To be able to take a clone, snapshot should be present and it should be in
              activated state.

        <b>snapshot</b> <b>delete</b> <b>(</b> <b>all</b> <b>|</b> <b>&lt;snapname&gt;</b> <b>|</b> <b>volume</b> <b>&lt;volname&gt;</b> <b>)</b>
              If snapname is specified then mentioned snapshot is deleted. If  volname  is  specified  then  all
              snapshots  belonging  to  that  particular  volume  is  deleted. If keyword *all* is used then all
              snapshots belonging to the system is deleted.

        <b>snapshot</b> <b>list</b> <b>[volname]</b>
              Lists all snapshots taken. If volname is provided, then  only  the  snapshots  belonging  to  that
              particular volume is listed.

        <b>snapshot</b> <b>info</b> <b>[snapname</b> <b>|</b> <b>(volume</b> <b>&lt;volname&gt;)]</b>
              This  command  gives  information such as snapshot name, snapshot UUID, time at which snapshot was
              created, and it lists down the snap-volume-name, number of snapshots already taken and  number  of
              snapshots  still  available for that particular volume, and the state of the snapshot. If snapname
              is specified then info of the  mentioned  snapshot is  displayed.  If volname  is  specified  then
              info  of all snapshots belonging to that volume is displayed.  If  both  snapname and  volname  is
              not specified then info of all the snapshots present in the system are displayed.

        <b>snapshot</b> <b>status</b> <b>[snapname</b> <b>|</b> <b>(volume</b> <b>&lt;volname&gt;)]</b>
              This command gives status of the snapshot. The details included are snapshot  brick  path,  volume
              group(LVM  details),  status of the snapshot bricks, PID of the bricks, data percentage filled for
              that particular volume group to which the snapshots belong to,  and  total  size  of  the  logical
              volume.

              If  snapname  is  specified  then  status  of  the  mentioned snapshot is displayed. If volname is
              specified then status of all snapshots belonging to that volume is displayed. If both snapname and
              volname is not specified then status of all the snapshots present in the system are displayed.

        <b>snapshot</b> <b>config</b> <b>[volname]</b> <b>([snap-max-hard-limit</b> <b>&lt;count&gt;]</b> <b>[snap-max-soft-limit</b> <b>&lt;percent&gt;])</b> <b>|</b> <b>([auto-</b>
       <b>delete</b> <b>&lt;enable|disable&gt;])</b> <b>|</b> <b>([activate-on-create</b> <b>&lt;enable|disable&gt;])</b>
              Displays and sets the snapshot config values.

              snapshot config without any keywords displays the snapshot config values of  all  volumes  in  the
              system. If volname is provided, then the snapshot config values of that volume is displayed.

              Snapshot  config  command along with keywords can be used to change the existing config values. If
              volname is provided then config value of that volume is  changed,  else  it  will  set/change  the
              system limit.

              snap-max-soft-limit  and  auto-delete are global options, that will be inherited by all volumes in
              the system and cannot be set to individual volumes.

              snap-max-hard-limit can be set globally, as well as per  volume.  The  lowest  limit  between  the
              global system limit and the volume specific limit, becomes the "Effective snap-max-hard-limit" for
              a volume.

              snap-max-soft-limit is a percentage value, which is applied on the "Effective snap-max-hard-limit"
              to get the "Effective snap-max-soft-limit".

              When  auto-delete feature is enabled, then upon reaching the "Effective snap-max-soft-limit", with
              every successful snapshot creation, the oldest snapshot will be deleted.

              When auto-delete feature is disabled, then upon reaching the "Effective snap-max-soft-limit",  the
              user gets a warning with every successful snapshot creation.

              When  auto-delete  feature  is  disabled,  then upon reaching the "Effective snap-max-hard-limit",
              further  snapshot  creations  will not be allowed.

              activate-on-create is disabled by default. If you enable activate-on-create, then further snapshot
              will be activated during the time of snapshot creation.

        <b>snapshot</b> <b>activate</b> <b>&lt;snapname&gt;</b>
              Activates the mentioned snapshot.

              Note : By default the snapshot is activated during snapshot creation.

        <b>snapshot</b> <b>deactivate</b> <b>&lt;snapname&gt;</b>
              Deactivates the mentioned snapshot.

        <b>snapshot</b> <b>help</b>
              Display help for the snapshot commands.

   <b>Self-heal</b> <b>Commands</b>
        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b>
              Triggers index self heal for the files that need healing.

        <b>volume</b> <b>heal</b>  <b>&lt;VOLNAME&gt;</b> <b>[enable</b> <b>|</b> <b>disable]</b>
              Enable/disable self-heal-daemon for volume &lt;VOLNAME&gt;.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>full</b>
              Triggers self heal on all the files.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>info</b>
              Lists the files that need healing.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>info</b> <b>split-brain</b>
              Lists the files which are in split-brain state.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>statistics</b>
              Lists the crawl statistics.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>statistics</b> <b>heal-count</b>
              Displays the count of files to be healed.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>statistics</b> <b>heal-count</b> <b>replica</b> <b>&lt;HOSTNAME:BRICKNAME&gt;</b>
              Displays the number of files to be healed from a particular replica subvolume to which  the  brick
              &lt;HOSTNAME:BRICKNAME&gt; belongs.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>split-brain</b> <b>bigger-file</b> <b>&lt;FILE&gt;</b>
              Performs  healing  of &lt;FILE&gt; which is in split-brain by choosing the bigger file in the replica as
              source.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>split-brain</b> <b>source-brick</b> <b>&lt;HOSTNAME:BRICKNAME&gt;</b>
              Selects &lt;HOSTNAME:BRICKNAME&gt; as the source for all the files  that  are  in  split-brain  in  that
              replica and heals them.

        <b>volume</b> <b>heal</b> <b>&lt;VOLNAME&gt;</b> <b>split-brain</b> <b>source-brick</b> <b>&lt;HOSTNAME:BRICKNAME&gt;</b> <b>&lt;FILE&gt;</b>
              Selects the split-brained &lt;FILE&gt; present in &lt;HOSTNAME:BRICKNAME&gt; as source and completes heal.

   <b>Other</b> <b>Commands</b>
        <b>get-state</b> <b>[&lt;daemon&gt;]</b> <b>[[odir</b> <b>&lt;/path/to/output/dir/&gt;]</b> <b>[file</b> <b>&lt;filename&gt;]]</b> <b>[detail|volumeoptions]</b>
              Get local state representation of mentioned daemon and store data in provided path information

        <b>help</b>  Display the command options.

        <b>quit</b>  Exit the gluster command line interface.

</pre><h4><b>FILES</b></h4><pre>
       /var/lib/glusterd/*

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man1/fusermount.1.html">fusermount</a></b>(1), <b><a href="../man8/mount.glusterfs.8.html">mount.glusterfs</a></b>(8), <b><a href="../man8/glusterfs.8.html">glusterfs</a></b>(8), <b><a href="../man8/glusterd.8.html">glusterd</a></b>(8)

</pre><h4><b>COPYRIGHT</b></h4><pre>
       Copyright(c) 2006-2011  Gluster, Inc.  &lt;<a href="http://www.gluster.com">http://www.gluster.com</a>&gt;

07 March 2011                             Gluster command line utility                                <u><a href="../man8/Gluster.8.html">Gluster</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>