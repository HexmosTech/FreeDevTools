<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>cpudist - On- and off-CPU task time as a histogram.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/bpfcc-tools">bpfcc-tools_0.31.0+ds-7ubuntu2_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       cpudist - On- and off-CPU task time as a histogram.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>cpudist</b> <b>[-h]</b> <b>[-O]</b> <b>[-T]</b> <b>[-m]</b> <b>[-P]</b> <b>[-L]</b> <b>[-p</b> <b>PID]</b> <b>[-I]</b> <b>[-e]</b> <b>[interval]</b> <b>[count]</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       This  measures  the  time  a  task  spends  on the CPU before being descheduled, and shows the times as a
       histogram. Tasks that spend a very short time on the CPU can be indicative of excessive  context-switches
       and  poor  workload  distribution,  and  possibly point to a shared source of contention that keeps tasks
       switching in and out as it becomes available (such as a mutex).

       Similarly, the tool can also measure the time a task spends off-CPU before it is  scheduled  again.  This
       can  be helpful in identifying long blocking and I/O operations, or alternatively very short descheduling
       times due to short-lived locks or timers.

       By default CPU idle time are excluded by simply excluding PID 0.

       This tool uses in-kernel eBPF maps for storing timestamps and  the  histogram,  for  efficiency.  Despite
       this, the overhead of this tool may become significant for some workloads: see the OVERHEAD section.

       Since this uses BPF, only the root user can use this tool.

</pre><h4><b>REQUIREMENTS</b></h4><pre>
       CONFIG_BPF and bcc.

</pre><h4><b>OPTIONS</b></h4><pre>
       -h     Print usage message.

       -O     Measure off-CPU time instead of on-CPU time.

       -T     Include timestamps on output.

       -m     Output histogram in milliseconds.

       -P     Print a histogram for each PID (tgid from the kernel's perspective).

       -L     Print a histogram for each TID (pid from the kernel's perspective).

       -p PID Only show this PID (filtered in kernel for efficiency).

       -I     Include CPU idle time (by default these are excluded).

       -e     Show extension summary (average/total/count).

       interval
              Output interval, in seconds.

       count  Number of outputs.

</pre><h4><b>EXAMPLES</b></h4><pre>
       Summarize task on-CPU time as a histogram:
              # <b>cpudist</b>

       Summarize task off-CPU time as a histogram:
              # <b>cpudist</b> <b>-O</b>

       Print 1 second summaries, 10 times:
              # <b>cpudist</b> <b>1</b> <b>10</b>

       Print 1 second summaries, using milliseconds as units for the histogram, and include timestamps on
       output:
              # <b>cpudist</b> <b>-mT</b> <b>1</b>

       Trace PID 185 only, 1 second summaries:
              # <b>cpudist</b> <b>-p</b> <b>185</b> <b>1</b>

       Include CPU idle time:
              # <b>cpudist</b> <b>-I</b>

       Also show extension summary:
              # <b>cpudist</b> <b>-e</b>

</pre><h4><b>FIELDS</b></h4><pre>
       usecs  Microsecond range

       msecs  Millisecond range

       count  How many times a task event fell into this range

       distribution
              An ASCII bar chart to visualize the distribution (count column)

</pre><h4><b>OVERHEAD</b></h4><pre>
       This  traces scheduler tracepoints, which can become very frequent. While eBPF has very low overhead, and
       this tool uses in-kernel maps for efficiency, the frequency of scheduler events for some workloads may be
       high enough that the overhead of this tool becomes significant. Measure in a lab environment to  quantify
       the overhead before use.

</pre><h4><b>SOURCE</b></h4><pre>
       This is from bcc.

              https://github.com/iovisor/bcc

       Also look in the bcc distribution for a companion _example.txt file containing example usage, output, and
       commentary for this tool.

</pre><h4><b>OS</b></h4><pre>
       Linux

</pre><h4><b>STABILITY</b></h4><pre>
       Unstable - in development.

</pre><h4><b>AUTHOR</b></h4><pre>
       Sasha Goldshtein, Rocky Xing

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man1/pidstat.1.html">pidstat</a>(1), <a href="../man8/runqlat.8.html">runqlat</a>(8)

USER COMMANDS                                      2016-06-28                                         <u><a href="../man8/cpudist.8.html">cpudist</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>