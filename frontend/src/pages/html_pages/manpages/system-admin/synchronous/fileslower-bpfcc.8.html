<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>fileslower - Trace slow synchronous file reads and writes.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/bpfcc-tools">bpfcc-tools_0.31.0+ds-7ubuntu2_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       fileslower - Trace slow synchronous file reads and writes.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>fileslower</b> <b>[-h]</b> <b>[-p</b> <b>PID]</b> <b>[-a]</b> <b>[min_ms]</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       This script uses kernel dynamic tracing of synchronous reads and writes at the VFS interface, to identify
       slow file reads and writes for any file system.

       This  version  traces  __vfs_read()  and  __vfs_write()  and  only  showing  synchronous I/O (the path to
       new_sync_read() and new_sync_write()), and I/O with filenames. This approach provides a view of just  two
       file  system  request  types:  file  reads and writes. There are typically many others: asynchronous I/O,
       directory operations, file handle operations, file open()s, fflush(), etc.

       WARNING: See the OVERHEAD section.

       By default, a minimum millisecond threshold of 10 is used.

       Since this works by tracing various kernel __vfs_*()  functions  using  dynamic  tracing,  it  will  need
       updating  to match any changes to these functions. A future version should switch to using FS tracepoints
       instead.

       Since this uses BPF, only the root user can use this tool.

</pre><h4><b>REQUIREMENTS</b></h4><pre>
       CONFIG_BPF and bcc.

</pre><h4><b>OPTIONS</b></h4><pre>
       -p PID Trace this PID only.

       -a     Include non-regular file types in output (sockets, FIFOs, etc).

       min_ms Minimum I/O latency (duration) to trace, in milliseconds. Default is 10 ms.

</pre><h4><b>EXAMPLES</b></h4><pre>
       Trace synchronous file reads and writes slower than 10 ms:
              # <b>fileslower</b>

       Trace slower than 1 ms:
              # <b>fileslower</b> <b>1</b>

       Trace slower than 1 ms, for PID 181 only:
              # <b>fileslower</b> <b>-p</b> <b>181</b> <b>1</b>

</pre><h4><b>FIELDS</b></h4><pre>
       TIME(s)
              Time of I/O completion since the first I/O seen, in seconds.

       COMM   Process name.

       PID    Process ID.

       D      Direction of I/O. R == read, W == write.

       BYTES  Size of I/O, in bytes.

       <a href="../manms/LAT.ms.html">LAT</a>(ms)
              Latency (duration) of I/O, measured from when  the  application  issued  it  to  VFS  to  when  it
              completed.  This time is inclusive of block device I/O, file system CPU cycles, file system locks,
              run queue latency, etc. It's a more accurate measure  of  the  latency  suffered  by  applications
              performing file system I/O, than to measure this down at the block device interface.

       FILENAME
              A cached kernel file name (comes from dentry-&gt;d_name.name).

</pre><h4><b>OVERHEAD</b></h4><pre>
       Depending on the frequency of application reads and writes, overhead can become severe, in the worst case
       slowing  applications  by  2x.  In  the  best  case, the overhead is negligible. Hopefully for real world
       workloads the overhead is often at the lower end of the spectrum -- test before use. The reason for  high
       overhead  is  that  this  traces  VFS reads and writes, which includes FS cache reads and writes, and can
       exceed one million events per second if the application  is  I/O  heavy.  While  the  instrumentation  is
       extremely  lightweight,  and  uses  in-kernel eBPF maps for efficient timing and filtering, multiply that
       cost by one million events per second and that cost becomes a million times worse. You can get an idea of
       the possible cost by just counting the instrumented events using the bcc funccount tool, eg:

       # ./funccount.py -i 1 -r '^__vfs_(read|write)$'

       This also costs overhead, but is somewhat less than fileslower.

       If the overhead is prohibitive for your workload, I'd recommend moving down-stack a little from VFS  into
       the  file system functions (ext4, xfs, etc).  Look for updates to bcc for specific file system tools that
       do this. The advantage of a per-file system approach is that we can trace  post-cache,  greatly  reducing
       events and overhead. The disadvantage is needing custom tracing approaches for each different file system
       (whereas VFS is generic).

</pre><h4><b>SOURCE</b></h4><pre>
       This is from bcc.

              https://github.com/iovisor/bcc

       Also  look  in  the bcc distribution for a companion _examples.txt file containing example usage, output,
       and commentary for this tool.

</pre><h4><b>OS</b></h4><pre>
       Linux

</pre><h4><b>STABILITY</b></h4><pre>
       Unstable - in development.

</pre><h4><b>AUTHOR</b></h4><pre>
       Brendan Gregg

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man8/biosnoop.8.html">biosnoop</a>(8), <a href="../man8/funccount.8.html">funccount</a>(8)

USER COMMANDS                                      2016-02-07                                      <u><a href="../man8/fileslower.8.html">fileslower</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>