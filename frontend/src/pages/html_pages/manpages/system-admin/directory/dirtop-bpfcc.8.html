<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>dirtop - File reads and writes by directory. Top for directories.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/bpfcc-tools">bpfcc-tools_0.31.0+ds-7ubuntu2_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       dirtop - File reads and writes by directory. Top for directories.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>dirtop</b>  <b>-d</b>  <b>directory1,directory2,...</b>  <b>[-h]</b>  <b>[-C]</b> <b>[-r</b> <b>MAXROWS]</b> <b>[-s</b> <b>{reads,writes,rbytes,wbytes}]</b> <b>[-p</b> <b>PID]</b>
       <b>[interval]</b> <b>[count]</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       This is top for directories.

       This traces file reads and writes, and prints a per-directory  summary  every  interval  (by  default,  1
       second).  By  default the summary is sorted on the highest read throughput (Kbytes). Sorting order can be
       changed via -s option.

       This uses in-kernel eBPF maps to store per process summaries for efficiency.

       This script works by tracing the __vfs_read() and __vfs_write() functions using kernel  dynamic  tracing,
       which  instruments  explicit  read and write calls. If files are read or written using another means (eg,
       via mmap()), then they will not be visible using this tool. Also, this tool will need updating  to  match
       any code changes to those vfs functions.

       This  should  be  useful  for  file  system  workload  characterization when analyzing the performance of
       applications.

       Note that tracing VFS level reads and writes can be a frequent activity, and this tool can begin to  cost
       measurable overhead at high I/O rates.

       Since this uses BPF, only the root user can use this tool.

</pre><h4><b>REQUIREMENTS</b></h4><pre>
       CONFIG_BPF and bcc.

</pre><h4><b>OPTIONS</b></h4><pre>
       -d     Defines  a  list  of  directories,  comma separated, to observe.  Wildcards are allowed if between
              single bracket.

       -C     Don't clear the screen.

       -r MAXROWS
              Maximum number of rows to print. Default is 20.

       -s {reads,writes,rbytes,wbytes}
              Sort column. Default is rbytes (read throughput).

       -p PID Trace this PID only.

       interval
              Interval between updates, seconds.

       count  Number of interval summaries.

</pre><h4><b>EXAMPLES</b></h4><pre>
       Summarize block device I/O by directory, 1 second screen refresh:
              # <b>dirtop</b> <b>-d</b> <b>'/hdfs/uuid/*/yarn'</b>

       Don't clear the screen, and top 8 rows only:
              # <b>dirtop</b> <b>-d</b> <b>'/hdfs/uuid/*/yarn'</b> <b>-Cr</b> <b>8</b>

       5 second summaries, 10 times only:
              # <b>dirtop</b> <b>-d</b> <b>'/hdfs/uuid/*/yarn'</b> <b>5</b> <b>10</b>

       Report read &amp; write IOs generated in mutliple yarn and data directories:
              # <b>dirtop</b> <b>-d</b> <b>'/hdfs/uuid/*/yarn,/hdfs/uuid/*/data'</b>

</pre><h4><b>FIELDS</b></h4><pre>
       loadavg:
              The contents of <a href="file:/proc/loadavg">/proc/loadavg</a>

       READS  Count of reads during interval.

       WRITES Count of writes during interval.

       R_Kb   Total read Kbytes during interval.

       W_Kb   Total write Kbytes during interval.

       PATH   The path were the IOs were accounted.

</pre><h4><b>OVERHEAD</b></h4><pre>
       Depending on the frequency of application reads and writes, overhead can become significant, in the worst
       case slowing applications by over 50%. Hopefully for real world workloads the overhead is  much  less  --
       test  before  use. The reason for the high overhead is that VFS reads and writes can be a frequent event,
       and despite the eBPF overhead being very small per event, if  you  multiply  this  small  overhead  by  a
       million events per second, it becomes a million times worse. Literally. You can gauge the number of reads
       and writes using the <a href="../man8/vfsstat.8.html">vfsstat</a>(8) tool, also from bcc.

</pre><h4><b>SOURCE</b></h4><pre>
       This is from bcc.

              https://github.com/iovisor/bcc

       Also  look  in  the bcc distribution for a companion _examples.txt file containing example usage, output,
       and commentary for this tool.

</pre><h4><b>OS</b></h4><pre>
       Linux

</pre><h4><b>STABILITY</b></h4><pre>
       Unstable - in development.

</pre><h4><b>AUTHOR</b></h4><pre>
       Erwan Velu

</pre><h4><b>INSPIRATION</b></h4><pre>
       <a href="../man8/filetop.8.html">filetop</a>(8) by Brendan Gregg

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man8/vfsstat.8.html">vfsstat</a>(8), <a href="../man8/vfscount.8.html">vfscount</a>(8), <a href="../man8/fileslower.8.html">fileslower</a>(8)

USER COMMANDS                                      2020-03-16                                          <u><a href="../man8/dirtop.8.html">dirtop</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>