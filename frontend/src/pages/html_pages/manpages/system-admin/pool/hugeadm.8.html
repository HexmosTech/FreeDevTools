<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>hugeadm - Configure the system huge page pools</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/libhugetlbfs-bin">libhugetlbfs-bin_2.24-1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       hugeadm - Configure the system huge page pools

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>hugeadm</b> <b>[options]</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>hugeadm</b>  displays  and  configures the systems huge page pools. The size of the pools is set as a minimum
       and maximum threshold.  The minimum value is allocated up front by the kernel and guaranteed to remain as
       hugepages until the pool is shrunk. If a maximum is set, the system will dynamically  allocate  pages  if
       applications  request  more  hugepages than the minimum size of the pool. There is no guarantee that more
       pages than this minimum pool size can be allocated.

       The following options create mounts hugetlbfs mount points.

       <b>--create-mounts</b>

              This creates mount points for each supported  huge  page  size  under  /var/lib/hugetlbfs.   After
              creation they are mounts and are owned by root:root with permissions set to 770.  Each mount point
              is named pagesize-&lt;size in bytes&gt;.

       <b>--create-user-mounts=&lt;user&gt;</b>

              This  creates mount points for each supported huge page size under /var/lib/hugetlbfs/user/&lt;user&gt;.
              Mount point naming is the same as --create-mounts.  After creation they are mounted and are  owned
              by &lt;user&gt;:root with permissions set to 700.

       <b>--create-group-mounts=&lt;group&gt;</b>

              This     creates     mount     points    for    each    supported    huge    page    size    under
              /var/lib/hugetlbfs/group/&lt;group&gt;.  Mount point naming  is  the  same  as  --create-mounts.   After
              creation they are mounted and are owned by root:&lt;group&gt; with permissions set to 070.

       <b>--create-global-mounts</b>

              This  creates  mount  points  for  each  supported huge page size under /var/lib/hugetlbfs/global.
              Mount point naming is the same as --create-mounts.  After creation they are mounted and are  owned
              by root:root with permissions set to 1777.

              The following options affect how mount points are created.

       <b>--max-size</b>

              This  option is used in conjunction with --create-*-mounts. It limits the maximum amount of memory
              used by files within the mount point rounded up to the nearest huge page size. This  can  be  used
              for example to grant different huge page quotas to individual users or groups.

       <b>--max-inodes</b>

              This  option  is  used in conjunction with --create-*-mounts. It limits the number of inodes (e.g.
              files) that can be created on the new mount points.  This limits the number of mappings  that  can
              be  created  on  a  mount  point.  It could be used for example to limit the number of application
              instances that used a mount point as long as  it  was  known  how  many  inodes  each  application
              instance required.

              The following options display information about the pools.

       <b>--pool-list</b>

              This  displays the Minimum, Current and Maximum number of huge pages in the pool for each pagesize
              supported by the system. The "Minimum" value is the size of the static pool and there will  always
              be  at  least this number of hugepages in use by the system, either by applications or kept by the
              kernel in a reserved pool. The "Current" value is the number of hugepages currently in use, either
              by applications or stored on the kernels free list. The "Maximum" value is the largest  number  of
              hugepages that can be in use at any given time.

       <b>--set-recommended-min_free_kbytes</b>

              Fragmentation  avoidance in the kernel depends on avoiding pages of different mobility types being
              mixed with a pageblock arena - typically the size of the default huge page size. The  more  mixing
              that  occurs,  the  less likely the huge page pool will be able to dynamically resize. The easiest
              means of  avoiding  mixing  is  to  increase  /proc/sys/vm/min_free_kbytes.  This  parameter  sets
              min_free_kbytes to a recommended value to aid fragmentation avoidance.

       <b>--set-recommended-shmmax</b>

              The  maximum  shared  memory segment size should be set to at least the size of the largest shared
              memory  segment   size   you   want   available   for   applications   using   huge   pages,   via
              /proc/sys/kernel/shmmax.  Optionally,  it  can  be set automatically to match the maximum possible
              size of all huge page allocations and thus the maximum possible shared memory segment size,  using
              this switch.

       <b>--set-shm-group=&lt;gid|groupname&gt;</b>

              Users  in  the  group  specified in <a href="file:/proc/sys/vm/hugetlb_shm_group">/proc/sys/vm/hugetlb_shm_group</a> are granted full access to huge
              pages. The sysctl takes a numeric gid, but this hugeadm option can set it for you, using either  a
              gid or group name.

       <b>--page-sizes</b>

              This displays every page size supported by the system and has a pool configured.

       <b>--page-sizes-all</b>

              This displays all page sizes supported by the system, even if no pool is available.

       <b>--list-all-mounts</b>

              This displays all active mount points for hugetlbfs.

       The following options configure the pool.

       <b>--pool-pages-min=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</b>

              This  option  sets  or adjusts the Minimum number of hugepages in the pool for pagesize <b>size</b>. <b>size</b>
              may be specified in bytes or in kilobytes, megabytes,  or  gigabytes  by  appending  K,  M,  or  G
              respectively,  or  as  DEFAULT,  which uses the system's default huge page size for <b>size</b>. The pool
              size adjustment can be specified by <b>pagecount</b> pages or by <b>memsize</b>, if postfixed with G, M,  or  K,
              for  gigabytes, megabytes, or kilobytes, respectively. If the adjustment is specified via <b>memsize</b>,
              then the <b>pagecount</b> will be calculated for you, based on page  size  <b>size</b>.   The  pool  is  set  to
              <b>pagecount</b>  pages  if  + or - are not specified. If + or - are specified, then the size of the pool
              will adjust by that amount.  Note that there is no guarantee that  the  system  can  allocate  the
              hugepages  requested for the Minimum pool. The size of the pools should be checked after executing
              this command to ensure they were successful.

       <b>--obey-numa-mempol</b>

              This option requests that allocation of huge pages to the static pool with  <b>--pool-pages-min</b>  obey
              the  NUMA  memory  policy  of  the  current process. This policy can be explicitly specified using
              numactl or inherited from a parent process.

       <b>--pool-pages-max=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</b>

              This option sets or adjusts the Maximum number of hugepages. Note that while the Minimum number of
              pages are guaranteed to be available to applications, there is not guarantee that the  system  can
              allocate  the  pages  on demand when the number of huge pages requested by applications is between
              the Minimum and Maximum pool sizes. See --pool-pages-min for usage syntax.

       <b>--enable-zone-movable</b>

              This option enables the use of the MOVABLE zone for the allocation  of  hugepages.  This  zone  is
              created  when kernelcore= or movablecore= are specified on the kernel command line but the zone is
              not used for the allocation of huge pages by default as the intended use for the zone  may  be  to
              guarantee  that  memory  can  be  off-lined  and hot-removed. The kernel guarantees that the pages
              within this zone can be reclaimed unlike some kernel buffers for example. Unless pages are  locked
              with mlock(), the hugepage pool can grow to at least the size of the movable zone once this option
              is  set.  Use  sysctl to permanently enable the use of the MOVABLE zone for the allocation of huge
              pages.

       <b>--disable-zone-movable</b>

              This option disables the use of the MOVABLE zone for the future allocation  of  huge  pages.  Note
              that  existing  huge pages are not reclaimed from the zone.  Use sysctl to permanently disable the
              use of the MOVABLE zone for the allocation of huge pages.

       <b>--hard</b>

              This option is specified with --pool-pages-min to retry allocations multiple times on  failure  to
              allocate  the  desired  count  of  pages.  It initially tries to resize the pool up to 5 times and
              continues to try if progress is being made towards the resize.

       <b>--add-temp-swap&lt;=count&gt;</b>

              This options is specified with --pool-pages-min to  initialize  a  temporary  swap  file  for  the
              duration  of the pool resize. When increasing the size of the pool, it can be necessary to reclaim
              pages so that contiguous memory is freed and this often requires swap to be  successful.  Swap  is
              only  created  for  a positive resize, and is then removed once the resize operation is completed.
              The default swap size is 5 huge pages, the optional argument &lt;count&gt; sets the swap size to &lt;count&gt;
              huge pages.

       <b>--add-ramdisk-swap</b>

              This option is specified with --pool-pages-min to initialize swap in memory on  ram  disks.   When
              increasing the size of the pool, it can be necessary to reclaim pages so that contiguous memory is
              freed  and this often requires swap to be successful.  If there isn't enough free disk space, swap
              can be initialized in RAM using this option.  If the size of one ramdisk is not greater  than  the
              huge  page  size,  then  swap  is  initialized  on  multiple ramdisks.  Swap is only created for a
              positive resize, and by default is removed once the resize operation is completed.

       <b>--persist</b>

              This option is specified with the --add-temp-swap or --add-ramdisk-swap to  make  the  swap  space
              persist  after  the  resize operation is completed.  The swap spaces can later be removed manually
              using the swapoff command.

       The following options tune the transparent huge page usage

       <b>--thp-always</b>

              Enable transparent huge pages always

       <b>--thp-madvise</b>

              Enable transparent huge pages only on madvised regions

       <b>--thp-never</b>

              Disable transparent huge pages

       <b>--thp-khugepaged-pages</b> <b>&lt;pages</b> <b>to</b> <b>scan&gt;</b>

              Configure the number of pages that khugepaged should scan on each pass

       <b>--thp-khugepaged-scan-sleep</b> <b>&lt;milliseconds&gt;</b>

              Configure how many milliseconds khugepaged should wait between passes

       <b>--thp-khugepaged-alloc-sleep</b> <b>&lt;milliseconds&gt;</b>

              Configure how many milliseconds khugepaged should wait after failing to allocate a  huge  page  to
              throttle the next attempt.

       The following options affect the verbosity of libhugetlbfs.

       <b>--verbose</b> <b>&lt;level&gt;,</b> <b>-v</b>

              The  default  value  for  the  verbosity  level  is  1  and the range of the value can be set with
              --verbose from 0 to 99. The higher the value, the more verbose the library will be. 0 is quiet and
              3 will output much debugging information. The verbosity level is increased by one each time -v  is
              specified.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <u><a href="../man1/oprofile.1.html">oprofile</a>(1),</u> <u><a href="../man1/pagesize.1.html">pagesize</a>(1),</u> <u><a href="../man7/libhugetlbfs.7.html">libhugetlbfs</a>(7),</u> <u><a href="../man8/hugectl.8.html">hugectl</a>(8),</u>

</pre><h4><b>AUTHORS</b></h4><pre>
       libhugetlbfs was written by various people on the libhugetlbfs-devel mailing list.

                                                 October 1, 2009                                      <u><a href="../man8/HUGEADM.8.html">HUGEADM</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>