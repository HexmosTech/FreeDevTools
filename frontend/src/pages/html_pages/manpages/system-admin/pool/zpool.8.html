<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>zpool - configures ZFS storage pools</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/zfs-fuse">zfs-fuse_0.7.0-30_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       zpool - configures ZFS storage pools

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>zpool</b> [<b>-?</b>]

       <b>zpool</b> <b>add</b> [<b>-fn</b>] [<b>-o</b> <u>property=value</u>] <u>pool</u> <u>vdev</u> ...

       <b>zpool</b> <b>attach</b> [<b>-f</b>] [<b>-o</b> <u>property=value</u>] <u>pool</u> <u>device</u> <u>new_device</u>

       <b>zpool</b> <b>clear</b> [<b>-F</b> [<b>-n</b>]] <u>pool</u> [<u>device</u>]

       <b>zpool</b> <b>create</b> [<b>-fn</b>] [<b>-o</b> <u>property=value</u>] ... [<b>-O</b> <u>file-system-property=value</u>]
            ... [<b>-m</b> <u>mountpoint</u>] [<b>-R</b> <u>root</u>] <u>pool</u> <u>vdev</u> ...

       <b>zpool</b> <b>destroy</b> [<b>-f</b>] <u>pool</u>

       <b>zpool</b> <b>detach</b> <u>pool</u> <u>device</u>

       <b>zpool</b> <b>export</b> [<b>-f</b>] <u>pool</u> ...

       <b>zpool</b> <b>get</b> "<u>all</u>" | <u>property</u>[,...] <u>pool</u> ...

       <b>zpool</b> <b>history</b> [<b>-il</b>] [<u>pool</u>] ...

       <b>zpool</b> <b>import</b> [<b>-d</b> <u>dir</u>] [<b>-D</b>]

       <b>zpool</b> <b>import</b> [<b>-o</b> <u>mntopts</u>] [<b>-o</b> <u>property=value</u>] ... [<b>-d</b> <u>dir</u> | <b>-c</b> <u>cachefile</u>]
            [<b>-D</b>] [<b>-f</b>] [<b>-R</b> <u>root</u>] [<b>-F</b> [<b>-n</b>]] <b>-a</b>

       <b>zpool</b> <b>import</b> [<b>-o</b> <u>mntopts</u>] [<b>-o</b> <u>property=value</u>] ... [<b>-d</b> <u>dir</u> | <b>-c</b> <u>cachefile</u>]
            [<b>-D</b>] [<b>-f</b>] [<b>-R</b> <u>root</u>] [<b>-F</b> [<b>-n</b>]] <u>pool</u> |<u>id</u> [<u>newpool</u>]

       <b>zpool</b> <b>iostat</b> [<b>-T</b> u | d ] [<b>-v</b>] [<u>pool</u>] ... [<u>interval</u>[<u>count</u>]]

       <b>zpool</b> <b>list</b> [<b>-H</b>] [<b>-o</b> <u>property</u>[,...]] [<u>pool</u>] ...

       <b>zpool</b> <b>offline</b> [<b>-t</b>] <u>pool</u> <u>device</u> ...

       <b>zpool</b> <b>online</b> <u>pool</u> <u>device</u> ...

       <b>zpool</b> <b>remove</b> <u>pool</u> <u>device</u> ...

       <b>zpool</b> <b>replace</b> [<b>-f</b>] <u>pool</u> <u>device</u> [<u>new_device</u>]

       <b>zpool</b> <b>scrub</b> [<b>-s</b>] <u>pool</u> ...

       <b>zpool</b> <b>set</b> <u>property</u>=<u>value</u> <u>pool</u>

       <b>zpool</b> <b>split</b> [<b>-R</b> <u>altroot</u>] [<b>-n</b>] [<b>-o</b> <u>mntopts</u>] [<b>-o</b> <u>property</u>=<u>value</u>] <u>pool</u>
            <u>newpool</u> [<u>device</u> ...]

       <b>zpool</b> <b>status</b> [<b>-xv</b>] [<u>pool</u>] ...

       <b>zpool</b> <b>upgrade</b>

       <b>zpool</b> <b>upgrade</b> <b>-v</b>

       <b>zpool</b> <b>upgrade</b> [<b>-V</b> <u>version</u>] <b>-a</b> | <u>pool</u> ...

</pre><h4><b>DESCRIPTION</b></h4><pre>
       The  <b>zpool</b>  command configures <b>ZFS</b> storage pools. A storage pool is a collection of devices that provides
       physical storage and data replication for <b>ZFS</b> datasets.

       All datasets within a storage pool share  the  same  space.  See  <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M)  for  information  on  managing
       datasets.

   <b>Virtual</b> <b>Devices</b> <b>(vdev</b>s)
       A  "virtual  device"  describes a single device or a collection of devices organized according to certain
       performance and fault characteristics. The following virtual devices are supported:

       <b>disk</b>

           A block device, typically located under <b>/dev/dsk</b>. <b>ZFS</b> can use individual slices or partitions, though
           the recommended mode of operation is to use whole disks. A disk can be specified by a full  path,  or
           it  can  be a shorthand name (the relative portion of the path under "/dev/dsk"). A whole disk can be
           specified by omitting the slice or partition designation. For  example,  "c0t0d0"  is  equivalent  to
           "/dev/dsk/c0t0d0s2". When given a whole disk, <b>ZFS</b> automatically labels the disk, if necessary.

       <b>file</b>

           A regular file. The use of files as a backing store is strongly discouraged. It is designed primarily
           for  experimental  purposes,  as  the fault tolerance of a file is only as good as the file system of
           which it is a part. A file must be specified by a full path.

       <b>mirror</b>

           A mirror of two or more devices. Data is replicated in an identical fashion across all components  of
           a  mirror.  A  mirror with <u>N</u> disks of size <u>X</u> can hold <u>X</u> bytes and can withstand (<u>N-1</u>) devices failing
           before data integrity is compromised.

       <b>raidz</b>
       <b>raidz1</b>
       <b>raidz2</b>
       <b>raidz3</b>

           A variation on <b>RAID-5</b> that allows for better distribution of parity and eliminates the "<b>RAID-5</b>  write
           hole"  (in  which data and parity become inconsistent after a power loss). Data and parity is striped
           across all disks within a <b>raidz</b> group.

           A <b>raidz</b> group can have single-, double- , or triple parity, meaning that the <b>raidz</b> group can  sustain
           one,  two, or three failures, respectively, without losing any data. The <b>raidz1</b> <b>vdev</b> type specifies a
           single-parity <b>raidz</b> group; the <b>raidz2</b> <b>vdev</b> type specifies a double-parity <b>raidz</b> group; and the <b>raidz3</b>
           <b>vdev</b> type specifies a triple-parity <b>raidz</b> group. The <b>raidz</b> <b>vdev</b> type is an alias for <b>raidz1</b>.

           A <b>raidz</b> group with <u>N</u> disks of size <u>X</u> with <u>P</u> parity disks can hold approximately (<u>N-P</u>)*<u>X</u> bytes and can
           withstand <u>P</u> device(s) failing before data integrity is compromised. The minimum number of devices  in
           a  <b>raidz</b> group is one more than the number of parity disks. The recommended number is between 3 and 9
           to help increase performance.

       <b>spare</b>

           A special pseudo-<b>vdev</b> which keeps track of available hot spares for a pool. For more information, see
           the "Hot Spares" section.

       <b>log</b>

           A separate-intent log device. If more than one log device is specified, then writes are load-balanced
           between devices. Log devices can be mirrored. However, <b>raidz</b> <b>vdev</b> types are  not  supported  for  the
           intent log. For more information, see the "Intent Log" section.

       <b>cache</b>

           A  device  used  to cache storage pool data. A cache device cannot be configured as a mirror or <b>raidz</b>
           group. For more information, see the "Cache Devices" section.

       Virtual devices cannot be nested, so a mirror or <b>raidz</b> virtual device can only contain  files  or  disks.
       Mirrors of mirrors (or other combinations) are not allowed.

       A  pool  can  have any number of virtual devices at the top of the configuration (known as "root vdevs").
       Data is dynamically distributed across all top-level devices  to  balance  data  among  devices.  As  new
       virtual devices are added, <b>ZFS</b> automatically places data on the newly available devices.

       Virtual  devices  are  specified one at a time on the command line, separated by whitespace. The keywords
       "mirror" and "raidz" are used to distinguish where a group ends and  another  begins.  For  example,  the
       following creates two root vdevs, each a mirror of two disks:

         # <b>zpool</b> <b>create</b> <b>mypool</b> <b>mirror</b> <b>c0t0d0</b> <b>c0t1d0</b> <b>mirror</b> <b>c1t0d0</b> <b>c1t1d0</b>

   <b>Device</b> <b>Failure</b> <b>and</b> <b>Recovery</b>
       <b>ZFS</b>  supports  a rich set of mechanisms for handling device failure and data corruption. All metadata and
       data is checksummed, and <b>ZFS</b> automatically repairs bad data from a good copy when corruption is detected.

       In order to take advantage of these features, a pool must make use of  some  form  of  redundancy,  using
       either  mirrored or <b>raidz</b> groups. While <b>ZFS</b> supports running in a non-redundant configuration, where each
       root vdev is simply a disk or file, this is strongly discouraged. A single case  of  bit  corruption  can
       render some or all of your data unavailable.

       A  pool's health status is described by one of three states: online, degraded, or faulted. An online pool
       has all devices operating normally. A degraded pool is one in which one or more devices have failed,  but
       the  data  is still available due to a redundant configuration. A faulted pool has corrupted metadata, or
       one or more faulted devices, and insufficient replicas to continue functioning.

       The health of the top-level vdev, such as mirror or <b>raidz</b> device, is potentially impacted by the state of
       its associated vdevs, or component devices. A top-level vdev  or  component  device  is  in  one  of  the
       following states:

       <b>DEGRADED</b>

           One  or  more  top-level  vdevs  is  in  the degraded state because one or more component devices are
           offline. Sufficient replicas exist to continue functioning.

           One or more component devices is in the degraded or faulted state, but sufficient replicas  exist  to
           continue functioning. The underlying conditions are as follows:

               o      The  number  of checksum errors exceeds acceptable levels and the device is degraded as an
                      indication that something may be wrong. <b>ZFS</b> continues to use the device as necessary.

               o      The number of I/O errors exceeds acceptable levels. The device  could  not  be  marked  as
                      faulted because there are insufficient replicas to continue functioning.

       <b>FAULTED</b>

           One  or  more  top-level  vdevs  is  in  the  faulted state because one or more component devices are
           offline. Insufficient replicas exist to continue functioning.

           One or more component devices is in the faulted state, and insufficient replicas  exist  to  continue
           functioning. The underlying conditions are as follows:

               o      The device could be opened, but the contents did not match expected values.

               o      The  number  of  I/O errors exceeds acceptable levels and the device is faulted to prevent
                      further use of the device.

       <b>OFFLINE</b>

           The device was explicitly taken offline by the "<b>zpool</b> <b>offline</b>" command.

       <b>ONLINE</b>

           The device is online and functioning.

       <b>REMOVED</b>

           The device was physically removed while the system was running. Device removal detection is hardware-
           dependent and may not be supported on all platforms.

       <b>UNAVAIL</b>

           The device could not be opened. If a pool is imported when a device was unavailable, then the  device
           will be identified by a unique identifier instead of its path since the path was never correct in the
           first place.

       If  a  device  is  removed  and  later  re-attached  to the system, <b>ZFS</b> attempts to put the device online
       automatically. Device attach detection is hardware-dependent and might not be supported on all platforms.

   <b>Hot</b> <b>Spares</b>
       <b>ZFS</b> allows devices to be associated with pools as "hot spares". These devices are not  actively  used  in
       the  pool, but when an active device fails, it is automatically replaced by a hot spare. To create a pool
       with hot spares, specify a "spare" <b>vdev</b> with any number of devices. For example,

         # <b>zpool</b> <b>create</b> <b>pool</b> <b>mirror</b> <b>c0d0</b> <b>c1d0</b> <b>spare</b> <b>c2d0</b> <b>c3d0</b>

       Spares can be shared across multiple pools, and can be added with the "<b>zpool</b>  <b>add</b>"  command  and  removed
       with  the  "<b>zpool</b>  <b>remove</b>"  command. Once a spare replacement is initiated, a new "spare" <b>vdev</b> is created
       within the configuration that will remain there until the original device is replaced. At this point, the
       hot spare becomes available again if another device fails.

       If a pool has a shared spare that is currently being used, the pool can not be exported since other pools
       may use this shared spare, which may lead to potential data corruption.

       An in-progress spare replacement can be cancelled by detaching the hot spare.  If  the  original  faulted
       device  is  detached,  then the hot spare assumes its place in the configuration, and is removed from the
       spare list of all active pools.

       Spares cannot replace log devices.

   <b>Intent</b> <b>Log</b>
       The <b>ZFS</b> Intent Log (<b>ZIL</b>)  satisfies  <b>POSIX</b>  requirements  for  synchronous  transactions.  For  instance,
       databases  often  require their transactions to be on stable storage devices when returning from a system
       call. <b>NFS</b> and other applications can also use <b>fsync</b>() to ensure data stability. By  default,  the  intent
       log  is  allocated  from  blocks  within  the  main  pool.  However,  it  might be possible to get better
       performance using separate intent log devices such as <b>NVRAM</b> or a dedicated disk. For example:

         # <b>zpool</b> <b>create</b> <b>pool</b> <b>c0d0</b> <b>c1d0</b> <b>log</b> <b>c2d0</b>

       Multiple log devices can also be specified, and they can be mirrored. See the  EXAMPLES  section  for  an
       example of mirroring multiple log devices.

       Log  devices  can be added, replaced, attached, detached, and imported and exported as part of the larger
       pool. Mirrored log devices can be removed by specifying the top-level mirror for the log.

   <b>Cache</b> <b>Devices</b>
       Devices can be added to a storage pool as "cache devices." These devices provide an additional  layer  of
       caching between main memory and disk. For read-heavy workloads, where the working set size is much larger
       than  what  can  be  cached in main memory, using cache devices allow much more of this working set to be
       served from low latency media. Using cache devices provides  the  greatest  performance  improvement  for
       random read-workloads of mostly static content.

       To create a pool with cache devices, specify a "cache" <b>vdev</b> with any number of devices. For example:

         # <b>zpool</b> <b>create</b> <b>pool</b> <b>c0d0</b> <b>c1d0</b> <b>cache</b> <b>c2d0</b> <b>c3d0</b>

       Cache  devices  cannot  be mirrored or part of a <b>raidz</b> configuration. If a read error is encountered on a
       cache device, that read <b>I/O</b> is reissued to the original storage pool device, which might  be  part  of  a
       mirrored or <b>raidz</b> configuration.

       The content of the cache devices is considered volatile, as is the case with other system caches.

   <b>Processes</b>
       Each  imported  pool has an associated process, named <b>zpool-</b><u>poolname</u>. The threads in this process are the
       pool's I/O processing threads, which handle the compression, checksumming, and other tasks  for  all  I/O
       associated  with  the  pool.  This  process exists to provides visibility into the CPU utilization of the
       system's storage pools. The existence of this process is an unstable interface.

   <b>Properties</b>
       Each pool has several properties associated with it.  Some  properties  are  read-only  statistics  while
       others are configurable and change the behavior of the pool. The following are read-only properties:

       <b>alloc</b>

           Amount of storage space within the pool that has been physically allocated.

       <b>capacity</b>

           Percentage  of  pool  space used. This property can also be referred to by its shortened column name,
           "cap".

       <b>dedupratio</b>

           The deduplication ratio specified for a pool, expressed  as a multiplier. Deduplication can be turned
           on by entering the command:

             # <b>zfs</b> <b>set</b> <b>dedup=on</b> <u>dataset</u>

           The default value is <b>off</b>.

           <b>dedupratio</b> is expressed as a single decimal number. For example, a <b>dedupratio</b> value of 1.76 indicates
           that 1.76 units of data were stored but only 1 unit of disk space was actually consumed.

       <b>free</b>

           Number of blocks within the pool that are not allocated.

       <b>guid</b>

           A unique identifier for the pool.

       <b>health</b>

           The current health of the pool. Health can be "<b>ONLINE</b>", "<b>DEGRADED</b>", "<b>FAULTED</b>", " <b>OFFLINE</b>", "<b>REMOVED</b>",
           or "<b>UNAVAIL</b>".

       <b>size</b>

           Total size of the storage pool.

       These space usage properties report actual physical space available to the  storage  pool.  The  physical
       space  can  be different from the total amount of space that any contained datasets can actually use. The
       amount of space used in a <b>raidz</b> configuration depends on the characteristics of the data  being  written.
       In addition, <b>ZFS</b> reserves some space for internal accounting that the <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M) command takes into account,
       but  the  <b>zpool</b>  command  does  not.  For  non-full  pools  of a reasonable size, these effects should be
       invisible. For small pools, or pools that are close to being completely  full,  these  discrepancies  may
       become more noticeable.  The following property can be set at creation time:

       <b>ashift</b>

           Pool  sector  size  exponent,  to the power of 2 (internally referred to as "ashift"). I/O operations
           will be aligned to the specified size boundaries. Additionally, the minimum (disk) write size will be
           set to the specified size, so this represents a space vs. performance trade-off. The typical case for
           setting this property is when performance is important and the underlying disks use 4KiB sectors  but
           report  512B  sectors  to  the  OS (for compatibility reasons); in that case, set <b>ashift=12</b> (which is
           1&lt;&lt;12 = 4096).  Since most large disks have had 4K sectors since 2011, ZFS defaults to ashift=12  for
           all disks larger than 512 GB.

           For  optimal  performance, the pool sector size should be greater than or equal to the sector size of
           the underlying disks. Since the property cannot be changed after pool creation, if in a  given  pool,
           you <u>ever</u> want to use drives that <u>report</u> 4KiB sectors, you must set <b>ashift=12</b> at pool creation time.

       The following property can be set at creation time and import time:

       <b>altroot</b>

           Alternate  root  directory.  If set, this directory is prepended to any mount points within the pool.
           This can be used when examining an unknown pool where the mount points cannot be trusted,  or  in  an
           alternate  boot  environment,  where  the  typical  paths  are not valid. <b>altroot</b> is not a persistent
           property. It is valid only while the system is up. Setting <b>altroot</b> defaults to using  <b>cachefile</b>=none,
           though this may be overridden   using an explicit setting.

       The  following  properties  can be set at creation time and import time, and later changed with the <b>zpool</b>
       <b>set</b> command:

       <b>autoexpand</b>=<b>on</b> | <b>off</b>

           Controls automatic pool expansion when the underlying LUN is grown. If set to <b>on</b>, the  pool  will  be
           resized according to the size of the expanded device. If the device is part of a mirror or <b>raidz</b> then
           all devices within that mirror/<b>raidz</b> group must be expanded before the new space is made available to
           the  pool. The default behavior is <b>off</b>. This property can also be referred to by its shortened column
           name, <b>expand</b>.

       <b>autoreplace</b>=<b>on</b> | <b>off</b>

           Controls automatic device replacement. If set to "<b>off</b>", device replacement must be initiated  by  the
           administrator by using the "<b>zpool</b> <b>replace</b>" command. If set to "<b>on</b>", any new device, found in the same
           physical  location  as  a device that previously belonged to the pool, is automatically formatted and
           replaced. The default behavior is "<b>off</b>". This property can also  be  referred  to  by  its  shortened
           column name, "replace".

       <b>bootfs</b>=<u>pool</u>/<u>dataset</u>

           Identifies the default bootable dataset for the root pool. This property is expected to be set mainly
           by the installation and upgrade programs.

       <b>cachefile</b>=<u>path</u> | <b>none</b>

           Controls  the  location  of  where  the pool configuration is cached. Discovering all pools on system
           startup requires a cached copy of the configuration data that is stored on the root file system.  All
           pools  in  this  cache  are  automatically imported when the system boots. Some environments, such as
           install and clustering, need to cache this information in a different location so that pools are  not
           automatically  imported.  Setting this property caches the pool configuration in a different location
           that can later be imported with "<b>zpool</b> <b>import</b> <b>-c</b>". Setting it to the special value "<b>none</b>"  creates  a
           temporary  pool  that  is  never  cached,  and  the  special value <b>''</b> (empty string) uses the default
           location.

           Multiple pools can share the same cache file. Because the kernel destroys  and  recreates  this  file
           when  pools are added and removed, care should be taken when attempting to access this file. When the
           last pool using a <b>cachefile</b> is exported or destroyed, the file is removed.

       <b>delegation</b>=<b>on</b> | <b>off</b>

           Controls whether a non-privileged user is granted access based on the dataset permissions defined  on
           the dataset. See <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M) for more information on <b>ZFS</b> delegated administration.

       <b>failmode</b>=<b>wait</b> | <b>continue</b> | <b>panic</b>

           Controls the system behavior in the event of catastrophic pool failure. This condition is typically a
           result  of  a  loss  of  connectivity to the underlying storage device(s) or a failure of all devices
           within the pool. The behavior of such an event is determined as follows:

           <b>wait</b>

               Blocks all <b>I/O</b> access to the pool until the device connectivity is recovered and the  errors  are
               cleared. A pool remains in the wait state until the device issue is resolved. This is the default
               behavior.

           <b>continue</b>

               Returns  <b>EIO</b>  to  any  new  write  <b>I/O</b>  requests but allows reads to any of the remaining healthy
               devices. Any write requests that have yet to be committed to disk would be blocked.

           <b>panic</b>

               Prints out a message to the console and generates a system crash dump.

       <b>listsnaps</b>=on | off

           Controls whether information about snapshots associated with this pool is output when "<b>zfs</b>  <b>list</b>"  is
           run without the <b>-t</b> option. The default value is "off".

       <b>version</b>=<u>version</u>

           The  current  on-disk  version of the pool. This can be increased, but never decreased. The preferred
           method of updating pools is with the "<b>zpool</b> <b>upgrade</b>" command, though this property can be used when a
           specific version is needed for backwards compatibility. This property can be any number between 1 and
           the current version reported by "<b>zpool</b> <b>upgrade</b> <b>-v</b>".

   <b>Subcommands</b>
       All subcommands that modify state are logged persistently to the pool in their original form.

       The <b>zpool</b> command provides subcommands to create and destroy  storage  pools,  add  capacity  to  storage
       pools, and provide information about the storage pools. The following subcommands are supported:

       <b>zpool</b> <b>-?</b>

           Displays a help message.

       <b>zpool</b> <b>add</b> [<b>-fn</b>]  [<b>-o</b> <u>property=value</u>] <u>pool</u> <u>vdev</u> ...

           Adds  the  specified  virtual  devices  to the given pool. The <u>vdev</u> specification is described in the
           "Virtual Devices" section. The behavior of the  <b>-f</b>  option,  and  the  device  checks  performed  are
           described in the "zpool create" subcommand.

           <b>-f</b>

               Forces  use  of <b>vdev</b>s, even if they appear in use or specify a conflicting replication level. Not
               all devices can be overridden in this manner.

           <b>-n</b>

               Displays the configuration that would be used without actually adding the <b>vdev</b>s. The actual  pool
               creation can still fail due to insufficient privileges or device sharing.

           <b>-o</b> <u>property=value</u>

               Sets  the given pool properties. See the "Properties" section for a list of valid properties that
               can be set. The only property supported at the moment is "ashift".

           Do not add a disk that is currently configured as a quorum device to a zpool. After a disk is in  the
           pool, that disk can then be configured as a quorum device.

       <b>zpool</b> <b>attach</b> [<b>-f</b>]  [<b>-o</b> <u>property=value</u>] <u>pool</u> <u>device</u> <u>new_device</u>

           Attaches  <u>new_device</u>  to  an  existing  <b>zpool</b>  device.  The existing device cannot be part of a <b>raidz</b>
           configuration. If <u>device</u> is not currently part of  a  mirrored  configuration,  <u>device</u>  automatically
           transforms  into  a  two-way  mirror of <u>device</u> and <u>new_device</u>. If <u>device</u> is part of a two-way mirror,
           attaching <u>new_device</u> creates a three-way mirror, and so on. In  either  case,  <u>new_device</u>  begins  to
           resilver immediately.

           <b>-f</b>

               Forces  use of <u>new_device</u>, even if its appears to be in use. Not all devices can be overridden in
               this manner.

           <b>-o</b> <u>property=value</u>

               Sets the given pool properties. See the "Properties" section for a list of valid properties  that
               can be set. The only property supported at the moment is "ashift".

       <b>zpool</b> <b>clear</b> [<b>-F</b> [<b>-n</b>]] <u>pool</u> [<u>device</u>] ...

           Clears  device errors in a pool. If no arguments are specified, all device errors within the pool are
           cleared. If one or more devices is specified, only those errors associated with the specified  device
           or devices are cleared.

           <b>-F</b>

               Initiates  recovery mode for an unopenable pool. Attempts to discard the last few transactions in
               the pool to return it to an openable state. Not all damaged pools can be recovered by using  this
               option. If successful, the data from the discarded transactions is irretrievably lost.

           <b>-n</b>

               Used  in  combination with the <b>-F</b> flag. Check whether discarding transactions would make the pool
               openable, but do not actually discard any transactions.

       <b>zpool</b> <b>create</b> [<b>-fn</b>] [<b>-o</b> <u>property=value</u>] ... [<b>-O</b> <u>file-system-property=value</u>] ... [<b>-m</b> <u>mountpoint</u>] [<b>-R</b> <u>root</u>]
       <u>pool</u> <u>vdev</u> ...

           Creates a new storage pool containing the virtual devices specified on the  command  line.  The  pool
           name  must  begin  with  a letter, and can only contain alphanumeric characters as well as underscore
           ("_"), dash ("-"), and period ("."). The pool names <b>mirror</b>, <b>raidz</b>, <b>spare</b>, and <b>log</b>  are  reserved,  as
           are  names  beginning  with  the  pattern <b>c</b>[<b>0-9</b>]. The <b>vdev</b> specification is described in the "Virtual
           Devices" section.

           The command verifies that each device specified is accessible and not currently  in  use  by  another
           subsystem.  There  are some uses, such as being currently mounted, or specified as the dedicated dump
           device, that prevents a device from ever being used by <b>ZFS</b>. Other uses, such as having a  preexisting
           <b>UFS</b> file system, can be overridden with the <b>-f</b> option.

           The  command  also  checks  that  the  replication strategy for the pool is consistent. An attempt to
           combine redundant and non-redundant storage in a single pool, or to mix disks and files,  results  in
           an error unless <b>-f</b> is specified. The use of differently sized devices within a single <b>raidz</b> or mirror
           group is also flagged as an error unless <b>-f</b> is specified.

           Unless the <b>-R</b> option is specified, the default mount point is "/<u>pool</u>". The mount point must not exist
           or  must  be  empty,  or  else the root dataset cannot be mounted. This can be overridden with the <b>-m</b>
           option.

           <b>-f</b>

               Forces use of <b>vdev</b>s, even if they appear in use or specify a conflicting replication  level.  Not
               all devices can be overridden in this manner.

           <b>-n</b>

               Displays the configuration that would be used without actually creating the pool. The actual pool
               creation can still fail due to insufficient privileges or device sharing.

           <b>-o</b> <u>property=value</u> [<b>-o</b> <u>property=value</u>] ...

               Sets  the given pool properties. See the "Properties" section for a list of valid properties that
               can be set.

           <b>-O</b> <u>file-system-property=value</u>
           <b>[-O</b> <u>file-system-property=value</u>] ...

               Sets the given file system properties in the root file system of the pool. See  the  "Properties"
               section of <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M) for a list of valid properties that can be set.

           <b>-R</b> <u>root</u>

               Equivalent to "-o cachefile=none,altroot=<u>root</u>"

           <b>-m</b> <u>mountpoint</u>

               Sets  the  mount point for the root dataset. The default mount point is "/<u>pool</u>" or "<b>altroot</b>/<u>pool</u>"
               if <b>altroot</b> is specified. The mount point must be an absolute path, "<b>legacy</b>", or "<b>none</b>". For  more
               information on dataset mount points, see <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M).

       <b>zpool</b> <b>destroy</b> [<b>-f</b>] <u>pool</u>

           Destroys  the  given  pool,  freeing  up any devices for other use. This command tries to unmount any
           active datasets before destroying the pool.

           <b>-f</b>

               Forces any active datasets contained within the pool to be unmounted.

       <b>zpool</b> <b>detach</b> <u>pool</u> <u>device</u>

           Detaches <u>device</u> from a mirror. The operation is refused if there are no other valid replicas  of  the
           data.

       <b>zpool</b> <b>export</b> [<b>-f</b>] <u>pool</u> ...

           Exports the given pools from the system. All devices are marked as exported, but are still considered
           in  use  by  other  subsystems.  The  devices  can  be moved between systems (even those of different
           endianness) and imported as long as a sufficient number of devices are present.

           Before exporting the pool, all datasets within the pool are unmounted. A pool can not be exported  if
           it has a shared spare that is currently being used.

           For  pools  to be portable, you must give the <b>zpool</b> command whole disks, not just slices, so that <b>ZFS</b>
           can label the disks with portable <b>EFI</b> labels. Otherwise,  disk  drivers  on  platforms  of  different
           endianness will not recognize the disks.

           <b>-f</b>

               Forcefully unmount all datasets, using the "<b>unmount</b> <b>-f</b>" command.

               This  command  will  forcefully  export  the pool even if it has a shared spare that is currently
               being used. This may lead to potential data corruption.

       <b>zpool</b> <b>get</b> "<u>all</u>" | <u>property</u>[,...] <u>pool</u> ...

           Retrieves the given list of properties (or all properties if "<b>all</b>" is used) for the specified storage
           pool(s). These properties are displayed with the following fields:

                    name          Name of storage pool
                     property      Property name
                     value         Property value
                     source        Property source, either 'default' or 'local'.

           See the "Properties" section for more information on the available pool properties.

       <b>zpool</b> <b>history</b> [<b>-il</b>] [<u>pool</u>] ...

           Displays the command history of the specified pools or all pools if no pool is specified.

           <b>-i</b>

               Displays internally logged <b>ZFS</b> events in addition to user initiated events.

           <b>-l</b>

               Displays log records in long format, which in addition to  standard  format  includes,  the  user
               name, the hostname, and the zone in which the operation was performed.

       <b>zpool</b> <b>import</b> [<b>-d</b> <u>dir</u> | <b>-c</b> <u>cachefile</u>] [<b>-D</b>]

           Lists pools available to import. If the <b>-d</b> option is not specified, this command searches for devices
           in  "/dev/dsk".  The  <b>-d</b> option can be specified multiple times, and all directories are searched. If
           the device appears to be part of an exported pool, this command displays a summary of the  pool  with
           the  name  of  the  pool,  a numeric identifier, as well as the <u>vdev</u> layout and current health of the
           device for each device or file. Destroyed pools, pools that were previously destroyed with the "<b>zpool</b>
           <b>destroy</b>" command, are not listed unless the <b>-D</b> option is specified.

           The numeric identifier is unique, and can be used instead of the pool  name  when  multiple  exported
           pools of the same name are available.

           <b>-c</b> <u>cachefile</u>

               Reads configuration from the given <b>cachefile</b> that was created with the "<b>cachefile</b>" pool property.
               This <b>cachefile</b> is used instead of searching for devices.

           <b>-d</b> <u>dir</u>

               Searches for devices or files in <u>dir</u>. The <b>-d</b> option can be specified multiple times.

           <b>-D</b>

               Lists destroyed pools only.

       <b>zpool</b> <b>import</b> [<b>-o</b> <u>mntopts</u>] [ <b>-o</b> <u>property</u>=<u>value</u>] ... [<b>-d</b> <u>dir</u> | <b>-c</b> <u>cachefile</u>] [<b>-D</b>] [<b>-f</b>] [<b>-R</b> <u>root</u>] [<b>-F</b> [<b>-n</b>]]
       <b>-a</b>

           Imports all pools found in the search directories. Identical to the previous command, except that all
           pools  with  a  sufficient number of devices available are imported. Destroyed pools, pools that were
           previously destroyed with the "<b>zpool</b> <b>destroy</b>" command, will not be imported unless the <b>-D</b>  option  is
           specified.

           <b>-o</b> <u>mntopts</u>

               Comma-separated  list of mount options to use when mounting datasets within the pool. See <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M)
               for a description of dataset properties and mount options.

           <b>-o</b> <u>property=value</u>

               Sets the specified property  on  the  imported  pool.  See  the  "Properties"  section  for  more
               information on the available pool properties.

           <b>-c</b> <u>cachefile</u>

               Reads configuration from the given <b>cachefile</b> that was created with the "<b>cachefile</b>" pool property.
               This <b>cachefile</b> is used instead of searching for devices.

           <b>-d</b> <u>dir</u>

               Searches  for devices or files in <u>dir</u>. The <b>-d</b> option can be specified multiple times. This option
               is incompatible with the <b>-c</b> option.

           <b>-D</b>

               Imports destroyed pools only. The <b>-f</b> option is also required.

           <b>-f</b>

               Forces import, even if the pool appears to be potentially active.

           <b>-F</b>

               Recovery mode for a non-importable pool. Attempt to return the pool to  an  importable  state  by
               discarding  the  last  few  transactions.  Not  all  damaged pools can be recovered by using this
               option. If successful, the data from the  discarded  transactions  is  irretrievably  lost.  This
               option is ignored if the pool is importable or already imported.

           <b>-a</b>

               Searches for and imports all pools found.

           <b>-R</b> <u>root</u>

               Sets the "<b>cachefile</b>" property to "<b>none</b>" and the "<u>altroot</u>" property to "<u>root</u>".

           <b>-n</b>

               Used with the <b>-F</b> recovery option. Determines whether a non-importable pool can be made importable
               again,  but  does  not  actually  perform the pool recovery. For more details about pool recovery
               mode, see the <b>-F</b> option, above.

       <b>zpool</b> <b>import</b> [<b>-o</b> <u>mntopts</u>] [ <b>-o</b> <u>property</u>=<u>value</u>] ... [<b>-d</b> <u>dir</u> | <b>-c</b> <u>cachefile</u>] [<b>-D</b>] [<b>-f</b>] [<b>-R</b> <u>root</u>] [<b>-F</b> [<b>-n</b>]]
       <u>pool</u> | <u>id</u> [<u>newpool</u>]

           Imports a specific pool. A pool can be identified by its name or the numeric identifier.  If  <u>newpool</u>
           is  specified,  the  pool is imported using the name <u>newpool</u>. Otherwise, it is imported with the same
           name as its exported name.

           If a device is removed from a system without running "<b>zpool</b> <b>export</b>"  first,  the  device  appears  as
           potentially  active.  It  cannot  be determined if this was a failed export, or whether the device is
           really in use from another host. To import a pool in this state, the <b>-f</b> option is required.

           <b>-o</b> <u>mntopts</u>

               Comma-separated list of mount options to use when mounting datasets within the pool. See  <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M)
               for a description of dataset properties and mount options.

           <b>-o</b> <u>property=value</u>

               Sets  the  specified  property  on  the  imported  pool.  See  the  "Properties" section for more
               information on the available pool properties.

           <b>-c</b> <u>cachefile</u>

               Reads configuration from the given <b>cachefile</b> that was created with the "<b>cachefile</b>" pool property.
               This <b>cachefile</b> is used instead of searching for devices.

           <b>-d</b> <u>dir</u>

               Searches for devices or files in <u>dir</u>. The <b>-d</b> option can be specified multiple times. This  option
               is incompatible with the <b>-c</b> option.

           <b>-D</b>

               Imports destroyed pool. The <b>-f</b> option is also required.

           <b>-f</b>

               Forces import, even if the pool appears to be potentially active.

           <b>-F</b>

               Recovery  mode  for  a  non-importable pool. Attempt to return the pool to an importable state by
               discarding the last few transactions. Not all damaged  pools  can  be  recovered  by  using  this
               option.  If  successful,  the  data  from  the discarded transactions is irretrievably lost. This
               option is ignored if the pool is importable or already imported.

           <b>-R</b> <u>root</u>

               Sets the "<b>cachefile</b>" property to "<b>none</b>" and the "<u>altroot</u>" property to "<u>root</u>".

           <b>-n</b>

               Used with the <b>-F</b> recovery option. Determines whether a non-importable pool can be made importable
               again, but does not actually perform the pool recovery. For  more  details  about  pool  recovery
               mode, see the <b>-F</b> option, above.

       <b>zpool</b> <b>iostat</b> [<b>-T</b> <b>u</b> | <b>d</b>] [<b>-v</b>] [<u>pool</u>] ... [<u>interval</u>[<u>count</u>]]

           Displays <b>I/O</b> statistics for the given pools. When given an interval, the statistics are printed every
           <u>interval</u> seconds until <b>Ctrl-C</b> is pressed. If no <u>pools</u> are specified, statistics for every pool in the
           system is shown. If <u>count</u> is specified, the command exits after <u>count</u> reports are printed.

           <b>-T</b> <b>u</b> | <b>d</b>

               Display a time stamp.

               Specify  <b>u</b>  for  a  printed  representation  of the internal representation of time. See <b><a href="../man2/time.2.html">time</a></b>(2).
               Specify <b>d</b> for standard date format. See <b><a href="../man1/date.1.html">date</a></b>(1).

           <b>-v</b>

               Verbose statistics. Reports usage statistics for individual <u>vdevs</u> within the pool, in addition to
               the pool-wide statistics.

       <b>zpool</b> <b>list</b> [<b>-H</b>] [<b>-o</b> <u>props</u>[,...]] [<u>pool</u>] ...

           Lists the given pools along with a health status and space usage. When given no arguments, all  pools
           in the system are listed.

           <b>-H</b>

               Scripted  mode.  Do not display headers, and separate fields by a single tab instead of arbitrary
               space.

           <b>-o</b> <u>props</u>

               Comma-separated list of properties to display. See the "Properties" section for a list  of  valid
               properties. The default list is <b>name</b>, <b>size</b>, <b>allocated</b>, <b>free</b>, <b>capacity</b>, <b>health</b>, <b>altroot</b>.

       <b>zpool</b> <b>offline</b> [<b>-t</b>] <u>pool</u> <u>device</u> ...

           Takes  the specified physical device offline. While the <u>device</u> is offline, no attempt is made to read
           or write to the device.

           This command is not applicable to spares or cache devices.

           <b>-t</b>

               Temporary. Upon reboot, the specified physical device reverts to its previous state.

       <b>zpool</b> <b>online</b> [<b>-e</b>] <u>pool</u> <u>device</u>...

           Brings the specified physical device online.

           This command is not applicable to spares or cache devices.

           <b>-e</b>

               Expand the device to use all available space. If the device is part of a mirror or <b>raidz</b> then all
               devices must be expanded before the new space will become available to the pool.

       <b>zpool</b> <b>remove</b> <u>pool</u> <u>device</u> ...

           Removes the specified device from the pool. This command currently only supports removing hot spares,
           cache, and log devices. A mirrored log device can be removed by specifying the top-level  mirror  for
           the  log.  Non-log  devices  that are part of a mirrored configuration can be removed using the <b>zpool</b>
           <b>detach</b> command. Non-redundant and <b>raidz</b> devices cannot be removed from a pool.

       <b>zpool</b> <b>replace</b> [<b>-f</b>] <u>pool</u> <u>old_device</u> [<u>new_device</u>]

           Replaces <u>old_device</u> with <u>new_device</u>. This is equivalent to attaching <u>new_device</u>, waiting  for  it  to
           resilver, and then detaching <u>old_device</u>.

           The  size  of  <u>new_device</u>  must  be greater than or equal to the minimum size of all the devices in a
           mirror or <b>raidz</b> configuration.

           <u>new_device</u> is required if the pool is not redundant. If <u>new_device</u> is not specified, it  defaults  to
           <u>old_device</u>.  This  form  of  replacement  is  useful  after  an existing disk has failed and has been
           physically replaced. In this case, the new disk may have the same <b>/dev/dsk</b> path as  the  old  device,
           even though it is actually a different disk. <b>ZFS</b> recognizes this.

           <b>-f</b>

               Forces  use of <u>new_device</u>, even if its appears to be in use. Not all devices can be overridden in
               this manner.

       <b>zpool</b> <b>scrub</b> [<b>-s</b>] <u>pool</u> ...

           Begins a scrub. The scrub examines all data in the  specified  pools  to  verify  that  it  checksums
           correctly.  For replicated (mirror or <b>raidz</b>) devices, <b>ZFS</b> automatically repairs any damage discovered
           during the scrub. The "<b>zpool</b> <b>status</b>" command reports the progress of the  scrub  and  summarizes  the
           results of the scrub upon completion.

           Scrubbing  and  resilvering  are  very  similar  operations.  The difference is that resilvering only
           examines data that <b>ZFS</b> knows to be out of date (for example, when attaching a new device to a  mirror
           or  replacing  an existing device), whereas scrubbing examines all data to discover silent errors due
           to hardware faults or disk failure.

           Because scrubbing and resilvering are <b>I/O</b>-intensive operations, <b>ZFS</b> only allows one at a time.  If  a
           scrub  is  already  in progress, the "<b>zpool</b> <b>scrub</b>" command terminates it and starts a new scrub. If a
           resilver is in progress, <b>ZFS</b> does not allow a scrub to be started until the resilver completes.

           <b>-s</b>

               Stop scrubbing.

       <b>zpool</b> <b>set</b> <u>property</u>=<u>value</u> <u>pool</u>

           Sets the given property on the specified pool. See the "Properties" section for more  information  on
           what properties can be set and acceptable values.

       <b>zpool</b> <b>split</b> [<b>-R</b> <u>altroot</u>] [<b>-n</b>] [<b>-o</b> <u>mntopts</u>] [<b>-o</b> <u>property</u>=<u>value</u>] <u>pool</u> <u>newpool</u> [<u>device</u> ...]

           Splits  off  one  disk  from  each  mirrored top-level vdev in a pool and creates a new pool from the
           split-off disks. The original pool must be made up of one or more mirrors and  must  not  be  in  the
           process  of  resilvering.  The  <b>split</b>  subcommand  chooses the last device in each mirror vdev unless
           overridden by a device specification on the command line.

           When using a <u>device</u> argument, <b>split</b> includes the specified device(s) in a new pool  and,  should  any
           devices  remain  unspecified,  assigns  the  last device in each mirror vdev to that pool, as it does
           normally. If you are uncertain about the outcome of a <b>split</b> command, use the <b>-n</b> ("dry-run") option to
           ensure your command will have the effect you intend.

           <b>-R</b> <u>altroot</u>

               Automatically import the  newly  created  pool  after  splitting,  using  the  specified  <u>altroot</u>
               parameter  for  the  new  pool's  alternate root. See the <b>altroot</b> description in the "Properties"
               section, above.

           <b>-n</b>

               Displays the configuration that would be created without actually splitting the pool. The  actual
               pool split could still fail due to insufficient privileges or device status.

           <b>-o</b> <u>mntopts</u>

               Comma-separated  list of mount options to use when mounting datasets within the pool. See <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M)
               for a description of dataset properties and mount options. Valid only in conjunction with the  <b>-R</b>
               option.

           <b>-o</b> <u>property</u>=<u>value</u>

               Sets  the  specified  property  on  the  new  pool. See the "Properties" section, above, for more
               information on the available pool properties.

       <b>zpool</b> <b>status</b> [<b>-xv</b>] [<u>pool</u>] ...

           Displays the detailed health status for the given pools. If no <u>pool</u> is specified, then the status  of
           each pool in the system is displayed. For more information on pool and device health, see the "Device
           Failure and Recovery" section.

           If  a  scrub  or  resilver is in progress, this command reports the percentage done and the estimated
           time to completion. Both of these are only approximate, because the amount of data in  the  pool  and
           the other workloads on the system can change.

           <b>-x</b>

               Only display status for pools that are exhibiting errors or are otherwise unavailable.

           <b>-v</b>

               Displays  verbose  data  error information, printing out a complete list of all data errors since
               the last complete pool scrub.

       <b>zpool</b> <b>upgrade</b>

           Displays all pools formatted using a different <b>ZFS</b> on-disk version. Older versions can continue to be
           used, but some features may not be available. These pools can be upgraded using "<b>zpool</b>  <b>upgrade</b>  <b>-a</b>".
           Pools  that are formatted with a more recent version are also displayed, although these pools will be
           inaccessible on the system.

       <b>zpool</b> <b>upgrade</b> <b>-v</b>

           Displays <b>ZFS</b> versions supported by the current software. The current <b>ZFS</b> versions  and  all  previous
           supported  versions  are  displayed,  along  with  an  explanation of the features provided with each
           version.

       <b>zpool</b> <b>upgrade</b> [<b>-V</b> <u>version</u>] <b>-a</b> | <u>pool</u> ...

           Upgrades the given pool to the latest on-disk version. Once this is done, the pool will no longer  be
           accessible on systems running older versions of the software.

           <b>-a</b>

               Upgrades all pools.

           <b>-V</b> <u>version</u>

               Upgrade  to  the  specified version. If the <b>-V</b> flag is not specified, the pool is upgraded to the
               most recent version. This option can only be used to increase the version number, and only up  to
               the most recent version supported by this software.

</pre><h4><b>EXAMPLES</b></h4><pre>
       <b>Example</b> <b>1</b> Creating a RAID-Z Storage Pool

       The following command creates a pool with a single <b>raidz</b> root <u>vdev</u> that consists of six disks.

         # <b>zpool</b> <b>create</b> <b>tank</b> <b>raidz</b> <b>c0t0d0</b> <b>c0t1d0</b> <b>c0t2d0</b> <b>c0t3d0</b> <b>c0t4d0</b> <b>c0t5d0</b>

       <b>Example</b> <b>2</b> Creating a Mirrored Storage Pool

       The following command creates a pool with two mirrors, where each mirror contains two disks.

         # <b>zpool</b> <b>create</b> <b>tank</b> <b>mirror</b> <b>c0t0d0</b> <b>c0t1d0</b> <b>mirror</b> <b>c0t2d0</b> <b>c0t3d0</b>

       <b>Example</b> <b>3</b> Creating a ZFS Storage Pool by Using Slices

       The following command creates an unmirrored pool using two disk slices.

         # <b>zpool</b> <b>create</b> <b>tank</b> <b>/dev/dsk/c0t0d0s1</b> <b>c0t1d0s4</b>

       <b>Example</b> <b>4</b> Creating a ZFS Storage Pool by Using Files

       The  following  command  creates  an  unmirrored pool using files. While not recommended, a pool based on
       files can be useful for experimental purposes.

         # <b>zpool</b> <b>create</b> <b>tank</b> <b>/path/to/file/a</b> <b>/path/to/file/b</b>

       <b>Example</b> <b>5</b> Adding a Mirror to a ZFS Storage Pool

       The following command adds two mirrored disks to the pool "<u>tank</u>", assuming the pool is already made up of
       two-way mirrors. The additional space is immediately available to any datasets within the pool.

         # <b>zpool</b> <b>add</b> <b>tank</b> <b>mirror</b> <b>c1t0d0</b> <b>c1t1d0</b>

       <b>Example</b> <b>6</b> Listing Available ZFS Storage Pools

       The following command lists all available pools on the system.

         # <b>zpool</b> <b>list</b>
         NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
         pool    136G   109M   136G     0%  3.00x  ONLINE  -
         rpool  67.5G  12.6G  54.9G    18%  1.01x  ONLINE  -

       <b>Example</b> <b>7</b> Listing All Properties for a Pool

       The following command lists all the properties for a pool.

         % <b>zpool</b> <b>get</b> <b>all</b> <b>pool</b>
         NAME  PROPERTY       VALUE       SOURCE
         pool  size           136G        -
         pool  capacity       0%          -
         pool  altroot        -           default
         pool  health         ONLINE      -
         pool  guid           15697759092019394988  default
         pool  version        21          default
         pool  bootfs         -           default
         pool  delegation     on          default
         pool  autoreplace    off         default
         pool  cachefile      -           default
         pool  failmode       wait        default
         pool  listsnapshots  off         default
         pool  autoexpand     off         default
         pool  dedupratio     3.00x       -
         pool  free           136G        -
         pool  allocated      109M        -

       <b>Example</b> <b>8</b> Destroying a ZFS Storage Pool

       The following command destroys the pool "<u>tank</u>" and any datasets contained within.

         # <b>zpool</b> <b>destroy</b> <b>-f</b> <b>tank</b>

       <b>Example</b> <b>9</b> Exporting a ZFS Storage Pool

       The following command exports the devices in pool <u>tank</u> so that they can be relocated or later imported.

         # <b>zpool</b> <b>export</b> <b>tank</b>

       <b>Example</b> <b>10</b> Importing a ZFS Storage Pool

       The following command displays available pools, and then imports the pool "tank" for use on the system.

       The results from this command are similar to the following:

         # <b>zpool</b> <b>import</b>
           pool: tank
             id: 7678868315469843843
          state: ONLINE
         action: The pool can be imported using its name or numeric identifier.
         config:

                 tank        ONLINE
                   mirror-0  ONLINE
                     c1t2d0  ONLINE
                     c1t3d0  ONLINE

         # <b>zpool</b> <b>import</b> <b>tank</b>

       <b>Example</b> <b>11</b> Upgrading All ZFS Storage Pools to the Current Version

       The following command upgrades all ZFS Storage pools to the current version of the software.

         # <b>zpool</b> <b>upgrade</b> <b>-a</b>
         This system is currently running ZFS pool version 19.

         All pools are formatted using this version.

       <b>Example</b> <b>12</b> Managing Hot Spares

       The following command creates a new pool with an available hot spare:

         # <b>zpool</b> <b>create</b> <b>tank</b> <b>mirror</b> <b>c0t0d0</b> <b>c0t1d0</b> <b>spare</b> <b>c0t2d0</b>

       If one of the disks were to fail, the pool would be reduced to the degraded state. The failed device  can
       be replaced using the following command:

         # <b>zpool</b> <b>replace</b> <b>tank</b> <b>c0t0d0</b> <b>c0t3d0</b>

       Once  the  data  has  been  resilvered,  the  spare is automatically removed and is made available should
       another device fails. The hot spare can be permanently removed from the pool using the following command:

         # <b>zpool</b> <b>remove</b> <b>tank</b> <b>c0t2d0</b>

       <b>Example</b> <b>13</b> Creating a ZFS Pool with Mirrored Separate Intent Logs

       The following command creates a ZFS storage pool consisting of two,  two-way  mirrors  and  mirrored  log
       devices:

         # <b>zpool</b> <b>create</b> <b>pool</b> <b>mirror</b> <b>c0d0</b> <b>c1d0</b> <b>mirror</b> <b>c2d0</b> <b>c3d0</b> <b>log</b> <b>mirror</b> <b>\</b>
            <b>c4d0</b> <b>c5d0</b>

       <b>Example</b> <b>14</b> Adding Cache Devices to a ZFS Pool

       The following command adds two disks for use as cache devices to a ZFS storage pool:

         # <b>zpool</b> <b>add</b> <b>pool</b> <b>cache</b> <b>c2d0</b> <b>c3d0</b>

       Once added, the cache devices gradually fill with content from main memory. Depending on the size of your
       cache devices, it could take over an hour for them to fill. Capacity and reads can be monitored using the
       <b>iostat</b> option as follows:

         # <b>zpool</b> <b>iostat</b> <b>-v</b> <b>pool</b> <b>5</b>

       <b>Example</b> <b>15</b> Removing a Mirrored Log Device

       The following command removes the mirrored log device <b>mirror-2</b>.

       Given this configuration:

            pool: tank
           state: ONLINE
           scrub: none requested
         config:

                  NAME        STATE     READ WRITE CKSUM
                  tank        ONLINE       0     0     0
                    mirror-0  ONLINE       0     0     0
                      c6t0d0  ONLINE       0     0     0
                      c6t1d0  ONLINE       0     0     0
                    mirror-1  ONLINE       0     0     0
                      c6t2d0  ONLINE       0     0     0
                      c6t3d0  ONLINE       0     0     0
                  logs
                    mirror-2  ONLINE       0     0     0
                      c4t0d0  ONLINE       0     0     0
                      c4t1d0  ONLINE       0     0     0

       The command to remove the mirrored log <b>mirror-2</b> is:

         # <b>zpool</b> <b>remove</b> <b>tank</b> <b>mirror-2</b>

       <b>Example</b> <b>16</b> Recovering a Faulted ZFS Pool

       If  a pool is faulted but recoverable, a message indicating this state is provided by <b>zpool</b> <b>status</b> if the
       pool was cached (see <b>cachefile</b> above), or as part of the error output from a failed <b>zpool</b> <b>import</b>  of  the
       pool.

       Recover a cached pool with the <b>zpool</b> <b>clear</b> command:

         # <b>zpool</b> <b>clear</b> <b>-F</b> <b>data</b>
         Pool data returned to its state as of Tue Sep 08 13:23:35 2009.
         Discarded approximately 29 seconds of transactions.

       If the pool configuration was not cached, use <b>zpool</b> <b>import</b> with the recovery mode flag:

         # <b>zpool</b> <b>import</b> <b>-F</b> <b>data</b>
         Pool data returned to its state as of Tue Sep 08 13:23:35 2009.
         Discarded approximately 29 seconds of transactions.

</pre><h4><b>EXIT</b> <b>STATUS</b></h4><pre>
       The following exit values are returned:

       <b>0</b>

           Successful completion.

       <b>1</b>

           An error occurred.

       <b>2</b>

           Invalid command line options were specified.

</pre><h4><b>ATTRIBUTES</b></h4><pre>
       See <b><a href="../man5/attributes.5.html">attributes</a></b>(5) for descriptions of the following attributes:

       ┌──────────────────────────────┬─────────────────────────────┐
       │       ATTRIBUTE TYPE         │      ATTRIBUTE VALUE        │
       ├──────────────────────────────┼─────────────────────────────┤
       │ Availability                 │SUNWzfsu                     │
       ├──────────────────────────────┼─────────────────────────────┤
       │ Interface Stability          │Committed                    │
       └──────────────────────────────┴─────────────────────────────┘

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man1M/zfs.1M.html">zfs</a></b>(1M), <b><a href="../man5/attributes.5.html">attributes</a></b>(5)

SunOS 5.11                                         4 Jan 2010                                           <u><a href="../man8/zpool.8.html">zpool</a></u>(8)
</pre>
 </div>
</div></section>
</div>
</body>
</html>