<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>fence_virt.conf - configuration file for fence_virtd</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/fence-virtd">fence-virtd_4.16.0-3ubuntu1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       fence_virt.conf - configuration file for fence_virtd

</pre><h4><b>DESCRIPTION</b></h4><pre>
       The  fence_virt.conf  file  contains configuration information for fence_virtd, a fencing request routing
       daemon for clusters of virtual machines.

       The file is tree-structured.  There are parent/child relationships and sibling relationships between  the
       nodes.

         foo {
           bar {
             baz = "1";
           }
         }

       There are three primary sections of fence_virt.conf.

</pre><h4><b>SECTIONS</b></h4><pre>
   <b>fence_virtd</b>
       This  section contains global information about how fence_virtd is to operate.  The most important pieces
       of information are as follows:

       <b>listener</b>
              the listener plugin for receiving fencing requests from clients

       <b>backend</b>
              the plugin to be used to carry out fencing requests

       <b>foreground</b>
              do not fork into the background.

       <b>wait_for_init</b>
              wait for the frontend and backends to become available rather than giving  up  immediately.   This
              replaces wait_for_backend in 0.2.x.

       <b>module_path</b>
              the module path to search for plugins

   <b>listeners</b>
       This section contains listener-specific configuration information; see the section about listeners below.

   <b>backends</b>
       This section contains listener-specific configuration information; see the section about listeners below.

   <b>groups</b>
       This  section  contains static maps of which virtual machines may fence which other virtual machines; see
       the section about groups below.

</pre><h4><b>LISTENERS</b></h4><pre>
       There are various listeners available for fence_virtd, each one handles decoding and authentication of  a
       given  fencing  request.   The  following  configuration  blocks  belong  in  the  <b>listeners</b>  section  of
       fence_virt.conf

   <b>multicast</b>
       <b>key_file</b>
              the shared key file to use (default: /etc/cluster/fence_xvm.key).

       <b>hash</b>   the weakest hashing algorithm allowed for client requests.  Clients may send packets with stronger
              hashes than the one specified, but not weaker ones.  (default: sha256, but could be sha1,  sha512,
              or none)

       <b>auth</b>   the  hashing  algorithm  to  use  for  the  simplistic challenge-response authentication (default:
              sha256, but could be sha1, sha512, or none)

       <b>family</b> the IP family to use (default: ipv4, but may be ipv6)

       <b>address</b>
              the multicast address to listen on (default: 225.0.0.12)

       <b>port</b>   the multicast port to listen on (default: 1229)

       <b>interface</b>
              interface to listen on.  By default, fence_virtd listens on all interfaces.  However, this  causes
              problems in some environments where the host computer is used as a gateway.

   <b>serial</b>
       The serial listener plugin utilizes libvirt's serial (or VMChannel) mapping to listen for requests.  When
       using  the serial listener, it is necessary to add a serial port (preferably pointing to /dev/ttyS1) or a
       channel (preferably pointing to 10.0.2.179:1229) to the libvirt domain description.  Note that only  type
       <b>unix</b>  ,  mode  <b>bind</b>  serial  ports  and  channels are supported and each VM should have a separate unique
       socket.  Example libvirt XML:

          &lt;<b>serial</b> type='unix'&gt;
            &lt;source mode='bind' path='/sandbox/guests/fence_socket_molly'/&gt;
            &lt;target port='1'/&gt;
          &lt;/serial&gt;
          &lt;<b>channel</b> type='unix'&gt;
            &lt;source mode='bind' path='/sandbox/guests/fence_molly_vmchannel'/&gt;
            &lt;target type='guestfwd' address='10.0.2.179' port='1229'/&gt;
          &lt;/channel&gt;

       <b>uri</b>    the URI to use when connecting to libvirt by the serial plugin (optional).

       <b>path</b>   The same directory that  is  defined  for  the  domain  serial  port  path  (From  example  above:
              /sandbox/guests).  Sockets must reside in this directory in order to be considered valid. This can
              be used to prevent fence_virtd from using the wrong sockets.

       <b>mode</b>   This selects  the  type  of  sockets  to  register.   Valid  values  are  "serial"  (default)  and
              "vmchannel".

   <b>tcp</b>
       The  tcp  listener  operates  similarly  to the multicast listener but uses TCP sockets for communication
       instead of using multicast packets.

       <b>key_file</b>
              the shared key file to use (default: /etc/cluster/fence_xvm.key).

       <b>hash</b>   the hashing algorithm to use for packet signing (default: sha256, but could be  sha1,  sha512,  or
              none)

       <b>auth</b>   the  hashing  algorithm  to  use  for  the  simplistic challenge-response authentication (default:
              sha256, but could be sha1, sha512, or none)

       <b>family</b> the IP family to use (default: ipv4, but may be ipv6)

       <b>address</b>
              the IP address to listen on (default: 127.0.0.1 for IPv4, ::1 for IPv6)

       <b>port</b>   the TCP port to listen on (default: 1229)

   <b>vsock</b>
       The vsock listener operates similarly  to  the  multicast  listener  but  uses  virtual  machine  sockets
       (AF_VSOCK) for communication instead of using multicast packets.

       <b>key_file</b>
              the shared key file to use (default: /etc/cluster/fence_xvm.key).

       <b>hash</b>   the  hashing  algorithm  to use for packet signing (default: sha256, but could be sha1, sha512, or
              none)

       <b>auth</b>   the hashing algorithm to  use  for  the  simplistic  challenge-response  authentication  (default:
              sha256, but could be sha1, sha512, or none)

       <b>port</b>   the vsock port to listen on (default: 1229)

</pre><h4><b>BACKENDS</b></h4><pre>
       There  are  various  backends  available for fence_virtd, each one handles routing a fencing request to a
       hypervisor or management tool.  The following configuration blocks belong  in  the  <b>backends</b>  section  of
       fence_virt.conf

   <b>libvirt</b>
       The  libvirt  plugin  is  the simplest plugin.  It is used in environments where routing fencing requests
       between multiple hosts is not required, for example by a user running a cluster of virtual machines on  a
       single desktop computer.

       <b>uri</b>    the URI to use when connecting to libvirt.

              All libvirt URIs are accepted and passed as-is.

              See https://libvirt.org/uri.html#remote-uris for examples.

              NOTE: When VMs are run as non-root user the socket path must be set as part of the URI.

              Example: qemu:///session?socket=<a href="file:/run/user/">/run/user/</a>&lt;UID&gt;/libvirt/virtqemud-sock

   <b>cpg</b>
       The  cpg plugin uses corosync CPG and libvirt to track virtual machines and route fencing requests to the
       appropriate computer.

       <b>uri</b>    the URI to use when connecting to libvirt by the cpg plugin.

       <b>name_mode</b>
              The cpg plugin, in order to retain compatibility with fence_xvm,  stores  virtual  machines  in  a
              certain  way.   The  default was to use 'name' when using fence_xvm and fence_xvmd, and so this is
              still the default.  However, it is strongly recommended to use 'uuid' instead  of  'name'  in  all
              cluster  environments  involving  more  than one physical host in order to avoid the potential for
              name collisions.

</pre><h4><b>GROUPS</b></h4><pre>
       Fence_virtd supports static maps which allow grouping of VMs.  The groups are arbitrary and  are  checked
       at  fence  time.   Any  member  of a group may fence any other member.  Hosts may be assigned to multiple
       groups if desired.

   <b>group</b>
       This defines a group.

       <b>name</b>   Optionally define the name of the group. Useful only for configuration readability  and  debugging
              of configuration parsing.

       <b>uuid</b>   Defines  UUID as a member of a group.  It can be used multiple times to specify both node name and
              UUID values that can be fenced.  When using the serial listener, the vm uuid is required and it is
              recommended to add also the vm name.

       <b>ip</b>     Defines an IP which is allowed to send fencing requests  for  members  of  this  group  (e.g.  for
              multicast).  It can be used multiple times to allow more than 1 IP to send fencing requests to the
              group.  It is highly recommended that this be used in conjunction with a key file.  When using the
              vsock  listener,  ip  should  contain the CID value assigned by libvirt to the vm.  When using the
              serial listener, ip value is not used and can be omitted.

</pre><h4><b>EXAMPLE</b></h4><pre>
        fence_virtd {
         listener = "multicast";
         backend = "cpg";
        }

        # this is the listeners section

        listeners {
         multicast {
          key_file = "/etc/cluster/fence_xvm.key";
         }
        }

        backends {
         libvirt {
          uri = "qemu:///system";
         }
        }

        groups {
         group {
          name = "cluster1";
          ip = "192.168.1.1";
          ip = "192.168.1.2";
          uuid = "44179d3f-6c63-474f-a212-20c8b4b25b16";
          uuid = "1ce02c4b-dfa1-42cb-b5b1-f0b1091ece60";
          uuid = "node1";
          uuid = "node2";
         }
        }

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <a href="../man8/fence_virtd.8.html">fence_virtd</a>(8), <a href="../man8/fence_virt.8.html">fence_virt</a>(8), <a href="../man8/fence_xvm.8.html">fence_xvm</a>(8), <a href="../man8/fence.8.html">fence</a>(8)

                                                                                              <u><a href="../man5/fence_virt.conf.5.html">fence_virt.conf</a></u>(5)
</pre>
 </div>
</div></section>
</div>
</body>
</html>