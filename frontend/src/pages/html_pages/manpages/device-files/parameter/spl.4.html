<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>spl — parameters of the SPL kernel module</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/zfsutils-linux">zfsutils-linux_2.3.1-1ubuntu2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       spl — parameters of the SPL kernel module

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>spl_kmem_cache_kmem_threads</b>=<b>4</b> (uint)
               The  number of threads created for the spl_kmem_cache task queue.  This task queue is responsible
               for allocating new slabs for use by the kmem caches.  For the majority of systems  and  workloads
               only a small number of threads are required.

       <b>spl_kmem_cache_obj_per_slab</b>=<b>8</b> (uint)
               The  preferred number of objects per slab in the cache.  In general, a larger value will increase
               the caches memory footprint  while  decreasing  the  time  required  to  perform  an  allocation.
               Conversely,  a  smaller  value  will  minimize  the  footprint and improve cache reclaim time but
               individual allocations may take longer.

       <b>spl_kmem_cache_max_size</b>=<b>32</b> (64-bit) or <b>4</b> (32-bit) (uint)
               The maximum size of a kmem cache slab in MiB.  This effectively limits the maximum  cache  object
               size to <b>spl_kmem_cache_max_size</b>/<b>spl_kmem_cache_obj_per_slab</b>.

               Caches may not be created with object sized larger than this limit.

       <b>spl_kmem_cache_slab_limit</b>=<b>16384</b> (uint)
               For  small  objects the Linux slab allocator should be used to make the most efficient use of the
               memory.  However, large objects are not supported  by  the  Linux  slab  and  therefore  the  SPL
               implementation  is  preferred.   This  value  is used to determine the cutoff between a small and
               large object.

               Objects of size <b>spl_kmem_cache_slab_limit</b> or smaller will  be  allocated  using  the  Linux  slab
               allocator, large objects use the SPL allocator.  A cutoff of 16K was determined to be optimal for
               architectures using 4K pages.

       <b>spl_kmem_alloc_warn</b>=<b>32768</b> (uint)
               As  a  general  rule <b>kmem_alloc</b>() allocations should be small, preferably just a few pages, since
               they must by physically contiguous.  Therefore, a rate limited warning will  be  printed  to  the
               console for any <b>kmem_alloc</b>() which exceeds a reasonable threshold.

               The  default  warning  threshold  is  set to eight pages but capped at 32K to accommodate systems
               using large pages.  This value was selected to be small enough to ensure the largest  allocations
               are  quickly noticed and fixed.  But large enough to avoid logging any warnings when a allocation
               size is larger than optimal but not a serious concern.  Since this value is  tunable,  developers
               are  encouraged  to  set it lower when testing so any new largish allocations are quickly caught.
               These warnings may be disabled by setting the threshold to zero.

       <b>spl_kmem_alloc_max</b>=<b>KMALLOC_MAX_SIZE</b>/<b>4</b> (uint)
               Large <b>kmem_alloc</b>() allocations will fail if they exceed <b>KMALLOC_MAX_SIZE</b>.  Allocations which  are
               marginally  smaller than this limit may succeed but should still be avoided due to the expense of
               locating a contiguous range of free pages.  Therefore, a maximum kmem size with reasonable safely
               margin of 4x is set.  <b>kmem_alloc</b>() allocations  larger  than  this  maximum  will  quickly  fail.
               <b>vmem_alloc</b>()  allocations  less  than  or  equal  to  this value will use <b>kmalloc</b>(), but shift to
               <b>vmalloc</b>() when exceeding this value.

       <b>spl_kmem_cache_magazine_size</b>=<b>0</b> (uint)
               Cache magazines are an optimization designed to minimize the cost of allocating memory.  They  do
               this  by keeping a per-cpu cache of recently freed objects, which can then be reallocated without
               taking a lock.  This can improve  performance  on  highly  contended  caches.   However,  because
               objects  in magazines will prevent otherwise empty slabs from being immediately released this may
               not be ideal for low memory machines.

               For this reason, <b>spl_kmem_cache_magazine_size</b> can be used to set a maximum magazine  size.   When
               this  value  is  set  to 0 the magazine size will be automatically determined based on the object
               size.  Otherwise magazines will  be  limited  to  2-256  objects  per  magazine  (i.e  per  cpu).
               Magazines may never be entirely disabled in this implementation.

       <b>spl_hostid</b>=<b>0</b> (ulong)
               The  system  hostid,  when  set  this can be used to uniquely identify a system.  By default this
               value is set to zero which indicates the hostid is disabled.  It can  be  explicitly  enabled  by
               placing a unique non-zero value in <u>/etc/hostid</u>.

       <b>spl_hostid_path</b>=<u>/etc/hostid</u> (charp)
               The  expected  path to locate the system hostid when specified.  This value may be overridden for
               non-standard configurations.

       <b>spl_panic_halt</b>=<b>0</b> (uint)
               Cause a kernel panic on assertion failures.  When not enabled, the thread is halted to facilitate
               further debugging.

               Set to a non-zero value to enable.

       <b>spl_taskq_kick</b>=<b>0</b> (uint)
               Kick stuck taskq to spawn threads.  When writing a non-zero value to it, it  will  scan  all  the
               taskqs.   If  any  of  them have a pending task more than 5 seconds old, it will kick it to spawn
               more threads.  This can be used if you find a rare deadlock occurs because  one  or  more  taskqs
               didn't spawn a thread when it should.

       <b>spl_taskq_thread_bind</b>=<b>0</b> (int)
               Bind  taskq  threads to specific CPUs.  When enabled all taskq threads will be distributed evenly
               across the available CPUs.  By default, this behavior is disabled to allow  the  Linux  scheduler
               the maximum flexibility to determine where a thread should run.

       <b>spl_taskq_thread_dynamic</b>=<b>1</b> (int)
               Allow  dynamic  taskqs.   When  enabled  taskqs  which set the <b>TASKQ_DYNAMIC</b> flag will by default
               create only a single thread.  New threads will be created on  demand  up  to  a  maximum  allowed
               number  to  facilitate  the  completion of outstanding tasks.  Threads which are no longer needed
               will be promptly destroyed.  By default this behavior is enabled but it can be  disabled  to  aid
               performance analysis or troubleshooting.

       <b>spl_taskq_thread_priority</b>=<b>1</b> (int)
               Allow  newly  created  taskq  threads to set a non-default scheduler priority.  When enabled, the
               priority specified when a taskq is created will be applied to all threads created by that  taskq.
               When  disabled  all  threads will use the default Linux kernel thread priority.  By default, this
               behavior is enabled.

       <b>spl_taskq_thread_sequential</b>=<b>4</b> (int)
               The number of items a taskq worker thread must handle without interruption  before  requesting  a
               new  worker  thread be spawned.  This is used to control how quickly taskqs ramp up the number of
               threads processing the queue.  Because Linux  thread  creation  and  destruction  are  relatively
               inexpensive  a  small  default value has been selected.  This means that normally threads will be
               created aggressively which is desirable.  Increasing this value will result in  a  slower  thread
               creation rate which may be preferable for some configurations.

       <b>spl_taskq_thread_timeout_ms</b>=<b>5000</b> (uint)
               Minimum  idle  threads  exit interval for dynamic taskqs.  Smaller values allow idle threads exit
               more often and potentially be respawned again on demand, causing more churn.

OpenZFS                                          August 24, 2020                                          <u><a href="../man4/SPL.4.html">SPL</a></u>(4)
</pre>
 </div>
</div></section>
</div>
</body>
</html>