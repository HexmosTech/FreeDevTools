<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>zfs — tuning of the ZFS kernel module</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/zfsutils-linux">zfsutils-linux_2.3.1-1ubuntu2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       zfs — tuning of the ZFS kernel module

</pre><h4><b>DESCRIPTION</b></h4><pre>
       The ZFS module supports these parameters:

       <b>dbuf_cache_max_bytes</b>=<b>UINT64_MAX</b>B (u64)
               Maximum  size  in  bytes  of  the  dbuf  cache.   The target size is determined by the MIN versus
               1/2^<b>dbuf_cache_shift</b> (1/32nd) of the target ARC size.  The behavior of the  dbuf  cache  and  its
               associated settings can be observed via the <u>/proc/spl/kstat/zfs/dbufstats</u> kstat.

       <b>dbuf_metadata_cache_max_bytes</b>=<b>UINT64_MAX</b>B (u64)
               Maximum  size  in  bytes  of  the  metadata dbuf cache.  The target size is determined by the MIN
               versus 1/2^<b>dbuf_metadata_cache_shift</b> (1/64th) of the  target  ARC  size.   The  behavior  of  the
               metadata    dbuf    cache    and    its   associated   settings   can   be   observed   via   the
               <u>/proc/spl/kstat/zfs/dbufstats</u> kstat.

       <b>dbuf_cache_hiwater_pct</b>=<b>10</b>% (uint)
               The percentage over <b>dbuf_cache_max_bytes</b> when dbufs must be evicted directly.

       <b>dbuf_cache_lowater_pct</b>=<b>10</b>% (uint)
               The percentage below <b>dbuf_cache_max_bytes</b> when the evict thread stops evicting dbufs.

       <b>dbuf_cache_shift</b>=<b>5</b> (uint)
               Set the size of the dbuf cache (<b>dbuf_cache_max_bytes</b>) to a log2 fraction of the target ARC size.

       <b>dbuf_metadata_cache_shift</b>=<b>6</b> (uint)
               Set the size of the dbuf metadata cache (<b>dbuf_metadata_cache_max_bytes</b>) to a log2 fraction of the
               target ARC size.

       <b>dbuf_mutex_cache_shift</b>=<b>0</b> (uint)
               Set the size of the mutex array for the dbuf cache.  When set to <b>0</b> the array is dynamically sized
               based on total system memory.

       <b>dmu_object_alloc_chunk_shift</b>=<b>7</b> (128) (uint)
               dnode slots allocated in a single operation as a power of 2.  The default  value  minimizes  lock
               contention for the bulk operation performed.

       <b>dmu_ddt_copies</b>=<b>3</b> (uint)
               Controls  the  number  of  copies  stored  for DeDup Table (DDT) objects.  Reducing the number of
               copies to  1  from  the  previous  default  of  3  can  reduce  the  write  inflation  caused  by
               deduplication.   This assumes redundancy for this data is provided by the vdev layer.  If the DDT
               is damaged, space may be leaked (not freed) when the DDT can not  report  the  correct  reference
               count.

       <b>dmu_prefetch_max</b>=<b>134217728</b>B (128 MiB) (uint)
               Limit  the amount we can prefetch with one call to this amount in bytes.  This helps to limit the
               amount of memory that can be used by prefetching.

       <b>ignore_hole_birth</b> (int)
               Alias for <b>send_holes_without_birth_time</b>.

       <b>l2arc_feed_again</b>=<b>1</b>|0 (int)
               Turbo L2ARC warm-up.  When the L2ARC is cold the fill interval will be set as fast as possible.

       <b>l2arc_feed_min_ms</b>=<b>200</b> (u64)
               Min feed interval in milliseconds.  Requires <b>l2arc_feed_again</b>=<u>1</u> and only  applicable  in  related
               situations.

       <b>l2arc_feed_secs</b>=<b>1</b> (u64)
               Seconds between L2ARC writing.

       <b>l2arc_headroom</b>=<b>8</b> (u64)
               How far through the ARC lists to search for L2ARC cacheable content, expressed as a multiplier of
               <b>l2arc_write_max</b>.  ARC persistence across reboots can be achieved with persistent L2ARC by setting
               this parameter to <b>0</b>, allowing the full length of ARC lists to be searched for cacheable content.

       <b>l2arc_headroom_boost</b>=<b>200</b>% (u64)
               Scales  <b>l2arc_headroom</b>  by  this percentage when L2ARC contents are being successfully compressed
               before writing.  A value of <b>100</b> disables this feature.

       <b>l2arc_exclude_special</b>=<b>0</b>|1 (int)
               Controls whether buffers present on special vdevs are eligible for caching into L2ARC.  If set to
               1, exclude dbufs on special vdevs from being cached to L2ARC.

       <b>l2arc_mfuonly</b>=<b>0</b>|1|2 (int)
               Controls whether only MFU metadata and data are cached from ARC into L2ARC.  This may be  desired
               to  avoid wasting space on L2ARC when reading/writing large amounts of data that are not expected
               to be accessed more than once.

               The default is 0, meaning both MRU and MFU data and metadata are cached.  When turning  off  this
               feature (setting it to 0), some MRU buffers will still be present in ARC and eventually cached on
               L2ARC.   If  <b>l2arc_noprefetch</b>=<b>0</b>, some prefetched buffers will be cached to L2ARC, and those might
               later transition to MRU, in which case the <b>l2arc_mru_asize</b> arcstat will not be <b>0</b>.

               Setting it to 1 means to L2 cache only MFU data and metadata.

               Setting it to 2 means to L2 cache all metadata (MRU+MFU) but only MFU data (ie: MRU data are  not
               cached).  This  can  be  the right setting to cache as much metadata as possible even when having
               high data turnover.

               Regardless of <b>l2arc_noprefetch</b>, some MFU buffers might be evicted from ARC, accessed later on  as
               prefetches  and  transition  to MRU as prefetches.  If accessed again they are counted as MRU and
               the <b>l2arc_mru_asize</b> arcstat will not be <b>0</b>.

               The ARC status of L2ARC buffers when they  were  first  cached  in  L2ARC  can  be  seen  in  the
               <b>l2arc_mru_asize</b>,  <b>l2arc_mfu_asize</b>,  and  <b>l2arc_prefetch_asize</b> arcstats when importing the pool or
               onlining a cache device if persistent L2ARC is enabled.

               The <b>evict_l2_eligible_mru</b> arcstat does not take into account if this option  is  enabled  as  the
               information  provided  by the <b>evict_l2_eligible_m[rf]u</b> arcstats can be used to decide if toggling
               this option is appropriate for the current workload.

       <b>l2arc_meta_percent</b>=<b>33</b>% (uint)
               Percent of ARC size allowed for L2ARC-only headers.  Since  L2ARC  buffers  are  not  evicted  on
               memory pressure, too many headers on a system with an irrationally large L2ARC can render it slow
               or unusable.  This parameter limits L2ARC writes and rebuilds to achieve the target.

       <b>l2arc_trim_ahead</b>=<b>0</b>% (u64)
               Trims  ahead  of  the current write size (<b>l2arc_write_max</b>) on L2ARC devices by this percentage of
               write size if we have filled the device.  If set to <b>100</b> we  TRIM  twice  the  space  required  to
               accommodate  upcoming  writes.  A minimum of <b>64</b> <b>MiB</b> will be trimmed.  It also enables TRIM of the
               whole L2ARC device upon creation or addition to an existing pool or if the header of  the  device
               is invalid upon importing a pool or onlining a cache device.  A value of <b>0</b> disables TRIM on L2ARC
               altogether and is the default as it can put significant stress on the underlying storage devices.
               This will vary depending of how well the specific device handles these commands.

       <b>l2arc_noprefetch</b>=<b>1</b>|0 (int)
               Do  not  write  buffers  to  L2ARC if they were prefetched but not used by applications.  In case
               there are prefetched buffers in L2ARC and this option is later set, we do not read the prefetched
               buffers from L2ARC.  Unsetting this option is useful for caching sequential reads from the  disks
               to  L2ARC  and  serve  those reads from L2ARC later on.  This may be beneficial in case the L2ARC
               device is significantly faster in sequential reads than the disks of the pool.

               Use <b>1</b> to disable and <b>0</b> to enable caching/reading prefetches to/from L2ARC.

       <b>l2arc_norw</b>=<b>0</b>|1 (int)
               No reads during writes.

       <b>l2arc_write_boost</b>=<b>33554432</b>B (32 MiB) (u64)
               Cold L2ARC devices will have <b>l2arc_write_max</b> increased by this amount while they remain cold.

       <b>l2arc_write_max</b>=<b>33554432</b>B (32 MiB) (u64)
               Max write bytes per interval.

       <b>l2arc_rebuild_enabled</b>=<b>1</b>|0 (int)
               Rebuild the L2ARC when importing a pool (persistent L2ARC).  This can be disabled  if  there  are
               problems  importing a pool or attaching an L2ARC device (e.g. the L2ARC device is slow in reading
               stored log metadata, or the metadata has become somehow fragmented/unusable).

       <b>l2arc_rebuild_blocks_min_l2size</b>=<b>1073741824</b>B (1 GiB) (u64)
               Mininum size of an L2ARC device required in order to write log blocks in it.  The log blocks  are
               used upon importing the pool to rebuild the persistent L2ARC.

               For  L2ARC  devices  less  than  1  GiB,  the  amount of data <b>l2arc_evict</b>() evicts is significant
               compared to the amount of restored L2ARC data.  In this case, do not write log blocks in L2ARC in
               order not to waste space.

       <b>metaslab_aliquot</b>=<b>1048576</b>B (1 MiB) (u64)
               Metaslab granularity, in bytes.  This is roughly similar to what would  be  referred  to  as  the
               "stripe size" in traditional RAID arrays.  In normal operation, ZFS will try to write this amount
               of data to each disk before moving on to the next top-level vdev.

       <b>metaslab_bias_enabled</b>=<b>1</b>|0 (int)
               Enable  metaslab  group  biasing based on their vdevs' over- or under-utilization relative to the
               pool.

       <b>metaslab_force_ganging</b>=<b>16777217</b>B (16 MiB + 1 B) (u64)
               Make some blocks above a certain size be gang blocks.  This option is used by the test  suite  to
               facilitate testing.

       <b>metaslab_force_ganging_pct</b>=<b>3</b>% (uint)
               For  blocks  that  could be forced to be a gang block (due to <b>metaslab_force_ganging</b>), force this
               many of them to be gang blocks.

       <b>brt_zap_prefetch</b>=<b>1</b>|0 (int)
               Controls prefetching BRT records for blocks which are going to be cloned.

       <b>brt_zap_default_bs</b>=<b>12</b> (4 KiB) (int)
               Default BRT ZAP data block size as a power of 2. Note that changing this after creating a BRT  on
               the pool will not affect existing BRTs, only newly created ones.

       <b>brt_zap_default_ibs</b>=<b>12</b> (4 KiB) (int)
               Default BRT ZAP indirect block size as a power of 2. Note that changing this after creating a BRT
               on the pool will not affect existing BRTs, only newly created ones.

       <b>ddt_zap_default_bs</b>=<b>15</b> (32 KiB) (int)
               Default  DDT ZAP data block size as a power of 2. Note that changing this after creating a DDT on
               the pool will not affect existing DDTs, only newly created ones.

       <b>ddt_zap_default_ibs</b>=<b>15</b> (32 KiB) (int)
               Default DDT ZAP indirect block size as a power of 2. Note that changing this after creating a DDT
               on the pool will not affect existing DDTs, only newly created ones.

       <b>zfs_default_bs</b>=<b>9</b> (512 B) (int)
               Default dnode block size as a power of 2.

       <b>zfs_default_ibs</b>=<b>17</b> (128 KiB) (int)
               Default dnode indirect block size as a power of 2.

       <b>zfs_dio_enabled</b>=<b>0</b>|1 (int)
               Enable Direct I/O.  If this setting is 0, then all I/O requests will be directed through the  ARC
               acting as though the dataset property <b>direct</b> was set to <b>disabled</b>.

       <b>zfs_history_output_max</b>=<b>1048576</b>B (1 MiB) (u64)
               When  attempting  to log an output nvlist of an ioctl in the on-disk history, the output will not
               be stored if it is larger than this size (in bytes).  This must be less than  <b>DMU_MAX_ACCESS</b>  (64
               MiB).  This applies primarily to <b>zfs_ioc_channel_program</b>() (cf. <u><a href="../man8/zfs-program.8.html">zfs-program</a></u>(8)).

       <b>zfs_keep_log_spacemaps_at_export</b>=<b>0</b>|1 (int)
               Prevent log spacemaps from being destroyed during pool exports and destroys.

       <b>zfs_metaslab_segment_weight_enabled</b>=<b>1</b>|0 (int)
               Enable/disable segment-based metaslab selection.

       <b>zfs_metaslab_switch_threshold</b>=<b>2</b> (int)
               When  using  segment-based metaslab selection, continue allocating from the active metaslab until
               this option's worth of buckets have been exhausted.

       <b>metaslab_debug_load</b>=<b>0</b>|1 (int)
               Load all metaslabs during pool import.

       <b>metaslab_debug_unload</b>=<b>0</b>|1 (int)
               Prevent metaslabs from being unloaded.

       <b>metaslab_fragmentation_factor_enabled</b>=<b>1</b>|0 (int)
               Enable use of the fragmentation metric in computing metaslab weights.

       <b>metaslab_df_max_search</b>=<b>16777216</b>B (16 MiB) (uint)
               Maximum distance to search forward from the last offset.  Without this  limit,  fragmented  pools
               can  see  <u>&gt;100`000</u> iterations and <b>metaslab_block_picker</b>() becomes the performance limiting factor
               on high-performance storage.

               With the default setting of <b>16</b> <b>MiB</b>, we typically see less than <u>500</u>  iterations,  even  with  very
               fragmented <b>ashift</b>=<b>9</b> pools.  The maximum number of iterations possible is <b>metaslab_df_max_search</b> <b>/</b>
               <b>2^(ashift+1)</b>.  With the default setting of <b>16</b> <b>MiB</b> this is <u>16*1024</u> (with <b>ashift</b>=<b>9</b>) or <u>2*1024</u> (with
               <b>ashift</b>=<b>12</b>).

       <b>metaslab_df_use_largest_segment</b>=<b>0</b>|1 (int)
               If   not   searching   forward   (due   to   <b>metaslab_df_max_search</b>,   <b>metaslab_df_free_pct</b>,   or
               <b>metaslab_df_alloc_threshold</b>), this tunable controls which segment is used.  If set, we  will  use
               the largest free segment.  If unset, we will use a segment of at least the requested size.

       <b>zfs_metaslab_max_size_cache_sec</b>=<b>3600</b>s (1 hour) (u64)
               When  we unload a metaslab, we cache the size of the largest free chunk.  We use that cached size
               to determine whether or not to load a metaslab for a given allocation.  As more frees  accumulate
               in  that metaslab while it's unloaded, the cached max size becomes less and less accurate.  After
               a number of seconds controlled by this tunable, we stop considering the cached max size and start
               considering only the histogram instead.

       <b>zfs_metaslab_mem_limit</b>=<b>25</b>% (uint)
               When we are loading a new metaslab, we check the amount of memory being used  to  store  metaslab
               range trees.  If it is over a threshold, we attempt to unload the least recently used metaslab to
               prevent  the  system  from  clogging  all  of its memory with range trees.  This tunable sets the
               percentage of total system memory that is the threshold.

       <b>zfs_metaslab_try_hard_before_gang</b>=<b>0</b>|1 (int)
               If unset, we will first try normal allocation.
               If that fails then we will do a gang allocation.
               If that fails then we will do a "try hard" gang allocation.
               If that fails then we will have a multi-layer gang block.

               If set, we will first try normal allocation.
               If that fails then we will do a "try hard" allocation.
               If that fails we will do a gang allocation.
               If that fails we will do a "try hard" gang allocation.
               If that fails then we will have a multi-layer gang block.

       <b>zfs_metaslab_find_max_tries</b>=<b>100</b> (uint)
               When not trying hard, we only  consider  this  number  of  the  best  metaslabs.   This  improves
               performance,  especially when there are many metaslabs per vdev and the allocation can't actually
               be satisfied (so we would otherwise iterate all metaslabs).

       <b>zfs_vdev_default_ms_count</b>=<b>200</b> (uint)
               When a vdev is added, target this number of metaslabs per top-level vdev.

       <b>zfs_vdev_default_ms_shift</b>=<b>29</b> (512 MiB) (uint)
               Default lower limit for metaslab size.

       <b>zfs_vdev_max_ms_shift</b>=<b>34</b> (16 GiB) (uint)
               Default upper limit for metaslab size.

       <b>zfs_vdev_max_auto_ashift</b>=<b>14</b> (uint)
               Maximum ashift used when optimizing for logical → physical sector size on  new  top-level  vdevs.
               May be increased up to <b>ASHIFT_MAX</b> (16), but this may negatively impact pool space efficiency.

       <b>zfs_vdev_direct_write_verify</b>=<b>Linux</b> <b>1</b> | <b>FreeBSD</b> <b>0</b> (uint)
               If  non-zero,  then a Direct I/O write's checksum will be verified every time the write is issued
               and before it is committed to the block pointer.  In the event the checksum is not valid then the
               I/O operation will return EIO.  This module parameter can be used to detect if  the  contents  of
               the  users  buffer  have changed in the process of doing a Direct I/O write.  It can also help to
               identify if reported checksum errors are tied to Direct I/O writes.  Each verify error  causes  a
               <b>dio_verify_wr</b>  zevent.  Direct Write I/O checksum verify errors can be seen with <b>zpool</b> <b>status</b> <b>-d</b>.
               The default value for this is 1 on Linux, but is 0 for FreeBSD because user pages can  be  placed
               under write protection in FreeBSD before the Direct I/O write is issued.

       <b>zfs_vdev_min_auto_ashift</b>=<b>ASHIFT_MIN</b> (9) (uint)
               Minimum ashift used when creating new top-level vdevs.

       <b>zfs_vdev_min_ms_count</b>=<b>16</b> (uint)
               Minimum number of metaslabs to create in a top-level vdev.

       <b>vdev_validate_skip</b>=<b>0</b>|1 (int)
               Skip label validation steps during pool import.  Changing is not recommended unless you know what
               you're doing and are recovering a damaged label.

       <b>zfs_vdev_ms_count_limit</b>=<b>131072</b> (128k) (uint)
               Practical upper limit of total metaslabs per top-level vdev.

       <b>metaslab_preload_enabled</b>=<b>1</b>|0 (int)
               Enable metaslab group preloading.

       <b>metaslab_preload_limit</b>=<b>10</b> (uint)
               Maximum number of metaslabs per group to preload

       <b>metaslab_preload_pct</b>=<b>50</b> (uint)
               Percentage of CPUs to run a metaslab preload taskq

       <b>metaslab_lba_weighting_enabled</b>=<b>1</b>|0 (int)
               Give  more  weight  to  metaslabs  with  lower  LBAs, assuming they have greater bandwidth, as is
               typically the case on a modern constant angular velocity disk drive.

       <b>metaslab_unload_delay</b>=<b>32</b> (uint)
               After a metaslab is used, we keep it loaded for this many TXGs, to attempt to reduce  unnecessary
               reloading.   Note  that  both  this many TXGs and <b>metaslab_unload_delay_ms</b> milliseconds must pass
               before unloading will occur.

       <b>metaslab_unload_delay_ms</b>=<b>600000</b>ms (10 min) (uint)
               After a metaslab is used, we keep it loaded for this many  milliseconds,  to  attempt  to  reduce
               unnecessary  reloading.   Note,  that  both this many milliseconds and <b>metaslab_unload_delay</b> TXGs
               must pass before unloading will occur.

       <b>reference_history</b>=<b>3</b> (uint)
               Maximum reference holders being tracked when reference_tracking_enable is active.

       <b>raidz_expand_max_copy_bytes</b>=<b>160MB</b> (ulong)
               Max amount of memory to use  for  RAID-Z  expansion  I/O.   This  limits  how  much  I/O  can  be
               outstanding at once.

       <b>raidz_expand_max_reflow_bytes</b>=<b>0</b> (ulong)
               For testing, pause RAID-Z expansion when reflow amount reaches this value.

       <b>raidz_io_aggregate_rows</b>=<b>4</b> (ulong)
               For expanded RAID-Z, aggregate reads that have more rows than this.

       <b>reference_history</b>=<b>3</b> (int)
               Maximum reference holders being tracked when reference_tracking_enable is active.

       <b>reference_tracking_enable</b>=<b>0</b>|1 (int)
               Track reference holders to <b>refcount_t</b> objects (debug builds only).

       <b>send_holes_without_birth_time</b>=<b>1</b>|0 (int)
               When  set, the <b>hole_birth</b> optimization will not be used, and all holes will always be sent during
               a <b>zfs</b> <b>send</b>.  This is useful if you suspect your datasets are affected by a bug in <b>hole_birth</b>.

       <b>spa_config_path</b>=<u>/etc/zfs/zpool.cache</u> (charp)
               SPA config file.

       <b>spa_asize_inflation</b>=<b>24</b> (uint)
               Multiplication factor used to estimate actual disk  consumption  from  the  size  of  data  being
               written.   The  default value is a worst case estimate, but lower values may be valid for a given
               pool depending on its configuration.  Pool administrators who understand the factors involved may
               wish to specify a more realistic inflation factor, particularly if they operate close to quota or
               capacity limits.

       <b>spa_load_print_vdev_tree</b>=<b>0</b>|1 (int)
               Whether to print the vdev tree in the debugging message buffer during pool import.

       <b>spa_load_verify_data</b>=<b>1</b>|0 (int)
               Whether to traverse data blocks during an "extreme rewind" (<b>-X</b>) import.

               An extreme rewind import normally performs a full  traversal  of  all  blocks  in  the  pool  for
               verification.   If  this  parameter is unset, the traversal skips non-metadata blocks.  It can be
               toggled once the import has started to stop or start the traversal of non-metadata blocks.

       <b>spa_load_verify_metadata</b>=<b>1</b>|0 (int)
               Whether to traverse blocks during an "extreme rewind" (<b>-X</b>) pool import.

               An extreme rewind import normally performs a full  traversal  of  all  blocks  in  the  pool  for
               verification.   If  this  parameter  is unset, the traversal is not performed.  It can be toggled
               once the import has started to stop or start the traversal.

       <b>spa_load_verify_shift</b>=<b>4</b> (1/16th) (uint)
               Sets the maximum number of bytes to consume during pool import to the log2 fraction of the target
               ARC size.

       <b>spa_slop_shift</b>=<b>5</b> (1/32nd) (int)
               Normally, we don't allow the last <b>3.2%</b> (<b>1/2^spa_slop_shift</b>) of space in the pool to be  consumed.
               This ensures that we don't run the pool completely out of space, due to unaccounted changes (e.g.
               to  the  MOS).   It also limits the worst-case time to allocate space.  If we have less than this
               amount of free space, most ZPL operations (e.g. write, create) will return <b>ENOSPC</b>.

       <b>spa_num_allocators</b>=<b>4</b> (int)
               Determines the number of block alloctators to use per spa instance.   Capped  by  the  number  of
               actual CPUs in the system via <b>spa_cpus_per_allocator</b>.

               Note  that  setting  this  value  too  high could result in performance degredation and/or excess
               fragmentation.  Set value only applies to pools imported/created after that.

       <b>spa_cpus_per_allocator</b>=<b>4</b> (int)
               Determines the minimum number of CPUs in a system for block alloctator  per  spa  instance.   Set
               value only applies to pools imported/created after that.

       <b>spa_upgrade_errlog_limit</b>=<b>0</b> (uint)
               Limits  the  number  of  on-disk  error log entries that will be converted to the new format when
               enabling the <b>head_errlog</b> feature.  The default is to convert all log entries.

       <b>vdev_removal_max_span</b>=<b>32768</b>B (32 KiB) (uint)
               During top-level vdev removal, chunks of data are copied from the vdev  which  may  include  free
               space  in  order to trade bandwidth for IOPS.  This parameter determines the maximum span of free
               space, in bytes, which will be included as "unnecessary" data in a chunk of copied data.

               The default value here was chosen to align  with  <b>zfs_vdev_read_gap_limit</b>,  which  is  a  similar
               concept when doing regular reads (but there's no reason it has to be the same).

       <b>vdev_file_logical_ashift</b>=<b>9</b> (512 B) (u64)
               Logical ashift for file-based devices.

       <b>vdev_file_physical_ashift</b>=<b>9</b> (512 B) (u64)
               Physical ashift for file-based devices.

       <b>zap_iterate_prefetch</b>=<b>1</b>|0 (int)
               If  set, when we start iterating over a ZAP object, prefetch the entire object (all leaf blocks).
               However, this is limited by <b>dmu_prefetch_max</b>.

       <b>zap_micro_max_size</b>=<b>131072</b>B (128 KiB) (int)
               Maximum micro ZAP size.  A "micro" ZAP is upgraded to a  "fat"  ZAP  once  it  grows  beyond  the
               specified  size.   Sizes  higher  than 128KiB will be clamped to 128KiB unless the <b>large_microzap</b>
               feature is enabled.

       <b>zap_shrink_enabled</b>=<b>1</b>|0 (int)
               If set, adjacent empty ZAP blocks will be collapsed, reducing disk space.

       <b>zfetch_min_distance</b>=<b>4194304</b>B (4 MiB) (uint)
               Min bytes to prefetch per stream.  Prefetch distance starts  from  the  demand  access  size  and
               quickly  grows  to  this  value, doubling on each hit.  After that it may grow further by 1/8 per
               hit, but only if some prefetch since last time  haven't  completed  in  time  to  satisfy  demand
               request, i.e.  prefetch depth didn't cover the read latency or the pool got saturated.

       <b>zfetch_max_distance</b>=<b>67108864</b>B (64 MiB) (uint)
               Max bytes to prefetch per stream.

       <b>zfetch_max_idistance</b>=<b>67108864</b>B (64 MiB) (uint)
               Max bytes to prefetch indirects for per stream.

       <b>zfetch_max_reorder</b>=<b>16777216</b>B (16 MiB) (uint)
               Requests within this byte distance from the current prefetch stream position are considered parts
               of  the  stream,  reordered  due to parallel processing.  Such requests do not advance the stream
               position immediately unless <b>zfetch_hole_shift</b> fill threshold is reached, but saved to fill  holes
               in the stream later.

       <b>zfetch_max_streams</b>=<b>8</b> (uint)
               Max number of streams per zfetch (prefetch streams per file).

       <b>zfetch_min_sec_reap</b>=<b>1</b> (uint)
               Min time before inactive prefetch stream can be reclaimed

       <b>zfetch_max_sec_reap</b>=<b>2</b> (uint)
               Max time before inactive prefetch stream can be deleted

       <b>zfs_abd_scatter_enabled</b>=<b>1</b>|0 (int)
               Enables  ARC  from  using  scatter/gather lists and forces all allocations to be linear in kernel
               memory.  Disabling can improve performance in some code paths at the expense of fragmented kernel
               memory.

       <b>zfs_abd_scatter_max_order</b>=<b>MAX_ORDER-1</b> (uint)
               Maximum number of consecutive memory pages allocated in a single block for scatter/gather lists.

               The value of <b>MAX_ORDER</b> depends on kernel configuration.

       <b>zfs_abd_scatter_min_size</b>=<b>1536</b>B (1.5 KiB) (uint)
               This is the minimum allocation size that will use scatter (page-based) ABDs.  Smaller allocations
               will use linear ABDs.

       <b>zfs_arc_dnode_limit</b>=<b>0</b>B (u64)
               When the number of bytes consumed by dnodes in the ARC exceeds this number of bytes, try to unpin
               some of it in response to demand for non-metadata.  This value acts as a ceiling to the amount of
               dnode  metadata,  and  defaults  to  <b>0</b>,  which  indicates  that  a  percent  which  is  based  on
               <b>zfs_arc_dnode_limit_percent</b> of the ARC meta buffers that may be used for dnodes.

       <b>zfs_arc_dnode_limit_percent</b>=<b>10</b>% (u64)
               Percentage that can be consumed by dnodes of ARC meta buffers.

               See  also  <b>zfs_arc_dnode_limit</b>,  which  serves  a  similar  purpose  but has a higher priority if
               nonzero.

       <b>zfs_arc_dnode_reduce_percent</b>=<b>10</b>% (u64)
               Percentage of ARC dnodes to try to scan in response to demand for non-metadata when the number of
               bytes consumed by dnodes exceeds <b>zfs_arc_dnode_limit</b>.

       <b>zfs_arc_average_blocksize</b>=<b>8192</b>B (8 KiB) (uint)
               The ARC's buffer hash table is sized based on the assumption of an average  block  size  of  this
               value.   This  works  out to roughly 1 MiB of hash table per 1 GiB of physical memory with 8-byte
               pointers.  For configurations with a known larger average block size, this value can be increased
               to reduce the memory footprint.

       <b>zfs_arc_eviction_pct</b>=<b>200</b>% (uint)
               When <b>arc_is_overflowing</b>(), <b>arc_get_data_impl</b>() waits for this percent of the requested amount  of
               data  to be evicted.  For example, by default, for every <u>2</u> <u>KiB</u> that's evicted, <u>1</u> <u>KiB</u> of it may be
               "reused" by a new allocation.  Since this is above <b>100</b>%, it ensures that progress is made towards
               getting <b>arc_size</b> under <b>arc_c</b>.  Since this is  finite,  it  ensures  that  allocations  can  still
               happen, even during the potentially long time that <b>arc_size</b> is more than <b>arc_c</b>.

       <b>zfs_arc_evict_batch_limit</b>=<b>10</b> (uint)
               Number ARC headers to evict per sub-list before proceeding to another sub-list.  This batch-style
               operation  prevents entire sub-lists from being evicted at once but comes at a cost of additional
               unlocking and locking.

       <b>zfs_arc_grow_retry</b>=<b>0</b>s (uint)
               If set to a non zero value, it will replace  the  <b>arc_grow_retry</b>  value  with  this  value.   The
               <b>arc_grow_retry</b>  value  (default  <b>5</b>s)  is the number of seconds the ARC will wait before trying to
               resume growth after a memory pressure event.

       <b>zfs_arc_lotsfree_percent</b>=<b>10</b>% (int)
               Throttle I/O when free system memory drops below this percentage of total system memory.  Setting
               this value to <b>0</b> will disable the throttle.

       <b>zfs_arc_max</b>=<b>0</b>B (u64)
               Max size of ARC in bytes.  If <b>0</b>, then the max size of ARC is determined by the amount  of  system
               memory  installed.   The  larger of <b>all_system_memory</b> - <b>1</b> <b>GiB</b> and <b>5/8</b> × <b>all_system_memory</b> will be
               used as the limit.  This value must be at least <b>67108864</b>B (64 MiB).

               This value can be changed dynamically, with some caveats.  It cannot  be  set  back  to  <b>0</b>  while
               running,  and  reducing  it  below  the current ARC size will not cause the ARC to shrink without
               memory pressure to induce shrinking.

       <b>zfs_arc_meta_balance</b>=<b>500</b> (uint)
               Balance between metadata and data on ghost hits.  Values above 100 increase metadata  caching  by
               proportionally reducing effect of ghost data hits on target data/metadata rate.

       <b>zfs_arc_min</b>=<b>0</b>B (u64)
               Min  size of ARC in bytes.  If set to <b>0</b>, <b>arc_c_min</b> will default to consuming the larger of <b>32</b> <b>MiB</b>
               and <b>all_system_memory</b> / <b>32</b>.

       <b>zfs_arc_min_prefetch_ms</b>=<b>0</b>ms(≡1s) (uint)
               Minimum time prefetched blocks are locked in the ARC.

       <b>zfs_arc_min_prescient_prefetch_ms</b>=<b>0</b>ms(≡6s) (uint)
               Minimum time "prescient prefetched" blocks are locked in the ARC.  These blocks are meant  to  be
               prefetched fairly aggressively ahead of the code that may use them.

       <b>zfs_arc_prune_task_threads</b>=<b>1</b> (int)
               Number  of  arc_prune threads.  FreeBSD does not need more than one.  Linux may theoretically use
               one per mount point up to number of CPUs, but that was not proven to be useful.

       <b>zfs_max_missing_tvds</b>=<b>0</b> (int)
               Number of missing top-level vdevs which will be allowed during pool  import  (only  in  read-only
               mode).

       <b>zfs_max_nvlist_src_size</b>= <b>0</b> (u64)
               Maximum  size  in  bytes allowed to be passed as <b>zc_nvlist_src_size</b> for ioctls on <u>/dev/zfs</u>.  This
               prevents a user from causing the kernel to allocate an excessive  amount  of  memory.   When  the
               limit  is  exceeded,  the  ioctl  fails with <b>EINVAL</b> and a description of the error is sent to the
               <u>zfs-dbgmsg</u> log.  This parameter should not need to be touched under normal circumstances.  If  <b>0</b>,
               equivalent  to a quarter of the user-wired memory limit under FreeBSD and to <b>134217728</b>B (128 MiB)
               under Linux.

       <b>zfs_multilist_num_sublists</b>=<b>0</b> (uint)
               To allow more fine-grained locking, each ARC state contains a series of lists for both  data  and
               metadata  objects.   Locking  is  performed  at  the level of these "sub-lists".  This parameters
               controls the number of sub-lists per ARC state, and also applies to other uses of  the  multilist
               data structure.

               If <b>0</b>, equivalent to the greater of the number of online CPUs and <b>4</b>.

       <b>zfs_arc_overflow_shift</b>=<b>8</b> (int)
               The ARC size is considered to be overflowing if it exceeds the current ARC target size (<b>arc_c</b>) by
               thresholds  determined  by  this  parameter.   Exceeding by (<b>arc_c</b> &gt;&gt; <b>zfs_arc_overflow_shift</b>) / <b>2</b>
               starts  ARC  reclamation  process.   If  that  appears  insufficient,  exceeding  by  (<b>arc_c</b>   &gt;&gt;
               <b>zfs_arc_overflow_shift</b>)  ×  <b>1.5</b> blocks new buffer allocation until the reclaim thread catches up.
               Started reclamation process continues till ARC size returns below the target size.

               The default value of <b>8</b> causes the ARC to start reclamation if it exceeds the target size by  <u>0.2%</u>
               of the target size, and block allocations by <u>0.6%</u>.

       <b>zfs_arc_shrink_shift</b>=<b>0</b> (uint)
               If nonzero, this will update <b>arc_shrink_shift</b> (default <b>7</b>) with the new value.

       <b>zfs_arc_pc_percent</b>=<b>0</b>% (off) (uint)
               Percent of pagecache to reclaim ARC to.

               This  tunable  allows  the  ZFS  ARC to play more nicely with the kernel's LRU pagecache.  It can
               guarantee that the ARC size won't collapse under scanning pressure on the  pagecache,  yet  still
               allows  the  ARC  to  be  reclaimed down to <b>zfs_arc_min</b> if necessary.  This value is specified as
               percent of pagecache size (as measured by <b>NR_FILE_PAGES</b>), where  that  percent  may  exceed  <b>100</b>.
               This only operates during memory pressure/reclaim.

       <b>zfs_arc_shrinker_limit</b>=<b>0</b> (int)
               This  is  a  limit on how many pages the ARC shrinker makes available for eviction in response to
               one page allocation attempt.  Note that in practice, the kernel's shrinker can ask us to evict up
               to about four times this for one allocation attempt.  To reduce OOM risk, this limit  is  applied
               for kswapd reclaims only.

               For  example  a  value  of  <b>10000</b>  (in practice, <u>160</u> <u>MiB</u> per allocation attempt with 4 KiB pages)
               limits the amount of time spent attempting to  reclaim  ARC  memory  to  less  than  100  ms  per
               allocation attempt, even with a small average compressed block size of ~8 KiB.

               The parameter can be set to 0 (zero) to disable the limit, and only applies on Linux.

       <b>zfs_arc_shrinker_seeks</b>=<b>2</b> (int)
               Relative  cost  of  ARC  eviction  on  Linux, AKA number of seeks needed to restore evicted page.
               Bigger values make ARC more precious and evictions smaller, comparing to other kernel subsystems.
               Value of 4 means parity with page cache.

       <b>zfs_arc_sys_free</b>=<b>0</b>B (u64)
               The target number of bytes the ARC  should  leave  as  free  memory  on  the  system.   If  zero,
               equivalent to the bigger of <b>512</b> <b>KiB</b> and <b>all_system_memory/64</b>.

       <b>zfs_autoimport_disable</b>=<b>1</b>|0 (int)
               Disable pool import at module load by ignoring the cache file (<b>spa_config_path</b>).

       <b>zfs_checksum_events_per_second</b>=<b>20</b>/s (uint)
               Rate  limit  checksum events to this many per second.  Note that this should not be set below the
               ZED thresholds (currently 10 checksums over 10 seconds) or else the daemon may  not  trigger  any
               action.

       <b>zfs_commit_timeout_pct</b>=<b>10</b>% (uint)
               This  controls the amount of time that a ZIL block (lwb) will remain "open" when it isn't "full",
               and it has a thread waiting for it to be committed to stable  storage.   The  timeout  is  scaled
               based  on  a  percentage  of the last lwb latency to avoid significantly impacting the latency of
               each individual transaction record (itx).

       <b>zfs_condense_indirect_commit_entry_delay_ms</b>=<b>0</b>ms (int)
               Vdev indirection layer (used for device removal) sleeps for this many milliseconds during mapping
               generation.  Intended for use with the test suite to throttle vdev removal speed.

       <b>zfs_condense_indirect_obsolete_pct</b>=<b>25</b>% (uint)
               Minimum percent of  obsolete  bytes  in  vdev  mapping  required  to  attempt  to  condense  (see
               <b>zfs_condense_indirect_vdevs_enable</b>).   Intended  for  use  with  the  test  suite  to  facilitate
               triggering condensing as needed.

       <b>zfs_condense_indirect_vdevs_enable</b>=<b>1</b>|0 (int)
               Enable condensing indirect vdev mappings.  When set, attempt to condense indirect  vdev  mappings
               if  the mapping uses more than <b>zfs_condense_min_mapping_bytes</b> bytes of memory and if the obsolete
               space map object uses more than <b>zfs_condense_max_obsolete_bytes</b> bytes  on-disk.   The  condensing
               process is an attempt to save memory by removing obsolete mappings.

       <b>zfs_condense_max_obsolete_bytes</b>=<b>1073741824</b>B (1 GiB) (u64)
               Only  attempt  to  condense  indirect vdev mappings if the on-disk size of the obsolete space map
               object is greater than this number of bytes (see <b>zfs_condense_indirect_vdevs_enable</b>).

       <b>zfs_condense_min_mapping_bytes</b>=<b>131072</b>B (128 KiB) (u64)
               Minimum size vdev mapping to attempt to condense (see <b>zfs_condense_indirect_vdevs_enable</b>).

       <b>zfs_dbgmsg_enable</b>=<b>1</b>|0 (int)
               Internally ZFS keeps a small log to facilitate debugging.  The log is enabled by default, and can
               be disabled by unsetting this option.  The contents  of  the  log  can  be  accessed  by  reading
               <u>/proc/spl/kstat/zfs/dbgmsg</u>.  Writing <b>0</b> to the file clears the log.

               This setting does not influence debug prints due to <b>zfs_flags</b>.

       <b>zfs_dbgmsg_maxsize</b>=<b>4194304</b>B (4 MiB) (uint)
               Maximum size of the internal ZFS debug log.

       <b>zfs_dbuf_state_index</b>=<b>0</b> (int)
               Historically  used  for  controlling  what reporting was available under <u>/proc/spl/kstat/zfs</u>.  No
               effect.

       <b>zfs_deadman_checktime_ms</b>=<b>60000</b>ms (1 min) (u64)
               Check time in milliseconds.  This defines the frequency at which we check for hung  I/O  requests
               and potentially invoke the <b>zfs_deadman_failmode</b> behavior.

       <b>zfs_deadman_enabled</b>=<b>1</b>|0 (int)
               When  a  pool sync operation takes longer than <b>zfs_deadman_synctime_ms</b>, or when an individual I/O
               operation takes longer than <b>zfs_deadman_ziotime_ms</b>,  then  the  operation  is  considered  to  be
               "hung".   If  <b>zfs_deadman_enabled</b>  is  set,  then the deadman behavior is invoked as described by
               <b>zfs_deadman_failmode</b>.  By default, the deadman is enabled and set to <b>wait</b> which results in "hung"
               I/O operations only being logged.  The  deadman  is  automatically  disabled  when  a  pool  gets
               suspended.

       <b>zfs_deadman_events_per_second</b>=<b>1</b>/s (int)
               Rate limit deadman zevents (which report hung I/O operations) to this many per second.

       <b>zfs_deadman_failmode</b>=<b>wait</b> (charp)
               Controls the failure behavior when the deadman detects a "hung" I/O operation.  Valid values are:
                   <b>wait</b>      Wait  for  a  "hung"  operation to complete.  For each "hung" operation a "deadman"
                             event will be posted describing that operation.
                   <b>continue</b>  Attempt to recover from a "hung" operation by re-dispatching it to the I/O pipeline
                             if possible.
                   <b>panic</b>     Panic the system.  This can be used to facilitate automatic fail-over to a properly
                             configured fail-over partner.

       <b>zfs_deadman_synctime_ms</b>=<b>600000</b>ms (10 min) (u64)
               Interval in milliseconds after which the deadman is triggered and also the interval after which a
               pool sync operation is considered to be "hung".  Once this limit is exceeded the deadman will  be
               invoked every <b>zfs_deadman_checktime_ms</b> milliseconds until the pool sync completes.

       <b>zfs_deadman_ziotime_ms</b>=<b>300000</b>ms (5 min) (u64)
               Interval  in milliseconds after which the deadman is triggered and an individual I/O operation is
               considered to be "hung".  As long as the operation remains "hung", the deadman  will  be  invoked
               every <b>zfs_deadman_checktime_ms</b> milliseconds until the operation completes.

       <b>zfs_dedup_prefetch</b>=<b>0</b>|1 (int)
               Enable prefetching dedup-ed blocks which are going to be freed.

       <b>zfs_dedup_log_flush_passes_max</b>=<b>8</b>(uint)
               Maximum number of dedup log flush passes (iterations) each transaction.

               At the start of each transaction, OpenZFS will estimate how many entries it needs to flush out to
               keep  up  with  the  change rate, taking the amount and time taken to flush on previous txgs into
               account (see <b>zfs_dedup_log_flush_flow_rate_txgs</b>).  It will spread this amount into  a  number  of
               passes.   At  each  pass,  it  will  use  the  amount already flushed and the total time taken by
               flushing and by other IO to recompute how much it should do for the remainder of the txg.

               Reducing the max number of passes will make flushing more aggressive, flushing out  more  entries
               on each pass.  This can be faster, but also more likely to compete with other IO.  Increasing the
               max number of passes will put fewer entries onto each pass, keeping the overhead of dedup changes
               to  a minimum but possibly causing a large number of changes to be dumped on the last pass, which
               can blow out the txg sync time beyond <b>zfs_txg_timeout</b>.

       <b>zfs_dedup_log_flush_min_time_ms</b>=<b>1000</b>(uint)
               Minimum time to spend on dedup log flush each transaction.

               At  least  this  long  will  be  spent  flushing  dedup  log  entries  each  transaction,  up  to
               <b>zfs_txg_timeout</b>.   This  occurs  even  if doing so would delay the transaction, that is, other IO
               completes under this time.

       <b>zfs_dedup_log_flush_entries_min</b>=<b>1000</b>(uint)
               Flush at least this many entries each transaction.

               OpenZFS will estimate how many entries it needs to flush each transaction to  keep  up  with  the
               ingest  rate  (see <b>zfs_dedup_log_flush_flow_rate_txgs</b>).  This sets the minimum for that estimate.
               Raising it can force OpenZFS to flush more aggressively, keeping the log small  and  so  reducing
               pool import times, but can make it less able to back off if log flushing would compete with other
               IO too much.

       <b>zfs_dedup_log_flush_flow_rate_txgs</b>=<b>10</b>(uint)
               Number of transactions to use to compute the flow rate.

               OpenZFS  will  estimate  how  many  entries  it needs to flush each transaction by monitoring the
               number of entries changed (ingest rate), number of entries flushed (flush rate)  and  time  spent
               flushing  (flush  time  rate)  and  combining  these into an overall "flow rate".  It will use an
               exponential weighted moving average over some number of  recent  transactions  to  compute  these
               rates.   This  sets the number of transactions to compute these averages over.  Setting it higher
               can help to smooth out the flow rate in the face of spiky workloads, but will take longer for the
               flow rate to adjust to a sustained change in the ingress rate.

       <b>zfs_dedup_log_txg_max</b>=<b>8</b>(uint)
               Max transactions to before starting to flush dedup logs.

               OpenZFS maintains two dedup logs, one receiving new changes, one flushing.  If there  is  nothing
               to flush, it will accumulate changes for no more than this many transactions before switching the
               logs and starting to flush entries out.

       <b>zfs_dedup_log_mem_max</b>=<b>0</b>(u64)
               Max memory to use for dedup logs.

               OpenZFS  will  spend  no  more  than  this  much  memory  on maintaining the in-memory dedup log.
               Flushing will begin when around half this amount is being spent on logs.  The default value of  <b>0</b>
               will cause it to be set by <b>zfs_dedup_log_mem_max_percent</b> instead.

       <b>zfs_dedup_log_mem_max_percent</b>=<b>1</b>% (uint)
               Max memory to use for dedup logs, as a percentage of total memory.

               If  <b>zfs_dedup_log_mem_max</b>  is not set, it will be initialised as a percentage of the total memory
               in the system.

       <b>zfs_delay_min_dirty_percent</b>=<b>60</b>% (uint)
               Start to delay each transaction once  there  is  this  amount  of  dirty  data,  expressed  as  a
               percentage      of     <b>zfs_dirty_data_max</b>.      This     value     should     be     at     least
               <b>zfs_vdev_async_write_active_max_dirty_percent</b>.  See “ZFS TRANSACTION DELAY”.

       <b>zfs_delay_scale</b>=<b>500000</b> (int)
               This controls how quickly the transaction delay approaches infinity.  Larger values cause  longer
               delays for a given amount of dirty data.

               For  the  smoothest  delay, this value should be about 1 billion divided by the maximum number of
               operations per second.  This will smoothly handle between ten times and a tenth of  this  number.
               See “ZFS TRANSACTION DELAY”.

               <b>zfs_delay_scale</b> × <b>zfs_dirty_data_max</b> <u>must</u> be smaller than <b>2^64</b>.

       <b>zfs_dio_write_verify_events_per_second</b>=<b>20</b>/s (uint)
               Rate limit Direct I/O write verify events to this many per second.

       <b>zfs_disable_ivset_guid_check</b>=<b>0</b>|1 (int)
               Disables  requirement  for  IVset  GUIDs  to  be  present  and  match when doing a raw receive of
               encrypted datasets.  Intended for  users  whose  pools  were  created  with  OpenZFS  pre-release
               versions and now have compatibility issues.

       <b>zfs_key_max_salt_uses</b>=<b>400000000</b> (4*10^8) (ulong)
               Maximum number of uses of a single salt value before generating a new one for encrypted datasets.
               The default value is also the maximum.

       <b>zfs_object_mutex_size</b>=<b>64</b> (uint)
               Size of the znode hashtable used for holds.

               Due  to  the need to hold locks on objects that may not exist yet, kernel mutexes are not created
               per-object and instead a hashtable is used where collisions will result in objects  waiting  when
               there is not actually contention on the same object.

       <b>zfs_slow_io_events_per_second</b>=<b>20</b>/s (int)
               Rate limit delay zevents (which report slow I/O operations) to this many per second.

       <b>zfs_unflushed_max_mem_amt</b>=<b>1073741824</b>B (1 GiB) (u64)
               Upper-bound  limit  for  unflushed  metadata changes to be held by the log spacemap in memory, in
               bytes.

       <b>zfs_unflushed_max_mem_ppm</b>=<b>1000</b>ppm (0.1%) (u64)
               Part of overall system memory that ZFS allows to be used for unflushed metadata  changes  by  the
               log spacemap, in millionths.

       <b>zfs_unflushed_log_block_max</b>=<b>131072</b> (128k) (u64)
               Describes  the  maximum  number  of log spacemap blocks allowed for each pool.  The default value
               means that the space in all the log spacemaps can add up to no more  than  <b>131072</b>  blocks  (which
               means <u>16</u> <u>GiB</u> of logical space before compression and ditto blocks, assuming that blocksize is <u>128</u>
               <u>KiB</u>).

               This  tunable  is  important because it involves a trade-off between import time after an unclean
               export and the frequency of flushing metaslabs.  The higher this number is, the more  log  blocks
               we allow when the pool is active which means that we flush metaslabs less often and thus decrease
               the  number  of I/O operations for spacemap updates per TXG.  At the same time though, that means
               that in the event of an unclean export, there will be more log spacemap blocks for  us  to  read,
               inducing  overhead  in the import time of the pool.  The lower the number, the amount of flushing
               increases, destroying log blocks quicker as they become obsolete faster, which leaves less blocks
               to be read during import time after a crash.

               Each log spacemap block existing during pool import leads to approximately one extra logical  I/O
               issued.   This  is  the  reason  why this tunable is exposed in terms of blocks rather than space
               used.

       <b>zfs_unflushed_log_block_min</b>=<b>1000</b> (u64)
               If the number of metaslabs is small and our incoming rate is high, we could get into a  situation
               that  we  are  flushing all our metaslabs every TXG.  Thus we always allow at least this many log
               blocks.

       <b>zfs_unflushed_log_block_pct</b>=<b>400</b>% (u64)
               Tunable used to determine the number of blocks that can be used for the spacemap  log,  expressed
               as a percentage of the total number of unflushed metaslabs in the pool.

       <b>zfs_unflushed_log_txg_max</b>=<b>1000</b> (u64)
               Tunable  limiting  maximum time in TXGs any metaslab may remain unflushed.  It effectively limits
               maximum number of unflushed per-TXG spacemap logs that need to be read after unclean pool export.

       <b>zfs_unlink_suspend_progress</b>=<b>0</b>|1 (uint)
               When enabled, files will not be asynchronously removed from the list of pending unlinks  and  the
               space  they  consume  will  be  leaked.   Once  this  option has been disabled and the dataset is
               remounted, the pending unlinks will be processed and the freed space returned to the pool.   This
               option is used by the test suite.

       <b>zfs_delete_blocks</b>=<b>20480</b> (ulong)
               This is the used to define a large file for the purposes of deletion.  Files containing more than
               <b>zfs_delete_blocks</b>  will be deleted asynchronously, while smaller files are deleted synchronously.
               Decreasing this value will reduce the time spent in an <u><a href="../man2/unlink.2.html">unlink</a></u>(2) system call, at the expense of a
               longer delay before the freed space is available.  This only applies on Linux.

       <b>zfs_dirty_data_max</b>= (int)
               Determines the dirty space limit in bytes.  Once this limit is exceeded, new  writes  are  halted
               until space frees up.  This parameter takes precedence over <b>zfs_dirty_data_max_percent</b>.  See “ZFS
               TRANSACTION DELAY”.

               Defaults to <b>physical_ram/10</b>, capped at <b>zfs_dirty_data_max_max</b>.

       <b>zfs_dirty_data_max_max</b>= (int)
               Maximum  allowable  value of <b>zfs_dirty_data_max</b>, expressed in bytes.  This limit is only enforced
               at module load time, and will be ignored if <b>zfs_dirty_data_max</b> is later changed.  This  parameter
               takes precedence over <b>zfs_dirty_data_max_max_percent</b>.  See “ZFS TRANSACTION DELAY”.

               Defaults to <b>min(physical_ram/4,</b> <b>4GiB)</b>, or <b>min(physical_ram/4,</b> <b>1GiB)</b> for 32-bit systems.

       <b>zfs_dirty_data_max_max_percent</b>=<b>25</b>% (uint)
               Maximum  allowable  value of <b>zfs_dirty_data_max</b>, expressed as a percentage of physical RAM.  This
               limit is only enforced at module load time, and will be ignored if  <b>zfs_dirty_data_max</b>  is  later
               changed.   The  parameter  <b>zfs_dirty_data_max_max</b>  takes  precedence  over  this  one.   See “ZFS
               TRANSACTION DELAY”.

       <b>zfs_dirty_data_max_percent</b>=<b>10</b>% (uint)
               Determines the dirty space limit, expressed as a percentage of all memory.  Once  this  limit  is
               exceeded,  new  writes  are  halted until space frees up.  The parameter <b>zfs_dirty_data_max</b> takes
               precedence over this one.  See “ZFS TRANSACTION DELAY”.

               Subject to <b>zfs_dirty_data_max_max</b>.

       <b>zfs_dirty_data_sync_percent</b>=<b>20</b>% (uint)
               Start syncing out a transaction group if there's at least this much dirty data (as  a  percentage
               of <b>zfs_dirty_data_max</b>).  This should be less than <b>zfs_vdev_async_write_active_min_dirty_percent</b>.

       <b>zfs_wrlog_data_max</b>= (int)
               The  upper limit of write-transaction zil log data size in bytes.  Write operations are throttled
               when approaching the limit until log data is cleared out after transaction group  sync.   Because
               of  some  overhead,  it  should be set at least 2 times the size of <b>zfs_dirty_data_max</b> to prevent
               harming normal write throughput.  It also should be smaller than the size of the slog  device  if
               slog is present.

               Defaults to <b>zfs_dirty_data_max*2</b>

       <b>zfs_fallocate_reserve_percent</b>=<b>110</b>% (uint)
               Since  ZFS is a copy-on-write filesystem with snapshots, blocks cannot be preallocated for a file
               in order to guarantee that later writes will not run out of space.  Instead,  <u><a href="../man2/fallocate.2.html">fallocate</a></u>(2)  space
               preallocation  only checks that sufficient space is currently available in the pool or the user's
               project quota allocation, and then creates a sparse file of the requested  size.   The  requested
               space  is  multiplied  by  <b>zfs_fallocate_reserve_percent</b>  to  allow additional space for indirect
               blocks and other internal metadata.  Setting this to <b>0</b>  disables  support  for  <u><a href="../man2/fallocate.2.html">fallocate</a></u>(2)  and
               causes it to return <b>EOPNOTSUPP</b>.

       <b>zfs_fletcher_4_impl</b>=<b>fastest</b> (string)
               Select a fletcher 4 implementation.

               Supported selectors are: <b>fastest</b>, <b>scalar</b>, <b>sse2</b>, <b>ssse3</b>, <b>avx2</b>, <b>avx512f</b>, <b>avx512bw</b>, and <b>aarch64_neon</b>.
               All  except  <b>fastest</b> and <b>scalar</b> require instruction set extensions to be available, and will only
               appear if ZFS detects that they are present at runtime.  If multiple implementations of  fletcher
               4 are available, the <b>fastest</b> will be chosen using a micro benchmark.  Selecting <b>scalar</b> results in
               the original CPU-based calculation being used.  Selecting any option other than <b>fastest</b> or <b>scalar</b>
               results in vector instructions from the respective CPU instruction set being used.

       <b>zfs_bclone_enabled</b>=<b>1</b>|0 (int)
               Enables   access   to   the  block  cloning  feature.   If  this  setting  is  0,  then  even  if
               feature@block_cloning is enabled, using functions and system calls that attempt to  clone  blocks
               will act as though the feature is disabled.

       <b>zfs_bclone_wait_dirty</b>=<b>0</b>|1 (int)
               When  set  to  1  the  FICLONE and FICLONERANGE ioctls wait for dirty data to be written to disk.
               This allows the clone operation to reliably succeed when a file is modified and then  immediately
               cloned.   For  small  files  this  may be slower than making a copy of the file.  Therefore, this
               setting defaults to 0 which causes a clone operation to  immediately  fail  when  encountering  a
               dirty block.

       <b>zfs_blake3_impl</b>=<b>fastest</b> (string)
               Select a BLAKE3 implementation.

               Supported  selectors  are: <b>cycle</b>, <b>fastest</b>, <b>generic</b>, <b>sse2</b>, <b>sse41</b>, <b>avx2</b>, <b>avx512</b>.  All except <b>cycle</b>,
               <b>fastest</b> and <b>generic</b> require instruction set extensions to be available, and will only  appear  if
               ZFS  detects  that  they  are  present  at  runtime.   If  multiple implementations of BLAKE3 are
               available, the <b>fastest</b> <b>will</b> <b>be</b> <b>chosen</b> <b>using</b> <b>a</b> <b>micro</b> <b>benchmark.</b> <b>You</b> <b>can</b> <b>see</b> <b>the</b> benchmark  results
               by reading this kstat file: <u>/proc/spl/kstat/zfs/chksum_bench</u>.

       <b>zfs_free_bpobj_enabled</b>=<b>1</b>|0 (int)
               Enable/disable the processing of the free_bpobj object.

       <b>zfs_async_block_max_blocks</b>=<b>UINT64_MAX</b> (unlimited) (u64)
               Maximum number of blocks freed in a single TXG.

       <b>zfs_max_async_dedup_frees</b>=<b>100000</b> (10^5) (u64)
               Maximum number of dedup blocks freed in a single TXG.

       <b>zfs_vdev_async_read_max_active</b>=<b>3</b> (uint)
               Maximum asynchronous read I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_async_read_min_active</b>=<b>1</b> (uint)
               Minimum asynchronous read I/O operation active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_async_write_active_max_dirty_percent</b>=<b>60</b>% (uint)
               When  the  pool  has more than this much dirty data, use <b>zfs_vdev_async_write_max_active</b> to limit
               active async writes.  If the dirty data is between the minimum and maximum, the active I/O  limit
               is linearly interpolated.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_async_write_active_min_dirty_percent</b>=<b>30</b>% (uint)
               When  the  pool  has less than this much dirty data, use <b>zfs_vdev_async_write_min_active</b> to limit
               active async writes.  If the dirty data is between the minimum and maximum, the active I/O  limit
               is linearly interpolated.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_async_write_max_active</b>=<b>10</b> (uint)
               Maximum asynchronous write I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_async_write_min_active</b>=<b>2</b> (uint)
               Minimum asynchronous write I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

               Lower  values  are  associated  with  better  latency  on  rotational  media  but poorer resilver
               performance.  The default value of <b>2</b> was chosen as a compromise.  A value of <b>3</b> has been shown  to
               improve resilver performance further at a cost of further increasing latency.

       <b>zfs_vdev_initializing_max_active</b>=<b>1</b> (uint)
               Maximum initializing I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_initializing_min_active</b>=<b>1</b> (uint)
               Minimum initializing I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_max_active</b>=<b>1000</b> (uint)
               The  maximum  number of I/O operations active to each device.  Ideally, this will be at least the
               sum of each queue's <b>max_active</b>.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_open_timeout_ms</b>=<b>1000</b> (uint)
               Timeout value to wait before determining a device is missing during import.  This is helpful  for
               transient  missing  paths  due  to  links being briefly removed and recreated in response to udev
               events.

       <b>zfs_vdev_rebuild_max_active</b>=<b>3</b> (uint)
               Maximum sequential resilver I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_rebuild_min_active</b>=<b>1</b> (uint)
               Minimum sequential resilver I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_removal_max_active</b>=<b>2</b> (uint)
               Maximum removal I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_removal_min_active</b>=<b>1</b> (uint)
               Minimum removal I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_scrub_max_active</b>=<b>2</b> (uint)
               Maximum scrub I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_scrub_min_active</b>=<b>1</b> (uint)
               Minimum scrub I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_sync_read_max_active</b>=<b>10</b> (uint)
               Maximum synchronous read I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_sync_read_min_active</b>=<b>10</b> (uint)
               Minimum synchronous read I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_sync_write_max_active</b>=<b>10</b> (uint)
               Maximum synchronous write I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_sync_write_min_active</b>=<b>10</b> (uint)
               Minimum synchronous write I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_trim_max_active</b>=<b>2</b> (uint)
               Maximum trim/discard I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_trim_min_active</b>=<b>1</b> (uint)
               Minimum trim/discard I/O operations active to each device.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_nia_delay</b>=<b>5</b> (uint)
               For non-interactive I/O (scrub,  resilver,  removal,  initialize  and  rebuild),  the  number  of
               concurrently-active  I/O  operations  is  limited to <b>zfs_*_min_active</b>, unless the vdev is "idle".
               When  there  are  no  interactive  I/O  operations  active  (synchronous   or   otherwise),   and
               <b>zfs_vdev_nia_delay</b>  operations have completed since the last interactive operation, then the vdev
               is considered to be "idle", and the number of concurrently-active non-interactive  operations  is
               increased to <b>zfs_*_max_active</b>.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_nia_credit</b>=<b>5</b> (uint)
               Some  HDDs  tend  to  prioritize  sequential  I/O so strongly, that concurrent random I/O latency
               reaches several seconds.  On some HDDs  this  happens  even  if  sequential  I/O  operations  are
               submitted  one  at  a  time,  and  so setting <b>zfs_*_max_active</b>= <b>1</b> does not help.  To prevent non-
               interactive I/O, like scrub, from monopolizing  the  device,  no  more  than  <b>zfs_vdev_nia_credit</b>
               <b>operations</b>  <b>can</b>  <b>be</b>  <b>sent</b>  while  there  are outstanding incomplete interactive operations.  This
               enforced wait ensures the HDD services the interactive I/O within a reasonable  amount  of  time.
               See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_queue_depth_pct</b>=<b>1000</b>% (uint)
               Maximum   number  of  queued  allocations  per  top-level  vdev  expressed  as  a  percentage  of
               <b>zfs_vdev_async_write_max_active</b>, which allows the system to detect devices that are more  capable
               of  handling  allocations  and to allocate more blocks to those devices.  This allows for dynamic
               allocation distribution when devices are imbalanced, as fuller devices will  tend  to  be  slower
               than empty devices.

               Also see <b>zio_dva_throttle_enabled</b>.

       <b>zfs_vdev_def_queue_depth</b>=<b>32</b> (uint)
               Default  queue  depth  for  each vdev IO allocator.  Higher values allow for better coalescing of
               sequential writes before sending them to the disk, but can increase transaction commit times.

       <b>zfs_vdev_failfast_mask</b>=<b>1</b> (uint)
               Defines if the driver should retire on a given error type.  The following options may be bitwise-
               ored together:
               ┌────────────────────────────────────────────────────────────────┐
               │     Value   Name        Description                            │
               ├────────────────────────────────────────────────────────────────┤
               │         1   Device      No driver retries on device errors     │
               │         2   Transport   No driver retries on transport errors. │
               │         4   Driver      No driver retries on driver errors.    │
               └────────────────────────────────────────────────────────────────┘

       <b>zfs_vdev_disk_max_segs</b>=<b>0</b> (uint)
               Maximum number of segments to add to a BIO (min 4).  If this is higher than the  maximum  allowed
               by  the device queue or the kernel itself, it will be clamped.  Setting it to zero will cause the
               kernel's ideal size to be used.  This parameter only applies on Linux.  This parameter is ignored
               if <b>zfs_vdev_disk_classic</b>=<b>1</b>.

       <b>zfs_vdev_disk_classic</b>=<b>0</b>|1 (uint)
               If set to 1, OpenZFS will submit IO to Linux using the method it used in 2.2 and  earlier.   This
               "classic"  method  has  known  issues  with  highly  fragmented IO requests and is slower on many
               workloads, but it has been in use for many years and is known to be very stable.  If you set this
               parameter, please also open a bug report why you did so, including the workload involved and  any
               error messages.

               This parameter and the classic submission method will be removed once we have total confidence in
               the new method.

               This parameter only applies on Linux, and can only be set at module load time.

       <b>zfs_expire_snapshot</b>=<b>300</b>s (int)
               Time before expiring <u>.zfs/snapshot</u>.

       <b>zfs_admin_snapshot</b>=<b>0</b>|1 (int)
               Allow  the  creation, removal, or renaming of entries in the <b>.zfs/snapshot</b> directory to cause the
               creation, destruction, or renaming of snapshots.  When enabled,  this  functionality  works  both
               locally and over NFS exports which have the <u>no_root_squash</u> option set.

       <b>zfs_snapshot_no_setuid</b>=<b>0</b>|1 (int)
               Whether  to  disable  <u>setuid/setgid</u>  support  for  snapshot  mounts  triggered  by  access to the
               <b>.zfs/snapshot</b> directory by setting the <u>nosuid</u> mount option.

       <b>zfs_flags</b>=<b>0</b> (int)
               Set additional debugging flags.  The following flags may be bitwise-ored together:
               ┌───────────────────────────────────────────────────────────────────────────────────────────────────────────┐
               │     Value   Name                         Description                                                      │
               ├───────────────────────────────────────────────────────────────────────────────────────────────────────────┤
               │         1   ZFS_DEBUG_DPRINTF            Enable dprintf entries in the debug log.                         │
               │ <b>*</b>       2   ZFS_DEBUG_DBUF_VERIFY        Enable extra dbuf verifications.                                 │
               │ <b>*</b>       4   ZFS_DEBUG_DNODE_VERIFY       Enable extra dnode verifications.                                │
               │         8   ZFS_DEBUG_SNAPNAMES          Enable snapshot name verification.                               │
               │ <b>*</b>      16   ZFS_DEBUG_MODIFY             Check for illegally modified ARC buffers.                        │
               │        64   ZFS_DEBUG_ZIO_FREE           Enable verification of block frees.                              │
               │       128   ZFS_DEBUG_HISTOGRAM_VERIFY   Enable extra spacemap histogram verifications.                   │
               │       256   ZFS_DEBUG_METASLAB_VERIFY    Verify space accounting on disk matches in-memory <b>range_trees</b>.   │
               │       512   ZFS_DEBUG_SET_ERROR          Enable <b>SET_ERROR</b> and dprintf entries in the debug log.           │
               │      1024   ZFS_DEBUG_INDIRECT_REMAP     Verify split blocks created by device removal.                   │
               │      2048   ZFS_DEBUG_TRIM               Verify TRIM ranges are always within the allocatable range tree. │
               │      4096   ZFS_DEBUG_LOG_SPACEMAP       Verify that the log summary is consistent with the spacemap log  │
               │                                                 and enable <b>zfs_dbgmsgs</b> for metaslab loading and flushing. │
               └───────────────────────────────────────────────────────────────────────────────────────────────────────────┘
                <b>*</b> Requires debug build.

       <b>zfs_btree_verify_intensity</b>=<b>0</b> (uint)
               Enables btree verification.  The following settings are cumulative:
               ┌───────────────────────────────────────────────────────────────┐
               │     Value   Description                                       │
               │                                                               │
               │         1   Verify height.                                    │
               │         2   Verify pointers from children to parent.          │
               │         3   Verify element counts.                            │
               │         4   Verify element order. (expensive)                 │
               │ <b>*</b>       5   Verify unused memory is poisoned. (expensive)     │
               └───────────────────────────────────────────────────────────────┘
                <b>*</b> Requires debug build.

       <b>zfs_free_leak_on_eio</b>=<b>0</b>|1 (int)
               If destroy encounters an <b>EIO</b> while reading metadata (e.g. indirect blocks), space  referenced  by
               the  missing  metadata  can  not be freed.  Normally this causes the background destroy to become
               "stalled", as it is unable to make forward progress.  While in this stalled state, all  remaining
               space  to  free from the error-encountering filesystem is "temporarily leaked".  Set this flag to
               cause it to ignore the <b>EIO</b>, permanently leak the space from indirect blocks that can not be read,
               and continue to free everything else that it can.

               The default "stalling" behavior is useful if the storage partially fails (i.e. some but  not  all
               I/O  operations  fail),  and then later recovers.  In this case, we will be able to continue pool
               operations while it is partially failed, and when it recovers, we can continue to free the space,
               with no leaks.  Note, however, that this case is actually fairly rare.

               Typically pools either
                   1. fail completely (but perhaps temporarily, e.g. due to a top-level vdev going offline), or
                   2. have localized, permanent errors (e.g. disk returns the wrong data  due  to  bit  flip  or
                     firmware bug).
               In  the former case, this setting does not matter because the pool will be suspended and the sync
               thread will not be able to make forward progress regardless.  In the latter, because the error is
               permanent, the best we can do is leak the minimum amount of space, which  is  what  setting  this
               flag will do.  It is therefore reasonable for this flag to normally be set, but we chose the more
               conservative  approach of not setting it, so that there is no possibility of leaking space in the
               "partial temporary" failure case.

       <b>zfs_free_min_time_ms</b>=<b>1000</b>ms (1s) (uint)
               During a <b>zfs</b> <b>destroy</b> operation using the <b>async_destroy</b> feature, a minimum of this much time  will
               be spent working on freeing blocks per TXG.

       <b>zfs_obsolete_min_time_ms</b>=<b>500</b>ms (uint)
               Similar to <b>zfs_free_min_time_ms</b>, but for cleanup of old indirection records for removed vdevs.

       <b>zfs_immediate_write_sz</b>=<b>32768</b>B (32 KiB) (s64)
               Largest  data  block  to write to the ZIL.  Larger blocks will be treated as if the dataset being
               written to had the <b>logbias</b>=<b>throughput</b> property set.

       <b>zfs_initialize_value</b>=<b>16045690984833335022</b> (0xDEADBEEFDEADBEEE) (u64)
               Pattern written to vdev free space by <u><a href="../man8/zpool-initialize.8.html">zpool-initialize</a></u>(8).

       <b>zfs_initialize_chunk_size</b>=<b>1048576</b>B (1 MiB) (u64)
               Size of writes used by <u><a href="../man8/zpool-initialize.8.html">zpool-initialize</a></u>(8).  This option is used by the test suite.

       <b>zfs_livelist_max_entries</b>=<b>500000</b> (5*10^5) (u64)
               The threshold size (in block pointers) at which we create a new  sub-livelist.   Larger  sublists
               are more costly from a memory perspective but the fewer sublists there are, the lower the cost of
               insertion.

       <b>zfs_livelist_min_percent_shared</b>=<b>75</b>% (int)
               If  the  amount  of shared space between a snapshot and its clone drops below this threshold, the
               clone turns off the livelist and reverts to the old deletion method.  This is  in  place  because
               livelists no long give us a benefit once a clone has been overwritten enough.

       <b>zfs_livelist_condense_new_alloc</b>=<b>0</b> (int)
               Incremented  each  time  an  extra  ALLOC  blkptr  is added to a livelist entry while it is being
               condensed.  This option is used by the test suite to track race conditions.

       <b>zfs_livelist_condense_sync_cancel</b>=<b>0</b> (int)
               Incremented each time livelist condensing  is  canceled  while  in  <b>spa_livelist_condense_sync</b>().
               This option is used by the test suite to track race conditions.

       <b>zfs_livelist_condense_sync_pause</b>=<b>0</b>|1 (int)
               When  set,  the  livelist  condense  process  pauses indefinitely before executing the synctask —
               <b>spa_livelist_condense_sync</b>().  This option is used by the test suite to trigger race conditions.

       <b>zfs_livelist_condense_zthr_cancel</b>=<b>0</b> (int)
               Incremented each time livelist condensing is canceled while in <b>spa_livelist_condense_cb</b>().   This
               option is used by the test suite to track race conditions.

       <b>zfs_livelist_condense_zthr_pause</b>=<b>0</b>|1 (int)
               When  set,  the  livelist  condense process pauses indefinitely before executing the open context
               condensing work in <b>spa_livelist_condense_cb</b>().  This option is used by the test suite to  trigger
               race conditions.

       <b>zfs_lua_max_instrlimit</b>=<b>100000000</b> (10^8) (u64)
               The maximum execution time limit that can be set for a ZFS channel program, specified as a number
               of Lua instructions.

       <b>zfs_lua_max_memlimit</b>=<b>104857600</b> (100 MiB) (u64)
               The maximum memory limit that can be set for a ZFS channel program, specified in bytes.

       <b>zfs_max_dataset_nesting</b>=<b>50</b> (int)
               The  maximum  depth  of  nested  datasets.   This  value can be tuned temporarily to fix existing
               datasets that exceed the predefined limit.

       <b>zfs_max_log_walking</b>=<b>5</b> (u64)
               The number of past TXGs that the flushing algorithm of the log spacemap feature uses to  estimate
               incoming log blocks.

       <b>zfs_max_logsm_summary_length</b>=<b>10</b> (u64)
               Maximum number of rows allowed in the summary of the spacemap log.

       <b>zfs_max_recordsize</b>=<b>16777216</b> (16 MiB) (uint)
               We  currently  support block sizes from <u>512</u> (512 B) to <u>16777216</u> (16 MiB).  The benefits of larger
               blocks, and thus larger I/O, need to be weighed against the cost  of  COWing  a  giant  block  to
               modify  one  byte.   Additionally,  very large blocks can have an impact on I/O latency, and also
               potentially on the memory allocator.  Therefore, we formerly forbade creating blocks larger  than
               1M.   Larger  blocks  could be created by changing it, and pools with larger blocks can always be
               imported and used, regardless of this setting.

               Note that it is still limited by default to <u>1</u> <u>MiB</u> on x86_32, because  Linux's  3/1  memory  split
               doesn't leave much room for 16M chunks.

       <b>zfs_allow_redacted_dataset_mount</b>=<b>0</b>|1 (int)
               Allow  datasets  received  with  redacted  send/receive to be mounted.  Normally disabled because
               these datasets may be missing key data.

       <b>zfs_min_metaslabs_to_flush</b>=<b>1</b> (u64)
               Minimum number of metaslabs to flush per dirty TXG.

       <b>zfs_metaslab_fragmentation_threshold</b>=<b>77</b>% (uint)
               Allow metaslabs to keep their active state as long as their fragmentation percentage is  no  more
               than  this  value.  An active metaslab that exceeds this threshold will no longer keep its active
               status allowing better metaslabs to be selected.

       <b>zfs_mg_fragmentation_threshold</b>=<b>95</b>% (uint)
               Metaslab groups are considered eligible for allocations if their fragmentation  metric  (measured
               as a percentage) is less than or equal to this value.  If a metaslab group exceeds this threshold
               then  it  will  be skipped unless all metaslab groups within the metaslab class have also crossed
               this threshold.

       <b>zfs_mg_noalloc_threshold</b>=<b>0</b>% (uint)
               Defines a threshold at which metaslab groups should be eligible for allocations.   The  value  is
               expressed  as  a  percentage  of  free space beyond which a metaslab group is always eligible for
               allocations.  If a metaslab group's free space is less  than  or  equal  to  the  threshold,  the
               allocator  will  avoid  allocating  to  that group unless all groups in the pool have reached the
               threshold.  Once all groups have  reached  the  threshold,  all  groups  are  allowed  to  accept
               allocations.   The  default  value of <b>0</b> disables the feature and causes all metaslab groups to be
               eligible for allocations.

               This parameter allows one to deal with pools having heavily imbalanced vdevs such as would be the
               case when a new vdev has been added.  Setting the threshold to a non-zero  percentage  will  stop
               allocations  from  being  made  to vdevs that aren't filled to the specified percentage and allow
               lesser filled vdevs to  acquire  more  allocations  than  they  otherwise  would  under  the  old
               <b>zfs_mg_alloc_failures</b> facility.

       <b>zfs_ddt_data_is_special</b>=<b>1</b>|0 (int)
               If enabled, ZFS will place DDT data into the special allocation class.

       <b>zfs_user_indirect_is_special</b>=<b>1</b>|0 (int)
               If enabled, ZFS will place user data indirect blocks into the special allocation class.

       <b>zfs_multihost_history</b>=<b>0</b> (uint)
               Historical   statistics   for   this   many   latest  multihost  updates  will  be  available  in
               <u>/proc/spl/kstat/zfs/</u>⟨<u>pool</u>⟩<u>/multihost</u>.

       <b>zfs_multihost_interval</b>=<b>1000</b>ms (1 s) (u64)
               Used to control the frequency of multihost writes which are performed  when  the  <b>multihost</b>  pool
               property  is  on.   This is one of the factors used to determine the length of the activity check
               during import.

               The multihost write period is <b>zfs_multihost_interval</b> / <b>leaf-vdevs</b>.  On average a multihost  write
               will  be  issued  for each leaf vdev every <b>zfs_multihost_interval</b> milliseconds.  In practice, the
               observed period can vary with the I/O load and this observed value is the delay which  is  stored
               in the uberblock.

       <b>zfs_multihost_import_intervals</b>=<b>20</b> (uint)
               Used   to   control   the   duration   of  the  activity  test  on  import.   Smaller  values  of
               <b>zfs_multihost_import_intervals</b> will reduce the import time but increase the risk  of  failing  to
               detect an active pool.  The total activity check time is never allowed to drop below one second.

               On  import the activity check waits a minimum amount of time determined by <b>zfs_multihost_interval</b>
               × <b>zfs_multihost_import_intervals</b>, or the same product computed on the host  which  last  had  the
               pool  imported,  whichever  is  greater.   The activity check time may be further extended if the
               value of MMP delay found in the best uberblock indicates actual  multihost  updates  happened  at
               longer intervals than <b>zfs_multihost_interval</b>.  A minimum of <u>100</u> <u>ms</u> is enforced.

               <b>0</b> is equivalent to <b>1</b>.

       <b>zfs_multihost_fail_intervals</b>=<b>10</b> (uint)
               Controls the behavior of the pool when multihost write failures or delays are detected.

               When  <b>0</b>,  multihost write failures or delays are ignored.  The failures will still be reported to
               the ZED which depending on its configuration may take action  such  as  suspending  the  pool  or
               offlining a device.

               Otherwise,  the  pool  will be suspended if <b>zfs_multihost_fail_intervals</b> × <b>zfs_multihost_interval</b>
               milliseconds pass without a successful MMP write.  This guarantees the activity test will see MMP
               writes if the pool is imported.  <b>1</b> is equivalent to <b>2</b>; this is necessary to prevent the pool from
               being suspended due to normal, small I/O latency variations.

       <b>zfs_no_scrub_io</b>=<b>0</b>|1 (int)
               Set to disable scrub I/O.  This results in scrubs not actually scrubbing data and simply doing  a
               metadata crawl of the pool instead.

       <b>zfs_no_scrub_prefetch</b>=<b>0</b>|1 (int)
               Set to disable block prefetching for scrubs.

       <b>zfs_nocacheflush</b>=<b>0</b>|1 (int)
               Disable cache flush operations on disks when writing.  Setting this will cause pool corruption on
               power loss if a volatile out-of-order write cache is enabled.

       <b>zfs_nopwrite_enabled</b>=<b>1</b>|0 (int)
               Allow  no-operation  writes.   The  occurrence  of  nopwrites  will  further depend on other pool
               properties (i.a. the checksumming and compression algorithms).

       <b>zfs_dmu_offset_next_sync</b>=<b>1</b>|0 (int)
               Enable forcing TXG sync to find holes.  When enabled forces ZFS to sync data  when  <b>SEEK_HOLE</b>  or
               <b>SEEK_DATA</b> flags are used allowing holes in a file to be accurately reported.  When disabled holes
               will not be reported in recently dirtied files.

       <b>zfs_pd_bytes_max</b>=<b>52428800</b>B (50 MiB) (int)
               The  number  of  bytes which should be prefetched during a pool traversal, like <b>zfs</b> <b>send</b> or other
               data crawling operations.

       <b>zfs_traverse_indirect_prefetch_limit</b>=<b>32</b> (uint)
               The number of blocks pointed by indirect (non-L0) block which should be prefetched during a  pool
               traversal, like <b>zfs</b> <b>send</b> or other data crawling operations.

       <b>zfs_per_txg_dirty_frees_percent</b>=<b>30</b>% (u64)
               Control  percentage  of  dirtied  indirect  blocks  from  frees allowed into one TXG.  After this
               threshold is crossed, additional frees will wait until the next TXG.  <b>0</b> disables this throttle.

       <b>zfs_prefetch_disable</b>=<b>0</b>|1 (int)
               Disable predictive prefetch.  Note that it leaves "prescient"  prefetch  (for,  e.g.,  <b>zfs</b>  <b>send</b>)
               intact.   Unlike  predictive prefetch, prescient prefetch never issues I/O that ends up not being
               needed, so it can't hurt performance.

       <b>zfs_qat_checksum_disable</b>=<b>0</b>|1 (int)
               Disable QAT hardware acceleration for SHA256 checksums.  May be unset after the ZFS modules  have
               been  loaded  to initialize the QAT hardware as long as support is compiled in and the QAT driver
               is present.

       <b>zfs_qat_compress_disable</b>=<b>0</b>|1 (int)
               Disable QAT hardware acceleration for gzip compression.  May be unset after the ZFS modules  have
               been  loaded  to initialize the QAT hardware as long as support is compiled in and the QAT driver
               is present.

       <b>zfs_qat_encrypt_disable</b>=<b>0</b>|1 (int)
               Disable QAT hardware acceleration for AES-GCM encryption.  May be unset  after  the  ZFS  modules
               have  been  loaded  to  initialize the QAT hardware as long as support is compiled in and the QAT
               driver is present.

       <b>zfs_vnops_read_chunk_size</b>=<b>1048576</b>B (1 MiB) (u64)
               Bytes to read per chunk.

       <b>zfs_read_history</b>=<b>0</b> (uint)
               Historical   statistics   for    this    many    latest    reads    will    be    available    in
               <u>/proc/spl/kstat/zfs/</u>⟨<u>pool</u>⟩<u>/reads</u>.

       <b>zfs_read_history_hits</b>=<b>0</b>|1 (int)
               Include cache hits in read history

       <b>zfs_rebuild_max_segment</b>=<b>1048576</b>B (1 MiB) (u64)
               Maximum read segment size to issue when sequentially resilvering a top-level vdev.

       <b>zfs_rebuild_scrub_enabled</b>=<b>1</b>|0 (int)
               Automatically  start  a pool scrub when the last active sequential resilver completes in order to
               verify the checksums of all blocks which have been resilvered.  This is enabled  by  default  and
               strongly recommended.

       <b>zfs_rebuild_vdev_limit</b>=<b>67108864</b>B (64 MiB) (u64)
               Maximum  amount of I/O that can be concurrently issued for a sequential resilver per leaf device,
               given in bytes.

       <b>zfs_reconstruct_indirect_combinations_max</b>=<b>4096</b> (int)
               If an indirect split block contains more than this many possible unique combinations  when  being
               reconstructed, consider it too computationally expensive to check them all.  Instead, try at most
               this  many  randomly  selected  combinations  each  time  the block is accessed.  This allows all
               segment copies to participate fairly in  the  reconstruction  when  all  combinations  cannot  be
               checked and prevents repeated use of one bad copy.

       <b>zfs_recover</b>=<b>0</b>|1 (int)
               Set  to  attempt  to recover from fatal errors.  This should only be used as a last resort, as it
               typically results in leaked space, or worse.

       <b>zfs_removal_ignore_errors</b>=<b>0</b>|1 (int)
               Ignore hard I/O errors during device removal.  When set, if a device encounters a hard I/O  error
               during  the  removal  process  the  removal will not be cancelled.  This can result in a normally
               recoverable block becoming permanently damaged and is hence not recommended.  This should only be
               used as a last resort when the pool cannot be returned to a healthy state prior to  removing  the
               device.

       <b>zfs_removal_suspend_progress</b>=<b>0</b>|1 (uint)
               This  is  used  by  the test suite so that it can ensure that certain actions happen while in the
               middle of a removal.

       <b>zfs_remove_max_segment</b>=<b>16777216</b>B (16 MiB) (uint)
               The largest contiguous segment that we will attempt to allocate when removing a device.  If there
               is a performance problem with attempting to allocate large blocks, consider decreasing this.  The
               default value is also the maximum.

       <b>zfs_resilver_disable_defer</b>=<b>0</b>|1 (int)
               Ignore the  <b>resilver_defer</b>  feature,  causing  an  operation  that  would  start  a  resilver  to
               immediately restart the one in progress.

       <b>zfs_resilver_defer_percent</b>=<b>10</b>% (uint)
               If  the  ongoing  resilver  progress  is  below  this threshold, a new resilver will restart from
               scratch instead of being deferred after the current one  finishes,  even  if  the  <b>resilver_defer</b>
               feature is enabled.

       <b>zfs_resilver_min_time_ms</b>=<b>3000</b>ms (3 s) (uint)
               Resilvers  are processed by the sync thread.  While resilvering, it will spend at least this much
               time working on a resilver between TXG flushes.

       <b>zfs_scan_ignore_errors</b>=<b>0</b>|1 (int)
               If set, remove the DTL (dirty time list) upon completion of a pool scan (scrub),  even  if  there
               were unrepairable errors.  Intended to be used during pool repair or recovery to stop resilvering
               when the pool is next imported.

       <b>zfs_scrub_after_expand</b>=<b>1</b>|0 (int)
               Automatically  start  a  pool  scrub  after  a  RAIDZ  expansion completes in order to verify the
               checksums of all blocks which have been copied during the expansion.  This is enabled by  default
               and strongly recommended.

       <b>zfs_scrub_min_time_ms</b>=<b>1000</b>ms (1 s) (uint)
               Scrubs  are processed by the sync thread.  While scrubbing, it will spend at least this much time
               working on a scrub between TXG flushes.

       <b>zfs_scrub_error_blocks_per_txg</b>=<b>4096</b> (uint)
               Error blocks to be scrubbed in one txg.

       <b>zfs_scan_checkpoint_intval</b>=<b>7200</b>s (2 hour) (uint)
               To preserve progress across reboots, the sequential scan algorithm  periodically  needs  to  stop
               metadata  scanning and issue all the verification I/O to disk.  The frequency of this flushing is
               determined by this tunable.

       <b>zfs_scan_fill_weight</b>=<b>3</b> (uint)
               This tunable affects how scrub and resilver I/O segments are ordered.  A higher number  indicates
               that  we  care more about how filled in a segment is, while a lower number indicates we care more
               about the size of the extent without considering the gaps within a segment.  This value  is  only
               tunable  upon  module  insertion.   Changing the value afterwards will have no effect on scrub or
               resilver performance.

       <b>zfs_scan_issue_strategy</b>=<b>0</b> (uint)
               Determines the order that data will be verified while scrubbing or resilvering:
                   <b>1</b>  Data will be verified as sequentially as possible, given the amount of memory reserved for
                      scrubbing (see <b>zfs_scan_mem_lim_fact</b>).  This may improve scrub performance if  the  pool's
                      data is very fragmented.
                   <b>2</b>  The  largest  mostly-contiguous  chunk of found data will be verified first.  By deferring
                      scrubbing of small segments, we may later find adjacent data to coalesce and increase  the
                      segment size.
                   <b>0</b>  Use strategy <b>1</b> during normal verification and strategy <b>2</b> while taking a checkpoint.

       <b>zfs_scan_legacy</b>=<b>0</b>|1 (int)
               If  unset,  indicates  that  scrubs  and  resilvers will gather metadata in memory before issuing
               sequential I/O.  Otherwise indicates that the  legacy  algorithm  will  be  used,  where  I/O  is
               initiated  as  soon  as it is discovered.  Unsetting will not affect scrubs or resilvers that are
               already in progress.

       <b>zfs_scan_max_ext_gap</b>=<b>2097152</b>B (2 MiB) (int)
               Sets the largest gap in bytes between scrub/resilver I/O operations that will still be considered
               sequential for sorting purposes.  Changing this value will not affect scrubs  or  resilvers  that
               are already in progress.

       <b>zfs_scan_mem_lim_fact</b>=<b>20</b>^-1 (uint)
               Maximum  fraction  of  RAM  used  for  I/O  sorting  by  sequential scan algorithm.  This tunable
               determines the hard limit for I/O sorting memory usage.  When the hard limit is reached  we  stop
               scanning  metadata  and start issuing data verification I/O.  This is done until we get below the
               soft limit.

       <b>zfs_scan_mem_lim_soft_fact</b>=<b>20</b>^-1 (uint)
               The fraction of the hard limit used  to  determined  the  soft  limit  for  I/O  sorting  by  the
               sequential  scan  algorithm.   When  we  cross this limit from below no action is taken.  When we
               cross this limit from above it is because we are issuing verification I/O.  In this case  (unless
               the  metadata  scan  is  done) we stop issuing verification I/O and start scanning metadata again
               until we get to the hard limit.

       <b>zfs_scan_report_txgs</b>=<b>0</b>|1 (uint)
               When reporting resilver throughput and estimated completion time  use  the  performance  observed
               over roughly the last <b>zfs_scan_report_txgs</b> TXGs.  When set to zero performance is calculated over
               the time between checkpoints.

       <b>zfs_scan_strict_mem_lim</b>=<b>0</b>|1 (int)
               Enforce  tight memory limits on pool scans when a sequential scan is in progress.  When disabled,
               the memory limit may be exceeded by fast disks.

       <b>zfs_scan_suspend_progress</b>=<b>0</b>|1 (int)
               Freezes  a  scrub/resilver   in   progress   without   actually   pausing   it.    Intended   for
               testing/debugging.

       <b>zfs_scan_vdev_limit</b>=<b>16777216</b>B (16 MiB) (int)
               Maximum  amount of data that can be concurrently issued at once for scrubs and resilvers per leaf
               device, given in bytes.

       <b>zfs_send_corrupt_data</b>=<b>0</b>|1 (int)
               Allow sending of corrupt data (ignore read/checksum errors when sending).

       <b>zfs_send_unmodified_spill_blocks</b>=<b>1</b>|0 (int)
               Include unmodified spill blocks in  the  send  stream.   Under  certain  circumstances,  previous
               versions  of  ZFS  could  incorrectly  remove the spill block from an existing object.  Including
               unmodified copies of the spill blocks creates a backwards-compatible stream which will recreate a
               spill block if it was incorrectly removed.

       <b>zfs_send_no_prefetch_queue_ff</b>=<b>20</b>^-1 (uint)
               The fill fraction of the <b>zfs</b> <b>send</b> internal queues.  The fill fraction controls  the  timing  with
               which internal threads are woken up.

       <b>zfs_send_no_prefetch_queue_length</b>=<b>1048576</b>B (1 MiB) (uint)
               The maximum number of bytes allowed in <b>zfs</b> <b>send</b>'s internal queues.

       <b>zfs_send_queue_ff</b>=<b>20</b>^-1 (uint)
               The  fill  fraction  of  the <b>zfs</b> <b>send</b> prefetch queue.  The fill fraction controls the timing with
               which internal threads are woken up.

       <b>zfs_send_queue_length</b>=<b>16777216</b>B (16 MiB) (uint)
               The maximum number of bytes allowed that will be prefetched by <b>zfs</b> <b>send</b>.  This value must  be  at
               least twice the maximum block size in use.

       <b>zfs_recv_queue_ff</b>=<b>20</b>^-1 (uint)
               The  fill  fraction  of  the <b>zfs</b> <b>receive</b> queue.  The fill fraction controls the timing with which
               internal threads are woken up.

       <b>zfs_recv_queue_length</b>=<b>16777216</b>B (16 MiB) (uint)
               The maximum number of bytes allowed in the <b>zfs</b> <b>receive</b> queue.  This value must be at least  twice
               the maximum block size in use.

       <b>zfs_recv_write_batch_size</b>=<b>1048576</b>B (1 MiB) (uint)
               The  maximum  amount of data, in bytes, that <b>zfs</b> <b>receive</b> will write in one DMU transaction.  This
               is the uncompressed size, even when receiving a compressed send stream.  This  setting  will  not
               reduce the write size below a single block.  Capped at a maximum of <b>32</b> <b>MiB</b>.

       <b>zfs_recv_best_effort_corrective</b>=<b>0</b> (int)
               When this variable is set to non-zero a corrective receive:
                   1. Does not enforce the restriction of source &amp; destination snapshot GUIDs matching.
                   2.  If  there  is  an  error during healing, the healing receive is not terminated instead it
                     moves on to the next record.

       <b>zfs_override_estimate_recordsize</b>=<b>0</b>|1 (uint)
               Setting this variable overrides the default logic for estimating block sizes  when  doing  a  <b>zfs</b>
               <b>send</b>.   The  default  heuristic  is  that  the average block size will be the current recordsize.
               Override this value if most data in your dataset is not of that size and you require accurate zfs
               send size estimates.

       <b>zfs_sync_pass_deferred_free</b>=<b>2</b> (uint)
               Flushing of data to disk is done in passes.  Defer frees starting in this pass.

       <b>zfs_spa_discard_memory_limit</b>=<b>16777216</b>B (16 MiB) (int)
               Maximum memory used for prefetching a checkpoint's space map on each vdev  while  discarding  the
               checkpoint.

       <b>zfs_special_class_metadata_reserve_pct</b>=<b>25</b>% (uint)
               Only  allow  small  data  blocks  to  be  allocated  on the special and dedup vdev types when the
               available free space percentage on these vdevs exceeds this value.  This ensures  reserved  space
               is available for pool metadata as the special vdevs approach capacity.

       <b>zfs_sync_pass_dont_compress</b>=<b>8</b> (uint)
               Starting  in  this  sync  pass,  disable  compression  (including of metadata).  With the default
               setting, in practice, we don't have this many sync passes, so this has no effect.

               The original intent was that disabling compression  would  help  the  sync  passes  to  converge.
               However,  in practice, disabling compression increases the average number of sync passes; because
               when we turn compression off, many blocks' size will change, and thus we have to re-allocate (not
               overwrite) them.  It also increases the number of <u>128</u> <u>KiB</u> allocations (e.g. for  indirect  blocks
               and  spacemaps)  because  these  will  not be compressed.  The <u>128</u> <u>KiB</u> allocations are especially
               detrimental to performance on highly fragmented systems, which may have very few free segments of
               this size, and may need to load new metaslabs to satisfy these allocations.

       <b>zfs_sync_pass_rewrite</b>=<b>2</b> (uint)
               Rewrite new block pointers starting in this pass.

       <b>zfs_trim_extent_bytes_max</b>=<b>134217728</b>B (128 MiB) (uint)
               Maximum size of TRIM command.  Larger ranges will be split into chunks no larger than this  value
               before issuing.

       <b>zfs_trim_extent_bytes_min</b>=<b>32768</b>B (32 KiB) (uint)
               Minimum  size  of  TRIM  commands.  TRIM ranges smaller than this will be skipped, unless they're
               part of a larger range which was chunked.  This is done because it's common for these small TRIMs
               to negatively impact overall performance.

       <b>zfs_trim_metaslab_skip</b>=<b>0</b>|1 (uint)
               Skip uninitialized  metaslabs  during  the  TRIM  process.   This  option  is  useful  for  pools
               constructed  from  large  thinly-provisioned  devices  where TRIM operations are slow.  As a pool
               ages, an increasing fraction of the pool's metaslabs will be initialized, progressively degrading
               the usefulness of this option.  This setting is stored when  starting  a  manual  TRIM  and  will
               persist for the duration of the requested TRIM.

       <b>zfs_trim_queue_limit</b>=<b>10</b> (uint)
               Maximum number of queued TRIMs outstanding per leaf vdev.  The number of concurrent TRIM commands
               issued to the device is controlled by <b>zfs_vdev_trim_min_active</b> and <b>zfs_vdev_trim_max_active</b>.

       <b>zfs_trim_txg_batch</b>=<b>32</b> (uint)
               The  number  of  transaction  groups'  worth  of  frees  which  should  be aggregated before TRIM
               operations are issued to the device.  This setting represents a trade-off between issuing larger,
               more efficient TRIM operations and the delay before the recently trimmed space is  available  for
               use by the device.

               Increasing  this  value will allow frees to be aggregated for a longer time.  This will result is
               larger TRIM operations and potentially increased memory usage.  Decreasing this value  will  have
               the opposite effect.  The default of <b>32</b> was determined to be a reasonable compromise.

       <b>zfs_txg_history</b>=<b>100</b> (uint)
               Historical    statistics    for    this    many    latest    TXGs    will    be    available   in
               <u>/proc/spl/kstat/zfs/</u>⟨<u>pool</u>⟩<u>/TXGs</u>.

       <b>zfs_txg_timeout</b>=<b>5</b>s (uint)
               Flush dirty data to disk at least every this many seconds (maximum TXG duration).

       <b>zfs_vdev_aggregation_limit</b>=<b>1048576</b>B (1 MiB) (uint)
               Max vdev I/O aggregation size.

       <b>zfs_vdev_aggregation_limit_non_rotating</b>=<b>131072</b>B (128 KiB) (uint)
               Max vdev I/O aggregation size for non-rotating media.

       <b>zfs_vdev_mirror_rotating_inc</b>=<b>0</b> (int)
               A number by which the balancing algorithm increments the load  calculation  for  the  purpose  of
               selecting  the least busy mirror member when an I/O operation immediately follows its predecessor
               on rotational vdevs for the purpose of making decisions based on load.

       <b>zfs_vdev_mirror_rotating_seek_inc</b>=<b>5</b> (int)
               A number by which the balancing algorithm increments the load  calculation  for  the  purpose  of
               selecting  the  least  busy  mirror  member  when  an  I/O operation lacks locality as defined by
               <b>zfs_vdev_mirror_rotating_seek_offset</b>.  Operations within this that are not immediately  following
               the previous operation are incremented by half.

       <b>zfs_vdev_mirror_rotating_seek_offset</b>=<b>1048576</b>B (1 MiB) (int)
               The maximum distance for the last queued I/O operation in which the balancing algorithm considers
               an operation to have locality.  See “ZFS I/O SCHEDULER”.

       <b>zfs_vdev_mirror_non_rotating_inc</b>=<b>0</b> (int)
               A  number  by  which  the  balancing algorithm increments the load calculation for the purpose of
               selecting the least busy mirror member  on  non-rotational  vdevs  when  I/O  operations  do  not
               immediately follow one another.

       <b>zfs_vdev_mirror_non_rotating_seek_inc</b>=<b>1</b> (int)
               A  number  by  which  the  balancing algorithm increments the load calculation for the purpose of
               selecting the least busy mirror member when an I/O operation lacks locality  as  defined  by  the
               <b>zfs_vdev_mirror_rotating_seek_offset</b>.   Operations within this that are not immediately following
               the previous operation are incremented by half.

       <b>zfs_vdev_read_gap_limit</b>=<b>32768</b>B (32 KiB) (uint)
               Aggregate read I/O operations if the on-disk gap between them is within this threshold.

       <b>zfs_vdev_write_gap_limit</b>=<b>4096</b>B (4 KiB) (uint)
               Aggregate write I/O operations if the on-disk gap between them is within this threshold.

       <b>zfs_vdev_raidz_impl</b>=<b>fastest</b> (string)
               Select the raidz parity implementation to use.

               Variants that don't depend on CPU-specific features may be selected on module load, as  they  are
               supported  on  all systems.  The remaining options may only be set after the module is loaded, as
               they are available only if the implementations are compiled  in  and  supported  on  the  running
               system.

               Once the module is loaded, <u>/sys/module/zfs/parameters/zfs_vdev_raidz_impl</u> will show the available
               options, with the currently selected one enclosed in square brackets.

               <b>fastest</b>           selected by built-in benchmark
               <b>original</b>          original implementation
               <b>scalar</b>            scalar implementation
               <b>sse2</b>              SSE2 instruction set                  64-bit x86
               <b>ssse3</b>             SSSE3 instruction set                 64-bit x86
               <b>avx2</b>              AVX2 instruction set                  64-bit x86
               <b>avx512f</b>           AVX512F instruction set               64-bit x86
               <b>avx512bw</b>          AVX512F &amp; AVX512BW instruction sets   64-bit x86
               <b>aarch64_neon</b>      NEON                                  Aarch64/64-bit ARMv8
               <b>aarch64_neonx2</b>    NEON with more unrolling              Aarch64/64-bit ARMv8
               <b>powerpc_altivec</b>   Altivec                               PowerPC

       <b>zfs_vdev_scheduler</b> (charp)
               <b>DEPRECATED</b>.  Prints warning to kernel log for compatibility.

       <b>zfs_zevent_len_max</b>=<b>512</b> (uint)
               Max event queue length.  Events in the queue can be viewed with <u><a href="../man8/zpool-events.8.html">zpool-events</a></u>(8).

       <b>zfs_zevent_retain_max</b>=<b>2000</b> (int)
               Maximum  recent  zevent  records  to  retain  for duplicate checking.  Setting this to <b>0</b> disables
               duplicate detection.

       <b>zfs_zevent_retain_expire_secs</b>=<b>900</b>s (15 min) (int)
               Lifespan for a recent ereport that was retained for duplicate checking.

       <b>zfs_zil_clean_taskq_maxalloc</b>=<b>1048576</b> (int)
               The maximum number of taskq entries that are allowed to be cached.  When this limit  is  exceeded
               transaction records (itxs) will be cleaned synchronously.

       <b>zfs_zil_clean_taskq_minalloc</b>=<b>1024</b> (int)
               The  number  of  taskq  entries  that  are  pre-populated when the taskq is first created and are
               immediately available for use.

       <b>zfs_zil_clean_taskq_nthr_pct</b>=<b>100</b>% (int)
               This controls the number of threads used by <b>dp_zil_clean_taskq</b>.  The default value of  <b>100%</b>  will
               create a maximum of one thread per cpu.

       <b>zil_maxblocksize</b>=<b>131072</b>B (128 KiB) (uint)
               This  sets  the  maximum  block  size  used  by the ZIL.  On very fragmented pools, lowering this
               (typically to <b>36</b> <b>KiB</b>) can improve performance.

       <b>zil_maxcopied</b>=<b>7680</b>B (7.5 KiB) (uint)
               This sets the maximum number of write bytes logged via WR_COPIED.  It tunes  a  tradeoff  between
               additional memory copy and possibly worse log space efficiency vs additional range lock/unlock.

       <b>zil_nocacheflush</b>=<b>0</b>|1 (int)
               Disable the cache flush commands that are normally sent to disk by the ZIL after an LWB write has
               completed.  Setting this will cause ZIL corruption on power loss if a volatile out-of-order write
               cache is enabled.

       <b>zil_replay_disable</b>=<b>0</b>|1 (int)
               Disable intent logging replay.  Can be disabled for recovery from corrupted ZIL.

       <b>zil_slog_bulk</b>=<b>67108864</b>B (64 MiB) (u64)
               Limit  SLOG write size per commit executed with synchronous priority.  Any writes above that will
               be executed with lower (asynchronous) priority to limit potential SLOG  device  abuse  by  single
               active ZIL writer.

       <b>zfs_zil_saxattr</b>=<b>1</b>|0 (int)
               Setting   this   tunable   to   zero  disables  ZIL  logging  of  new  <b>xattr</b>=<b>sa</b>  records  if  the
               <b>org.openzfs:zilsaxattr</b> feature is enabled on the pool.  This would  only  be  necessary  to  work
               around bugs in the ZIL logging or replay code for this record type.  The tunable has no effect if
               the feature is disabled.

       <b>zfs_embedded_slog_min_ms</b>=<b>64</b> (uint)
               Usually,  one  metaslab  from  each  normal-class  vdev  is  dedicated  for use by the ZIL to log
               synchronous writes.  However, if there are fewer than <b>zfs_embedded_slog_min_ms</b> metaslabs  in  the
               vdev,  this  functionality  is  disabled.   This  ensures that we don't set aside an unreasonable
               amount of space for the ZIL.

       <b>zstd_earlyabort_pass</b>=<b>1</b> (uint)
               Whether heuristic for detection of incompressible data with zstd levels &gt;= 3 using LZ4 and zstd-1
               passes is enabled.

       <b>zstd_abort_size</b>=<b>131072</b> (uint)
               Minimal uncompressed size (inclusive) of a record  before  the  early  abort  heuristic  will  be
               attempted.

       <b>zio_deadman_log_all</b>=<b>0</b>|1 (int)
               If  non-zero,  the  zio  deadman  will produce debugging messages (see <b>zfs_dbgmsg_enable</b>) for all
               zios, rather than only for leaf zios possessing a vdev.  This is meant to be used  by  developers
               to  gain  diagnostic information for hang conditions which don't involve a mutex or other locking
               primitive: typically conditions in which a thread in the zio pipeline is looping indefinitely.

       <b>zio_slow_io_ms</b>=<b>30000</b>ms (30 s) (int)
               When an I/O operation takes more than this much time to complete, it's marked as slow.  Each slow
               operation causes a delay zevent.  Slow I/O counters can be seen with <b>zpool</b> <b>status</b> <b>-s</b>.

       <b>zio_dva_throttle_enabled</b>=<b>1</b>|0 (int)
               Throttle block allocations in the I/O pipeline.  This allows for dynamic allocation  distribution
               when  devices  are  imbalanced.  When enabled, the maximum number of pending allocations per top-
               level vdev is limited by <b>zfs_vdev_queue_depth_pct</b>.

       <b>zfs_xattr_compat</b>=0|1 (int)
               Control the naming scheme used when setting new xattrs in the user namespace.  If <b>0</b> (the  default
               on Linux), user namespace xattr names are prefixed with the namespace, to be backwards compatible
               with  previous  versions  of  ZFS  on Linux.  If <b>1</b> (the default on FreeBSD), user namespace xattr
               names are not prefixed, to be backwards compatible with previous versions of ZFS on  illumos  and
               FreeBSD.

               Either  naming scheme can be read on this and future versions of ZFS, regardless of this tunable,
               but legacy ZFS on illumos or FreeBSD are unable to read user  namespace  xattrs  written  in  the
               Linux  format,  and  legacy  versions  of  ZFS  on Linux are unable to read user namespace xattrs
               written in the legacy ZFS format.

               An existing xattr with the alternate naming scheme is removed when overwriting the xattr so as to
               not accumulate duplicates.

       <b>zio_requeue_io_start_cut_in_line</b>=<b>0</b>|1 (int)
               Prioritize requeued I/O.

       <b>zio_taskq_batch_pct</b>=<b>80</b>% (uint)
               Percentage of online CPUs which will run a worker thread for I/O.  These workers are  responsible
               for  I/O  work  such  as  compression,  encryption, checksum and parity calculations.  Fractional
               number of CPUs will be rounded down.

               The default value of <b>80%</b> was chosen to avoid using all CPUs which can result  in  latency  issues
               and  inconsistent application performance, especially when slower compression and/or checksumming
               is enabled.  Set value only applies to pools imported/created after that.

       <b>zio_taskq_batch_tpq</b>=<b>0</b> (uint)
               Number of worker threads per taskq.  Higher values improve  I/O  ordering  and  CPU  utilization,
               while lower reduce lock contention.  Set value only applies to pools imported/created after that.

               If  <b>0</b>, generate a system-dependent value close to 6 threads per taskq.  Set value only applies to
               pools imported/created after that.

       <b>zio_taskq_write_tpq</b>=<b>16</b> (uint)
               Determines the minimum number of threads per  write  issue  taskq.   Higher  values  improve  CPU
               utilization  on  high  throughput,  while  lower reduce taskq locks contention on high IOPS.  Set
               value only applies to pools imported/created after that.

       <b>zio_taskq_read</b>=<b>fixed,1,8</b> <b>null</b> <b>scale</b> <b>null</b> (charp)
               Set the queue and thread configuration for the IO read queues.  This  is  an  advanced  debugging
               parameter.  Don't change this unless you understand what it does.  Set values only apply to pools
               imported/created after that.

       <b>zio_taskq_write</b>=<b>sync</b> <b>null</b> <b>scale</b> <b>null</b> (charp)
               Set  the  queue  and thread configuration for the IO write queues.  This is an advanced debugging
               parameter.  Don't change this unless you understand what it does.  Set values only apply to pools
               imported/created after that.

       <b>zvol_inhibit_dev</b>=<b>0</b>|1 (uint)
               Do not create zvol device nodes.  This may slightly improve startup time on systems with  a  very
               large number of zvols.

       <b>zvol_major</b>=<b>230</b> (uint)
               Major number for zvol block devices.

       <b>zvol_max_discard_blocks</b>=<b>16384</b> (long)
               Discard  (TRIM) operations done on zvols will be done in batches of this many blocks, where block
               size is determined by the <b>volblocksize</b> property of a zvol.

       <b>zvol_prefetch_bytes</b>=<b>131072</b>B (128 KiB) (uint)
               When adding a zvol to the system, prefetch this many bytes from the start and end of the  volume.
               Prefetching  these  regions  of  the  volume is desirable, because they are likely to be accessed
               immediately by <u><a href="../man8/blkid.8.html">blkid</a></u>(8) or the kernel partitioner.

       <b>zvol_request_sync</b>=<b>0</b>|1 (uint)
               When processing I/O requests for a zvol, submit them synchronously.  This effectively limits  the
               queue  depth  to  <u>1</u> for each I/O submitter.  When unset, requests are handled asynchronously by a
               thread pool.  The number  of  requests  which  can  be  handled  concurrently  is  controlled  by
               <b>zvol_threads</b>.   <b>zvol_request_sync</b>  is  ignored  when  running  on  a  kernel  that supports block
               multiqueue (<b>blk-mq</b>).

       <b>zvol_num_taskqs</b>=<b>0</b> (uint)
               Number of zvol taskqs.  If <b>0</b> (the default) then scaling is done internally to  prefer  6  threads
               per taskq.  This only applies on Linux.

       <b>zvol_threads</b>=<b>0</b> (uint)
               The  number of system wide threads to use for processing zvol block IOs.  If <b>0</b> (the default) then
               internally set <b>zvol_threads</b> to the number of CPUs present or 32 (whichever is greater).

       <b>zvol_blk_mq_threads</b>=<b>0</b> (uint)
               The number of threads per zvol to use for queuing IO requests.  This parameter will  only  appear
               if  your  kernel supports <b>blk-mq</b> and is only read and assigned to a zvol at zvol load time.  If <b>0</b>
               (the default) then internally set <b>zvol_blk_mq_threads</b> to the number of CPUs present.

       <b>zvol_use_blk_mq</b>=<b>0</b>|1 (uint)
               Set to <b>1</b> to use the <b>blk-mq</b> API for zvols.  Set to <b>0</b> (the default) to use the  legacy  zvol  APIs.
               This setting can give better or worse zvol performance depending on the workload.  This parameter
               will  only  appear if your kernel supports <b>blk-mq</b> and is only read and assigned to a zvol at zvol
               load time.

       <b>zvol_blk_mq_blocks_per_thread</b>=<b>8</b> (uint)
               If <b>zvol_use_blk_mq</b> is enabled, then process this number of  <b>volblocksize</b>-sized  blocks  per  zvol
               thread.  This  tunable  can  be  use to favor better performance for zvol reads (lower values) or
               writes (higher values).  If set to <b>0</b>, then the zvol layer will  process  the  maximum  number  of
               blocks  per  thread  that it can.  This parameter will only appear if your kernel supports <b>blk-mq</b>
               and is only applied at each zvol's load time.

       <b>zvol_blk_mq_queue_depth</b>=<b>0</b> (uint)
               The queue_depth value for the zvol <b>blk-mq</b> interface.  This parameter will  only  appear  if  your
               kernel supports <b>blk-mq</b> and is only applied at each zvol's load time.  If <b>0</b> (the default) then use
               the  kernel's  default  queue  depth.   Values  are  clamped  to  the  kernel's BLKDEV_MIN_RQ and
               BLKDEV_MAX_RQ/BLKDEV_DEFAULT_RQ limits.

       <b>zvol_volmode</b>=<b>1</b> (uint)
               Defines zvol block devices behaviour when <b>volmode</b>=<b>default</b>:
                   <b>1</b>  equivalent to <b>full</b>
                   <b>2</b>  equivalent to <b>dev</b>
                   <b>3</b>  equivalent to <b>none</b>

       <b>zvol_enforce_quotas</b>=<b>0</b>|1 (uint)
               Enable strict ZVOL quota enforcement.  The  strict  quota  enforcement  may  have  a  performance
               impact.

</pre><h4><b>ZFS</b> <b>I/O</b> <b>SCHEDULER</b></h4><pre>
       ZFS issues I/O operations to leaf vdevs to satisfy and complete I/O operations.  The scheduler determines
       when  and  in  what  order  those  operations are issued.  The scheduler divides operations into five I/O
       classes, prioritized in the following order:  sync  read,  sync  write,  async  read,  async  write,  and
       scrub/resilver.   Each  queue defines the minimum and maximum number of concurrent operations that may be
       issued to the device.  In addition, the device has an aggregate maximum, <b>zfs_vdev_max_active</b>.  Note  that
       the  sum  of  the  per-queue  minima  must not exceed the aggregate maximum.  If the sum of the per-queue
       maxima exceeds the aggregate maximum, then the number of active operations may reach <b>zfs_vdev_max_active</b>,
       in which case no further operations will be issued, regardless of whether all per-queue minima have  been
       met.

       For  many  physical  devices,  throughput increases with the number of concurrent operations, but latency
       typically suffers.  Furthermore, physical devices  typically  have  a  limit  at  which  more  concurrent
       operations have no effect on throughput or can actually cause it to decrease.

       The scheduler selects the next operation to issue by first looking for an I/O class whose minimum has not
       been  satisfied.   Once all are satisfied and the aggregate maximum has not been hit, the scheduler looks
       for classes whose maximum has not been satisfied.  Iteration through the I/O classes is done in the order
       specified above.  No further operations  are  issued  if  the  aggregate  maximum  number  of  concurrent
       operations  has  been  hit,  or  if  there are no operations queued for an I/O class that has not hit its
       maximum.  Every time an I/O operation is queued or an operation completes, the scheduler  looks  for  new
       operations to issue.

       In general, smaller <b>max_active</b>s will lead to lower latency of synchronous operations.  Larger <b>max_active</b>s
       may lead to higher overall throughput, depending on underlying storage.

       The  ratio  of  the  queues' <b>max_active</b>s determines the balance of performance between reads, writes, and
       scrubs.  For example, increasing <b>zfs_vdev_scrub_max_active</b> will cause the scrub or resilver  to  complete
       more quickly, but reads and writes to have higher latency and lower throughput.

       All  I/O classes have a fixed maximum number of outstanding operations, except for the async write class.
       Asynchronous writes represent the data that is committed to stable storage during the syncing  stage  for
       transaction  groups.   Transaction  groups  enter the syncing state periodically, so the number of queued
       async writes will quickly burst up and then bleed down to zero.  Rather than servicing them as quickly as
       possible, the I/O scheduler changes the maximum number of active async write operations according to  the
       amount  of  dirty data in the pool.  Since both throughput and latency typically increase with the number
       of concurrent  operations  issued  to  physical  devices,  reducing  the  burstiness  in  the  number  of
       simultaneous  operations also stabilizes the response time of operations from other queues, in particular
       synchronous ones.  In broad strokes, the I/O scheduler will issue more  concurrent  operations  from  the
       async write queue as there is more dirty data in the pool.

   <b>Async</b> <b>Writes</b>
       The  number  of  concurrent  operations  issued for the async write I/O class follows a piece-wise linear
       function defined by a few adjustable points:

              |              o---------| &lt;-- <b>zfs_vdev_async_write_max_active</b>
         ^    |             /^         |
         |    |            / |         |
       active |           /  |         |
        I/O   |          /   |         |
       count  |         /    |         |
              |        /     |         |
              |-------o      |         | &lt;-- <b>zfs_vdev_async_write_min_active</b>
             0|_______^______|_________|
              0%      |      |       100% of <b>zfs_dirty_data_max</b>
                      |      |
                      |      `-- <b>zfs_vdev_async_write_active_max_dirty_percent</b>
                      `--------- <b>zfs_vdev_async_write_active_min_dirty_percent</b>

       Until the amount of dirty data exceeds a minimum percentage of the dirty data allowed in  the  pool,  the
       I/O  scheduler  will  limit  the  number  of  concurrent operations to the minimum.  As that threshold is
       crossed, the number of concurrent operations issued increases linearly to the maximum  at  the  specified
       maximum percentage of the dirty data allowed in the pool.

       Ideally,  the  amount  of  dirty data on a busy pool will stay in the sloped part of the function between
       <b>zfs_vdev_async_write_active_min_dirty_percent</b> and <b>zfs_vdev_async_write_active_max_dirty_percent</b>.   If  it
       exceeds  the  maximum  percentage, this indicates that the rate of incoming data is greater than the rate
       that the backend storage can handle.  In  this  case,  we  must  further  throttle  incoming  writes,  as
       described in the next section.

</pre><h4><b>ZFS</b> <b>TRANSACTION</b> <b>DELAY</b></h4><pre>
       We  delay  transactions when we've determined that the backend storage isn't able to accommodate the rate
       of incoming writes.

       If there is already a transaction waiting, we  delay  relative  to  when  that  transaction  will  finish
       waiting.   This  way  the  calculated  delay  time  is  independent of the number of threads concurrently
       executing transactions.

       If we are the only waiter, wait relative to when the transaction started, rather than the  current  time.
       This credits the transaction for "time already served", e.g. reading indirect blocks.

       The minimum time for a transaction to take is calculated as
             min_time = min(<b>zfs_delay_scale</b> × (<b>dirty</b> - <b>min</b>) / (<b>max</b> - <b>dirty</b>), 100ms)

       The  delay has two degrees of freedom that can be adjusted via tunables.  The percentage of dirty data at
       which we start to delay is defined by <b>zfs_delay_min_dirty_percent</b>.  This should typically be at or  above
       <b>zfs_vdev_async_write_active_max_dirty_percent</b>, so that we only start to delay after writing at full speed
       has  failed  to  keep  up  with  the  incoming  write  rate.   The  scale  of  the  curve  is  defined by
       <b>zfs_delay_scale</b>.  Roughly speaking, this variable determines the amount of delay at the midpoint  of  the
       curve.

       delay
        10ms +-------------------------------------------------------------*+
             |                                                             *|
         9ms +                                                             *+
             |                                                             *|
         8ms +                                                             *+
             |                                                            * |
         7ms +                                                            * +
             |                                                            * |
         6ms +                                                            * +
             |                                                            * |
         5ms +                                                           *  +
             |                                                           *  |
         4ms +                                                           *  +
             |                                                           *  |
         3ms +                                                          *   +
             |                                                          *   |
         2ms +                                              (midpoint) *    +
             |                                                  |    **     |
         1ms +                                                  v ***       +
             |             <b>zfs_delay_scale</b> ----------&gt;     ********         |
           0 +-------------------------------------*********----------------+
             0%                    &lt;- <b>zfs_dirty_data_max</b> -&gt;               100%

       Note, that since the delay is added to the outstanding time remaining on the most recent transaction it's
       effectively the inverse of IOPS.  Here, the midpoint of <u>500</u> <u>us</u> translates to <u>2000</u> <u>IOPS</u>.  The shape of the
       curve  was  chosen  such  that  small  changes in the amount of accumulated dirty data in the first three
       quarters of the curve yield relatively small differences in the amount of delay.

       The effects can be easier to understand when the amount of delay is represented on a logarithmic scale:

       delay
       100ms +-------------------------------------------------------------++
             +                                                              +
             |                                                              |
             +                                                             *+
        10ms +                                                             *+
             +                                                           ** +
             |                                              (midpoint)  **  |
             +                                                  |     **    +
         1ms +                                                  v ****      +
             +             <b>zfs_delay_scale</b> ----------&gt;        *****         +
             |                                             ****             |
             +                                          ****                +
       100us +                                        **                    +
             +                                       *                      +
             |                                      *                       |
             +                                     *                        +
        10us +                                     *                        +
             +                                                              +
             |                                                              |
             +                                                              +
             +--------------------------------------------------------------+
             0%                    &lt;- <b>zfs_dirty_data_max</b> -&gt;               100%

       Note here that only as the amount of dirty data approaches its limit does the  delay  start  to  increase
       rapidly.   The  goal  of  a  properly tuned system should be to keep the amount of dirty data out of that
       range by first ensuring that the appropriate limits are set  for  the  I/O  scheduler  to  reach  optimal
       throughput  on  the  back-end  storage, and then by changing the value of <b>zfs_delay_scale</b> to increase the
       steepness of the curve.

OpenZFS                                         November 1, 2024                                          <u><a href="../man4/ZFS.4.html">ZFS</a></u>(4)
</pre>
 </div>
</div></section>
</div>
</body>
</html>