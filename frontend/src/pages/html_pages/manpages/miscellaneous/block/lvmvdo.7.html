<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>lvmvdo — Support for Virtual Data Optimizer in LVM</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/lvm2">lvm2_2.03.31-2ubuntu1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       lvmvdo — Support for Virtual Data Optimizer in LVM

</pre><h4><b>DESCRIPTION</b></h4><pre>
       VDO  is  software  that  provides  inline  block-level  deduplication, compression, and thin provisioning
       capabilities for primary storage.

       Deduplication is a technique for reducing the consumption of storage resources  by  eliminating  multiple
       copies  of  duplicate  blocks.  Compression  takes  the individual unique blocks and shrinks them.  These
       reduced blocks are then efficiently packed together into physical blocks. Thin provisioning  manages  the
       mapping  from  logical blocks presented by VDO to where the data has actually been physically stored, and
       also eliminates any blocks of all zeroes.

       With deduplication, instead of writing the same data  more  than  once,  VDO  detects  and  records  each
       duplicate  block  as  a  reference  to  the  original  block.  VDO maintains a mapping from Logical Block
       Addresses (LBA) (used by the storage layer above VDO) to physical block addresses (used  by  the  storage
       layer  under  VDO).  After  deduplication,  multiple  logical  block  addresses may be mapped to the same
       physical block address; these are called shared blocks and are reference-counted by the software.

       With compression, VDO compresses multiple blocks (or shared blocks) with the fast LZ4 algorithm, and bins
       them together where possible so that multiple compressed blocks fit within a 4 KB block on the underlying
       storage. Mapping from LBA is to a physical block address and index within it for the  desired  compressed
       data. All compressed blocks are individually reference counted for correctness.

       Block sharing and block compression are invisible to applications using the storage, which read and write
       blocks as they would if VDO were not present. When a shared block is overwritten, a new physical block is
       allocated  for storing the new block data to ensure that other logical block addresses that are mapped to
       the shared physical block are not modified.

       To use VDO with <b><a href="../man8/lvm.8.html">lvm</a></b>(8), you must install the standard VDO user-space tools <b><a href="../man8/vdoformat.8.html">vdoformat</a></b>(8) and kernel module
       "<u>dm_vdo</u>" (For older kernels &lt;6.9 the out of tree kernel VDO module "<u>kvdo</u>" is necessary).

       The kernel module implements fine-grained  storage  virtualization,  thin  provisioning,  block  sharing,
       compression  and  memory-efficient duplicate identification. The user-space tools include <b><a href="../man8/vdostats.8.html">vdostats</a></b>(8) for
       extracting statistics from VDO volumes.

</pre><h4><b>VDO</b> <b>TERMS</b></h4><pre>
       VDODataLV
              VDO data LV
              A large hidden LV with the _vdata suffix. It is created in a VG
              used by the VDO kernel target to store all data and metadata blocks.

       VDOPoolLV
              VDO pool LV
              A pool for virtual VDOLV(s), which are the size of used VDODataLV.
              Only a single VDOLV is currently supported.

       VDOLV
              VDO LV
              Created from VDOPoolLV.
              Appears blank after creation.

</pre><h4><b>VDO</b> <b>USAGE</b></h4><pre>
       The primary methods for using VDO with lvm2:

   <b>1.</b> <b>Create</b> <b>a</b> <b>VDOPoolLV</b> <b>and</b> <b>a</b> <b>VDOLV</b>
       Create a VDOPoolLV that will hold VDO data, and a virtual size VDOLV that the user can use.   If  you  do
       not  specify the virtual size, then the VDOLV is created with the maximum size that always fits into data
       volume even if no deduplication or compression can happen (i.e. it can hold the incompressible content of
       /dev/urandom).  If you do not specify the name of VDOPoolLV, it is taken from  the  sequence  of  vpool0,
       vpool1 ...

       Note:  The  performance  of  TRIM/Discard operations is slow for large volumes of VDO type. Please try to
       avoid sending discard requests unless necessary because it might take  considerable  amount  of  time  to
       finish the discard operation.

       <b>lvcreate</b> <b>--type</b> <b>vdo</b> <b>-n</b> <b>VDOLV</b> <b>-L</b> <b>DataSize</b> <b>-V</b> <b>LargeVirtualSize</b> <b>VG/VDOPoolLV</b>
       <b>lvcreate</b> <b>--vdo</b> <b>-L</b> <b>DataSize</b> <b>VG</b>

       <u>Example</u>
       # lvcreate --type vdo -n vdo0 -L 10G -V 100G vg/vdopool0
       # mkfs.ext4 -E nodiscard /dev/vg/vdo0

   <b>2.</b> <b>Convert</b> <b>an</b> <b>existing</b> <b>LV</b> <b>into</b> <b>VDOPoolLV</b>
       Convert  an  already  created  or  existing LV into a VDOPoolLV, which is a volume that can hold data and
       metadata.  You will be prompted to confirm such conversion because it <b>IRREVERSIBLY</b> <b>DESTROYS</b>  the  content
       of such volume and the volume is immediately formatted by <b><a href="../man8/vdoformat.8.html">vdoformat</a></b>(8) as a VDO pool data volume. You can
       specify  the virtual size of the VDOLV associated with this VDOPoolLV.  If you do not specify the virtual
       size, it will be set to the maximum size that can keep 100% incompressible data there.

       <b>lvconvert</b> <b>--type</b> <b>vdo-pool</b> <b>-n</b> <b>VDOLV</b> <b>-V</b> <b>VirtualSize</b> <b>VG/VDOPoolLV</b>
       <b>lvconvert</b> <b>--vdopool</b> <b>VG/VDOPoolLV</b>

       <u>Example</u>
       # lvconvert --type vdo-pool -n vdo0 -V10G vg/ExistingLV

   <b>3.</b> <b>Change</b> <b>the</b> <b>compression</b> <b>and</b> <b>deduplication</b> <b>of</b> <b>a</b> <b>VDOPoolLV</b>
       Disable or enable the compression and deduplication for VDOPoolLV (the  volume  that  maintains  all  VDO
       LV(s) associated with it).

       <b>lvchange</b> <b>--compression</b> <b>y|n</b> <b>--deduplication</b> <b>y|n</b> <b>VG/VDOPoolLV</b>

       <u>Example</u>
       # lvchange --compression n  vg/vdopool0
       # lvchange --deduplication y vg/vdopool1

   <b>4.</b> <b>Change</b> <b>the</b> <b>default</b> <b>settings</b> <b>used</b> <b>for</b> <b>creating</b> <b>a</b> <b>VDOPoolLV</b>
       VDO  allows  to  set  a  large variety of options. Lots of these settings can be specified in lvm.conf or
       profile settings. You can prepare a number of different profiles in the  <u><a href="file:/etc/lvm/profile">/etc/lvm/profile</a></u>  directory  and
       just  specify  the  profile file name.  Check the output of <b>lvmconfig</b> <b>--type</b> <b>default</b> <b>--withcomments</b> for a
       detailed description of all individual VDO settings.

       <u>Example</u>
       # cat &lt;&lt;EOF &gt; /etc/lvm/profile/vdo_create.profile
       allocation {
              vdo_use_compression=1
              vdo_use_deduplication=1
              vdo_minimum_io_size=4096
              vdo_block_map_cache_size_mb=128
              vdo_block_map_period=16380
              vdo_use_sparse_index=0
              vdo_index_memory_size_mb=256
              vdo_slab_size_mb=2048
              vdo_ack_threads=1
              vdo_bio_threads=1
              vdo_bio_rotation=64
              vdo_cpu_threads=2
              vdo_hash_zone_threads=1
              vdo_logical_threads=1
              vdo_physical_threads=1
              vdo_max_discard=1
       }
       EOF

       # lvcreate --vdo -L10G --metadataprofile vdo_create vg/vdopool0
       # lvcreate --vdo -L10G --config 'allocation/vdo_cpu_threads=4' vg/vdopool1

   <b>5.</b> <b>Set</b> <b>or</b> <b>change</b> <b>VDO</b> <b>settings</b> <b>with</b> <b>option</b> <b>--vdosettings</b>
       Use the form 'option=value' or 'option1=value option2=value', or repeat  --vdosettings  for  each  option
       being  set.   Options  are listed in the Example section above, for the full description see <b><a href="../man5/lvm.conf.5.html">lvm.conf</a></b>(5).
       Options can omit 'vdo_' and 'vdo_use_' prefixes and all its underscores.  So i.e. vdo_use_deduplication=1
       and deduplication=1 are equivalent.   To  change  the  option  for  an  already  existing  VDOPoolLV  use
       <b><a href="../man8/lvchange.8.html">lvchange</a></b>(8)  command.  However not all option can be changed.  Only compression and deduplication options
       can be also changed for an active VDO LV.  Lowest priority options are specified with configuration file,
       then with --vdosettings and highest are explicit option --compression and --deduplication.

       <u>Example</u>

       # lvcreate --vdo -L10G --vdosettings 'ack_threads=1 hash_zone_threads=2' vg/vdopool0
       # lvchange --vdosettings 'bio_threads=2 deduplication=1' vg/vdopool0

   <b>6.</b> <b>Checking</b> <b>the</b> <b>usage</b> <b>of</b> <b>VDOPoolLV</b>
       To quickly check how much data on a VDOPoolLV is already consumed, use <b><a href="../man8/lvs.8.html">lvs</a></b>(8). The  Data%  field  reports
       how  much data is occupied in the content of the virtual data for the VDOLV and how much space is already
       consumed with all the data and metadata blocks in the VDOPoolLV.  For a  detailed  description,  use  the
       <b><a href="../man8/vdostats.8.html">vdostats</a></b>(8) command.

       Note: <b><a href="../man8/vdostats.8.html">vdostats</a></b>(8) currently understands only <u><a href="file:/dev/mapper">/dev/mapper</a></u> device names.

       <u>Example</u>
       # lvcreate --type vdo -L10G -V20G -n vdo0 vg/vdopool0
       # mkfs.ext4 -E nodiscard /dev/vg/vdo0
       # lvs -a vg

         LV               VG Attr       LSize  Pool     Origin Data%
         vdo0             vg vwi-a-v--- 20.00g vdopool0        0.01
         vdopool0         vg dwi-ao---- 10.00g                 30.16
         [vdopool0_vdata] vg Dwi-ao---- 10.00g

       # vdostats --all /dev/mapper/vg-vdopool0-vpool
       /dev/mapper/vg-vdopool0 :
         version                             : 30
         release version                     : 133524
         data blocks used                    : 79
         ...

   <b>7.</b> <b>Extending</b> <b>the</b> <b>VDOPoolLV</b> <b>size</b>
       You  can  add  more  space  to  hold  VDO data and metadata by extending the VDODataLV using the commands
       <b><a href="../man8/lvresize.8.html">lvresize</a></b>(8) and <b><a href="../man8/lvextend.8.html">lvextend</a></b>(8).  The extension needs to add at least one new VDO slab. You can configure the
       slab size with the <b>allocation/vdo_slab_size_mb</b> setting.

       You   can   also   enable   automatic   size   extension   of   a   monitored    VDOPoolLV    with    the
       <b>activation/vdo_pool_autoextend_percent</b> and <b>activation/vdo_pool_autoextend_threshold</b> settings.

       Note: You cannot reduce the size of a VDOPoolLV.

       <b>lvextend</b> <b>-L+AddingSize</b> <b>VG/VDOPoolLV</b>

       <u>Example</u>
       # lvextend -L+50G vg/vdopool0
       # lvresize -L300G vg/vdopool1

   <b>8.</b> <b>Extending</b> <b>or</b> <b>reducing</b> <b>the</b> <b>VDOLV</b> <b>size</b>
       You  can  extend  or  reduce  a  virtual  VDO  LV as a standard LV with the <b><a href="../man8/lvresize.8.html">lvresize</a></b>(8), <b><a href="../man8/lvextend.8.html">lvextend</a></b>(8), and
       <b><a href="../man8/lvreduce.8.html">lvreduce</a></b>(8) commands.

       Note: The reduction needs to process TRIM for reduced disk area  to  unmap  used  data  blocks  from  the
       VDOPoolLV, which might take a long time.

       <b>lvextend</b> <b>-L+AddingSize</b> <b>VG/VDOLV</b>
       <b>lvreduce</b> <b>-L-ReducingSize</b> <b>VG/VDOLV</b>

       <u>Example</u>
       # lvextend -L+50G vg/vdo0
       # lvreduce -L-50G vg/vdo1
       # lvresize -L200G vg/vdo2

   <b>9.</b> <b>Component</b> <b>activation</b> <b>of</b> <b>a</b> <b>VDODataLV</b>
       You  can  activate  a VDODataLV separately as a component LV for examination purposes.  The activation of
       the VDODataLV activates the data LV in read-only mode, and the  data  LV  cannot  be  modified.   If  the
       VDODataLV  is  active  as  a  component, any upper LV using this volume CANNOT be activated.  You have to
       deactivate the VDODataLV first to continue to use the VDOPoolLV.

       <u>Example</u>
       # lvchange -ay vg/vpool0_vdata
       # lvchange -an vg/vpool0_vdata

</pre><h4><b>VDO</b> <b>TOPICS</b></h4><pre>
   <b>1.</b> <b>Stacking</b> <b>VDO</b>
       You can convert or stack a VDOPooLV with these currently supported volume types: linear, stripe, raid and
       cache with cachepool.

   <b>1.</b> <b>Using</b> <b>multiple</b> <b>volumes</b> <b>using</b> <b>same</b> <b>VDOPoolLV</b>
       You can convert existing VDO LV into a thin volume. After this conversion you can create a thin  snapshot
       or  you  can add more thin volumes with thin-pool named after original LV name LV_tpool0.  See <b><a href="../man7/lvmthin.7.html">lvmthin</a></b>(7)
       for more details.

       <u>Example</u>
       # lvcreate --type vdo -L 5G -V 10G -n vdo1 vg/vdopool
       # lvconvert --type thin vg/vdo1
       # lvcreate -V20 vg/vdo1_tpool0

   <b>2.</b> <b>VDOPoolLV</b> <b>on</b> <b>top</b> <b>of</b> <b>raid</b>
       Using a raid type LV for a VDODataLV.

       <u>Example</u>
       # lvcreate --type raid1 -L 5G -n vdopool vg
       # lvconvert --type vdo-pool -V 10G vg/vdopool

   <b>3.</b> <b>Caching</b> <b>a</b> <b>VDOPoolLV</b>
       VDOPoolLV (accepts also VDODataLV volume name) caching provides  a  mechanism  to  accelerate  reads  and
       writes of already compressed and deduplicated data blocks together with VDO metadata.

       <u>Example</u>
       # lvcreate --type vdo -L 5G -V 10G -n vdo1 vg/vdopool
       # lvcreate --type cache-pool -L 1G -n cachepool vg
       # lvconvert --cache --cachepool vg/cachepool vg/vdopool
       # lvconvert --uncache vg/vdopool

   <b>4.</b> <b>Caching</b> <b>a</b> <b>VDOLV</b>
       VDO  LV  cache  allow you to 'cache' a device for better performance before it hits the processing of the
       VDO Pool LV layer.

       <u>Example</u>
       # lvcreate --type vdo -L 5G -V 10G -n vdo1 vg/vdopool
       # lvcreate --type cache-pool -L 1G -n cachepool vg
       # lvconvert --cache --cachepool vg/cachepool vg/vdo1
       # lvconvert --uncache vg/vdo1

   <b>5.</b> <b>Usage</b> <b>of</b> <b>Discard/TRIM</b> <b>with</b> <b>a</b> <b>VDOLV</b>
       You can discard data on a VDO LV and reduce used blocks on a VDOPoolLV.  However, the current performance
       of discard operations is still not optimal and takes a considerable amount of time and CPU.   Unless  you
       really need it, you should avoid using discard.

       When  a  block  device  is  going  to be rewritten, its blocks will be automatically reused for new data.
       Discard is useful in situations when user knows that the given portion of a VDO LV is  not  going  to  be
       used  and the discarded space can be used for block provisioning in other regions of the VDO LV.  For the
       same reason, you should avoid using mkfs with discard for a freshly created VDO LV to save a lot of  time
       that this operation would take otherwise as device is already expected to be empty.

   <b>6.</b> <b>Memory</b> <b>usage</b>
       The VDO target requires 38 MiB of RAM and several variable amounts:

       • 1.15  MiB  of  RAM  for  each 1 MiB of configured block map cache size.  The block map cache requires a
         minimum of 150 MiB RAM.

       • 1.6 MiB of RAM for each 1 TiB of logical space.

       • 268 MiB of RAM for each 1 TiB of physical storage managed by the volume.

       UDS requires a minimum of 250 MiB of RAM, which is also the default amount that deduplication uses.

       The memory required for the UDS index is determined by the index  type  and  the  required  size  of  the
       deduplication window and is controlled by the <b>allocation/vdo_use_sparse_index</b> setting.

       With  enabled UDS sparse indexing, it relies on the temporal locality of data and attempts to retain only
       the most relevant index entries in memory and can maintain a  deduplication  window  that  is  ten  times
       larger than with dense while using the same amount of memory.

       Although  the  sparse  index  provides the greatest coverage, the dense index provides more deduplication
       advice.  For most workloads, given the same amount of  memory,  the  difference  in  deduplication  rates
       between dense and sparse indexes is negligible.

       A  dense  index with 1 GiB of RAM maintains a 1 TiB deduplication window, while a sparse index with 1 GiB
       of RAM maintains a 10 TiB deduplication window.  In general, 1 GiB is sufficient for 4  TiB  of  physical
       space with a dense index and 40 TiB with a sparse index.

   <b>7.</b> <b>Storage</b> <b>space</b> <b>requirements</b>
       You  can  configure  a  VDOPoolLV  to  use up to 256 TiB of physical storage.  Only a certain part of the
       physical storage is usable to store data.  This section provides the calculations to determine the usable
       size of a VDO-managed volume.

       The VDO target requires storage for two types of VDO metadata and for the UDS index:

       • The first type of VDO metadata uses approximately 1 MiB for each 4 GiB  of  physical  storage  plus  an
         additional 1 MiB per slab.

       • The  second  type  of  VDO  metadata consumes approximately 1.25 MiB for each 1 GiB of logical storage,
         rounded up to the nearest slab.

       • The amount of storage required for the UDS index depends on the type of index and  the  amount  of  RAM
         allocated  to  the  index. For each 1 GiB of RAM, a dense UDS index uses 17 GiB of storage and a sparse
         UDS index will use 170 GiB of storage.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man8/lvm.8.html">lvm</a></b>(8), <b><a href="../man5/lvm.conf.5.html">lvm.conf</a></b>(5), <b><a href="../man8/lvmconfig.8.html">lvmconfig</a></b>(8), <b><a href="../man8/lvcreate.8.html">lvcreate</a></b>(8), <b><a href="../man8/lvconvert.8.html">lvconvert</a></b>(8), <b><a href="../man8/lvchange.8.html">lvchange</a></b>(8), <b><a href="../man8/lvextend.8.html">lvextend</a></b>(8), <b><a href="../man8/lvreduce.8.html">lvreduce</a></b>(8),
       <b><a href="../man8/lvresize.8.html">lvresize</a></b>(8), <b><a href="../man8/lvremove.8.html">lvremove</a></b>(8), <b><a href="../man8/lvs.8.html">lvs</a></b>(8),

       <b><a href="../man7/lvmthin.7.html">lvmthin</a></b>(7), <b><a href="../man8/vdoformat.8.html">vdoformat</a></b>(8), <b><a href="../man8/vdostats.8.html">vdostats</a></b>(8),

       <b><a href="../man8/mkfs.8.html">mkfs</a></b>(8)

Red Hat, Inc                            LVM TOOLS <a href="../man2/2.03.31.2.html">2.03.31</a>(2) (2025-02-27)                              <u><a href="../man7/LVMVDO.7.html">LVMVDO</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>