<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>o2cb - Default cluster stack of the OCFS2 file system.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/ocfs2-tools">ocfs2-tools_1.8.8-3build1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       o2cb - Default cluster stack of the <u>OCFS2</u> file system.

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>o2cb</b>  is  the  default  cluster  stack  of  the  <u>OCFS2</u> file system. It is an in-kernel cluster stack that
       includes a node manager (o2nm) to keep track of the nodes in the cluster, a disk heartbeat  agent  (o2hb)
       to  detect node live-ness, a network agent (o2net) for intra-cluster node communication and a distributed
       lock manager (o2dlm) to keep track of lock resources.  It also includes a synthetic file  system,  dlmfs,
       to allow applications to access the in-kernel dlm.

</pre><h4><b>CONFIGURATION</b></h4><pre>
       The   stack   is   configured   using   the   <b><a href="../man8/o2cb.8.html">o2cb</a>(8)</b>   cluster   configuration   utility   and  operated
       (online/offline/status) using the <u>o2cb</u> init service.

       <b>CLUSTER</b> <b>CONFIGURATION</b>

              It has two configuration files. One for the cluster layout (/etc/ocfs2/cluster.conf) and the other
              for the cluster timeouts, etc. (/etc/sysconfig/o2cb). More information about these two  files  can
              be found in <b><a href="../man5/ocfs2.cluster.conf.5.html">ocfs2.cluster.conf</a>(5)</b> and <b><a href="../man5/o2cb.sysconfig.5.html">o2cb.sysconfig</a>(5)</b>.

              The <b>o2cb</b> cluster stack supports two heartbeat modes, namely, <b>local</b> and <b>global</b>.  Only one heartbeat
              mode can be active at any one time.

              <b>Local</b>  <b>heartbeat</b> refers to disk heartbeating on all shared devices. In this mode, the <u>heartbeat</u> <u>is</u>
              <u>started</u> <u>during</u> <u>mount</u> <u>and</u> <u>stopped</u> <u>during</u> <u>umount</u>. This mode is easy to setup as it does not  require
              configuring  heartbeat  devices. The one drawback in this mode is the overhead on servers having a
              large number of <u>OCFS2</u> mounts. For example, a server with 50 mounts will have 50 heartbeat threads.
              This is the default heartbeat mode.

              <b>Global</b> <b>heartbeat</b>, on the other hand, refers to heartbeating on  specific  shared  devices.   These
              devices  are  normal <u>OCFS2</u> formatted volumes that could also be mounted and used as clustered file
              systems. In this mode, the <u>heartbeat</u> <u>is</u> <u>started</u> <u>during</u> <u>cluster</u> <u>online</u> <u>and</u> <u>stopped</u>  <u>during</u>  <u>cluster</u>
              <u>offline</u>.  While  this  mode  can be used for all clusters, it is <u>strongly</u> recommended for clusters
              having a large number of mounts.

              More information on disk heartbeat is provided below.

       <b>KERNEL</b> <b>CONFIGURATION</b>

              Two sysctl values need to be set for <b>o2cb</b> to function properly. The first, panic_on_oops, must  be
              enabled  to  turn  a  kernel  oops  into a panic. If a kernel thread required for <b>o2cb</b> to function
              crashes, the system must be reset to prevent a cluster hang. If it is not set,  another  node  may
              not be able to distinguish whether a node is unable to respond or slow to respond.

              The  other  related sysctl parameter is panic, which specifies the number of seconds after a panic
              that the system will be auto-reset. Setting this parameter to zero disables autoreset; the cluster
              will require manual intervention. This is not preferred in a cluster environment.

              To manually enable panic on oops and set a 30 sec timeout for reboot on panic, do:

              # echo 1 &gt; <a href="file:/proc/sys/kernel/panic_on_oops">/proc/sys/kernel/panic_on_oops</a>
              # echo 30 &gt; <a href="file:/proc/sys/kernel/panic">/proc/sys/kernel/panic</a>

              <b>To</b> <b>enable</b> <b>the</b> <b>above</b> <b>on</b> <b>every</b> <b>boot,</b> <b>add</b> <b>the</b> <b>following</b> <b>to</b> <b><a href="file:/etc/sysctl.conf">/etc/sysctl.conf</a>:</b>

              <b>kernel.panic_on_oops</b> <b>=</b> <b>1</b>
              <b>kernel.panic</b> <b>=</b> <b>30</b>

       <b>OS</b> <b>CONFIGURATION</b>

              The <b>o2cb</b> cluster stack also requires iptables (firewalling) to be either disabled or  modified  to
              allow  network  traffic  on  the  private network interface. The port used by <b>o2cb</b> is specified in
              /etc/ocfs2/cluster.conf.

</pre><h4><b>DISK</b> <b>HEARTBEAT</b></h4><pre>
       O2CB uses disk heartbeat to detect node liveness. The disk heartbeat thread, <b>o2hb</b>, periodically reads and
       writes to a heartbeat file in a OCFS2 file system. Its write payload contains a sequence number  that  it
       increments  in  each  write. This allows other nodes reading the same heartbeat file to detect the change
       and associate that with a live node.  Conversely, a node whose sequence number has  stopped  changing  is
       marked as a possible dead node. Possible. Not confirmed. That is because it just could be slow I/Os.

       To  differentiate  between  a  dead  node and one that has slow I/Os, O2CB has a disk heartbeat threshold
       (timeout). Only nodes whose sequence number has not incremented for that duration are marked dead.

       However that node may not be dead but just experiencing slow I/O. To prevent that, the  heartbeat  thread
       keeps  track  of  the  time  elapsed since the last completed write. If that time exceeds the timeout, it
       forces a self-fence. It does so to prevent other nodes from marking it as dead while it is still alive.

       This self-fencing scheme has proven to be very reliable as it relies on kernel timers and pci bus  reset.
       External  fencing, while attractive, is rarely as reliable as it relies on external hardware and software
       that is prone to failure due to misconfiguration, etc.

       Having said that, O2CB  disk  heartbeat  has  had  its  share  of  problems  with  self  fencing.   Nodes
       experiencing slow I/O on only one of multiple devices have to initiate self-fence.

       This  is because in the default <b>local</b> <b>heartbeat</b> scheme, nodes in a cluster may not be heartbeating on the
       same set of devices.

       The <b>global</b> <b>heartbeat</b> mode addresses this shortcoming by introducing a scheme that  forces  all  nodes  to
       heartbeat  on  the same set of devices. In this scheme, a node experiencing a slowdown in I/O on a device
       may not need to initiate self-fence. It will only have to do so if it encounters slowdown on 50% or  more
       of  the  heartbeat  devices.   In  a  cluster  with  3  heartbeat regions, a slowdown in 1 region will be
       tolerated. In a cluster with 5 regions, a slowdown in 2 will be tolerated.

       It is for this reason, this mode is recommended for users that have 3 or more OCFS2 mounts.

       O2CB allows up to <b>32</b> heartbeat regions to be configured in the global heartbeat mode.

</pre><h4><b>ONLINE</b> <b>CLUSTER</b> <b>MODIFICATION</b></h4><pre>
       The O2CB cluster stack allows <u>adding</u> <u>and</u> <u>removing</u> <u>nodes</u> <u>in</u> <u>an</u> <u>online</u>  <u>cluster</u>  when  run  in  the  <b>global</b>
       heartbeat  mode.  Use  the  <b><a href="../man8/o2cb.8.html">o2cb</a>(8)</b>  utility  to make the changes in the configuration and (re)online the
       cluster using the <u>o2cb</u> init script. The user <b>must</b> do the same on <u>all</u> nodes in the  cluster.  The  cluster
       will not allow any new cluster mounts if the node configuration on all nodes is not the same.

       The  removal  of  nodes will only succeed if that node is no longer in use. If the user removes an active
       node from the configuration, the re-online will fail.

       The cluster stack also allows <u>adding</u> <u>and</u> <u>removing</u> <u>heartbeat</u>  <u>regions</u>  <u>in</u>  <u>an</u>  <u>online</u>  <u>cluster</u>.   Use  the
       <b><a href="../man8/o2cb.8.html">o2cb</a>(8)</b>  utility  to make the changes in the configuration file and (re)online the cluster using the <u>o2cb</u>
       init script. The user <b>must</b> do the same on <u>all</u> nodes in the cluster. The cluster will not  allow  any  new
       cluster mounts if the heartbeat region configuration on all nodes is not the same.

       The  removal  of heartbeat regions will only succeed if the active heartbeat region count is greater than
       <b>3</b>. This is to protect against edge conditions that can destabilize the cluster.

</pre><h4><b>GETTING</b> <b>STARTED</b></h4><pre>
       The first step in configuring <b>o2cb</b> is deciding whether to setup <b>local</b>  or  <b>global</b>  heartbeat.  If  <b>global</b>
       heartbeat, then one has to format atleast one heartbeat device.

       To format a OCFS2 volume with global heartbeat enabled, do:

       # mkfs.ocfs2 --cluster-stack=o2cb --cluster-name=webcluster --global-heartbeat -L "hbvol1" /dev/sdb1

       <b>Once</b> <b>formatted,</b> <b>setup</b> <b>/etc/ocfs2/cluster.conf</b> <b>following</b> <b>the</b> <b>example</b> <b>provided</b> <b>in</b> <b><a href="../man5/ocfs2.cluster.conf.5.html">ocfs2.cluster.conf</a>(5)</b>.

       If  <b>local</b>  heartbeat,  then  one  can  setup cluster.conf without any heartbeat devices. The next step is
       starting the cluster.

       To online the cluster stack, do:

       # service o2cb online
       Loading stack plugin "o2cb": OK
       Loading filesystem "ocfs2_dlmfs": OK
       Mounting ocfs2_dlmfs filesystem at /dlm: OK
       Setting cluster stack "o2cb": OK
       Registering O2CB cluster "webcluster": OK
       Setting O2CB cluster timeouts : OK
       Starting global heartbeat for cluster "webcluster": OK

       <b>Once</b> <b>the</b> <b>cluster</b> <b>stack</b> <b>is</b> <b>online,</b> <b>new</b> <b>OCFS2</b> volumes can be  formatted  normally  without  specifying  the
       cluster stack information. <u><a href="../man8/mkfs.ocfs2.8.html">mkfs.ocfs2</a>(8)</u> will pick up that information automatically.

       # mkfs.ocfs2 -L "datavol" /dev/sdc1

       <u>Meanwhile</u> <u>existing</u> <u>volumes</u> <u>can</u> <u>be</u> <u>converted</u> <u>to</u> <u>the</u> <u>new</u> <u>cluster</u> <u>stack</u> <u>using</u> <b><a href="../man8/tunefs.ocfs2.8.html">tunefs.ocfs2</a>(8)</b> utility.

       # tunefs.ocfs2 --update-cluster-stack /dev/sdd1
       Updating on-disk cluster information to match the running cluster.
       DANGER: YOU MUST BE ABSOLUTELY SURE THAT NO OTHER NODE IS USING THIS FILESYSTEM
       BEFORE MODIFYING ITS CLUSTER CONFIGURATION.
       Update the on-disk cluster information? y

       <b>Another</b>  <b>utility</b> <b><a href="../man8/mounted.ocfs2.8.html">mounted.ocfs2</a>(8)</b> is useful is listing all the <u>OCFS2</u> volumes alonghwith the cluster stack
       information.

       To get a list of OCFS2 volumes, do:

       # mounted.ocfs2 -d
       Device     Stack  Cluster     F  UUID                              Label
       /dev/sdb1  o2cb   webcluster  G  DCDA2845177F4D59A0F2DCD8DE507CC3  hbvol1
       /dev/sdc1  None                  23878C320CF3478095D1318CB5C99EED  localmount
       /dev/sdd1  o2cb   webcluster  G  8AB016CD59FC4327A2CDAB69F08518E3  webvol
       /dev/sdg1  o2cb   webcluster  G  77D95EF51C0149D2823674FCC162CF8B  logsvol
       /dev/sdh1  o2cb   webcluster  G  BBA1DBD0F73F449384CE75197D9B7098  scratch

       <u>The</u> <u>o2cb</u> init script can also be used to check the status of the cluster, offline the cluster, etc.

       To check the status of the cluster stack, do:

       # service o2cb status
       Driver for "configfs": Loaded
       Filesystem "configfs": Mounted
       Stack glue driver: Loaded
       Stack plugin "o2cb": Loaded
       Driver for "ocfs2_dlmfs": Loaded
       Filesystem "ocfs2_dlmfs": Mounted
       Checking O2CB cluster "webcluster": Online
         Heartbeat dead threshold: 62
         Network idle timeout: 60000
         Network keepalive delay: 2000
         Network reconnect delay: 2000
         Heartbeat mode: Global
       Checking O2CB heartbeat: Active
         77D95EF51C0149D2823674FCC162CF8B /dev/sdg1
         DCDA2845177F4D59A0F2DCD8DE507CC3 /dev/sdk1
         BBA1DBD0F73F449384CE75197D9B7098 /dev/sdh1
       Nodes in O2CB cluster: 6 7 10
       Active userdlm domains:  ovm

       <u>To</u> <u>offline</u> <u>and</u> <u>unload</u> <u>the</u> <u>cluster</u> <u>stack,</u> <u>do:</u>

       <u>#</u> <u>service</u> <u>o2cb</u> <u>offline</u>
       <u>Clean</u> <u>userdlm</u> <u>domains:</u> <u>OK</u>
       <u>Stopping</u> <u>global</u> <u>heartbeat</u> <u>on</u> <u>cluster</u> <u>"webcluster":</u> <u>OK</u>
       <u>Stopping</u> <u>O2CB</u> <u>cluster</u> <u>webcluster:</u> <u>OK</u>
       <u>Unregistering</u> <u>O2CB</u> <u>cluster</u> <u>"webcluster":</u> <u>OK</u>

       <u>#</u> <u>service</u> <u>o2cb</u> <u>unload</u>
       <u>Clean</u> <u>userdlm</u> <u>domains:</u> <u>OK</u>
       <u>Unmounting</u> <u>ocfs2_dlmfs</u> <u>filesystem:</u> <u>OK</u>
       <u>Unloading</u> <u>module</u> <u>"ocfs2_dlmfs":</u> <u>OK</u>
       <u>Unloading</u> <u>module</u> <u>"ocfs2_stack_o2cb":</u> <u>OK</u>

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man8/o2cb.8.html">o2cb</a>(8)</b> <b><a href="../man5/o2cb.sysconfig.5.html">o2cb.sysconfig</a>(5)</b> <b><a href="../man5/ocfs2.cluster.conf.5.html">ocfs2.cluster.conf</a>(5)</b> <b><a href="../man8/o2hbmonitor.8.html">o2hbmonitor</a>(8)</b>

</pre><h4><b>AUTHORS</b></h4><pre>
       Oracle Corporation

</pre><h4><b>COPYRIGHT</b></h4><pre>
       Copyright Â© 2004, 2011 Oracle. All rights reserved.

Version 1.8.8                                      August 2011                                           <u><a href="../man7/o2cb.7.html">o2cb</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>