<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>io_uring - Asynchronous I/O facility</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/liburing-dev">liburing-dev_2.11-1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       io_uring - Asynchronous I/O facility

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>#include</b> <b>&lt;<a href="file:/usr/include/linux/io_uring.h">linux/io_uring.h</a>&gt;</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>io_uring</b>  is  a  Linux-specific  API  for asynchronous I/O.  It allows the user to submit one or more I/O
       requests, which are processed asynchronously without blocking the calling  process.   <b>io_uring</b>  gets  its
       name  from ring buffers which are shared between user space and kernel space. This arrangement allows for
       efficient I/O, while avoiding the overhead  of  copying  buffers  between  them,  where  possible.   This
       interface  makes  <b>io_uring</b>  different  from  other  UNIX  I/O APIs, wherein, rather than just communicate
       between kernel and user space with system calls, ring buffers are used as the main mode of communication.
       This arrangement has various performance benefits which are discussed in a separate section below.   This
       man page uses the terms shared buffers, shared ring buffers and queues interchangeably.

       The general programming model you need to follow for <b>io_uring</b> is outlined below

       •      Set  up  shared buffers with <b><a href="../man2/io_uring_setup.2.html">io_uring_setup</a></b>(2) and <b><a href="../man2/mmap.2.html">mmap</a></b>(2), mapping into user space shared buffers
              for the submission queue (SQ) and the completion queue (CQ).  You place I/O requests you  want  to
              make on the SQ, while the kernel places the results of those operations on the CQ.

       •      For  every  I/O  request  you  need  to  make  (like to read a file, write a file, accept a socket
              connection, etc), you create a submission queue entry, or SQE, describe the I/O operation you need
              to get done and add it to the tail of the submission  queue  (SQ).   Each  I/O  operation  is,  in
              essence,  the  equivalent  of  a  system call you would have made otherwise, if you were not using
              <b>io_uring</b>.  For instance, a SQE with the <u>opcode</u> set to <b>IORING_OP_READ</b> will request a read operation
              to be issued that is similar to the <b><a href="../man2/read.2.html">read</a></b>(2) system call. Refer  to  the  opcode  documentation  in
              <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2)  for  all supported opcodes and their properties.  You can add more than one SQE
              to the queue depending on the number of operations you want to request.

       •      After you add one or more SQEs, you need to call <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2) to tell the kernel  to  dequeue
              your I/O requests off the SQ and begin processing them.

       •      For  each  SQE  you submit, once it is done processing the request, the kernel places a completion
              queue event or CQE at the tail of the completion queue or  CQ.   The  kernel  places  exactly  one
              matching  CQE  in the CQ for every SQE you submit on the SQ.  After you retrieve a CQE, minimally,
              you might be interested in checking the <u>res</u> field of the CQE structure, which corresponds  to  the
              return  value  of  the  system call's equivalent, had you used it directly without using <b>io_uring</b>.
              Given that <b>io_uring</b> is an async interface, <u>errno</u> is never used for passing back error information.
              Instead, <u>res</u> will contain what the equivalent system call would have returned in case of  success,
              and  in  case of error <u>res</u> will contain <u>-errno</u>.  For example, if the normal read system call would
              have returned -1 and set <u>errno</u> to <b>EINVAL</b>, then <u>res</u> would contain <b>-EINVAL</b>.  If  the  normal  system
              call would have returned a read size of 1024, then <u>res</u> would contain 1024.

       •      Optionally,  <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2) can also wait for a specified number of requests to be processed by
              the kernel before it returns.  If you specified a certain number of completions to wait  for,  the
              kernel  would have placed at least those many number of CQEs on the CQ, which you can then readily
              read, right after the return from <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2).

       •      It is important to remember that I/O requests submitted to the kernel can complete in  any  order.
              It  is  not necessary for the kernel to process one request after another, in the order you placed
              them.  Given that the interface is a ring, the requests  are  attempted  in  order,  however  that
              doesn't  imply  any sort of ordering on their execution or completion.  When more than one request
              is in flight, it is not possible to determine which one will execute or complete first.  When  you
              dequeue  CQEs  off the CQ, you should always check which submitted request it corresponds to.  The
              most common method for doing so is utilizing the <u>user_data</u> field in the request, which  is  passed
              back on the completion side.

       •      Concretely,  for operations where strict ordering is required, such as for sends and receives on a
              stream-oriented TCP socket, it is generally unsafe to have more than one outstanding send, or more
              than one outstanding receive (the two directions are independent) on a given socket at a time,  as
              the  kernel  may  reorder their execution if poll arming or other background kernel activities are
              involved.  However, <b>io_uring</b> provides various facilities to  enable  applications  to  efficiently
              pipeline their operations safely. If the requests are submitted in a single batch, the application
              may  use  <b>IOSQE_IO_LINK</b>  to enforce an execution order in the kernel. Otherwise, <b>io_uring</b> provides
              advanced features like <u>multi</u> <u>shot</u> and send/receive <u>bundles</u> to allow applications to  provide  more
              data  in  fewer, more efficient trips to the kernel. Even if these features are used, applications
              must still ensure they do not overlap different sends or different receives on a given file.

       Adding to and reading from the queues:

       •      You add SQEs to the tail of the SQ.  The kernel reads SQEs off the head of the queue.

       •      The kernel adds CQEs to the tail of the CQ.  You read CQEs off the head of the queue.

       It should be noted that depending on the configuration io_uring's behavior can deviate from the  behavior
       outlined  above,  like  not posting a CQE for every SQE when setting <b>IOSQE_CQE_SKIP_SUCCESS</b> in the SQE or
       posting multiple CQEs for a single SQE for  multi  shot  operations  or  requiring  an  <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2)
       syscall to make the kernel begin processing newly added SQEs when using submission queue polling.

   <b>Submission</b> <b>queue</b> <b>polling</b>
       One  of  the goals of <b>io_uring</b> is to provide a means for efficient I/O.  To this end, <b>io_uring</b> supports a
       polling mode that lets you avoid the call to <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2), which you use to inform the  kernel  that
       you  have  queued  SQEs  on  to  the SQ.  With SQ Polling, <b>io_uring</b> starts a kernel thread that polls the
       submission queue for any I/O requests you submit by adding SQEs.  With SQ Polling enabled,  there  is  no
       need  for  you  to  call <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2), letting you avoid the overhead of system calls.  A designated
       kernel thread dequeues SQEs off the SQ as you add them and dispatches them for asynchronous processing.

   <b>Setting</b> <b>up</b> <b>io_uring</b>
       The main steps in setting up <b>io_uring</b> consist of mapping in the shared buffers with  <b><a href="../man2/mmap.2.html">mmap</a></b>(2)  calls.   In
       the  example  program  included  in this man page, the function <b>app_setup_uring</b>() sets up <b>io_uring</b> with a
       QUEUE_DEPTH deep submission queue.  Pay attention  to  the  2  <b><a href="../man2/mmap.2.html">mmap</a></b>(2)  calls  that  set  up  the  shared
       submission  and  completion  queues.   If  your kernel is older than version 5.4, three <b><a href="../man2/mmap.2.html">mmap</a>(2)</b> calls are
       required.

   <b>Submitting</b> <b>I/O</b> <b>requests</b>
       The process of submitting a request consists of describing the I/O operation you need to get  done  using
       an  <b>io_uring_sqe</b>  structure  instance.   These  details  describe  the  equivalent  system  call  and its
       parameters.  Because the range of I/O operations Linux supports are  very  varied  and  the  <b>io_uring_sqe</b>
       structure  needs  to  be  able to describe them, it has several fields, some packed into unions for space
       efficiency.  Here is a simplified version of struct <b>io_uring_sqe</b> with some of the most often used fields:

           struct io_uring_sqe {
                   __u8    opcode;         /* type of operation for this sqe */
                   __s32   fd;             /* file descriptor to do IO on */
                   __u64   off;            /* offset into file */
                   __u64   addr;           /* pointer to buffer or iovecs */
                   __u32   len;            /* buffer size or number of iovecs */
                   __u64   user_data;      /* data to be passed back at completion time */
                   __u8    flags;          /* IOSQE_ flags */
                   ...
           };

       Here is struct <b>io_uring_sqe</b> in full:

           struct io_uring_sqe {
                __u8 opcode;        /* type of operation for this sqe */
                __u8 flags;         /* IOSQE_ flags */
                __u16     ioprio;        /* ioprio for the request */
                __s32     fd;       /* file descriptor to do IO on */
                union {
                     __u64     off; /* offset into file */
                     __u64     addr2;
                     struct {
                          __u32     cmd_op;
                          __u32     __pad1;
                     };
                };
                union {
                     __u64     addr;     /* pointer to buffer or iovecs */
                     __u64     splice_off_in;
                     struct {
                          __u32     level;
                          __u32     optname;
                     };
                };
                __u32     len;      /* buffer size or number of iovecs */
                union {
                     __kernel_rwf_t rw_flags;
                     __u32          fsync_flags;
                     __u16          poll_events;   /* compatibility */
                     __u32          poll32_events; /* word-reversed for BE */
                     __u32          sync_range_flags;
                     __u32          msg_flags;
                     __u32          timeout_flags;
                     __u32          accept_flags;
                     __u32          cancel_flags;
                     __u32          open_flags;
                     __u32          statx_flags;
                     __u32          fadvise_advice;
                     __u32          splice_flags;
                     __u32          rename_flags;
                     __u32          unlink_flags;
                     __u32          hardlink_flags;
                     __u32          xattr_flags;
                     __u32          msg_ring_flags;
                     __u32          uring_cmd_flags;
                     __u32          waitid_flags;
                     __u32          futex_flags;
                     __u32          install_fd_flags;
                     __u32          nop_flags;
                };
                __u64     user_data;     /* data to be passed back at completion time */
                /* pack this to avoid bogus arm OABI complaints */
                union {
                     /* index into fixed buffers, if used */
                     __u16     buf_index;
                     /* for grouped buffer selection */
                     __u16     buf_group;
                } __attribute__((packed));
                /* personality to use, if used */
                __u16     personality;
                union {
                     __s32     splice_fd_in;
                     __u32     file_index;
                     __u32     optlen;
                     struct {
                          __u16     addr_len;
                          __u16     __pad3[1];
                     };
                };
                union {
                     struct {
                          __u64     addr3;
                          __u64     __pad2[1];
                     };
                     __u64     optval;
                     /*
                      * If the ring is initialized with IORING_SETUP_SQE128, then
                      * this field is used for 80 bytes of arbitrary command data
                      */
                     __u8 cmd[0];
                };
           };

       To submit an I/O request to <b>io_uring</b>, you need to  acquire  a  submission  queue  entry  (SQE)  from  the
       submission  queue  (SQ),  fill  it  up  with  details  of  the  operation  you  want  to  submit and call
       <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2).  There are helper functions of the form io_uring_prep_X to enable proper setup of  the
       SQE.  If  you want to avoid calling <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2), you have the option of setting up Submission Queue
       Polling.

       SQEs are added to the tail of the submission queue.  The kernel picks up SQEs off the  head  of  the  SQ.
       The general algorithm to get the next available SQE and update the tail is as follows.

           struct io_uring_sqe *sqe;
           unsigned tail, index;
           tail = *sqring-&gt;tail;
           index = tail &amp; (*sqring-&gt;ring_mask);
           sqe = &amp;sqring-&gt;sqes[index];
           /* fill up details about this I/O request */
           describe_io(sqe);
           /* fill the sqe index into the SQ ring array */
           sqring-&gt;array[index] = index;
           tail++;
           atomic_store_explicit(sqring-&gt;tail, tail, memory_order_release);

       To  get the index of an entry, the application must mask the current tail index with the size mask of the
       ring.  This holds true for both SQs and CQs.  Once the SQE is acquired, the necessary fields  are  filled
       in,  describing the request.  While the CQ ring directly indexes the shared array of CQEs, the submission
       side has an indirection array between them.  The submission side ring buffer is an index into this array,
       which in turn contains the index into the SQEs.

       The following code snippet demonstrates how a read operation, an equivalent of a <b><a href="../man2/preadv2.2.html">preadv2</a></b>(2)  system  call
       is described by filling up an SQE with the necessary parameters.

           struct iovec iovecs[16];
            ...
           sqe-&gt;opcode = IORING_OP_READV;
           sqe-&gt;fd = fd;
           sqe-&gt;addr = (unsigned long) iovecs;
           sqe-&gt;len = 16;
           sqe-&gt;off = offset;
           sqe-&gt;flags = 0;

       <b>Memory</b> <b>ordering</b>
              Modern  compilers and CPUs freely reorder reads and writes without affecting the program's outcome
              to optimize performance.  Some aspects of this need to be  kept  in  mind  on  SMP  systems  since
              <b>io_uring</b>  involves  buffers  shared between kernel and user space.  These buffers are both visible
              and modifiable from kernel and user space.  As heads and tails belonging to these  shared  buffers
              are  updated  by  kernel  and  user  space,  changes need to be coherently visible on either side,
              irrespective of whether a CPU switch took place after the kernel-user mode  switch  happened.   We
              use  memory  barriers to enforce this coherency.  Being significantly large subjects on their own,
              memory barriers are out of scope for further discussion on this man page.  For more information on
              modern memory models the reader may refer to the Documentation/memory-barriers.txt in  the  kernel
              tree or to the documentation of the formal C11 or kernel memory model.

       <b>Letting</b> <b>the</b> <b>kernel</b> <b>know</b> <b>about</b> <b>I/O</b> <b>submissions</b>
              Once you place one or more SQEs on to the SQ, you need to let the kernel know that you've done so.
              You can do this by calling the <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2) system call.  This system call is also capable of
              waiting for a specified count of events to complete.  This way, you can be sure to find completion
              events in the completion queue without having to poll it for events later.

   <b>Reading</b> <b>completion</b> <b>events</b>
       Similar to the submission queue (SQ), the completion queue (CQ) is a shared buffer between the kernel and
       user  space.   Whereas  you placed submission queue entries on the tail of the SQ and the kernel read off
       the head, when it comes to the CQ, the kernel places completion queue events or CQEs on the tail  of  the
       CQ and you read off its head.

       Submission  is  flexible  (and thus a bit more complicated) since it needs to be able to encode different
       types of system calls that take various parameters.  Completion, on the other hand is simpler since we're
       looking only for a return value back from the kernel.  This  is  easily  understood  by  looking  at  the
       completion queue event structure, struct <b>io_uring_cqe</b>:

           struct io_uring_cqe {
                __u64     user_data;  /* sqe-&gt;data submission passed back */
                __s32     res;        /* result code for this event */
                __u32     flags;
           };

       Here,  <u>user_data</u>  is  custom  data that is passed unchanged from submission to completion.  That is, from
       SQEs to CQEs.  This field can  be  used  to  set  context,  uniquely  identifying  submissions  that  got
       completed.   Given  that  I/O  requests  can complete in any order, this field can be used to correlate a
       submission with a completion.  <u>res</u> is the result from the system call that was performed as part  of  the
       submission; its return value.

       The  <u>flags</u>  field  carries  request-specific  information. As of the 6.12 kernel, the following flags are
       defined:

       <b>IORING_CQE_F_BUFFER</b>
              If set, the upper 16 bits of the flags field carries the  buffer  ID  that  was  chosen  for  this
              request.  The  request must have been issued with <b>IOSQE_BUFFER_SELECT</b> set, and used with a request
              type that supports buffer selection. Additionally, buffers must have been provided upfront  either
              via the <b>IORING_OP_PROVIDE_BUFFERS</b> or the <b>IORING_REGISTER_PBUF_RING</b> methods.

       <b>IORING_CQE_F_MORE</b>
              If set, the application should expect more completions from the request. This is used for requests
              that can generate multiple completions, such as multi-shot requests, receive, or accept.

       <b>IORING_CQE_F_SOCK_NONEMPTY</b>
              If  set, upon receiving the data from the socket in the current request, the socket still had data
              left on completion of this request.

       <b>IORING_CQE_F_NOTIF</b>
              Set for notification CQEs, as seen with the zero-copy networking send and receive support.

       <b>IORING_CQE_F_BUF_MORE</b>
              If set, the buffer ID set in the completion  will  get  more  completions.  This  means  that  the
              provided  buffer  has  been  partially  consumed and there's more buffer space left, and hence the
              application should expect more completions with this buffer  ID.  Each  completion  will  continue
              where  the  previous one left off. This can only happen if the provided buffer ring has been setup
              with <b>IOU_PBUF_RING_INC</b> to allow for incremental / partial consumption of buffers.

       The general sequence to read completion events off the completion queue is as follows:

           unsigned head;
           head = *cqring-&gt;head;
           if (head != atomic_load_acquire(cqring-&gt;tail)) {
               struct io_uring_cqe *cqe;
               unsigned index;
               index = head &amp; (cqring-&gt;mask);
               cqe = &amp;cqring-&gt;cqes[index];
               /* process completed CQE */
               process_cqe(cqe);
               /* CQE consumption complete */
               head++;
           }
           atomic_store_explicit(cqring-&gt;head, head, memory_order_release);

       It helps to be reminded that the kernel adds CQEs to the tail of the CQ, while you need to  dequeue  them
       off the head.  To get the index of an entry at the head, the application must mask the current head index
       with  the  size  mask  of  the  ring.   Once the CQE has been consumed or processed, the head needs to be
       updated to reflect the consumption of the CQE.  Attention should be paid to the read and  write  barriers
       to ensure successful read and update of the head.

   <b>io_uring</b> <b>performance</b>
       Because  of  the  shared  ring buffers between kernel and user space, <b>io_uring</b> can be a zero-copy system.
       Copying buffers to and from becomes necessary when system calls that transfer  data  between  kernel  and
       user  space  are  involved.   But  since  the bulk of the communication in <b>io_uring</b> is via buffers shared
       between the kernel and user space, this huge performance overhead is completely avoided.

       While system calls may not seem like a significant overhead, in high performance applications,  making  a
       lot  of  them  will  begin  to  matter.  While workarounds the operating system has in place to deal with
       Spectre and Meltdown are ideally best done away with, unfortunately, some of these workarounds are around
       the system call interface, making system calls not as cheap as before on affected hardware.  While  newer
       hardware  should not need these workarounds, hardware with these vulnerabilities can be expected to be in
       the wild for a long time.  While using synchronous programming interfaces or even when using asynchronous
       programming interfaces under Linux, there is at least one system call involved in the submission of  each
       request.  In <b>io_uring</b>, on the other hand, you can batch several requests in one go, simply by queueing up
       multiple  SQEs,  each  describing  an I/O operation you want and make a single call to <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2).
       This is possible due to <b>io_uring</b>'s shared buffers based design.

       While this batching in itself can avoid the overhead associated with potentially  multiple  and  frequent
       system  calls,  you  can  reduce  even this overhead further with Submission Queue Polling, by having the
       kernel poll and pick up your SQEs for processing as you add them to the submission queue. This avoids the
       <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2) call you need to make to tell  the  kernel  to  pick  SQEs  up.   For  high-performance
       applications, this means even fewer system call overheads.

</pre><h4><b>CONFORMING</b> <b>TO</b></h4><pre>
       <b>io_uring</b> is Linux-specific.

</pre><h4><b>EXAMPLES</b></h4><pre>
       The following example uses <b>io_uring</b> to copy stdin to stdout.  Using shell redirection, you should be able
       to  copy  files with this example.  Because it uses a queue depth of only one, this example processes I/O
       requests one after the other.  It is purposefully kept this way  to  aid  understanding.   In  real-world
       scenarios  however,  you'll want to have a larger queue depth to parallelize I/O request processing so as
       to gain the kind of performance benefits <b>io_uring</b> provides with its asynchronous processing of requests.

       #include &lt;<a href="file:/usr/include/stdio.h">stdio.h</a>&gt;
       #include &lt;<a href="file:/usr/include/stdlib.h">stdlib.h</a>&gt;
       #include &lt;sys/stat.h&gt;
       #include &lt;sys/ioctl.h&gt;
       #include &lt;sys/syscall.h&gt;
       #include &lt;sys/mman.h&gt;
       #include &lt;sys/uio.h&gt;
       #include &lt;<a href="file:/usr/include/linux/fs.h">linux/fs.h</a>&gt;
       #include &lt;<a href="file:/usr/include/fcntl.h">fcntl.h</a>&gt;
       #include &lt;<a href="file:/usr/include/unistd.h">unistd.h</a>&gt;
       #include &lt;<a href="file:/usr/include/string.h">string.h</a>&gt;
       #include &lt;stdatomic.h&gt;

       #include &lt;<a href="file:/usr/include/linux/io_uring.h">linux/io_uring.h</a>&gt;

       #define QUEUE_DEPTH 1
       #define BLOCK_SZ    1024

       /* Macros for barriers needed by io_uring */
       #define io_uring_smp_store_release(p, v)            \
           atomic_store_explicit((_Atomic typeof(*(p)) *)(p), (v), \
                         memory_order_release)
       #define io_uring_smp_load_acquire(p)                \
           atomic_load_explicit((_Atomic typeof(*(p)) *)(p),   \
                        memory_order_acquire)

       int ring_fd;
       unsigned *sring_tail, *sring_mask, *sring_array,
                   *cring_head, *cring_tail, *cring_mask;
       struct io_uring_sqe *sqes;
       struct io_uring_cqe *cqes;
       char buff[BLOCK_SZ];
       off_t offset;

       /*
        * System call wrappers provided since glibc does not yet
        * provide wrappers for io_uring system calls.
       * */

       int io_uring_setup(unsigned entries, struct io_uring_params *p)
       {
           int ret;
           ret = syscall(__NR_io_uring_setup, entries, p);
           return (ret &lt; 0) ? -errno : ret;
       }

       int io_uring_enter(int ring_fd, unsigned int to_submit,
                          unsigned int min_complete, unsigned int flags)
       {
           int ret;
           ret = syscall(__NR_io_uring_enter, ring_fd, to_submit,
                         min_complete, flags, NULL, 0);
           return (ret &lt; 0) ? -errno : ret;
       }

       int app_setup_uring(void) {
           struct io_uring_params p;
           void *sq_ptr, *cq_ptr;

           /* See <a href="../man2/io_uring_setup.2.html">io_uring_setup</a>(2) for io_uring_params.flags you can set */
           memset(&amp;p, 0, sizeof(p));
           ring_fd = io_uring_setup(QUEUE_DEPTH, &amp;p);
           if (ring_fd &lt; 0) {
               perror("io_uring_setup");
               return 1;
           }

           /*
            * io_uring communication happens via 2 shared kernel-user space ring
            * buffers, which can be jointly mapped with a single mmap() call in
            * kernels &gt;= 5.4.
            */

           int sring_sz = p.sq_off.array + p.sq_entries * sizeof(unsigned);
           int cring_sz = p.cq_off.cqes + p.cq_entries * sizeof(struct io_uring_cqe);

           /* Rather than check for kernel version, the recommended way is to
            * check the features field of the io_uring_params structure, which is a
            * bitmask. If IORING_FEAT_SINGLE_MMAP is set, we can do away with the
            * second mmap() call to map in the completion ring separately.
            */
           if (p.features &amp; IORING_FEAT_SINGLE_MMAP) {
               if (cring_sz &gt; sring_sz)
                   sring_sz = cring_sz;
               cring_sz = sring_sz;
           }

           /* Map in the submission and completion queue ring buffers.
            *  Kernels &lt; 5.4 only map in the submission queue, though.
            */
           sq_ptr = mmap(0, sring_sz, PROT_READ | PROT_WRITE,
                         MAP_SHARED | MAP_POPULATE,
                         ring_fd, IORING_OFF_SQ_RING);
           if (sq_ptr == MAP_FAILED) {
               perror("mmap");
               return 1;
           }

           if (p.features &amp; IORING_FEAT_SINGLE_MMAP) {
               cq_ptr = sq_ptr;
           } else {
               /* Map in the completion queue ring buffer in older kernels separately */
               cq_ptr = mmap(0, cring_sz, PROT_READ | PROT_WRITE,
                             MAP_SHARED | MAP_POPULATE,
                             ring_fd, IORING_OFF_CQ_RING);
               if (cq_ptr == MAP_FAILED) {
                   perror("mmap");
                   return 1;
               }
           }
           /* Save useful fields for later easy reference */
           sring_tail = sq_ptr + p.sq_off.tail;
           sring_mask = sq_ptr + p.sq_off.ring_mask;
           sring_array = sq_ptr + p.sq_off.array;

           /* Map in the submission queue entries array */
           sqes = mmap(0, p.sq_entries * sizeof(struct io_uring_sqe),
                          PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE,
                          ring_fd, IORING_OFF_SQES);
           if (sqes == MAP_FAILED) {
               perror("mmap");
               return 1;
           }

           /* Save useful fields for later easy reference */
           cring_head = cq_ptr + p.cq_off.head;
           cring_tail = cq_ptr + p.cq_off.tail;
           cring_mask = cq_ptr + p.cq_off.ring_mask;
           cqes = cq_ptr + p.cq_off.cqes;

           return 0;
       }

       /*
       * Read from completion queue.
       * In this function, we read completion events from the completion queue.
       * We dequeue the CQE, update and head and return the result of the operation.
       * */

       int read_from_cq() {
           struct io_uring_cqe *cqe;
           unsigned head;

           /* Read barrier */
           head = io_uring_smp_load_acquire(cring_head);
           /*
           * Remember, this is a ring buffer. If head == tail, it means that the
           * buffer is empty.
           * */
           if (head == *cring_tail)
               return -1;

           /* Get the entry */
           cqe = &amp;cqes[head &amp; (*cring_mask)];
           if (cqe-&gt;res &lt; 0)
               fprintf(stderr, "Error: %s\n", strerror(abs(cqe-&gt;res)));

           head++;

           /* Write barrier so that update to the head are made visible */
           io_uring_smp_store_release(cring_head, head);

           return cqe-&gt;res;
       }

       /*
       * Submit a read or a write request to the submission queue.
       * */

       int submit_to_sq(int fd, int op) {
           unsigned index, tail;

           /* Add our submission queue entry to the tail of the SQE ring buffer */
           tail = *sring_tail;
           index = tail &amp; *sring_mask;
           struct io_uring_sqe *sqe = &amp;sqes[index];
           /* Fill in the parameters required for the read or write operation */
           sqe-&gt;opcode = op;
           sqe-&gt;fd = fd;
           sqe-&gt;addr = (unsigned long) buff;
           if (op == IORING_OP_READ) {
               memset(buff, 0, sizeof(buff));
               sqe-&gt;len = BLOCK_SZ;
           }
           else {
               sqe-&gt;len = strlen(buff);
           }
           sqe-&gt;off = offset;

           sring_array[index] = index;
           tail++;

           /* Update the tail */
           io_uring_smp_store_release(sring_tail, tail);

           /*
           * Tell the kernel we have submitted events with the io_uring_enter()
           * system call. We also pass in the IORING_ENTER_GETEVENTS flag which
           * causes the io_uring_enter() call to wait until min_complete
           * (the 3rd param) events complete.
           * */
           int ret =  io_uring_enter(ring_fd, 1,1,
                                     IORING_ENTER_GETEVENTS);
           if(ret &lt; 0) {
               perror("io_uring_enter");
               return -1;
           }

           return ret;
       }

       int main(int argc, char *argv[]) {
           int res;

           /* Setup io_uring for use */
           if(app_setup_uring()) {
               fprintf(stderr, "Unable to setup uring!\n");
               return 1;
           }

           /*
           * A while loop that reads from stdin and writes to stdout.
           * Breaks on EOF.
           */
           while (1) {
               /* Initiate read from stdin and wait for it to complete */
               submit_to_sq(STDIN_FILENO, IORING_OP_READ);
               /* Read completion queue entry */
               res = read_from_cq();
               if (res &gt; 0) {
                   /* Read successful. Write to stdout. */
                   submit_to_sq(STDOUT_FILENO, IORING_OP_WRITE);
                   read_from_cq();
               } else if (res == 0) {
                   /* reached EOF */
                   break;
               }
               else if (res &lt; 0) {
                   /* Error reading file */
                   fprintf(stderr, "Error: %s\n", strerror(abs(res)));
                   break;
               }
               offset += res;
           }

           return 0;
       }

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man2/io_uring_enter.2.html">io_uring_enter</a></b>(2) <b><a href="../man2/io_uring_register.2.html">io_uring_register</a></b>(2) <b><a href="../man2/io_uring_setup.2.html">io_uring_setup</a></b>(2)

Linux                                              2020-07-26                                        <u><a href="../man7/io_uring.7.html">io_uring</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>