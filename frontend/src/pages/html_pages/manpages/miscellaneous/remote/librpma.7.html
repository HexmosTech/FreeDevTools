<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>librpma - remote persistent memory access library</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/librpma-dev">librpma-dev_1.3.0-2build2_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       <b>librpma</b> - remote persistent memory access library

</pre><h4><b>SYNOPSIS</b></h4><pre>
             #include &lt;librpma.h&gt;
             <b>cc</b> ... <b>-lrpma</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       librpma  is a C library to simplify accessing persistent memory (PMem) on remote hosts over Remote Direct
       Memory Access (RDMA).

       The librpma library provides two possible schemes of operation: Remote Memory Access and Messaging.  Both
       of them are available over a connection established between two peers. Both of these schemes can make use
       of  PMem  as  well  as  DRAM  for  the  sake  of building efficient and scalable Remote Persistent Memory
       Accessing (RPMA) applications.

</pre><h4><b>REMOTE</b> <b>MEMORY</b> <b>ACCESS</b></h4><pre>
       The librpma library implements four basic API calls dedicated for accessing a remote memory:

       •  <b>rpma_read</b>() - initiates transferring data from the remote memory to the local memory,

       •  <b>rpma_write</b>() - initiates transferring data from the local memory to the remote memory),

       •  <b>rpma_atomic_write</b>()  -  works  like  <b>rpma_write</b>(),  but  it  allows  transferring  8  bytes  of   data
          (RPMA_ATOMIC_WRITE_ALIGNMENT)    and   storing   them   atomically   in   the   remote   memory   (see
          <b><a href="../man3/rpma_atomic_write.3.html">rpma_atomic_write</a></b>(3) for details and restrictions), and:

       •  <b>rpma_flush</b>() - initiates finalizing a transfer of  data  to  the  remote  memory.  Possible  types  of
          <b>rpma_flush</b>() operation:

          •  RPMA_FLUSH_TYPE_PERSISTENT - flush data down to the persistent domain,

          •  RPMA_FLUSH_TYPE_VISIBILITY - flush data deep enough to make it visible on the remote node.

       All the above functions use the attribute flags to set the completion notification indicator:

       •  RPMA_F_COMPLETION_ON_ERROR - generates the completion only on error

       •  RPMA_F_COMPLETION_ALWAYS - generates the completion regardless of a result of the operation.

       All of these operations are considered as finished when the respective completion is generated.

</pre><h4><b>DIRECT</b> <b>WRITE</b> <b>TO</b> <b>PMEM</b></h4><pre>
       <b>Direct</b>  <b>Write</b>  <b>to</b>  <b>PMem</b>  is  a  feature  of a platform and its configuration which allows an RDMA-capable
       network interface to write data to platform's PMem in a persistent way. It may be impossible  because  of
       e.g. caching mechanisms existing on the data's way. When <b>Direct</b> <b>Write</b> <b>to</b> <b>PMem</b> is impossible, operating in
       the  way  assuming  it  is  possible may corrupt data on PMem, so this is why <b>Direct</b> <b>Write</b> <b>to</b> <b>PMem</b> is not
       enabled by default.

       On the current Intel platforms, the only thing you have to do in order to enable <b>Direct</b> <b>Write</b> <b>to</b> <b>PMem</b>  is
       turning  off Intel Direct Data I/O (DDIO). Sometimes, you can turn off DDIO either globally for the whole
       platform or for a specific PCIe Root Port.  For details, please see the manual of your platform.

       When you have a platform which allows <b>Direct</b> <b>Write</b> <b>to</b> <b>PMem</b>, you have to declare this is the case in  your
       peer's  configuration.  The  peer's  configuration  has  to be transferred to all the peers which want to
       execute <b>rpma_flush</b>() with RPMA_FLUSH_TYPE_PERSISTENT against the  platform's  PMem  and  applied  to  the
       connection object which safeguards access to PMem.

       •  <b>rpma_peer_cfg_set_direct_write_to_pmem</b>() - declare <b>Direct</b> <b>Write</b> <b>to</b> <b>PMem</b> support

       •  <b>rpma_peer_cfg_get_descriptor</b>() - get the descriptor of the peer configuration

       •  <b>rpma_peer_cfg_from_descriptor</b>() - create a peer configuration from the descriptor

       •  <b>rpma_conn_apply_remote_peer_cfg</b>() - apply remote peer cfg to the connection

       For details on how to use these APIs please see https://github.com/pmem/rpma/tree/main/examples/05-flush-
       to-persistent.

</pre><h4><b>CLIENT</b> <b>OPERATION</b></h4><pre>
       A  client  is  the active side of the process of establishing a connection. A role of the peer during the
       process of establishing connection does not determine direction of the  data  flow  (neither  via  Remote
       Memory  Access  nor  via  Messaging).  After  establishing  the  connection  both  peers  have  the  same
       capabilities.

       The client, in order to establish a connection, has to perform the following steps:

       •  <b>rpma_conn_req_new</b>() - create a new outgoing connection request object

       •  <b>rpma_conn_req_connect</b>() - initiate processing the connection request

       •  <b>rpma_conn_next_event</b>() - wait for the RPMA_CONN_ESTABLISHED event

       After establishing the connection both peers can perform Remote Memory Access and/or Messaging  over  the
       connection.

       The client, in order to close a connection, has to perform the following steps:

       •  <b>rpma_conn_disconnect</b>() - initiate disconnection

       •  <b>rpma_conn_next_event</b>() - wait for the RPMA_CONN_CLOSED event

       •  <b>rpma_conn_delete</b>() - delete the closed connection

</pre><h4><b>SERVER</b> <b>OPERATION</b></h4><pre>
       A  server  is  the passive side of the process of establishing a connection. Note that after establishing
       the connection both peers have the same capabilities.

       The server, in order to establish a connection, has to perform the following steps:

       •  <b>rpma_ep_listen</b>() - create a listening endpoint

       •  <b>rpma_ep_next_conn_req</b>() - obtain an incoming connection request

       •  <b>rpma_conn_req_connect</b>() - initiate connecting the connection request

       •  <b>rpma_conn_next_event</b>() - wait for the RPMA_CONN_ESTABLISHED event

       After establishing the connection both peers can perform Remote Memory Access and/or Messaging  over  the
       connection.

       The server, in order to close a connection, has to perform the following steps:

       •  <b>rpma_conn_next_event</b>() - wait for the RPMA_CONN_CLOSED event

       •  <b>rpma_conn_disconnect</b>() - disconnect the connection

       •  <b>rpma_conn_delete</b>() - delete the closed connection

       When no more incoming connections are expected, the server can stop waiting for them:

       •  <b>rpma_ep_shutdown</b>() - stop listening and delete the endpoint

</pre><h4><b>MEMORY</b> <b>MANAGEMENT</b></h4><pre>
       Every  piece of memory (either volatile or persistent) must be registered and its usage must be specified
       in order to be used in Remote Memory Access or Messaging. This can be done  using  the  following  memory
       management librpma functions:

       •  <b>rpma_mr_reg</b>() which registers a memory region and creates a local memory registration object and

       •  <b>rpma_mr_dereg</b>() which deregisters the memory region and deletes the local memory registration object.

       A  description  of  the registered memory region sometimes has to be transferred via network to the other
       side of the connection. In order to do that a network-transferable description  of  the  provided  memory
       region  (called  'descriptor') has to be created using <b>rpma_mr_get_descriptor</b>(). On the other side of the
       connection the received descriptor should be decoded using <b>rpma_mr_remote_from_descriptor</b>(). It creates a
       remote memory region's structure that allows for Remote Memory Access.

</pre><h4><b>MESSAGING</b></h4><pre>
       The librpma messaging API allows transferring messages (buffers of arbitrary  data)  between  the  peers.
       Transferring  messages requires preparing buffers (memory regions) on the remote side to receive the sent
       data. The received data are written to those dedicated buffers and the sender does not  have  to  have  a
       respective  remote memory region object to send a message.  The memory buffers used for messaging have to
       be registered using <b>rpma_mr_reg</b>() prior to <b>rpma_send</b>() or <b>rpma_recv</b>() function call.

       The librpma library implements the following messaging API:

       •  <b>rpma_send</b>() - initiates the send operation which transfers a message from the local  memory  to  other
          side of the connection,

       •  <b>rpma_recv</b>()  -  initiates  the receive operation which prepares a buffer for a message sent from other
          side of the connection,

       •  <b>rpma_conn_req_recv</b>() works as <b>rpma_recv</b>(), but it may be used before the connection is established.

       All of these operations are considered as finished when the respective completion is generated.

</pre><h4><b>COMPLETIONS</b></h4><pre>
       RDMA operations generate complitions that notify a user that the respective operation has been completed.

       The following operations are available in librpma:

       •  IBV_WC_RDMA_READ - RMA read operation

       •  IBV_WC_RDMA_WRITE - RMA write operation

       •  IBV_WC_SEND - messaging send operation

       •  IBV_WC_RECV - messaging receive operation

       •  IBV_WC_RECV_RDMA_WITH_IMM - messaging receive operation for RMA write operation with immediate data

       All operations generate completion on error. The operations posted with the <b>RPMA_F_COMPLETION_ALWAYS</b> flag
       also generate a completion on success.  Completion codes are reused from the  libibverbs  library,  where
       the  IBV_WC_SUCCESS status indicates the successful completion of an operation. Completions are collected
       in the completion queue (CQ) (see the <b>QUEUES,</b> <b>PERFORMANCE</b> <b>AND</b> <b>RESOURCE</b> <b>USE</b> section for  more  details  on
       queues).

       The librpma library implements the following API for handling completions:

       •  <b>rpma_conn_get_cq</b>() gets the connection's main CQ,

       •  <b>rpma_conn_get_rcq</b>() gets the connection's receive CQ,

       •  <b>rpma_cq_wait</b>()  waits  for  an  incoming completion from the specified CQ (main or receive CQ) - if it
          succeeds the completion can be collected using <b>rpma_cq_get_wc</b>(),

       •  <b>rpma_cq_get_wc</b>() receives the next available completion of an already posted operation.

</pre><h4><b>PEER</b></h4><pre>
       A peer is an abstraction representing an RDMA-capable device.  All other RPMA objects have to be  created
       in the context of a peer.  A peer allows one to:

       •  establish connections (Client Operation)

       •  register memory regions (Memory Management)

       •  create endpoints for listening for incoming connections (Server Operation)

       At  the  beginning,  in  order to create a peer, a user has to obtain an RDMA device context by the given
       IPv4/IPv6 address using <b>rpma_utils_get_ibv_context</b>(). Then  a  new  peer  object  can  be  created  using
       <b>rpma_peer_new</b>() and deleted using <b>rpma_peer_delete</b>().

</pre><h4><b>SYNCHRONOUS</b> <b>AND</b> <b>ASYNCHRONOUS</b> <b>MODES</b></h4><pre>
       By default, all endpoints and connections operate in the synchronous mode where:

       •  <b>rpma_ep_next_conn_req</b>(),

       •  <b>rpma_cq_wait</b>() and

       •  <b>rpma_conn_get_next_event</b>()

       are  blocking  calls.  You  can  make  those  API  calls  non-blocking  by  modifying the respective file
       descriptors:

       •  <b>rpma_ep_get_fd</b>() - provides a file descriptor for <b>rpma_ep_next_conn_req</b>()

       •  <b>rpma_cq_get_fd</b>() - provides a file descriptor for <b>rpma_cq_wait</b>()

       •  <b>rpma_conn_get_event_fd</b>() - provides a file descriptor for <b>rpma_conn_get_next_event</b>()

       When you have a file descriptor, you can make it non-blocking using <b><a href="../man2/fcntl.2.html">fcntl</a></b>(2) as follows:

               int ret = fcntl(fd, F_GETFL);
               fcntl(fd, F_SETFL, flags | O_NONBLOCK);

       Such change makes the respective API call non-blocking automatically.

       The provided file descriptors can also be used for scalable I/O handling like <b><a href="../man7/epoll.7.html">epoll</a></b>(7).

       Please   see   the   example    showing    how    to    make    use    of    RPMA    file    descriptors:
       https://github.com/pmem/rpma/tree/main/examples/06-multiple-connections

</pre><h4><b>QUEUES,</b> <b>PERFORMANCE</b> <b>AND</b> <b>RESOURCE</b> <b>USE</b></h4><pre>
       <b>Remote</b>  <b>Memory</b>  <b>Access</b>  operations,  <b>Messaging</b>  operations  and their <b>Completions</b> consume space in queues
       allocated in an RDMA-capable network interface (RNIC) hardware for each of the connections. You  must  be
       aware of the existence of these queues:

       •  completion  queue  <b>(CQ)</b>  where  completions  of  operations  are  placed, either when a completion was
          required by a user (RPMA_F_COMPLETION_ALWAYS) or a completion  with  an  error  occurred.  All  <b>Remote</b>
          <b>Memory</b> <b>Access</b> operations and <b>Messaging</b> operations can consume <b>CQ</b> space.

       •  send queue <b>(SQ)</b> where all <b>Remote</b> <b>Memory</b> <b>Access</b> operations and <b>rpma_send</b>() operations are placed before
          they are executed by RNIC.

       •  receive  queue  <b>(RQ)</b>  where <b>rpma_recv</b>() entries are placed before they are consumed by the <b>rpma_send</b>()
          coming from another side of the connection.

       You must assume <b>SQ</b> and <b>RQ</b> entries occupy the place in their respective queue till:

       •  a respective operation's completion is generated or

       •  a completion of an operation, which was scheduled later, is generated.

       You must also be aware that RNIC has limited resources so it is impossible to store a very  long  set  of
       queues for many possibly existing connections. If all of the queues will not fit into RNIC's resources it
       will  start  using the platform's memory for this purpose. In this case, the performance will be degraded
       because of inevitable cache misses.

       Because the length of queues has so profound impact on  the  performance  of  RPMA  application  you  can
       configure the length of each of the queues separately for each of the connections:

       •  <b>rpma_conn_cfg_set_cq_size</b>() - set length of <b>CQ</b>

       •  <b>rpma_conn_cfg_set_sq_size</b>() - set length of <b>SQ</b>

       •  <b>rpma_conn_cfg_set_rq_size</b>() - set length of <b>RQ</b>

       When  the  connection  configuration  object is ready it has to be used for either <b>rpma_conn_req_new</b>() or
       <b>rpma_ep_next_conn_req</b>() for the settings to take effect.

</pre><h4><b>THREAD</b> <b>SAFETY</b></h4><pre>
       The analysis of thread safety of the librpma library is described  in  details  in  the  THREAD_SAFETY.md
       file:

               https://github.com/pmem/rpma/blob/main/THREAD_SAFETY.md

</pre><h4><b>ON-DEMAND</b> <b>PAGING</b> <b>SUPPORT</b></h4><pre>
       On-Demand-Paging  (ODP)  is  a  technique  that  simplifies the memory registration process (for example,
       applications no longer need to pin down the underlying physical pages of the address space and track  the
       validity  of the mappings). On-Demand Paging is available if both the hardware and the kernel support it.
       The detailed description of ODP can be found here:

            https://community.mellanox.com/s/article/understanding-on-demand-paging--odp-x

       State of ODP support can be  checked  using  the  <b>rpma_utils_ibv_context_is_odp_capable</b>()  function  that
       queries the RDMA device context's capabilities and checks if it supports On-Demand Paging.

       The  librpma  library uses ODP automatically if it is supported. ODP support is required to register PMem
       memory region mapped from File System DAX (FSDAX).

</pre><h4><b>DEBUGGING</b> <b>AND</b> <b>ERROR</b> <b>HANDLING</b></h4><pre>
       If a librpma function may fail, it returns a negative error code. Checking if the returned value is  non-
       negative  is  the  only  programmatically  available  way to verify if the API call succeeded.  The exact
       meaning of all error codes is described in the manual of each function.

       The librpma library implements the logging API which may give additional information in case of an  error
       and during normal operation as well, according to the current logging threshold levels.

       The  function  that  will handle all generated log messages can be set using <b>rpma_log_set_function</b>(). The
       logging function can be either the default logging function (built into the library) or  a  user-defined,
       thread-safe,  function.  The  default logging function can write messages to <b><a href="../man3/syslog.3.html">syslog</a></b>(3) and <b><a href="../man3/stderr.3.html">stderr</a></b>(3). The
       logging threshold level can be set or  got  using  <b>rpma_log_set_threshold</b>()  or  <b>rpma_log_get_threshold</b>()
       respectively.

       There      is      an      example      of      the      usage      of     the     logging     functions:
       https://github.com/pmem/rpma/tree/main/examples/log

</pre><h4><b>EXAMPLES</b></h4><pre>
       See https://github.com/pmem/rpma/tree/main/examples for examples of using the librpma API.

</pre><h4><b>ACKNOWLEDGEMENTS</b></h4><pre>
       librpma is built on the top of libibverbs and librdmacm APIs.

</pre><h4><b>DEPRECATING</b></h4><pre>
       Using of the API calls which are marked as deprecated should be avoided, because they will be removed  in
       a new major release.

       NOTE: API calls deprecated in 0.X release will be removed in <b>0.</b>(X+1) release usually.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       https://pmem.io/rpma/

RPMA                                              01 April 2024                                       <u><a href="../man7/librpma.7.html">librpma</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>