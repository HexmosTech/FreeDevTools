<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EXAMPLE: Working as xargs -n1. Argument appending</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/parallel">parallel_20240222+ds-2_all</a> <br><br><pre>
</pre><h4><b>GNU</b> <b>PARALLEL</b> <b>EXAMPLES</b></h4><pre>
   <b>EXAMPLE:</b> <b>Working</b> <b>as</b> <b>xargs</b> <b>-n1.</b> <b>Argument</b> <b>appending</b>
       GNU <b>parallel</b> can work similar to <b>xargs</b> <b>-n1</b>.

       To compress all html files using <b>gzip</b> run:

         find . -name '*.html' | parallel gzip --best

       If the file names may contain a newline use <b>-0</b>. Substitute FOO BAR with FUBAR in all files in this dir
       and subdirs:

         find . -type f -print0 | \
           parallel -q0 perl -i -pe 's/FOO BAR/FUBAR/g'

       Note <b>-q</b> is needed because of the space in 'FOO BAR'.

   <b>EXAMPLE:</b> <b>Simple</b> <b>network</b> <b>scanner</b>
       <b>prips</b> can generate IP-addresses from CIDR notation. With GNU <b>parallel</b> you can build a simple network
       scanner to see which addresses respond to <b>ping</b>:

         prips 130.229.16.0/20 | \
           parallel --timeout 2 -j0 \
             'ping -c 1 {} &gt;/dev/null &amp;&amp; echo {}' 2&gt;/dev/null

   <b>EXAMPLE:</b> <b>Reading</b> <b>arguments</b> <b>from</b> <b>command</b> <b>line</b>
       GNU <b>parallel</b> can take the arguments from command line instead of stdin (standard input). To compress all
       html files in the current dir using <b>gzip</b> run:

         parallel gzip --best ::: *.html

       To convert *.wav to *.mp3 using LAME running one process per CPU run:

         parallel lame {} -o {.}.mp3 ::: *.wav

   <b>EXAMPLE:</b> <b>Inserting</b> <b>multiple</b> <b>arguments</b>
       When moving a lot of files like this: <b>mv</b> <b>*.log</b> <b>destdir</b> you will sometimes get the error:

         bash: <a href="file:/bin/mv">/bin/mv</a>: Argument list too long

       because there are too many files. You can instead do:

         ls | grep -E '\.log$' | parallel mv {} destdir

       This will run <b>mv</b> for each file. It can be done faster if <b>mv</b> gets as many arguments that will fit on the
       line:

         ls | grep -E '\.log$' | parallel -m mv {} destdir

       In many shells you can also use <b>printf</b>:

         printf '%s\0' *.log | parallel -0 -m mv {} destdir

   <b>EXAMPLE:</b> <b>Context</b> <b>replace</b>
       To remove the files <u>pict0000.jpg</u> .. <u>pict9999.jpg</u> you could do:

         seq -w 0 9999 | parallel rm pict{}.jpg

       You could also do:

         seq -w 0 9999 | perl -pe 's/(.*)/pict$1.jpg/' | parallel -m rm

       The first will run <b>rm</b> 10000 times, while the last will only run <b>rm</b> as many times needed to keep the
       command line length short enough to avoid <b>Argument</b> <b>list</b> <b>too</b> <b>long</b> (it typically runs 1-2 times).

       You could also run:

         seq -w 0 9999 | parallel -X rm pict{}.jpg

       This will also only run <b>rm</b> as many times needed to keep the command line length short enough.

   <b>EXAMPLE:</b> <b>Compute</b> <b>intensive</b> <b>jobs</b> <b>and</b> <b>substitution</b>
       If ImageMagick is installed this will generate a thumbnail of a jpg file:

         convert -geometry 120 foo.jpg thumb_foo.jpg

       This will run with number-of-cpus jobs in parallel for all jpg files in a directory:

         ls *.jpg | parallel convert -geometry 120 {} thumb_{}

       To do it recursively use <b>find</b>:

         find . -name '*.jpg' | \
           parallel convert -geometry 120 {} {}_thumb.jpg

       Notice how the argument has to start with <b>{}</b> as <b>{}</b> will include path (e.g. running <b>convert</b> <b>-geometry</b> <b>120</b>
       <b>./foo/bar.jpg</b> <b>thumb_./foo/bar.jpg</b> would clearly be wrong). The command will generate files like
       ./foo/bar.jpg_thumb.jpg.

       Use <b>{.}</b> to avoid the extra .jpg in the file name. This command will make files like ./foo/bar_thumb.jpg:

         find . -name '*.jpg' | \
           parallel convert -geometry 120 {} {.}_thumb.jpg

   <b>EXAMPLE:</b> <b>Substitution</b> <b>and</b> <b>redirection</b>
       This will generate an uncompressed version of .gz-files next to the .gz-file:

         parallel zcat {} "&gt;"{.} ::: *.gz

       Quoting of &gt; is necessary to postpone the redirection. Another solution is to quote the whole command:

         parallel "zcat {} &gt;{.}" ::: *.gz

       Other special shell characters (such as * ; $ &gt; &lt; |  &gt;&gt; &lt;&lt;) also need to be put in quotes, as they may
       otherwise be interpreted by the shell and not given to GNU <b>parallel</b>.

   <b>EXAMPLE:</b> <b>Composed</b> <b>commands</b>
       A job can consist of several commands. This will print the number of files in each directory:

         ls | parallel 'echo -n {}" "; ls {}|wc -l'

       To put the output in a file called &lt;name&gt;.dir:

         ls | parallel '(echo -n {}" "; ls {}|wc -l) &gt;{}.dir'

       Even small shell scripts can be run by GNU <b>parallel</b>:

         find . | parallel 'a={}; name=${a##*/};' \
           'upper=$(echo "$name" | tr "[:lower:]" "[:upper:]");'\
           'echo "$name - $upper"'

         ls | parallel 'mv {} "$(echo {} | tr "[:upper:]" "[:lower:]")"'

       Given a list of URLs, list all URLs that fail to download. Print the line number and the URL.

         cat urlfile | parallel "wget {} 2&gt;/dev/null || grep -n {} urlfile"

       Create a mirror directory with the same file names except all files and symlinks are empty files.

         cp -rs /the/source/dir mirror_dir
         find mirror_dir -type l | parallel -m rm {} '&amp;&amp;' touch {}

       Find the files in a list that do not exist

         cat file_list | parallel 'if [ ! -e {} ] ; then echo {}; fi'

   <b>EXAMPLE:</b> <b>Composed</b> <b>command</b> <b>with</b> <b>perl</b> <b>replacement</b> <b>string</b>
       You have a bunch of file. You want them sorted into dirs. The dir of each file should be named the first
       letter of the file name.

         parallel 'mkdir -p {=s/(.).*/$1/=}; mv {} {=s/(.).*/$1/=}' ::: *

   <b>EXAMPLE:</b> <b>Composed</b> <b>command</b> <b>with</b> <b>multiple</b> <b>input</b> <b>sources</b>
       You have a dir with files named as 24 hours in 5 minute intervals: 00:00, 00:05, 00:10 .. 23:55. You want
       to find the files missing:

         parallel [ -f {1}:{2} ] "||" echo {1}:{2} does not exist \
           ::: {00..23} ::: {00..55..5}

   <b>EXAMPLE:</b> <b>Calling</b> <b>Bash</b> <b>functions</b>
       If the composed command is longer than a line, it becomes hard to read. In Bash you can use functions.
       Just remember to <b>export</b> <b>-f</b> the function.

         doit() {
           echo Doing it for $1
           sleep 2
           echo Done with $1
         }
         export -f doit
         parallel doit ::: 1 2 3

         doubleit() {
           echo Doing it for $1 $2
           sleep 2
           echo Done with $1 $2
         }
         export -f doubleit
         parallel doubleit ::: 1 2 3 ::: a b

       To do this on remote servers you need to transfer the function using <b>--env</b>:

         parallel --env doit -S server doit ::: 1 2 3
         parallel --env doubleit -S server doubleit ::: 1 2 3 ::: a b

       If your environment (aliases, variables, and functions) is small you can copy the full environment
       without having to <b>export</b> <b>-f</b> anything. See <b>env_parallel</b>.

   <b>EXAMPLE:</b> <b>Function</b> <b>tester</b>
       To test a program with different parameters:

         tester() {
           if (eval "$@") &gt;&amp;/dev/null; then
             perl -e 'printf "\033[30;102m[ OK ]\033[0m @ARGV\n"' "$@"
           else
             perl -e 'printf "\033[30;101m[FAIL]\033[0m @ARGV\n"' "$@"
           fi
         }
         export -f tester
         parallel tester my_program ::: arg1 arg2
         parallel tester exit ::: 1 0 2 0

       If <b>my_program</b> fails a red FAIL will be printed followed by the failing command; otherwise a green OK will
       be printed followed by the command.

   <b>EXAMPLE:</b> <b>Identify</b> <b>few</b> <b>failing</b> <b>jobs</b>
       <b>--bar</b> works best if jobs have no output. If the failing jobs have output you can identify the jobs like
       this:

         job-with-few-failures() {
             # Force reproducibility
             RANDOM=$1
             # This fails 1% (328 of 32768)
             if [ $RANDOM -lt 328 ] ; then
               echo Failed $1
             fi
         }
         export -f job-with-few-failures
         seq 1000 | parallel --bar --tag job-with-few-failures

   <b>EXAMPLE:</b> <b>Continously</b> <b>show</b> <b>the</b> <b>latest</b> <b>line</b> <b>of</b> <b>output</b>
       It can be useful to monitor the output of running jobs.

       This shows the most recent output line until a job finishes. After which the output of the job is printed
       in full:

         parallel '{} | tee &gt;(cat &gt;&amp;3)' ::: 'command 1' 'command 2' \
           3&gt; &gt;(perl -ne '$|=1;chomp;printf"%.'$COLUMNS's\r",$_." "x100')

   <b>EXAMPLE:</b> <b>Log</b> <b>rotate</b>
       Log rotation renames a logfile to an extension with a higher number: log.1 becomes log.2, log.2 becomes
       log.3, and so on. The oldest log is removed. To avoid overwriting files the process starts backwards from
       the high number to the low number.  This will keep 10 old versions of the log:

         seq 9 -1 1 | parallel -j1 mv log.{} log.'{= $_++ =}'
         mv log log.1

   <b>EXAMPLE:</b> <b>Removing</b> <b>file</b> <b>extension</b> <b>when</b> <b>processing</b> <b>files</b>
       When processing files removing the file extension using <b>{.}</b> is often useful.

       Create a directory for each zip-file and unzip it in that dir:

         parallel 'mkdir {.}; cd {.}; unzip ../{}' ::: *.zip

       Recompress all .gz files in current directory using <b>bzip2</b> running 1 job per CPU in parallel:

         parallel "zcat {} | bzip2 &gt;{.}.bz2 &amp;&amp; rm {}" ::: *.gz

       Convert all WAV files to MP3 using LAME:

         find sounddir -type f -name '*.wav' | parallel lame {} -o {.}.mp3

       Put all converted in the same directory:

         find sounddir -type f -name '*.wav' | \
           parallel lame {} -o mydir/{<a href="file:/.">/.</a>}.mp3

   <b>EXAMPLE:</b> <b>Replacing</b> <b>parts</b> <b>of</b> <b>file</b> <b>names</b>
       If you deal with paired end reads, you will have files like barcode1_R1.fq.gz, barcode1_R2.fq.gz,
       barcode2_R1.fq.gz, and barcode2_R2.fq.gz.

       You want barcode<u>N</u>_R1 to be processed with barcode<u>N</u>_R2.

           parallel --plus myprocess {} {/_R1.fq.gz/_R2.fq.gz} ::: *_R1.fq.gz

       If the barcode does not contain '_R1', you can do:

           parallel --plus myprocess {} {/_R1/_R2} ::: *_R1.fq.gz

   <b>EXAMPLE:</b> <b>Removing</b> <b>strings</b> <b>from</b> <b>the</b> <b>argument</b>
       If you have directory with tar.gz files and want these extracted in the corresponding dir (e.g foo.tar.gz
       will be extracted in the dir foo) you can do:

         parallel --plus 'mkdir {..}; tar -C {..} -xf {}' ::: *.tar.gz

       If you want to remove a different ending, you can use {%string}:

         parallel --plus echo {%_demo} ::: mycode_demo keep_demo_here

       You can also remove a starting string with {#string}

         parallel --plus echo {#demo_} ::: demo_mycode keep_demo_here

       To remove a string anywhere you can use regular expressions with {/regexp/replacement} and leave the
       replacement empty:

         parallel --plus echo {/demo_/} ::: demo_mycode remove_demo_here

   <b>EXAMPLE:</b> <b>Download</b> <b>24</b> <b>images</b> <b>for</b> <b>each</b> <b>of</b> <b>the</b> <b>past</b> <b>30</b> <b>days</b>
       Let us assume a website stores images like:

         https://www.example.com/path/to/YYYYMMDD_##.jpg

       where YYYYMMDD is the date and ## is the number 01-24. This will download images for the past 30 days:

         getit() {
           date=$(date -d "today -$1 days" +%Y%m%d)
           num=$2
           echo wget https://www.example.com/path/to/${date}_${num}.jpg
         }
         export -f getit

         parallel getit ::: $(seq 30) ::: $(seq -w 24)

       <b>$(date</b> <b>-d</b> <b>"today</b> <b>-$1</b> <b>days"</b> <b>+%Y%m%d)</b> will give the dates in YYYYMMDD with <b>$1</b> days subtracted.

   <b>EXAMPLE:</b> <b>Download</b> <b>world</b> <b>map</b> <b>from</b> <b>NASA</b>
       NASA provides tiles to download on earthdata.nasa.gov. Download tiles for Blue Marble world map and
       create a 10240x20480 map.

         base=https://map1a.vis.earthdata.nasa.gov/wmts-geo/wmts.cgi
         service="SERVICE=WMTS&amp;REQUEST=GetTile&amp;VERSION=1.0.0"
         layer="LAYER=BlueMarble_ShadedRelief_Bathymetry"
         set="STYLE=&amp;TILEMATRIXSET=EPSG4326_500m&amp;TILEMATRIX=5"
         tile="TILEROW={1}&amp;TILECOL={2}"
         format="FORMAT=image%2Fjpeg"
         url="$base?$service&amp;$layer&amp;$set&amp;$tile&amp;$format"

         parallel -j0 -q wget "$url" -O {1}_{2}.jpg ::: {0..19} ::: {0..39}
         parallel eval convert +append {}_{0..39}.jpg line{}.jpg ::: {0..19}
         convert -append line{0..19}.jpg world.jpg

   <b>EXAMPLE:</b> <b>Download</b> <b>Apollo-11</b> <b>images</b> <b>from</b> <b>NASA</b> <b>using</b> <b>jq</b>
       Search NASA using their API to get JSON for images related to 'apollo 11' and has 'moon landing' in the
       description.

       The search query returns JSON containing URLs to JSON containing collections of pictures. One of the
       pictures in each of these collection is <u>large</u>.

       <b>wget</b> is used to get the JSON for the search query. <b>jq</b> is then used to extract the URLs of the
       collections. <b>parallel</b> then calls <b>wget</b> to get each collection, which is passed to <b>jq</b> to extract the URLs
       of all images. <b>grep</b> filters out the <u>large</u> images, and <b>parallel</b> finally uses <b>wget</b> to fetch the images.

         base="https://images-api.nasa.gov/search"
         q="q=apollo 11"
         description="description=moon landing"
         media_type="media_type=image"
         wget -O - "$base?$q&amp;$description&amp;$media_type" |
           jq -r .collection.items[].href |
           parallel wget -O - |
           jq -r .[] |
           grep large |
           parallel wget

   <b>EXAMPLE:</b> <b>Download</b> <b>video</b> <b>playlist</b> <b>in</b> <b>parallel</b>
       <b>youtube-dl</b> is an excellent tool to download videos. It can, however, not download videos in parallel.
       This takes a playlist and downloads 10 videos in parallel.

         url='youtu.be/watch?v=0wOf2Fgi3DE&amp;list=UU_cznB5YZZmvAmeq7Y3EriQ'
         export url
         youtube-dl --flat-playlist "https://$url" |
           parallel --tagstring {#} --lb -j10 \
             youtube-dl --playlist-start {#} --playlist-end {#} '"https://$url"'

   <b>EXAMPLE:</b> <b>Prepend</b> <b>last</b> <b>modified</b> <b>date</b> <b>(ISO8601)</b> <b>to</b> <b>file</b> <b>name</b>
         parallel mv {} '{= $a=pQ($_); $b=$_;' \
           '$_=qx{date -r "$a" +%FT%T}; chomp; $_="$_ $b" =}' ::: *

       <b>{=</b> and <b>=}</b> mark a perl expression. <b>pQ</b> perl-quotes the string. <b>date</b> <b>+%FT%T</b> is the date in ISO8601 with
       time.

   <b>EXAMPLE:</b> <b>Save</b> <b>output</b> <b>in</b> <b>ISO8601</b> <b>dirs</b>
       Save output from <b>ps</b> <b>aux</b> every second into dirs named yyyy-mm-ddThh:mm:ss+zz:zz.

         seq 1000 | parallel -N0 -j1 --delay 1 \
           --results '{= $_=`date -Isec`; chomp=}/' ps aux

   <b>EXAMPLE:</b> <b>Digital</b> <b>clock</b> <b>with</b> <b>"blinking"</b> <b>:</b>
       The : in a digital clock blinks. To make every other line have a ':' and the rest a ' ' a perl expression
       is used to look at the 3rd input source. If the value modulo 2 is 1: Use ":" otherwise use " ":

         parallel -k echo {1}'{=3 $_=$_%2?":":" "=}'{2}{3} \
           ::: {0..12} ::: {0..5} ::: {0..9}

   <b>EXAMPLE:</b> <b>Aggregating</b> <b>content</b> <b>of</b> <b>files</b>
       This:

         parallel --header : echo x{X}y{Y}z{Z} \&gt; x{X}y{Y}z{Z} \
         ::: X {1..5} ::: Y {01..10} ::: Z {1..5}

       will generate the files x1y01z1 .. x5y10z5. If you want to aggregate the output grouping on x and z you
       can do this:

         parallel eval 'cat {=s/y01/y*/=} &gt; {=s/y01//=}' ::: *y01*

       For all values of x and z it runs commands like:

         cat x1y*z1 &gt; x1z1

       So you end up with x1z1 .. x5z5 each containing the content of all values of y.

   <b>EXAMPLE:</b> <b>Breadth</b> <b>first</b> <b>parallel</b> <b>web</b> <b>crawler/mirrorer</b>
       This script below will crawl and mirror a URL in parallel.  It downloads first pages that are 1 click
       down, then 2 clicks down, then 3; instead of the normal depth first, where the first link link on each
       page is fetched first.

       Run like this:

         PARALLEL=-j100 ./parallel-crawl <a href="http://gatt.org.yeslab.org/">http://gatt.org.yeslab.org/</a>

       Remove the <b>wget</b> part if you only want a web crawler.

       It works by fetching a page from a list of URLs and looking for links in that page that are within the
       same starting URL and that have not already been seen. These links are added to a new queue. When all the
       pages from the list is done, the new queue is moved to the list of URLs and the process is started over
       until no unseen links are found.

         #!<a href="file:/bin/bash">/bin/bash</a>

         # E.g. <a href="http://gatt.org.yeslab.org/">http://gatt.org.yeslab.org/</a>
         URL=$1
         # Stay inside the start dir
         BASEURL=$(echo $URL | perl -pe 's:#.*::; s:(/<a href="file:/.">/.</a>*/)[^/]*:$1:')
         URLLIST=$(mktemp urllist.XXXX)
         URLLIST2=$(mktemp urllist.XXXX)
         SEEN=$(mktemp seen.XXXX)

         # Spider to get the URLs
         echo $URL &gt;$URLLIST
         cp $URLLIST $SEEN

         while [ -s $URLLIST ] ; do
           cat $URLLIST |
             parallel lynx -listonly -image_links -dump {} \; \
               wget -qm -l1 -Q1 {} \; echo Spidered: {} \&gt;\&amp;2 |
               perl -ne 's/#.*//; s/\s+\d+.\s(\S+)$/$1/ and
                 do { $seen{$1}++ or print }' |
             grep -F $BASEURL |
             grep -v -x -F -f $SEEN | tee -a $SEEN &gt; $URLLIST2
           mv $URLLIST2 $URLLIST
         done

         rm -f $URLLIST $URLLIST2 $SEEN

   <b>EXAMPLE:</b> <b>Process</b> <b>files</b> <b>from</b> <b>a</b> <b>tar</b> <b>file</b> <b>while</b> <b>unpacking</b>
       If the files to be processed are in a tar file then unpacking one file and processing it immediately may
       be faster than first unpacking all files.

         tar xvf foo.tgz | perl -ne 'print $l;$l=$_;END{print $l}' | \
           parallel echo

       The Perl one-liner is needed to make sure the file is complete before handing it to GNU <b>parallel</b>.

   <b>EXAMPLE:</b> <b>Rewriting</b> <b>a</b> <b>for-loop</b> <b>and</b> <b>a</b> <b>while-read-loop</b>
       for-loops like this:

         (for x in `cat list` ; do
           do_something $x
         done) | process_output

       and while-read-loops like this:

         cat list | (while read x ; do
           do_something $x
         done) | process_output

       can be written like this:

         cat list | parallel do_something | process_output

       For example: Find which host name in a list has IP address 1.2.3 4:

         cat hosts.txt | parallel -P 100 host | grep 1.2.3.4

       If the processing requires more steps the for-loop like this:

         (for x in `cat list` ; do
           no_extension=${x%.*};
           do_step1 $x scale $no_extension.jpg
           do_step2 &lt;$x $no_extension
         done) | process_output

       and while-loops like this:

         cat list | (while read x ; do
           no_extension=${x%.*};
           do_step1 $x scale $no_extension.jpg
           do_step2 &lt;$x $no_extension
         done) | process_output

       can be written like this:

         cat list | parallel "do_step1 {} scale {.}.jpg ; do_step2 &lt;{} {.}" |\
           process_output

       If the body of the loop is bigger, it improves readability to use a function:

         (for x in `cat list` ; do
           do_something $x
           [... 100 lines that do something with $x ...]
         done) | process_output

         cat list | (while read x ; do
           do_something $x
           [... 100 lines that do something with $x ...]
         done) | process_output

       can both be rewritten as:

         doit() {
           x=$1
           do_something $x
           [... 100 lines that do something with $x ...]
         }
         export -f doit
         cat list | parallel doit

   <b>EXAMPLE:</b> <b>Rewriting</b> <b>nested</b> <b>for-loops</b>
       Nested for-loops like this:

         (for x in `cat xlist` ; do
           for y in `cat ylist` ; do
             do_something $x $y
           done
         done) | process_output

       can be written like this:

         parallel do_something {1} {2} :::: xlist ylist | process_output

       Nested for-loops like this:

         (for colour in red green blue ; do
           for size in S M L XL XXL ; do
             echo $colour $size
           done
         done) | sort

       can be written like this:

         parallel echo {1} {2} ::: red green blue ::: S M L XL XXL | sort

   <b>EXAMPLE:</b> <b>Finding</b> <b>the</b> <b>lowest</b> <b>difference</b> <b>between</b> <b>files</b>
       <b>diff</b> is good for finding differences in text files. <b>diff</b> <b>|</b> <b>wc</b> <b>-l</b> gives an indication of the size of the
       difference. To find the differences between all files in the current dir do:

         parallel --tag 'diff {1} {2} | wc -l' ::: * ::: * | sort -nk3

       This way it is possible to see if some files are closer to other files.

   <b>EXAMPLE:</b> <b>for-loops</b> <b>with</b> <b>column</b> <b>names</b>
       When doing multiple nested for-loops it can be easier to keep track of the loop variable if is is named
       instead of just having a number. Use <b>--header</b> <b>:</b> to let the first argument be an named alias for the
       positional replacement string:

         parallel --header : echo {colour} {size} \
           ::: colour red green blue ::: size S M L XL XXL

       This also works if the input file is a file with columns:

         cat addressbook.tsv | \
           parallel --colsep '\t' --header : echo {Name} {E-mail address}

   <b>EXAMPLE:</b> <b>All</b> <b>combinations</b> <b>in</b> <b>a</b> <b>list</b>
       GNU <b>parallel</b> makes all combinations when given two lists.

       To make all combinations in a single list with unique values, you repeat the list and use replacement
       string <b>{choose_k}</b>:

         parallel --plus echo {choose_k} ::: A B C D ::: A B C D

         parallel --plus echo 2{2choose_k} 1{1choose_k} ::: A B C D ::: A B C D

       <b>{choose_k}</b> works for any number of input sources:

         parallel --plus echo {choose_k} ::: A B C D ::: A B C D ::: A B C D

       Where <b>{choose_k}</b> does not care about order, <b>{uniq}</b> cares about order. It simply skips jobs where values
       from different input sources are the same:

         parallel --plus echo {uniq} ::: A B C  ::: A B C  ::: A B C
         parallel --plus echo {1uniq}+{2uniq}+{3uniq} \
           ::: A B C  ::: A B C  ::: A B C

       The behaviour of <b>{choose_k}</b> is undefined, if the input values of each source are different.

   <b>EXAMPLE:</b> <b>Which</b> <b>git</b> <b>branches</b> <b>are</b> <b>the</b> <b>most</b> <b>similar</b>
       If you have a ton of branches in git, it may be useful to see how similar the branches are. This gives a
       rough estimate:

         parallel --trim rl --plus --tag 'git diff {choose_k} | wc -c' \
           :::: &lt;(git branch | grep -v '*')  &lt;(git branch | grep -v '*') |
           sort -k3n

   <b>EXAMPLE:</b> <b>From</b> <b>a</b> <b>to</b> <b>b</b> <b>and</b> <b>b</b> <b>to</b> <b>c</b>
       Assume you have input like:

         aardvark
         babble
         cab
         dab
         each

       and want to run combinations like:

         aardvark babble
         babble cab
         cab dab
         dab each

       If the input is in the file in.txt:

         parallel echo {1} - {2} ::::+ &lt;(head -n -1 in.txt) &lt;(tail -n +2 in.txt)

       If the input is in the array $a here are two solutions:

         seq $((${#a[@]}-1)) | \
           env_parallel --env a echo '${a[{=$_--=}]} - ${a[{}]}'
         parallel echo {1} - {2} ::: "${a[@]::${#a[@]}-1}" :::+ "${a[@]:1}"

   <b>EXAMPLE:</b> <b>Count</b> <b>the</b> <b>differences</b> <b>between</b> <b>all</b> <b>files</b> <b>in</b> <b>a</b> <b>dir</b>
       Using <b>--results</b> the results are saved in /tmp/diffcount*.

         parallel --results /tmp/diffcount "diff -U 0 {1} {2} | \
           tail -n +3 |grep -v '^@'|wc -l" ::: * ::: *

       To see the difference between file A and file B look at the file '/tmp/diffcount/1/A/2/B'.

   <b>EXAMPLE:</b> <b>Speeding</b> <b>up</b> <b>fast</b> <b>jobs</b>
       Starting a job on the local machine takes around 3-10 ms. This can be a big overhead if the job takes
       very few ms to run. Often you can group small jobs together using <b>-X</b> which will make the overhead less
       significant. Compare the speed of these:

         seq -w 0 9999 | parallel touch pict{}.jpg
         seq -w 0 9999 | parallel -X touch pict{}.jpg

       If your program cannot take multiple arguments, then you can use GNU <b>parallel</b> to spawn multiple GNU
       <b>parallel</b>s:

         seq -w 0 9999999 | \
           parallel -j10 -q -I,, --pipe parallel -j0 touch pict{}.jpg

       If <b>-j0</b> normally spawns 252 jobs, then the above will try to spawn 2520 jobs. On a normal GNU/Linux system
       you can spawn 32000 jobs using this technique with no problems. To raise the 32000 jobs limit raise
       <a href="file:/proc/sys/kernel/pid_max">/proc/sys/kernel/pid_max</a> to 4194303.

       If you do not need GNU <b>parallel</b> to have control over each job (so no need for <b>--retries</b> or <b>--joblog</b> or
       similar), then it can be even faster if you can generate the command lines and pipe those to a shell. So
       if you can do this:

         mygenerator | sh

       Then that can be parallelized like this:

         mygenerator | parallel --pipe --block 10M sh

       E.g.

         mygenerator() {
           seq 10000000 | perl -pe 'print "echo This is fast job number "';
         }
         mygenerator | parallel --pipe --block 10M sh

       The overhead is 100000 times smaller namely around 100 nanoseconds per job.

   <b>EXAMPLE:</b> <b>Using</b> <b>shell</b> <b>variables</b>
       When using shell variables you need to quote them correctly as they may otherwise be interpreted by the
       shell.

       Notice the difference between:

         ARR=("My brother's 12\" records are worth &lt;\$\$\$&gt;"'!' Foo Bar)
         parallel echo ::: ${ARR[@]} # This is probably not what you want

       and:

         ARR=("My brother's 12\" records are worth &lt;\$\$\$&gt;"'!' Foo Bar)
         parallel echo ::: "${ARR[@]}"

       When using variables in the actual command that contains special characters (e.g. space) you can quote
       them using <b>'"$VAR"'</b> or using "'s and <b>-q</b>:

         VAR="My brother's 12\" records are worth &lt;\$\$\$&gt;"
         parallel -q echo "$VAR" ::: '!'
         export VAR
         parallel echo '"$VAR"' ::: '!'

       If <b>$VAR</b> does not contain ' then <b>"'$VAR'"</b> will also work (and does not need <b>export</b>):

         VAR="My 12\" records are worth &lt;\$\$\$&gt;"
         parallel echo "'$VAR'" ::: '!'

       If you use them in a function you just quote as you normally would do:

         VAR="My brother's 12\" records are worth &lt;\$\$\$&gt;"
         export VAR
         myfunc() { echo "$VAR" "$1"; }
         export -f myfunc
         parallel myfunc ::: '!'

   <b>EXAMPLE:</b> <b>Group</b> <b>output</b> <b>lines</b>
       When running jobs that output data, you often do not want the output of multiple jobs to run together.
       GNU <b>parallel</b> defaults to grouping the output of each job, so the output is printed when the job finishes.
       If you want full lines to be printed while the job is running you can use <b>--line-buffer</b>. If you want
       output to be printed as soon as possible you can use <b>-u</b>.

       Compare the output of:

         parallel wget --progress=dot --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}
         parallel --line-buffer wget --progress=dot --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}
         parallel --latest-line wget --progress=dot --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}
         parallel -u wget --progress=dot --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}

   <b>EXAMPLE:</b> <b>Tag</b> <b>output</b> <b>lines</b>
       GNU <b>parallel</b> groups the output lines, but it can be hard to see where the different jobs begin. <b>--tag</b>
       prepends the argument to make that more visible:

         parallel --tag wget --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}

       <b>--tag</b> works with <b>--line-buffer</b> but not with <b>-u</b>:

         parallel --tag --line-buffer wget --limit-rate=100k \
           https://ftpmirror.gnu.org/parallel/parallel-20{}0822.tar.bz2 \
           ::: {12..16}

       Check the uptime of the servers in <u><a href="file:~/.parallel/sshloginfile">~/.parallel/sshloginfile</a></u>:

         parallel --tag -S .. --nonall uptime

   <b>EXAMPLE:</b> <b>Colorize</b> <b>output</b>
       Give each job a new color. Most terminals support ANSI colors with the escape code "\033[30;3Xm" where 0
       &lt;= X &lt;= 7:

           seq 10 | \
             parallel --tagstring '\033[30;3{=$_=++$::color%8=}m' seq {}
           parallel --rpl '{color} $_="\033[30;3".(++$::color%8)."m"' \
             --tagstring {color} seq {} ::: {1..10}

       To get rid of the initial \t (which comes from <b>--tagstring</b>):

           ... | perl -pe 's/\t//'

   <b>EXAMPLE:</b> <b>Keep</b> <b>order</b> <b>of</b> <b>output</b> <b>same</b> <b>as</b> <b>order</b> <b>of</b> <b>input</b>
       Normally the output of a job will be printed as soon as it completes. Sometimes you want the order of the
       output to remain the same as the order of the input. This is often important, if the output is used as
       input for another system. <b>-k</b> will make sure the order of output will be in the same order as input even
       if later jobs end before earlier jobs.

       Append a string to every line in a text file:

         cat textfile | parallel -k echo {} append_string

       If you remove <b>-k</b> some of the lines may come out in the wrong order.

       Another example is <b>traceroute</b>:

         parallel traceroute ::: qubes-os.org debian.org freenetproject.org

       will give traceroute of qubes-os.org, debian.org and freenetproject.org, but it will be sorted according
       to which job completed first.

       To keep the order the same as input run:

         parallel -k traceroute ::: qubes-os.org debian.org freenetproject.org

       This will make sure the traceroute to qubes-os.org will be printed first.

       A bit more complex example is downloading a huge file in chunks in parallel: Some internet connections
       will deliver more data if you download files in parallel. For downloading files in parallel see:
       "EXAMPLE: Download 10 images for each of the past 30 days". But if you are downloading a big file you can
       download the file in chunks in parallel.

       To download byte 10000000-19999999 you can use <b>curl</b>:

         curl -r 10000000-19999999 https://example.com/the/big/file &gt;file.part

       To download a 1 GB file we need 100 10MB chunks downloaded and combined in the correct order.

         seq 0 99 | parallel -k curl -r \
           {}0000000-{}9999999 https://example.com/the/big/file &gt; file

   <b>EXAMPLE:</b> <b>Parallel</b> <b>grep</b>
       <b>grep</b> <b>-r</b> greps recursively through directories. GNU <b>parallel</b> can often speed this up.

         find . -type f | parallel -k -j150% -n 1000 -m grep -H -n STRING {}

       This will run 1.5 job per CPU, and give 1000 arguments to <b>grep</b>.

       There are situations where the above will be slower than <b>grep</b> <b>-r</b>:

       • If  data is already in RAM. The overhead of starting jobs and buffering output may outweigh the benefit
         of running in parallel.

       • If the files are big. If a file cannot be read in a single seek, the disk may start thrashing.

       The speedup is caused by two factors:

       • On rotating harddisks small files often require a seek for each file. By searching for  more  files  in
         parallel, the arm may pass another wanted file on its way.

       • NVMe drives often perform better by having multiple command running in parallel.

   <b>EXAMPLE:</b> <b>Grepping</b> <b>n</b> <b>lines</b> <b>for</b> <b>m</b> <b>regular</b> <b>expressions.</b>
       The simplest solution to grep a big file for a lot of regexps is:

         grep -f regexps.txt bigfile

       Or if the regexps are fixed strings:

         grep -F -f regexps.txt bigfile

       There are 3 limiting factors: CPU, RAM, and disk I/O.

       RAM  is  easy  to measure: If the <b>grep</b> process takes up most of your free memory (e.g. when running <b>top</b>),
       then RAM is a limiting factor.

       CPU is also easy to measure: If the <b>grep</b> takes &gt;90% CPU in <b>top</b>, then the CPU is a  limiting  factor,  and
       parallelization will speed this up.

       It is harder to see if disk I/O is the limiting factor, and depending on the disk system it may be faster
       or slower to parallelize. The only way to know for certain is to test and measure.

       <u>Limiting</u> <u>factor:</u> <u>RAM</u>

       The  normal <b>grep</b> <b>-f</b> <b>regexps.txt</b> <b>bigfile</b> works no matter the size of bigfile, but if regexps.txt is so big
       it cannot fit into memory, then you need to split this.

       <b>grep</b> <b>-F</b> takes around 100 bytes of RAM and <b>grep</b> takes about 500 bytes of RAM per 1 byte of regexp.  So  if
       regexps.txt is 1% of your RAM, then it may be too big.

       If  you  can  convert  your  regexps into fixed strings do that. E.g. if the lines you are looking for in
       bigfile all looks like:

         ID1 foo bar baz Identifier1 quux
         fubar ID2 foo bar baz Identifier2

       then your regexps.txt can be converted from:

         ID1.*Identifier1
         ID2.*Identifier2

       into:

         ID1 foo bar baz Identifier1
         ID2 foo bar baz Identifier2

       This way you can use <b>grep</b> <b>-F</b> which takes around 80% less memory and is much faster.

       If it still does not fit in memory you can do this:

         parallel --pipe-part -a regexps.txt --block 1M grep -F -f - -n bigfile | \
           sort -un | perl -pe 's/^\d+://'

       The 1M should be your free memory divided by the number of CPU threads and divided by 200 for <b>grep</b> <b>-F</b> and
       by 1000 for normal <b>grep</b>. On GNU/Linux you can do:

         free=$(awk '/^((Swap)?Cached|MemFree|Buffers):/ { sum += $2 }
                     END { print sum }' <a href="file:/proc/meminfo">/proc/meminfo</a>)
         percpu=$((free / 200 / $(parallel --number-of-threads)))k

         parallel --pipe-part -a regexps.txt --block $percpu --compress \
           grep -F -f - -n bigfile | \
           sort -un | perl -pe 's/^\d+://'

       If you can live with duplicated lines and wrong order, it is faster to do:

         parallel --pipe-part -a regexps.txt --block $percpu --compress \
           grep -F -f - bigfile

       <u>Limiting</u> <u>factor:</u> <u>CPU</u>

       If the CPU is the limiting factor parallelization should be done on the regexps:

         cat regexps.txt | parallel --pipe -L1000 --round-robin --compress \
           grep -f - -n bigfile | \
           sort -un | perl -pe 's/^\d+://'

       The command will start one <b>grep</b> per CPU and read <u>bigfile</u> one time  per  CPU,  but  as  that  is  done  in
       parallel,  all  reads except the first will be cached in RAM. Depending on the size of <u>regexps.txt</u> it may
       be faster to use <b>--block</b> <b>10m</b> instead of <b>-L1000</b>.

       Some storage systems perform better when reading multiple chunks in parallel. This is true for some  RAID
       systems and for some network file systems. To parallelize the reading of <u>bigfile</u>:

         parallel --pipe-part --block 100M -a bigfile -k --compress \
           grep -f regexps.txt

       This  will  split  <u>bigfile</u>  into  100MB  chunks and run <b>grep</b> on each of these chunks. To parallelize both
       reading of <u>bigfile</u> and <u>regexps.txt</u> combine the two using <b>--cat</b>:

         parallel --pipe-part --block 100M -a bigfile --cat cat regexps.txt \
           \| parallel --pipe -L1000 --round-robin grep -f - {}

       If a line matches multiple regexps, the line may be duplicated.

       <u>Bigger</u> <u>problem</u>

       If the problem is too big to be solved by this, you are probably ready for Lucene.

   <b>EXAMPLE:</b> <b>Using</b> <b>remote</b> <b>computers</b>
       To run commands on a remote computer SSH needs to be set up  and  you  must  be  able  to  login  without
       entering a password (The commands <b>ssh-copy-id</b>, <b>ssh-agent</b>, and <b>sshpass</b> may help you do that).

       If you need to login to a whole cluster, you typically do not want to accept the host key for every host.
       You want to accept them the first time and be warned if they are ever changed. To do that:

         # Add the servers to the sshloginfile
         (echo servera; echo serverb) &gt; .parallel/my_cluster
         # Make sure .ssh/config exist
         touch .ssh/config
         cp .ssh/config .ssh/config.backup
         # Disable StrictHostKeyChecking temporarily
         (echo 'Host *'; echo StrictHostKeyChecking no) &gt;&gt; .ssh/config
         parallel --slf my_cluster --nonall true
         # Remove the disabling of StrictHostKeyChecking
         mv .ssh/config.backup .ssh/config

       The servers in <b>.parallel/my_cluster</b> are now added in <b>.ssh/known_hosts</b>.

       To run <b>echo</b> on <b>server.example.com</b>:

         seq 10 | parallel --sshlogin server.example.com echo

       To run commands on more than one remote computer run:

         seq 10 | parallel --sshlogin s1.example.com,s2.example.net echo

       Or:

         seq 10 | parallel --sshlogin server.example.com \
           --sshlogin server2.example.net echo

       If the login username is <u>foo</u> on <u>server2.example.net</u> use:

         seq 10 | parallel --sshlogin server.example.com \
           --sshlogin <a href="mailto:foo@server2.example.net">foo@server2.example.net</a> echo

       If your list of hosts is <u>server1-88.example.net</u> with login <u>foo</u>:

         seq 10 | parallel -Sfoo@server{1..88}.example.net echo

       To distribute the commands to a list of computers, make a file <u>mycomputers</u> with all the computers:

         server.example.com
         <a href="mailto:foo@server2.example.com">foo@server2.example.com</a>
         server3.example.com

       Then run:

         seq 10 | parallel --sshloginfile mycomputers echo

       To include the local computer add the special sshlogin ':' to the list:

         server.example.com
         <a href="mailto:foo@server2.example.com">foo@server2.example.com</a>
         server3.example.com
         :

       GNU  <b>parallel</b>  will  try to determine the number of CPUs on each of the remote computers, and run one job
       per CPU - even if the remote computers do not have the same number of CPUs.

       If the number of CPUs on the remote computers is not identified correctly the number of CPUs can be added
       in front. Here the computer has 8 CPUs.

         seq 10 | parallel --sshlogin 8/server.example.com echo

   <b>EXAMPLE:</b> <b>Transferring</b> <b>of</b> <b>files</b>
       To recompress gzipped files with <b>bzip2</b> using a remote computer run:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com \
           --transfer "zcat {} | bzip2 -9 &gt;{.}.bz2"

       This will list the .gz-files in the <u>logs</u> directory and all directories below. Then it will  transfer  the
       files  to <u>server.example.com</u> to the corresponding directory in <u>$HOME/logs</u>. On <u>server.example.com</u> the file
       will be recompressed using <b>zcat</b> and <b>bzip2</b> resulting in the corresponding  file  with  <u>.gz</u>  replaced  with
       <u>.bz2</u>.

       If you want the resulting bz2-file to be transferred back to the local computer add <u>--return</u> <u>{.}.bz2</u>:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com \
           --transfer --return {.}.bz2 "zcat {} | bzip2 -9 &gt;{.}.bz2"

       After  the  recompressing is done the <u>.bz2</u>-file is transferred back to the local computer and put next to
       the original <u>.gz</u>-file.

       If you want to delete the transferred files on the remote computer add <u>--cleanup</u>. This will  remove  both
       the file transferred to the remote computer and the files transferred from the remote computer:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com \
           --transfer --return {.}.bz2 --cleanup "zcat {} | bzip2 -9 &gt;{.}.bz2"

       If  you  want  run  on  several  computers  add  the computers to <u>--sshlogin</u> either using ',' or multiple
       <u>--sshlogin</u>:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com,server2.example.com \
           --sshlogin server3.example.com \
           --transfer --return {.}.bz2 --cleanup "zcat {} | bzip2 -9 &gt;{.}.bz2"

       You can add the local computer using <u>--sshlogin</u> <u>:</u>. This will disable the removing  and  transferring  for
       the local computer only:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com,server2.example.com \
           --sshlogin server3.example.com \
           --sshlogin : \
           --transfer --return {.}.bz2 --cleanup "zcat {} | bzip2 -9 &gt;{.}.bz2"

       Often <u>--transfer</u>, <u>--return</u> and <u>--cleanup</u> are used together. They can be shortened to <u>--trc</u>:

         find logs/ -name '*.gz' | \
           parallel --sshlogin server.example.com,server2.example.com \
           --sshlogin server3.example.com \
           --sshlogin : \
           --trc {.}.bz2 "zcat {} | bzip2 -9 &gt;{.}.bz2"

       With the file <u>mycomputers</u> containing the list of computers it becomes:

         find logs/ -name '*.gz' | parallel --sshloginfile mycomputers \
           --trc {.}.bz2 "zcat {} | bzip2 -9 &gt;{.}.bz2"

       If  the  file <u><a href="file:~/.parallel/sshloginfile">~/.parallel/sshloginfile</a></u> contains the list of computers the special short hand <u>-S</u> <u>..</u> can be
       used:

         find logs/ -name '*.gz' | parallel -S .. \
           --trc {.}.bz2 "zcat {} | bzip2 -9 &gt;{.}.bz2"

   <b>EXAMPLE:</b> <b>Advanced</b> <b>file</b> <b>transfer</b>
       Assume you have files in in/*, want them processed on server, and transferred back into /other/dir:

         parallel -S server --trc /other/dir/./{/}.out \
           cp {/} {/}.out ::: in/./*

   <b>EXAMPLE:</b> <b>Distributing</b> <b>work</b> <b>to</b> <b>local</b> <b>and</b> <b>remote</b> <b>computers</b>
       Convert *.mp3 to *.ogg running one process per CPU on local computer and server2:

         parallel --trc {.}.ogg -S server2,: \
           'mpg321 -w - {} | oggenc -q0 - -o {.}.ogg' ::: *.mp3

   <b>EXAMPLE:</b> <b>Running</b> <b>the</b> <b>same</b> <b>command</b> <b>on</b> <b>remote</b> <b>computers</b>
       To run the command <b>uptime</b> on remote computers you can do:

         parallel --tag --nonall -S server1,server2 uptime

       <b>--nonall</b> reads no arguments. If you have a list of jobs you want to run on each computer you can do:

         parallel --tag --onall -S server1,server2 echo ::: 1 2 3

       Remove <b>--tag</b> if you do not want the sshlogin added before the output.

       If you have a lot of hosts use '-j0' to access more hosts in parallel.

   <b>EXAMPLE:</b> <b>Running</b> <b>'sudo'</b> <b>on</b> <b>remote</b> <b>computers</b>
       Put the password into passwordfile then run:

         parallel --ssh 'cat passwordfile | ssh' --nonall \
           -S user@server1,user@server2 sudo -S ls -l <a href="file:/root">/root</a>

   <b>EXAMPLE:</b> <b>Using</b> <b>remote</b> <b>computers</b> <b>behind</b> <b>NAT</b> <b>wall</b>
       If the workers are behind a NAT wall, you need some trickery to get to them.

       If you can <b>ssh</b> to a jumphost, and reach the workers from there, then the obvious solution would be  this,
       but it <b>does</b> <b>not</b> <b>work</b>:

         parallel --ssh 'ssh jumphost ssh' -S host1 echo ::: DOES NOT WORK

       It does not work because the command is dequoted by <b>ssh</b> twice where as GNU <b>parallel</b> only expects it to be
       dequoted once.

       You can use a bash function and have GNU <b>parallel</b> quote the command:

         jumpssh() { ssh -A jumphost ssh $(parallel --shellquote ::: "$@"); }
         export -f jumpssh
         parallel --ssh jumpssh -S host1 echo ::: this works

       Or you can instead put this in <b><a href="file:~/.ssh/config">~/.ssh/config</a></b>:

         Host host1 host2 host3
           ProxyCommand ssh jumphost.domain nc -w 1 %h 22

       It requires <b>nc(netcat)</b> to be installed on jumphost. With this you can simply:

         parallel -S host1,host2,host3 echo ::: This does work

       <u>No</u> <u>jumphost,</u> <u>but</u> <u>port</u> <u>forwards</u>

       If there is no jumphost but each server has port 22 forwarded from the firewall (e.g. the firewall's port
       22001 = port 22 on host1, 22002 = host2, 22003 = host3) then you can use <b><a href="file:~/.ssh/config">~/.ssh/config</a></b>:

         Host host1.v
           Port 22001
         Host host2.v
           Port 22002
         Host host3.v
           Port 22003
         Host *.v
           Hostname firewall

       And then use host{1..3}.v as normal hosts:

         parallel -S host1.v,host2.v,host3.v echo ::: a b c

       <u>No</u> <u>jumphost,</u> <u>no</u> <u>port</u> <u>forwards</u>

       If  ports cannot be forwarded, you need some sort of VPN to traverse the NAT-wall. TOR is one options for
       that, as it is very easy to get working.

       You need to install TOR and setup a hidden service. In <b>torrc</b> put:

         HiddenServiceDir /var/lib/tor/hidden_service/
         HiddenServicePort 22 127.0.0.1:22

       Then start TOR: <b>/etc/init.d/tor</b> <b>restart</b>

       The  TOR  hostname  is  now  in  <b>/var/lib/tor/hidden_service/hostname</b>  and  is   something   similar   to
       <b>izjafdceobowklhz.onion</b>. Now you simply prepend <b>torsocks</b> to <b>ssh</b>:

         parallel --ssh 'torsocks ssh' -S izjafdceobowklhz.onion \
           -S zfcdaeiojoklbwhz.onion,auclucjzobowklhi.onion echo ::: a b c

       If not all hosts are accessible through TOR:

         parallel -S 'torsocks ssh izjafdceobowklhz.onion,host2,host3' \
           echo ::: a b c

       See more <b>ssh</b> tricks on https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Proxies_and_Jump_Hosts

   <b>EXAMPLE:</b> <b>Use</b> <b>sshpass</b> <b>with</b> <b>ssh</b>
       If you cannot use passwordless login, you may be able to use <b>sshpass</b>:

         seq 10 | parallel -S user-with-password:MyPassword@server echo

       or:

         export SSHPASS='MyPa$$w0rd'
         seq 10 | parallel -S user-with-password:@server echo

   <b>EXAMPLE:</b> <b>Use</b> <b>outrun</b> <b>instead</b> <b>of</b> <b>ssh</b>
       <b>outrun</b>  lets  you  run  a  command on a remote server. <b>outrun</b> sets up a connection to access files at the
       source server, and automatically transfers files. <b>outrun</b> must be installed on the remote system.

       You can use <b>outrun</b> in an sshlogin this way:

         parallel -S 'outrun user@server' command

       or:

         parallel --ssh outrun -S server command

   <b>EXAMPLE:</b> <b>Slurm</b> <b>cluster</b>
       The Slurm Workload Manager is used in many clusters.

       Here is a simple example of using GNU <b>parallel</b> to call <b>srun</b>:

         #!<a href="file:/bin/bash">/bin/bash</a>

         #SBATCH --time 00:02:00
         #SBATCH --ntasks=4
         #SBATCH --job-name GnuParallelDemo
         #SBATCH --output gnuparallel.out

         module purge
         module load gnu_parallel

         my_parallel="parallel --delay .2 -j $SLURM_NTASKS"
         my_srun="srun --export=all --exclusive -n1"
         my_srun="$my_srun --cpus-per-task=1 --cpu-bind=cores"
         $my_parallel "$my_srun" echo This is job {} ::: {1..20}

   <b>EXAMPLE:</b> <b>Parallelizing</b> <b>rsync</b>
       <b>rsync</b> is a great tool, but sometimes it will not fill up the available bandwidth. Running multiple  <b>rsync</b>
       in parallel can fix this.

         cd src-dir
         find . -type f |
           parallel -j10 -X rsync -zR -Ha ./{} fooserver:/dest-dir/

       Adjust <b>-j10</b> until you find the optimal number.

       <b>rsync</b>  <b>-R</b>  will  create  the needed subdirectories, so all files are not put into a single dir. The <b>./</b> is
       needed so the resulting command looks similar to:

         rsync -zR ././sub/dir/file fooserver:/dest-dir/

       The <b><a href="file:/./">/./</a></b> is what <b>rsync</b> <b>-R</b> works on.

       If you are unable to push data, but need  to  pull  them  and  the  files  are  called  digits.png  (e.g.
       000000.png) you might be able to do:

         seq -w 0 99 | parallel rsync -Havessh fooserver:src/*{}.png destdir/

   <b>EXAMPLE:</b> <b>Use</b> <b>multiple</b> <b>inputs</b> <b>in</b> <b>one</b> <b>command</b>
       Copy files like foo.es.ext to foo.ext:

         ls *.es.* | perl -pe 'print; s/\.es//' | parallel -N2 cp {1} {2}

       The  perl  command spits out 2 lines for each input. GNU <b>parallel</b> takes 2 inputs (using <b>-N2</b>) and replaces
       {1} and {2} with the inputs.

       Count in binary:

         parallel -k echo ::: 0 1 ::: 0 1 ::: 0 1 ::: 0 1 ::: 0 1 ::: 0 1

       Print the number on the opposing sides of a six sided die:

         parallel --link -a &lt;(seq 6) -a &lt;(seq 6 -1 1) echo
         parallel --link echo :::: &lt;(seq 6) &lt;(seq 6 -1 1)

       Convert files from all subdirs to PNG-files with consecutive numbers (useful for making input  PNG's  for
       <b>ffmpeg</b>):

         parallel --link -a &lt;(find . -type f | sort) \
           -a &lt;(seq $(find . -type f|wc -l)) convert {1} {2}.png

       Alternative version:

         find . -type f | sort | parallel convert {} {#}.png

   <b>EXAMPLE:</b> <b>Use</b> <b>a</b> <b>table</b> <b>as</b> <b>input</b>
       Content of table_file.tsv:

         foo&lt;TAB&gt;bar
         baz &lt;TAB&gt; quux

       To run:

         cmd -o bar -i foo
         cmd -o quux -i baz

       you can run:

         parallel -a table_file.tsv --colsep '\t' cmd -o {2} -i {1}

       Note: The default for GNU <b>parallel</b> is to remove the spaces around the columns. To keep the spaces:

         parallel -a table_file.tsv --trim n --colsep '\t' cmd -o {2} -i {1}

   <b>EXAMPLE:</b> <b>Output</b> <b>to</b> <b>database</b>
       GNU <b>parallel</b> can output to a database table and a CSV-file:

         dburl=csv:///%2Ftmp%2Fmydir
         dbtableurl=$dburl/mytable.csv
         parallel --sqlandworker $dbtableurl seq ::: {1..10}

       It  is rather slow and takes up a lot of CPU time because GNU <b>parallel</b> parses the whole CSV file for each
       update.

       A better approach is to use an SQLite-base and then convert that to CSV:

         dburl=sqlite3:///%2Ftmp%2Fmy.sqlite
         dbtableurl=$dburl/mytable
         parallel --sqlandworker $dbtableurl seq ::: {1..10}
         sql $dburl '.headers on' '.mode csv' 'SELECT * FROM mytable;'

       This takes around a second per job.

       If you have access to a real database system, such as PostgreSQL, it is even faster:

         dburl=pg://user:pass@host/mydb
         dbtableurl=$dburl/mytable
         parallel --sqlandworker $dbtableurl seq ::: {1..10}
         sql $dburl \
           "COPY (SELECT * FROM mytable) TO stdout DELIMITER ',' CSV HEADER;"

       Or MySQL:

         dburl=mysql://user:pass@host/mydb
         dbtableurl=$dburl/mytable
         parallel --sqlandworker $dbtableurl seq ::: {1..10}
         sql -p -B $dburl "SELECT * FROM mytable;" &gt; mytable.tsv
         perl -pe 's/"/""/g; s/\t/","/g; s/^/"/; s/$/"/;
           %s=("\\" =&gt; "\\", "t" =&gt; "\t", "n" =&gt; "\n");
           s/\\([\\tn])/$s{$1}/g;' mytable.tsv

   <b>EXAMPLE:</b> <b>Output</b> <b>to</b> <b>CSV-file</b> <b>for</b> <b>R</b>
       If you have no need for the advanced job distribution control that a database provides,  but  you  simply
       want output into a CSV file that you can read into R or LibreCalc, then you can use <b>--results</b>:

         parallel --results my.csv seq ::: 10 20 30
         R
         &gt; mydf &lt;- read.csv("my.csv");
         &gt; print(mydf[2,])
         &gt; write(as.character(mydf[2,c("Stdout")]),'')

   <b>EXAMPLE:</b> <b>Use</b> <b>XML</b> <b>as</b> <b>input</b>
       The   show   Aflyttet   on   Radio   24syv   publishes   an  RSS  feed  with  their  audio  podcasts  on:
       <a href="http://arkiv.radio24syv.dk/audiopodcast/channel/4466232">http://arkiv.radio24syv.dk/audiopodcast/channel/4466232</a>

       Using <b>xpath</b> you can extract the URLs for 2019 and download them using GNU <b>parallel</b>:

         wget -O - <a href="http://arkiv.radio24syv.dk/audiopodcast/channel/4466232">http://arkiv.radio24syv.dk/audiopodcast/channel/4466232</a> | \
           xpath -e "//pubDate[contains(text(),'2019')]/../enclosure/@url" | \
           parallel -u wget '{= s/ url="//; s/"//; =}'

   <b>EXAMPLE:</b> <b>Run</b> <b>the</b> <b>same</b> <b>command</b> <b>10</b> <b>times</b>
       If you want to run the same command with the same arguments 10 times in parallel you can do:

         seq 10 | parallel -n0 my_command my_args

   <b>EXAMPLE:</b> <b>Working</b> <b>as</b> <b>cat</b> <b>|</b> <b>sh.</b> <b>Resource</b> <b>inexpensive</b> <b>jobs</b> <b>and</b> <b>evaluation</b>
       GNU <b>parallel</b> can work similar to <b>cat</b> <b>|</b> <b>sh</b>.

       A resource inexpensive job is a job that takes very little CPU, disk I/O and  network  I/O.  Ping  is  an
       example of a resource inexpensive job. wget is too - if the webpages are small.

       The content of the file jobs_to_run:

         ping -c 1 10.0.0.1
         wget <a href="http://example.com/status.cgi">http://example.com/status.cgi</a>?ip=10.0.0.1
         ping -c 1 10.0.0.2
         wget <a href="http://example.com/status.cgi">http://example.com/status.cgi</a>?ip=10.0.0.2
         ...
         ping -c 1 10.0.0.255
         wget <a href="http://example.com/status.cgi">http://example.com/status.cgi</a>?ip=10.0.0.255

       To run 100 processes simultaneously do:

         parallel -j 100 &lt; jobs_to_run

       As there is not a <u>command</u> the jobs will be evaluated by the shell.

   <b>EXAMPLE:</b> <b>Call</b> <b>program</b> <b>with</b> <b>FASTA</b> <b>sequence</b>
       FASTA files have the format:

         &gt;Sequence name1
         sequence
         sequence continued
         &gt;Sequence name2
         sequence
         sequence continued
         more sequence

       To call <b>myprog</b> with the sequence as argument run:

         cat file.fasta |
           parallel --pipe -N1 --recstart '&gt;' --rrs \
             'read a; echo Name: "$a"; myprog $(tr -d "\n")'

   <b>EXAMPLE:</b> <b>Call</b> <b>program</b> <b>with</b> <b>interleaved</b> <b>FASTQ</b> <b>records</b>
       FASTQ files have the format:

         @M10991:61:000000000-A7EML:1:1101:14011:1001 1:N:0:28
         CTCCTAGGTCGGCATGATGGGGGAAGGAGAGCATGGGAAGAAATGAGAGAGTAGCAAGG
         +
         #8BCCGGGGGFEFECFGGGGGGGGG@;FFGGGEG@FF&lt;EE&lt;@FFC,CEGCCGGFF&lt;FGF

       Interleaved FASTQ starts with a line like these:

         @HWUSI-EAS100R:6:73:941:1973#0/1
         @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG
         @EAS139:136:FC706VJ:2:2104:15343:197393 1:N:18:1

       where '/1' and ' 1:' determines this is read 1.

       This  will  cut big.fq into one chunk per CPU thread and pass it on stdin (standard input) to the program
       fastq-reader:

         parallel --pipe-part -a big.fq --block -1 --regexp \
           --recend '\n' --recstart '@.*(/1| 1:.*)\n[A-Za-z\n\.~]' \
           fastq-reader

   <b>EXAMPLE:</b> <b>Processing</b> <b>a</b> <b>big</b> <b>file</b> <b>using</b> <b>more</b> <b>CPUs</b>
       To process a big file or some output you can use <b>--pipe</b> to split up the data into  blocks  and  pipe  the
       blocks into the processing program.

       If the program is <b>gzip</b> <b>-9</b> you can do:

         cat bigfile | parallel --pipe --recend '' -k gzip -9 &gt; bigfile.gz

       This  will  split  <b>bigfile</b> into blocks of 1 MB and pass that to <b>gzip</b> <b>-9</b> in parallel. One <b>gzip</b> will be run
       per CPU. The output of <b>gzip</b> <b>-9</b> will be kept in order and saved to <b>bigfile.gz</b>

       <b>gzip</b> works fine if the output is appended, but some processing does not work  like  that  -  for  example
       sorting.  For this GNU <b>parallel</b> can put the output of each command into a file. This will sort a big file
       in parallel:

         cat bigfile | parallel --pipe --files sort |\
           parallel -Xj1 sort -m {} ';' rm {} &gt;bigfile.sort

       Here <b>bigfile</b> is split into blocks of around 1MB, each block ending in '\n'  (which  is  the  default  for
       <b>--recend</b>).  Each  block  is  passed to <b>sort</b> and the output from <b>sort</b> is saved into files. These files are
       passed to the second <b>parallel</b> that runs <b>sort</b> <b>-m</b> on the files before it removes the files. The  output  is
       saved to <b>bigfile.sort</b>.

       GNU  <b>parallel</b>'s  <b>--pipe</b>  maxes  out  at  around  100 MB/s because every byte has to be copied through GNU
       <b>parallel</b>. But if <b>bigfile</b> is a real (seekable) file GNU <b>parallel</b> can by-pass  the  copying  and  send  the
       parts directly to the program:

         parallel --pipe-part --block 100m -a bigfile --files sort |\
           parallel -Xj1 sort -m {} ';' rm {} &gt;bigfile.sort

   <b>EXAMPLE:</b> <b>Grouping</b> <b>input</b> <b>lines</b>
       When processing with <b>--pipe</b> you may have lines grouped by a value. Here is <u>my.csv</u>:

          Transaction Customer Item
               1       a       53
               2       b       65
               3       b       82
               4       c       96
               5       c       67
               6       c       13
               7       d       90
               8       d       43
               9       d       91
               10      d       84
               11      e       72
               12      e       102
               13      e       63
               14      e       56
               15      e       74

       Let  us  assume  you  want  GNU  <b>parallel</b>  to  process  each  customer.  In other words: You want all the
       transactions for a single customer to be treated as a single record.

       To do this we preprocess the data with a program that inserts a record  separator  before  each  customer
       (column 2 = $F[1]). Here we first make a 50 character random string, which we then use as the separator:

         sep=`perl -e 'print map { ("a".."z","A".."Z")[<a href="../man52/rand.52.html">rand</a>(52)] } (1..50);'`
         cat my.csv | \
            perl -ape '$F[1] ne $l and print "'$sep'"; $l = $F[1]' | \
            parallel --recend $sep --rrs --pipe -N1 wc

       If your program can process multiple customers replace <b>-N1</b> with a reasonable <b>--blocksize</b>.

   <b>EXAMPLE:</b> <b>Running</b> <b>more</b> <b>than</b> <b>250</b> <b>jobs</b> <b>workaround</b>
       If  you  need  to run a massive amount of jobs in parallel, then you will likely hit the filehandle limit
       which  is  often  around  250  jobs.  If  you   are   super   user   you   can   raise   the   limit   in
       <a href="file:/etc/security/limits.conf">/etc/security/limits.conf</a> but you can also use this workaround. The filehandle limit is per process. That
       means that if you just spawn more GNU <b>parallel</b>s then each of them can run 250 jobs. This will spawn up to
       2500 jobs:

         cat myinput |\
           parallel --pipe -N 50 --round-robin -j50 parallel -j50 your_prg

       This  will  spawn up to 62500 jobs (use with caution - you need 64 GB RAM to do this, and you may need to
       increase <a href="file:/proc/sys/kernel/pid_max">/proc/sys/kernel/pid_max</a>):

         cat myinput |\
           parallel --pipe -N 250 --round-robin -j250 parallel -j250 your_prg

   <b>EXAMPLE:</b> <b>Working</b> <b>as</b> <b>mutex</b> <b>and</b> <b>counting</b> <b>semaphore</b>
       The command <b>sem</b> is an alias for <b>parallel</b> <b>--semaphore</b>.

       A counting semaphore will allow a given number of jobs to be started in the background.  When the  number
       of  jobs  are  running  in the background, GNU <b>sem</b> will wait for one of these to complete before starting
       another command. <b>sem</b> <b>--wait</b> will wait for all jobs to complete.

       Run 10 jobs concurrently in the background:

         for i in *.log ; do
           echo $i
           sem -j10 gzip $i ";" echo done
         done
         sem --wait

       A mutex is a counting semaphore allowing only one job to run. This will edit the file <u>myfile</u> and prepends
       the file with lines with the numbers 1 to 3.

         seq 3 | parallel sem sed -i -e '1i{}' myfile

       As <u>myfile</u> can be very big it is important only one process edits the file at the same time.

       Name the semaphore to have multiple different semaphores active at the same time:

         seq 3 | parallel sem --id mymutex sed -i -e '1i{}' myfile

   <b>EXAMPLE:</b> <b>Mutex</b> <b>for</b> <b>a</b> <b>script</b>
       Assume a script is called from cron or from a web service, but only one instance can be run  at  a  time.
       With <b>sem</b> and <b>--shebang-wrap</b> the script can be made to wait for other instances to finish. Here in <b>bash</b>:

         #!/usr/bin/sem --shebang-wrap -u --id $0 --fg <a href="file:/bin/bash">/bin/bash</a>

         echo This will run
         sleep 5
         echo exclusively

       Here <b>perl</b>:

         #!/usr/bin/sem --shebang-wrap -u --id $0 --fg <a href="file:///usr/lib/w3m/cgi-bin/w3mman2html.cgi?perl">/usr/bin/perl</a>

         print "This will run ";
         sleep 5;
         print "exclusively\n";

       Here <b>python</b>:

         #!/usr/local/bin/sem --shebang-wrap -u --id $0 --fg /usr/bin/python

         import time
         print "This will run ";
         <a href="../man5/time.sleep.5.html">time.sleep</a>(5)
         print "exclusively";

   <b>EXAMPLE:</b> <b>Start</b> <b>editor</b> <b>with</b> <b>file</b> <b>names</b> <b>from</b> <b>stdin</b> <b>(standard</b> <b>input)</b>
       You can use GNU <b>parallel</b> to start interactive programs like emacs or vi:

         cat filelist | parallel --tty -X emacs
         cat filelist | parallel --tty -X vi

       If there are more files than will fit on a single command line, the editor will be started again with the
       remaining files.

   <b>EXAMPLE:</b> <b>Running</b> <b>sudo</b>
       <b>sudo</b>  requires  a  password to run a command as root. It caches the access, so you only need to enter the
       password again if you have not used <b>sudo</b> for a while.

       The command:

         parallel sudo echo ::: This is a bad idea

       is no good, as you would be prompted for the sudo password for each of the jobs. Instead do:

         sudo parallel echo ::: This is a good idea

       This way you only have to enter the sudo password once.

   <b>EXAMPLE:</b> <b>Run</b> <b>ping</b> <b>in</b> <b>parallel</b>
       <b>ping</b> prints out statistics when killed with CTRL-C.

       Unfortunately, CTRL-C will also normally kill GNU <b>parallel</b>.

       But by using <b>--open-tty</b> and ignoring SIGINT you can get the wanted effect:

         parallel -j0 --open-tty --lb --tag ping '{= $SIG{INT}=sub {} =}' \
           ::: 1.1.1.1 8.8.8.8 9.9.9.9 21.21.21.21 80.80.80.80 88.88.88.88

       <b>--open-tty</b> will make the <b>ping</b>s receive SIGINT (from CTRL-C).  CTRL-C will not kill GNU <b>parallel</b>, so  that
       will only exit after <b>ping</b> is done.

   <b>EXAMPLE:</b> <b>GNU</b> <b>Parallel</b> <b>as</b> <b>queue</b> <b>system/batch</b> <b>manager</b>
       GNU  <b>parallel</b> can work as a simple job queue system or batch manager.  The idea is to put the jobs into a
       file and have GNU <b>parallel</b> read from that continuously. As GNU <b>parallel</b> will stop at end of file  we  use
       <b>tail</b> to continue reading:

         true &gt;jobqueue; tail -n+0 -f jobqueue | parallel

       To submit your jobs to the queue:

         echo my_command my_arg &gt;&gt; jobqueue

       You can of course use <b>-S</b> to distribute the jobs to remote computers:

         true &gt;jobqueue; tail -n+0 -f jobqueue | parallel -S ..

       Output only will be printed when reading the next input after a job has finished: So you need to submit a
       job after the first has finished to see the output from the first job.

       If  you  keep this running for a long time, jobqueue will grow. A way of removing the jobs already run is
       by making GNU <b>parallel</b> stop when it hits a special value and then restart.  To  use  <b>--eof</b>  to  make  GNU
       <b>parallel</b> exit, <b>tail</b> also needs to be forced to exit:

         true &gt;jobqueue;
         while true; do
           tail -n+0 -f jobqueue |
             (parallel -E StOpHeRe -S ..; echo GNU Parallel is now done;
              perl -e 'while(&lt;&gt;){/StOpHeRe/ and last};print &lt;&gt;' jobqueue &gt; j2;
              (seq 1000 &gt;&gt; jobqueue &amp;);
              echo Done appending dummy data forcing tail to exit)
           echo tail exited;
           mv j2 jobqueue
         done

       In some cases you can run on more CPUs and computers during the night:

         # Day time
         echo 50% &gt; jobfile
         cp day_server_list <a href="file:~/.parallel/sshloginfile">~/.parallel/sshloginfile</a>
         # Night time
         echo 100% &gt; jobfile
         cp night_server_list <a href="file:~/.parallel/sshloginfile">~/.parallel/sshloginfile</a>
         tail -n+0 -f jobqueue | parallel --jobs jobfile -S ..

       GNU <b>parallel</b> discovers if <b>jobfile</b> or <b><a href="file:~/.parallel/sshloginfile">~/.parallel/sshloginfile</a></b> changes.

   <b>EXAMPLE:</b> <b>GNU</b> <b>Parallel</b> <b>as</b> <b>dir</b> <b>processor</b>
       If  you  have a dir in which users drop files that needs to be processed you can do this on GNU/Linux (If
       you know what <b>inotifywait</b> is called on other platforms file a bug report):

         inotifywait -qmre MOVED_TO -e CLOSE_WRITE --format %w%f my_dir |\
           parallel -u echo

       This will run the command <b>echo</b> on each file put into <b>my_dir</b> or subdirs of <b>my_dir</b>.

       You can of course use <b>-S</b> to distribute the jobs to remote computers:

         inotifywait -qmre MOVED_TO -e CLOSE_WRITE --format %w%f my_dir |\
           parallel -S ..  -u echo

       If the files to be processed are in a tar file then unpacking one file and processing it immediately  may
       be faster than first unpacking all files. Set up the dir processor as above and unpack into the dir.

       Using  GNU <b>parallel</b> as dir processor has the same limitations as using GNU <b>parallel</b> as queue system/batch
       manager.

   <b>EXAMPLE:</b> <b>Locate</b> <b>the</b> <b>missing</b> <b>package</b>
       If you have downloaded source and tried compiling it, you may have seen:

         $ ./configure
         [...]
         checking for something.h... no
         configure: error: "libsomething not found"

       Often it is not obvious which package you should install to get  that  file.  Debian  has  `apt-file`  to
       search  for a file. `tracefile` from https://codeberg.org/tange/tangetools can tell which files a program
       tried to access. In this case we are interested in one of the last files:

         $ tracefile -un ./configure | tail | parallel -j0 apt-file search

</pre><h4><b>AUTHOR</b></h4><pre>
       When using GNU <b>parallel</b> for a publication please cite:

       O. Tange (2011): GNU Parallel - The Command-Line  Power  Tool,  ;login:  The  USENIX  Magazine,  February
       2011:42-47.

       This  helps  funding  further development; and it won't cost you a cent.  If you pay 10000 EUR you should
       feel free to use GNU Parallel without citing.

       Copyright (C) 2007-10-18 Ole Tange, <a href="http://ole.tange.dk">http://ole.tange.dk</a>

       Copyright (C) 2008-2010 Ole Tange, <a href="http://ole.tange.dk">http://ole.tange.dk</a>

       Copyright (C) 2010-2024 Ole Tange, <a href="http://ole.tange.dk">http://ole.tange.dk</a> and Free Software Foundation, Inc.

       Parts of the manual concerning <b>xargs</b> compatibility is inspired by the manual of <b>xargs</b> from GNU  findutils
       4.4.2.

</pre><h4><b>LICENSE</b></h4><pre>
       This  program  is  free  software;  you  can  redistribute it and/or modify it under the terms of the GNU
       General Public License as published by the Free Software Foundation; either version 3 of the License,  or
       at your option any later version.

       This  program  is  distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even
       the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General  Public
       License for more details.

       You  should  have received a copy of the GNU General Public License along with this program.  If not, see
       &lt;https://www.gnu.org/licenses/&gt;.

   <b>Documentation</b> <b>license</b> <b>I</b>
       Permission is granted to copy, distribute and/or modify this documentation under the  terms  of  the  GNU
       Free  Documentation  License, Version 1.3 or any later version published by the Free Software Foundation;
       with no Invariant Sections, with no Front-Cover Texts, and with no  Back-Cover  Texts.   A  copy  of  the
       license is included in the file LICENSES/GFDL-1.3-or-later.txt.

   <b>Documentation</b> <b>license</b> <b>II</b>
       You are free:

       <b>to</b> <b>Share</b> to copy, distribute and transmit the work

       <b>to</b> <b>Remix</b> to adapt the work

       Under the following conditions:

       <b>Attribution</b>
                You  must  attribute  the work in the manner specified by the author or licensor (but not in any
                way that suggests that they endorse you or your use of the work).

       <b>Share</b> <b>Alike</b>
                If you alter, transform, or build upon this work, you may distribute  the  resulting  work  only
                under the same, similar or a compatible license.

       With the understanding that:

       <b>Waiver</b>   Any of the above conditions can be waived if you get permission from the copyright holder.

       <b>Public</b> <b>Domain</b>
                Where  the work or any of its elements is in the public domain under applicable law, that status
                is in no way affected by the license.

       <b>Other</b> <b>Rights</b>
                In no way are any of the following rights affected by the license:

                • Your  fair  dealing  or  fair  use  rights,  or  other  applicable  copyright  exceptions  and
                  limitations;

                • The author's moral rights;

                • Rights  other  persons  may have either in the work itself or in how the work is used, such as
                  publicity or privacy rights.

       <b>Notice</b>   For any reuse or distribution, you must make clear to others the license terms of this work.

       A copy of the full license is included in the file as LICENCES/CC-BY-SA-4.0.txt

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man1/parallel.1.html">parallel</a></b>(1),  <b><a href="../man7/parallel_tutorial.7.html">parallel_tutorial</a></b>(7),  <b><a href="../man1/env_parallel.1.html">env_parallel</a></b>(1),  <b><a href="../man1/parset.1.html">parset</a></b>(1),  <b><a href="../man1/parsort.1.html">parsort</a></b>(1),  <b><a href="../man7/parallel_alternatives.7.html">parallel_alternatives</a></b>(7),
       <b><a href="../man7/parallel_design.7.html">parallel_design</a></b>(7), <b><a href="../man1/niceload.1.html">niceload</a></b>(1), <b><a href="../man1/sql.1.html">sql</a></b>(1), <b><a href="../man1/ssh.1.html">ssh</a></b>(1), <b><a href="../man1/ssh-agent.1.html">ssh-agent</a></b>(1), <b><a href="../man1/sshpass.1.html">sshpass</a></b>(1), <b><a href="../man1/ssh-copy-id.1.html">ssh-copy-id</a></b>(1), <b><a href="../man1/rsync.1.html">rsync</a></b>(1)

20240222                                           2024-03-22                               <u><a href="../man7/PARALLEL_EXAMPLES.7.html">PARALLEL_EXAMPLES</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>