<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pacemaker-schedulerd - Pacemaker scheduler options</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/pacemaker">pacemaker_3.0.0-2ubuntu1_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       pacemaker-schedulerd - Pacemaker scheduler options

</pre><h4><b>SYNOPSIS</b></h4><pre>
       [<b>no-quorum-policy</b>=select] [<b>shutdown-lock</b>=boolean] [<b>shutdown-lock-limit</b>=time] [<b>symmetric-cluster</b>=boolean]
       [<b>maintenance-mode</b>=boolean] [<b>start-failure-is-fatal</b>=boolean] [<b>enable-startup-probes</b>=boolean]
       [<b>stonith-enabled</b>=boolean] [<b>stonith-action</b>=select] [<b>stonith-timeout</b>=time] [<b>have-watchdog</b>=boolean]
       [<b>concurrent-fencing</b>=boolean] [<b>startup-fencing</b>=boolean] [<b>priority-fencing-delay</b>=time]
       [<b>node-pending-timeout</b>=time] [<b>cluster-delay</b>=time] [<b>batch-limit</b>=integer] [<b>migration-limit</b>=integer]
       [<b>stop-all-resources</b>=boolean] [<b>stop-orphan-resources</b>=boolean] [<b>stop-orphan-actions</b>=boolean]
       [<b>pe-error-series-max</b>=integer] [<b>pe-warn-series-max</b>=integer] [<b>pe-input-series-max</b>=integer]
       [<b>node-health-strategy</b>=select] [<b>node-health-base</b>=integer] [<b>node-health-green</b>=integer]
       [<b>node-health-yellow</b>=integer] [<b>node-health-red</b>=integer] [<b>placement-strategy</b>=select]

</pre><h4><b>DESCRIPTION</b></h4><pre>
       Cluster options used by Pacemaker's scheduler

</pre><h4><b>SUPPORTED</b> <b>PARAMETERS</b></h4><pre>
       <b>no-quorum-policy</b> = select [stop]
           What to do when the cluster does not have quorum

           What to do when the cluster does not have quorum Allowed values: stop, freeze, ignore, demote, fence,
           suicide

       <b>shutdown-lock</b> = boolean [false]
           Whether to lock resources to a cleanly shut down node

           When true, resources active on a node when it is cleanly shut down are kept "locked" to that node
           (not allowed to run elsewhere) until they start again on that node after it rejoins (or for at most
           shutdown-lock-limit, if set). Stonith resources and Pacemaker Remote connections are never locked.
           Clone and bundle instances and the promoted role of promotable clones are currently never locked,
           though support could be added in a future release.

       <b>shutdown-lock-limit</b> = time [0]
           Do not lock resources to a cleanly shut down node longer than this

           If shutdown-lock is true and this is set to a nonzero time duration, shutdown locks will expire after
           this much time has passed since the shutdown was initiated, even if the node has not rejoined.

       <b>symmetric-cluster</b> = boolean [true]
           Whether resources can run on any node by default

       <b>maintenance-mode</b> = boolean [false]
           Whether the cluster should refrain from monitoring, starting, and stopping resources

       <b>start-failure-is-fatal</b> = boolean [true]
           Whether a start failure should prevent a resource from being recovered on the same node

           When true, the cluster will immediately ban a resource from a node if it fails to start there. When
           false, the cluster will instead check the resource's fail count against its migration-threshold.

       <b>enable-startup-probes</b> = boolean [true]
           Whether the cluster should check for active resources during start-up

       <b>stonith-enabled</b> = boolean [true]
           *** Advanced Use Only *** Whether nodes may be fenced as part of recovery

           If false, unresponsive nodes are immediately assumed to be harmless, and resources that were active
           on them may be recovered elsewhere. This can result in a "split-brain" situation, potentially leading
           to data loss and/or service unavailability.

       <b>stonith-action</b> = select [reboot]
           Action to send to fence device when a node needs to be fenced

           Action to send to fence device when a node needs to be fenced Allowed values: reboot, off

       <b>stonith-timeout</b> = time [60s]
           How long to wait for on, off, and reboot fence actions to complete by default

       <b>have-watchdog</b> = boolean [false]
           Whether watchdog integration is enabled

           This is set automatically by the cluster according to whether SBD is detected to be in use.
           User-configured values are ignored. The value `true` is meaningful if diskless SBD is used and
           `stonith-watchdog-timeout` is nonzero. In that case, if fencing is required, watchdog-based
           self-fencing will be performed via SBD without requiring a fencing resource explicitly configured.

       <b>concurrent-fencing</b> = boolean [true]
           *** Deprecated ***

           Allow performing fencing operations in parallel

       <b>startup-fencing</b> = boolean [true]
           *** Advanced Use Only *** Whether to fence unseen nodes at start-up

           Setting this to false may lead to a "split-brain" situation, potentially leading to data loss and/or
           service unavailability.

       <b>priority-fencing-delay</b> = time [0]
           Apply fencing delay targeting the lost nodes with the highest total resource priority

           Apply specified delay for the fencings that are targeting the lost nodes with the highest total
           resource priority in case we don't have the majority of the nodes in our cluster partition, so that
           the more significant nodes potentially win any fencing match, which is especially meaningful under
           split-brain of 2-node cluster. A promoted resource instance takes the base priority + 1 on
           calculation if the base priority is not 0. Any static/random delays that are introduced by
           `pcmk_delay_base/max` configured for the corresponding fencing resources will be added to this delay.
           This delay should be significantly greater than, safely twice, the maximum `pcmk_delay_base/max`. By
           default, priority fencing delay is disabled.

       <b>node-pending-timeout</b> = time [0]
           How long to wait for a node that has joined the cluster to join the controller process group

           Fence nodes that do not join the controller process group within this much time after joining the
           cluster, to allow the cluster to continue managing resources. A value of 0 means never fence pending
           nodes. Setting the value to 2h means fence nodes after 2 hours.

       <b>cluster-delay</b> = time [60s]
           Maximum time for node-to-node communication

           The node elected Designated Controller (DC) will consider an action failed if it does not get a
           response from the node executing the action within this time (after considering the action's own
           timeout). The "correct" value will depend on the speed and load of your network and cluster nodes.

       <b>batch-limit</b> = integer [0]
           Maximum number of jobs that the cluster may execute in parallel across all nodes

           The "correct" value will depend on the speed and load of your network and cluster nodes. If set to 0,
           the cluster will impose a dynamically calculated limit when any node has a high load.

       <b>migration-limit</b> = integer [-1]
           The number of live migration actions that the cluster is allowed to execute in parallel on a node (-1
           means no limit)

       <b>stop-all-resources</b> = boolean [false]
           Whether the cluster should stop all active resources

       <b>stop-orphan-resources</b> = boolean [true]
           Whether to stop resources that were removed from the configuration

       <b>stop-orphan-actions</b> = boolean [true]
           Whether to cancel recurring actions removed from the configuration

       <b>pe-error-series-max</b> = integer [-1]
           The number of scheduler inputs resulting in errors to save

           Zero to disable, -1 to store unlimited.

       <b>pe-warn-series-max</b> = integer [5000]
           The number of scheduler inputs resulting in warnings to save

           Zero to disable, -1 to store unlimited.

       <b>pe-input-series-max</b> = integer [4000]
           The number of scheduler inputs without errors or warnings to save

           Zero to disable, -1 to store unlimited.

       <b>node-health-strategy</b> = select [none]
           How cluster should react to node health attributes

           Requires external entities to create node attributes (named with the prefix "#health") with values
           "red", "yellow", or "green". Allowed values: none, migrate-on-red, only-green, progressive, custom

       <b>node-health-base</b> = integer [0]
           Base health score assigned to a node

           Only used when "node-health-strategy" is set to "progressive".

       <b>node-health-green</b> = integer [0]
           The score to use for a node health attribute whose value is "green"

           Only used when "node-health-strategy" is set to "custom" or "progressive".

       <b>node-health-yellow</b> = integer [0]
           The score to use for a node health attribute whose value is "yellow"

           Only used when "node-health-strategy" is set to "custom" or "progressive".

       <b>node-health-red</b> = integer [-INFINITY]
           The score to use for a node health attribute whose value is "red"

           Only used when "node-health-strategy" is set to "custom" or "progressive".

       <b>placement-strategy</b> = select [default]
           How the cluster should allocate resources to nodes

           How the cluster should allocate resources to nodes Allowed values: default, utilization, minimal,
           balanced

</pre><h4><b>AUTHOR</b></h4><pre>
       <b>Andrew</b> <b>Beekhof</b> &lt;<a href="mailto:andrew@beekhof.net">andrew@beekhof.net</a>&gt;
           Author.

Pacemaker Configuration                            06/16/2025                             <u><a href="../man7/PACEMAKER-SCHEDULER.7.html">PACEMAKER-SCHEDULER</a></u>(7)
</pre>
 </div>
</div></section>
</div>
</body>
</html>