<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>gres.conf - Slurm configuration file for Generic RESource (GRES) management.</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/questing/+package/slurm-client">slurm-client_24.11.5-4_amd64</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       gres.conf - Slurm configuration file for Generic RESource (GRES) management.

</pre><h4><b>DESCRIPTION</b></h4><pre>
       <b>gres.conf</b>  is  an  ASCII  file  which  describes  the configuration of Generic RESource(s) (GRES) on each
       compute node.  If the GRES information in the slurm.conf file does not fully  describe  those  resources,
       then  a  gres.conf  file  should be included on each compute node. For cloud nodes, a gres.conf file that
       includes all the cloud nodes must be on all cloud nodes and the  controller.  The  file  will  always  be
       located in the same directory as <b>slurm.conf</b>.

       If  the  GRES information in the slurm.conf file fully describes those resources (i.e. no "Cores", "File"
       or "Links" specification is required for that GRES type or that information is  automatically  detected),
       that  information  may  be  omitted from the gres.conf file and only the configuration information in the
       slurm.conf file will be used.  The  gres.conf  file  may  be  omitted  completely  if  the  configuration
       information in the slurm.conf file fully describes all GRES.

       If using the <b>gres.conf</b> file to describe the resources available to nodes, the first parameter on the line
       should be <b>NodeName</b>. If configuring Generic Resources without specifying nodes, the first parameter on the
       line should be <b>Name</b>.

       Parameter names are case insensitive.  Any text following a "#" in the configuration file is treated as a
       comment  through  the  end  of  that line.  Changes to the configuration file take effect upon restart of
       Slurm daemons, daemon receipt of the SIGHUP signal, or execution of the  command  "scontrol  reconfigure"
       unless otherwise noted.

       <b>NOTE</b>:  Slurm  support  for  gres/[mps|shard]  requires  the  use of the select/cons_tres plugin. For more
       information on how to configure MPS, see  <u>https://slurm.schedmd.com/gres.html#MPS_Management</u>.   For  more
       information on how to configure Sharding, see <u>https://slurm.schedmd.com/gres.html#Sharding</u>.

       For more information on GRES scheduling in general, see <u>https://slurm.schedmd.com/gres.html</u>.

       The overall configuration parameters available include:

       <b>AutoDetect</b>
              The  hardware  detection  mechanisms  to  enable for automatic GRES configuration.  Currently, the
              options are:

              <b>nrt</b>    Automatically detect AWS Trainium/Inferentia devices.

              <b>nvidia</b> Automatically detect NVIDIA GPUs. No library required, but doesn't detect MIGs or  NVlinks.
                     Added in Slurm 24.11.

              <b>nvml</b>   Automatically detect NVIDIA GPUs. Requires the NVIDIA Management Library (NVML).

              <b>off</b>    Do not automatically detect any GPUs. Used to override other options.

              <b>oneapi</b> Automatically  detect  Intel  GPUs.  Requires the Intel Graphics Compute Runtime for oneAPI
                     Level Zero and OpenCL Driver (oneapi).

              <b>rsmi</b>   Automatically detect AMD GPUs. Requires the ROCm System  Management  Interface  (ROCm  SMI)
                     Library.

              <u>AutoDetect</u>  can  be  on  a  line  by  itself, in which case it will globally apply to all lines in
              gres.conf by default. In addition, <u>AutoDetect</u> can be combined  with  <b>NodeName</b>  to  only  apply  to
              certain  nodes.  Node-specific  <u>AutoDetect</u>s  will  trump  the  global  <u>AutoDetect</u>. A node-specific
              <u>AutoDetect</u> only needs to be specified once per node. If specified  multiple  times  for  the  same
              nodes, they must all be the same value. To unset <u>AutoDetect</u> for a node when a global <u>AutoDetect</u> is
              set,  simply  set  it  to  "off" in a node-specific GRES line.  E.g.: <u>NodeName=tux3</u> <u>AutoDetect=off</u>
              <u>Name=gpu</u> <u>File=/dev/nvidia[0-3]</u>.  <u>AutoDetect</u> cannot be used with cloud nodes.

              <u>AutoDetect</u> will automatically detect files, cores, links, and any other hardware. If  a  parameter
              such as <b>File</b>, <b>Cores</b>, or <b>Links</b> are specified when <u>AutoDetect</u> is used, then the specified values are
              used  to  sanity  check the auto detected values. If there is a mismatch, then the node's state is
              set to invalid and the node is drained.

       <b>Count</b>  Number of resources of this name/type available on this node.  The default value  is  set  to  the
              number  of  <b>File</b>  values  specified (if any), otherwise the default value is one. A suffix of "K",
              "M", "G", "T" or "P" may be used to  multiply  the  number  by  1024,  1048576,  1073741824,  etc.
              respectively.  For example: "Count=10G".

       <b>Cores</b>  Optionally  specify  the  core  index  numbers  matching  the specific sockets* which can use this
              resource.

              If this option is used, all the cores in a socket* must be specified together.   While  Slurm  can
              track  and  assign  resources  at  the  CPU  or  thread  level,  its scheduling algorithms used to
              co-allocate GRES devices with CPUs operates at a socket* level for job allocations.  Therefore, it
              is not possible to preferentially assign GRES with different specific CPUs on the same socket*.

              *Sockets may be substituted for NUMA nodes with  SlurmdParameters=numa_node_as_socket  or  l3cache
              with SlurmdParameters=l3cache_as_socket.

              Multiple  cores  may be specified using a comma-delimited list or a range may be specified using a
              "-" separator (e.g. "0,1,2,3" or "0-3").  If a job  specifies  <b>--gres-flags=enforce-binding</b>,  then
              only  the  identified cores can be allocated with each generic resource. This will tend to improve
              performance of jobs, but delay the allocation of resources to them.  If specified and a job is <u>not</u>
              submitted with the <b>--gres-flags=enforce-binding</b> option the identified cores will be preferred  for
              scheduling with each generic resource.

              If  <b>--gres-flags=disable-binding</b> is specified, then any core can be used with the resources, which
              also increases the  speed  of  Slurm's  scheduling  algorithm  but  can  degrade  the  application
              performance.   The <b>--gres-flags=disable-binding</b> option is currently required to use more CPUs than
              are bound to a GRES (e.g. if a GPU is bound to the CPUs on one socket, but resources on more  than
              one  socket are required to run the job).  If any core can be effectively used with the resources,
              then do not specify the <b>cores</b> option for improved speed in the Slurm scheduling logic.  A  restart
              of the slurmctld is needed for changes to the Cores option to take effect.

              <b>NOTE</b>:  Since  Slurm  must  be able to perform resource management on heterogeneous clusters having
              various processing unit numbering schemes, a logical core index must be specified instead  of  the
              physical  core  index.   That  logical core index might not correspond to your physical core index
              number.  Core 0 will be the first core on the first socket, while core 1 will be the  second  core
              on  the  first  socket.   This  numbering coincides with the logical core number (Core L#) seen in
              "lstopo -l" command output.

       <b>File</b>   Fully qualified pathname of the device files associated with a resource.  The name can  include  a
              numeric range suffix to be interpreted by Slurm (e.g. <u>File=/dev/nvidia[0-3]</u>).

              This field is generally required if enforcement of generic resource allocations is to be supported
              (i.e.  prevents users from making use of resources allocated to a different user).  Enforcement of
              the file allocation relies upon Linux Control Groups (cgroups)  and  Slurm's  task/cgroup  plugin,
              which will place the allocated files into the job's cgroup and prevent use of other files.  Please
              see Slurm's Cgroups Guide for more information: <u>https://slurm.schedmd.com/cgroups.html</u>.

              If  <b>File</b>  is  specified then <b>Count</b> must be either set to the number of file names specified or not
              set (the default value is the number of files specified).  The exception to this is  MPS/Sharding.
              For either of these GRES, each GPU would be identified by device file using the <b>File</b> parameter and
              <b>Count</b>  would  specify  the number of entries that would correspond to that GPU. For MPS, typically
              100 or some multiple of 100. For  Sharding  typically  the  maximum  number  of  jobs  that  could
              simultaneously share that GPU.

              If  using  a  card  with  Multi-Instance  GPU  functionality,  use <b>MultipleFiles</b> instead. <b>File</b> and
              <b>MultipleFiles</b> are mutually exclusive.

              <b>NOTE</b>: <b>File</b> is required for all <u>gpu</u> typed GRES.

              <b>NOTE</b>: If you specify the <b>File</b> parameter for a resource on some node, the option must be  specified
              on all nodes and Slurm will track the assignment of each specific resource on each node. Otherwise
              Slurm  will  only  track  a  count of allocated resources rather than the state of each individual
              device file.

              <b>NOTE</b>: Drain a node before changing the count of records with <b>File</b> parameters (e.g. if you want  to
              add  or  remove  GPUs from a node's configuration).  Failure to do so will result in any job using
              those GRES being aborted.

              <b>NOTE</b>: When specifying <b>File</b>, <b>Count</b> is limited in size (currently 1024) for each node.

       <b>Flags</b>  Optional flags that can be specified to change configured behavior of the GRES.

              Allowed values at present are:

              <b>CountOnly</b>           Do not attempt to load a plugin of the GRES type as this  GRES  will  only  be
                                  used to track counts of GRES used. This avoids attempting to load non-existent
                                  plugin  which can affect filesystems with high latency metadata operations for
                                  non-existent files.

                                  <b>NOTE</b>: If a gres has this flag configured it is global, so all other nodes with
                                  that gres will have this flag implied.

              <b>explicit</b>            If the flag is set, GRES is not allocated to the job as  part  of  whole  node
                                  allocation (--exclusive or OverSubscribe=EXCLUSIVE set on partition) unless it
                                  was explicitly requested by the job.

                                  <b>NOTE</b>: If a gres has this flag configured it is global, so all other nodes with
                                  that gres will have this flag implied.

              <b>one_sharing</b>         To  be used on a shared gres. If using a shared gres (mps) on top of a sharing
                                  gres (gpu) only allow one of the sharing gres to be used by the  shared  gres.
                                  This is the default for MPS.

                                  <b>NOTE</b>: If a gres has this flag configured it is global, so all other nodes with
                                  that  gres  will  have  this  flag  implied.  This flag is not compatible with
                                  all_sharing for a specific gres.

              <b>all_sharing</b>         To be used on a shared gres. This is the opposite of one_sharing  and  can  be
                                  used  to  allow  all  sharing  gres (gpu) on a node to be used for shared gres
                                  (mps).

                                  <b>NOTE</b>: If a gres has this flag configured it is global, so all other nodes with
                                  that gres will have this flag  implied.  This  flag  is  not  compatible  with
                                  one_sharing for a specific gres.

              <b>nvidia_gpu_env</b>      Set  environment  variable  <u>CUDA_VISIBLE_DEVICES</u> for all GPUs on the specified
                                  node(s).

              <b>amd_gpu_env</b>         Set environment variable <u>ROCR_VISIBLE_DEVICES</u> for all GPUs  on  the  specified
                                  node(s).

              <b>intel_gpu_env</b>       Set  environment  variable  <u>ZE_AFFINITY_MASK</u>  for  all  GPUs  on the specified
                                  node(s).

              <b>opencl_env</b>          Set environment variable <u>GPU_DEVICE_ORDINAL</u> for  all  GPUs  on  the  specified
                                  node(s).

              <b>no_gpu_env</b>          Set  no  GPU-specific environment variables. This is mutually exclusive to all
                                  other environment-related flags.

              If no environment-related flags are specified, then  <u>nvidia_gpu_env</u>,  <u>amd_gpu_env</u>,  <u>intel_gpu_env</u>,
              and  <u>opencl_env</u>  will be implicitly set by default.  If <b>AutoDetect</b> is used and environment-related
              flags are not specified,  then  <u>AutoDetect=nvml</u>  or  <u>AutoDetect=nvidia</u>  will  set  <u>nvidia_gpu_env</u>,
              <u>AutoDetect=rsmi</u>  will  set <u>amd_gpu_env</u>, and <u>AutoDetect=oneapi</u> will set <u>intel_gpu_env</u>.  Conversely,
              specified environment-related flags will always override <b>AutoDetect</b>.

              Environment-related flags set on one GRES line will be inherited by the GRES line  directly  below
              it if no environment-related flags are specified on that line and if it is of the same node, name,
              and type. Environment-related flags must be the same for GRES of the same node, name, and type.

              Note that there is a known issue with the AMD ROCm runtime where <u>ROCR_VISIBLE_DEVICES</u> is processed
              first,  and  then  <u>CUDA_VISIBLE_DEVICES</u>  is  processed.  To  avoid  the issues caused by this, set
              <u>Flags=amd_gpu_env</u> for AMD GPUs so only <u>ROCR_VISIBLE_DEVICES</u> is set.

       <b>Links</b>  A comma-delimited list of numbers identifying the number of connections between  this  device  and
              other devices to allow coscheduling of better connected devices.  This is an ordered list in which
              the  number  of  connections  this  specific  device  has to device number 0 would be in the first
              position, the number of connections it has to device number 1 in the second position, etc.   A  -1
              indicates  the  device  itself  and a 0 indicates no connection.  If specified, then this line can
              only contain a single GRES device (i.e. can only contain a single file via <b>File</b>).

              This is an optional value and is usually automatically determined if  <b>AutoDetect</b>  is  enabled.   A
              typical  use  case  would be to identify GPUs having NVLink connectivity.  Note that for GPUs, the
              minor number assigned by the OS and used in the device file (i.e. the X in  <u>/dev/nvidiaX</u>)  is  not
              necessarily  the same as the device number/index. The device number is created by sorting the GPUs
              by  PCI  bus  ID  and  then  numbering  them   starting   from   the   smallest   bus   ID.    See
              <u>https://slurm.schedmd.com/gres.html#GPU_Management</u>

       <b>MultipleFiles</b>
              Fully  qualified  pathname  of  the device files associated with a resource.  Graphics cards using
              Multi-Instance GPU (MIG) technology will present multiple device files that should be managed as a
              single generic resource. The file names can be a comma separated list or it can include a  numeric
              range suffix (e.g. MultipleFiles=/dev/nvidia[0-3]).

              Drain  a  node before changing the count of records with the <b>MultipleFiles</b> parameter, such as when
              adding or removing GPUs from a node's configuration.  Failure to do so  will  result  in  any  job
              using those GRES being aborted.

              When not using GPUs with MIG functionality, use <b>File</b> instead.  <b>MultipleFiles</b> and <b>File</b> are mutually
              exclusive.

       <b>Name</b>   Name  of  the  generic  resource.  Any  desired  name may be used.  The name must match a value in
              <b>GresTypes</b> in <u>slurm.conf</u>.   Each  generic  resource  has  an  optional  plugin  which  can  provide
              resource-specific functionality.  Generic resources that currently include an optional plugin are:

              <b>gpu</b>    Graphics Processing Unit

              <b>mps</b>    CUDA Multi-Process Service (MPS)

              <b>nic</b>    Network Interface Card

              <b>shard</b>  Shards of a gpu

       <b>NodeName</b>
              An  optional  NodeName  specification  can be used to permit one gres.conf file to be used for all
              compute nodes in a cluster by specifying the node(s) that each line should apply to.  The NodeName
              specification can use a Slurm hostlist specification as shown in the example below.

       <b>Type</b>   An optional arbitrary string identifying the type of generic resource.  For example, this might be
              used to identify a specific model of GPU, which users can then specify  in  a  job  request.   For
              changes  to  the  <b>Type</b>  option to take effect with a scontrol reconfig all affected <b>slurmd</b> daemons
              must be responding to the <b>slurmctld</b>.  Otherwise a restart of the <b>slurmctld</b> and <b>slurmd</b>  daemons  is
              required.

              <b>NOTE</b>:  If  using  autodetect  functionality and defining the Type in your gres.conf file, the Type
              specified should match or be a substring of the value that is detected,  using  an  underscore  in
              lieu of any spaces.

</pre><h4><b>EXAMPLES</b></h4><pre>
       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Define GPU devices with MPS support, with AutoDetect sanity checking
       ##################################################################
       AutoDetect=nvml
       Name=gpu Type=gtx560 File=/dev/nvidia0 COREs=0,1
       Name=gpu Type=tesla  File=/dev/nvidia1 COREs=2,3
       Name=mps Count=100 File=/dev/nvidia0 COREs=0,1
       Name=mps Count=100  File=/dev/nvidia1 COREs=2,3

       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Overwrite system defaults and explicitly configure three GPUs
       ##################################################################
       Name=gpu Type=tesla File=/dev/nvidia[0-1] COREs=0,1
       # Name=gpu Type=tesla  File=/dev/nvidia[2-3] COREs=2,3
       # NOTE: nvidia2 device is out of service
       Name=gpu Type=tesla  File=/dev/nvidia3 COREs=2,3

       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Use a single gres.conf file for all compute nodes - positive method
       ##################################################################
       ## Explicitly specify devices on nodes tux0-tux15
       # NodeName=tux[0-15]  Name=gpu File=/dev/nvidia[0-3]
       # NOTE: tux3 nvidia1 device is out of service
       NodeName=tux[0-2]  Name=gpu File=/dev/nvidia[0-3]
       NodeName=tux3  Name=gpu File=/dev/nvidia[0,2-3]
       NodeName=tux[4-15]  Name=gpu File=/dev/nvidia[0-3]

       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Use NVML to gather GPU configuration information
       # for all nodes except one
       ##################################################################
       AutoDetect=nvml
       NodeName=tux3 AutoDetect=off Name=gpu File=/dev/nvidia[0-3]

       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Specify some nodes with NVML, some with RSMI, and some with no AutoDetect
       ##################################################################
       NodeName=tux[0-7] AutoDetect=nvml
       NodeName=tux[8-11] AutoDetect=rsmi
       NodeName=tux[12-15] Name=gpu File=/dev/nvidia[0-3]

       ##################################################################
       # Slurm's Generic Resource (GRES) configuration file
       # Define 'bandwidth' GRES to use as a way to limit the
       # resource use on these nodes for workflow purposes
       ##################################################################
       NodeName=tux[0-7] Name=bandwidth Type=lustre Count=4G Flags=CountOnly

</pre><h4><b>COPYING</b></h4><pre>
       Copyright  (C) 2010 The Regents of the University of California.  Produced at Lawrence Livermore National
       Laboratory (cf, DISCLAIMER).
       Copyright (C) 2010-2022 SchedMD LLC.

       This   file   is   part   of   Slurm,   a   resource    management    program.     For    details,    see
       &lt;https://slurm.schedmd.com/&gt;.

       Slurm  is  free  software;  you  can  redistribute it and/or modify it under the terms of the GNU General
       Public License as published by the Free Software Foundation; either version 2 of the License, or (at your
       option) any later version.

       Slurm is distributed in the hope that it will be useful, but  WITHOUT  ANY  WARRANTY;  without  even  the
       implied  warranty  of  MERCHANTABILITY  or  FITNESS  FOR A PARTICULAR PURPOSE. See the GNU General Public
       License for more details.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <b><a href="../man5/slurm.conf.5.html">slurm.conf</a></b>(5)

December 2024                               Slurm Configuration File                                <u><a href="../man5/gres.conf.5.html">gres.conf</a></u>(5)
</pre>
 </div>
</div></section>
</div>
</body>
</html>