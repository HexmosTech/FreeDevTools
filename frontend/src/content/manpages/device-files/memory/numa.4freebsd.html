<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NUMA — Non-Uniform Memory Access</title>
    <style>
        body { font-family: monospace; margin: 20px; line-height: 1.4; }
        a { color: #0066cc; }
        pre { white-space: pre-wrap; }
    </style>
</head>
<body>
    <div id="main-content">
<section class="p-strip is-bordered">
<div class="row">
<div class="col-3 u-hide--small u-hide" id="toc">
</div>
<div id="tableWrapper">
<p id="distroAndSection"></p>

Provided by: <a href="https://launchpad.net/ubuntu/plucky/+package/freebsd-manpages">freebsd-manpages_12.2-2_all</a> <br><br><pre>
</pre><h4><b>NAME</b></h4><pre>
       NUMA — Non-Uniform Memory Access

</pre><h4><b>SYNOPSIS</b></h4><pre>
       <b>options</b> <b>MAXMEMDOM</b>
       <b>options</b> <b>NUMA</b>

</pre><h4><b>DESCRIPTION</b></h4><pre>
       Non-Uniform  Memory  Access  is  a  computer  architecture  design  which  involves unequal costs between
       processors, memory and IO devices in a given system.

       In a <b>NUMA</b> architecture, the latency to access specific memory or IO devices depends upon which  processor
       the  memory  or  device  is  attached to.  Accessing memory local to a processor is faster than accessing
       memory that is connected to one of the other processors.  FreeBSD implements NUMA-aware memory allocation
       policies.  By default it attempts to ensure that allocations are balanced across each domain.  Users  may
       override the default domain selection policy using <u><a href="../man1/cpuset.1.html">cpuset</a></u>(1).

       <b>NUMA</b>  support  is  enabled  when  the  <b>NUMA</b>  option  is specified in the kernel configuration file.  Each
       platform defines the <b>MAXMEMDOM</b> constant, which specifies the maximum number of  supported  NUMA  domains.
       This  constant  may  be specified in the kernel configuration file.  <b>NUMA</b> support can be disabled at boot
       time by setting the <u>vm.numa.disabled</u> tunable to 1.  Other values for this tunable are currently ignored.

       Thread and process <b>NUMA</b> policies are controlled  with  the  <u><a href="../man2/cpuset_getdomain.2.html">cpuset_getdomain</a></u>(2)  and  <u><a href="../man2/cpuset_setdomain.2.html">cpuset_setdomain</a></u>(2)
       syscalls.  The <u><a href="../man1/cpuset.1.html">cpuset</a></u>(1) tool is available for starting processes with a non-default policy, or to change
       the policy of an existing thread or process.

       Systems  with  non-uniform  access  to  I/O  devices  may  mark  those  devices  with the local VM domain
       identifier.  Drivers can find out their local domain information by calling <u><a href="../man9/bus_get_domain.9.html">bus_get_domain</a></u>(9).

   <b>MIB</b> <b>Variables</b>
       The operation of <b>NUMA</b> is controlled and exposes information with these <u><a href="../man8/sysctl.8.html">sysctl</a></u>(8) MIB variables:

       <u>vm.ndomains</u>
               The number of VM domains which have been detected.

       <u>vm.phys_locality</u>
               A table indicating the relative cost of each VM domain to each other.  A value  of  10  indicates
               equal  cost.  A value of -1 means the locality map is not available or no locality information is
               available.

       <u>vm.phys_segs</u>
               The map of physical memory, grouped by VM domain.

</pre><h4><b>IMPLEMENTATION</b> <b>NOTES</b></h4><pre>
       The current <b>NUMA</b> implementation is VM-focused.  The hardware <b>NUMA</b> domains are mapped into  a  contiguous,
       non-sparse  VM  domain  space,  starting  from  0.   Thus, VM domain information (for example, the domain
       identifier) is not necessarily the same as  is  found  in  the  hardware  specific  information.   Policy
       information is available in both struct thread and struct proc.

</pre><h4><b>SEE</b> <b>ALSO</b></h4><pre>
       <u><a href="../man1/cpuset.1.html">cpuset</a></u>(1), <u><a href="../man2/cpuset_getaffinity.2.html">cpuset_getaffinity</a></u>(2), <u><a href="../man2/cpuset_setaffinity.2.html">cpuset_setaffinity</a></u>(2), <u><a href="../man9/bus_get_domain.9.html">bus_get_domain</a></u>(9)

</pre><h4><b>HISTORY</b></h4><pre>
       <b>NUMA</b>  first  appeared  in  FreeBSD 9.0 as a first-touch allocation policy with a fail-over to round-robin
       allocation and was not configurable.  It was then modified in FreeBSD 10.0  to  implement  a  round-robin
       allocation policy and was also not configurable.

       The  <u><a href="../man2/numa_getaffinity.2.html">numa_getaffinity</a></u>(2)  and  <u><a href="../man2/numa_setaffinity.2.html">numa_setaffinity</a></u>(2)  syscalls  and  the  <u><a href="../man1/numactl.1.html">numactl</a></u>(1) tool first appeared in
       FreeBSD 11.0 and were removed in FreeBSD 12.0.  The current implementation appeared in FreeBSD 12.0.

</pre><h4><b>AUTHORS</b></h4><pre>
       This manual page written by Adrian Chadd &lt;<u><a href="mailto:adrian@FreeBSD.org">adrian@FreeBSD.org</a></u>&gt;.

</pre><h4><b>NOTES</b></h4><pre>
       No statistics are kept to indicate how often <b>NUMA</b> allocation policies succeed or fail.

Debian                                          October 22, 2018                                         <u><a href="../man4/NUMA.4.html">NUMA</a></u>(4)
</pre>
 </div>
</div></section>
</div>
</body>
</html>