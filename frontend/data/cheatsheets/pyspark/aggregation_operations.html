<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Aggregation Operations - Data Aggregation Functions</title>
    <meta name="description" content="Learn PySpark aggregation operations with examples for count, sum, mean, max, min, collect_set, collect_list, and using window functions for row aggregation.">
    <meta name="keywords" content="pyspark, aggregation, data aggregation, count, sum, mean, max, min, collect_set, collect_list, window functions, dataframe, spark sql">
    <link rel="canonical" href="https://your-domain.com/pyspark/aggregation_operations.html">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="PySpark Aggregation Operations - Data Aggregation Functions">
    <meta property="og:description" content="Learn PySpark aggregation operations with examples for count, sum, mean, max, min, collect_set, collect_list, and using window functions for row aggregation.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://your-domain.com/pyspark/aggregation_operations.html">
    <meta property="og:image" content="https://your-domain.com/images/pyspark-aggregation-og.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="PySpark Aggregation Operations - Data Aggregation Functions">
    <meta name="twitter:description" content="Learn PySpark aggregation operations with examples for count, sum, mean, max, min, collect_set, collect_list, and using window functions for row aggregation.">
    <meta name="twitter:image" content="https://your-domain.com/images/pyspark-aggregation-twitter.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .header {
            background: #2c3e50;
            color: white;
            padding: 20px;
            border-bottom: 1px solid #34495e;
        }
        .header h1 {
            margin: 0;
            font-size: 1.5em;
        }
        .content {
            padding: 20px;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
            margin: 0;
        }
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
        }
        .markdown-content {
            line-height: 1.7;
        }
        .markdown-content h1, .markdown-content h2, .markdown-content h3 {
            color: #2c3e50;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 8px;
        }
        .markdown-content pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
        }
        .markdown-content code {
            background: #f1f3f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        .markdown-content pre code {
            background: none;
            padding: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>PySpark Aggregation Operations</h1>
        </div>
        <div class="content">
            <div class="markdown-content">
                <h2>Understanding PySpark Aggregation Functions</h2>
                <p>PySpark provides a rich set of aggregation functions to perform data summarization and analysis on DataFrames. These operations are crucial for deriving insights from large datasets. Aggregations are typically performed after a <code>groupBy</code> operation, allowing you to compute aggregate values for each group.</p>

                <h2>Common Aggregation Operations</h2>
                <p>Here are some of the most frequently used PySpark aggregation functions:</p>
                <ul>
                    <li><strong>Row Count:</strong> <code>F.count()</code> - Counts the number of rows in each group.</li>
                    <li><strong>Sum of Rows in Group:</strong> <code>F.sum(*cols)</code> - Calculates the sum of values in specified columns for each group.</li>
                    <li><strong>Mean of Rows in Group:</strong> <code>F.mean(*cols)</code> - Computes the average of values in specified columns for each group.</li>
                    <li><strong>Max of Rows in Group:</strong> <code>F.max(*cols)</code> - Finds the maximum value in specified columns for each group.</li>
                    <li><strong>Min of Rows in Group:</strong> <code>F.min(*cols)</code> - Finds the minimum value in specified columns for each group.</li>
                    <li><strong>First Row in Group:</strong> <code>F.alias(*cols)</code> - Selects the first value encountered in a column within each group.</li>
                </ul>

                <h3>Example: Grouping by Gender and Finding Max Age</h3>
                <pre class="codehilite"><code class="language-python">from pyspark.sql import functions as F

# Assuming 'df' is your PySpark DataFrame
df = df.groupBy('gender').agg(F.max('age').alias('max_age_by_gender'))
</code></pre>

                <h2>Collecting Data within Groups</h2>
                <p>Beyond simple numerical aggregations, PySpark allows you to collect data from within groups into lists or sets.</p>
                <ul>
                    <li><strong>Collect a Set of all Rows in Group:</strong> <code>F.collect_set(col)</code> - Gathers unique values from a column into an array for each group.</li>
                    <li><strong>Collect a List of all Rows in Group:</strong> <code>F.collect_list(col)</code> - Gathers all values (including duplicates) from a column into an array for each group.</li>
                </ul>

                <h3>Example: Collecting Unique Names by Age</h3>
                <pre class="codehilite"><code class="language-python">from pyspark.sql import functions as F

# Assuming 'df' is your PySpark DataFrame
df = df.groupBy('age').agg(F.collect_set('name').alias('person_names'))
</code></pre>

                <h2>Advanced Aggregation with Window Functions</h2>
                <p>Window functions offer a powerful way to perform calculations across a set of table rows that are somehow related to the current row. This is particularly useful for tasks like ranking, calculating running totals, or selecting the latest record within partitions.</p>

                <h3>Example: Selecting the Latest Row per Person</h3>
                <p>This example demonstrates how to use window functions to filter for the most recent record for each unique combination of first and last names, ordered by date.</p>
                <pre class="codehilite"><code class="language-python">from pyspark.sql import functions as F
from pyspark.sql import Window as W

# Assuming 'df' is your PySpark DataFrame with columns 'first_name', 'last_name', 'date', and other data

# Define the window specification: partition by first and last name, order by date descending
window = W.partitionBy("first_name", "last_name").orderBy(F.desc("date"))

# Add a row number based on the defined window
df = df.withColumn("row_number", F.row_number().over(window))

# Filter to keep only the rows with row_number = 1 (the latest row for each partition)
df = df.filter(F.col("row_number") == 1)

# Drop the temporary row_number column
df = df.drop("row_number")
</code></pre>

                <h2>Further Reading on PySpark</h2>
                <ul>
                    <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api.html#dataframe-functions" target="_blank">PySpark SQL Functions Documentation</a></li>
                    <li><a href="https://stackoverflow.com/questions/tagged/pyspark" target="_blank">PySpark Questions on Stack Overflow</a></li>
                    <li><a href="https://www.databricks.com/glossary/aggregation-functions" target="_blank">Understanding Aggregation Functions</a></li>
                </ul>
            </div>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
</body>
</html>