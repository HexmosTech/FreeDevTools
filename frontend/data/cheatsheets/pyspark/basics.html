<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Basics - Essential DataFrame Operations</title>
    <meta name="description" content="Master PySpark basics with essential DataFrame operations. Learn to show, limit, count, and write data, and convert to Pandas. Essential for Spark developers.">
    <meta name="keywords" content="PySpark basics, PySpark DataFrame, Spark operations, show DataFrame, limit DataFrame, count DataFrame, write DataFrame, toPandas, Spark tutorial, PySpark guide">
    <link rel="canonical" href="https://your-website.com/pyspark/basics.html">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="PySpark Basics - Essential DataFrame Operations">
    <meta property="og:description" content="Master PySpark basics with essential DataFrame operations. Learn to show, limit, count, and write data, and convert to Pandas. Essential for Spark developers.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://your-website.com/pyspark/basics.html">
    <meta property="og:image" content="https://your-website.com/path/to/og-image.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="PySpark Basics - Essential DataFrame Operations">
    <meta name="twitter:description" content="Master PySpark basics with essential DataFrame operations. Learn to show, limit, count, and write data, and convert to Pandas. Essential for Spark developers.">
    <meta name="twitter:image" content="https://your-website.com/path/to/twitter-image.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .header {
            background: #2c3e50;
            color: white;
            padding: 20px;
            border-bottom: 1px solid #34495e;
        }
        .header h1 {
            margin: 0;
            font-size: 1.5em;
        }
        .content {
            padding: 20px;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
            margin: 0;
        }
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
        }
        .markdown-content {
            line-height: 1.7;
        }
        .markdown-content h1, .markdown-content h2, .markdown-content h3 {
            color: #2c3e50;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 8px;
        }
        .markdown-content pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
        }
        .markdown-content code {
            background: #f1f3f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        .markdown-content pre code {
            background: none;
            padding: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>PySpark Basics</h1>
        </div>
        <div class="content">
            <div class="markdown-content">
                <p>This section covers fundamental PySpark DataFrame operations essential for data manipulation and analysis. Understanding these basics is crucial for anyone working with Apache Spark and Python.</p>

                <h2>Displaying DataFrame Content</h2>
                <p>To view the contents of a PySpark DataFrame, you can use the <code>show()</code> method. This is useful for quickly inspecting your data.</p>
                <pre class="codehilite"><code class="language-python"># Show a preview of the DataFrame
df.show()

# Show preview of the first or last n rows
df.head(5)
df.tail(5)
</code></pre>

                <h2>Previewing Data as JSON</h2>
                <p>For a more structured preview, especially for debugging or understanding nested data, you can convert the DataFrame to a JSON string. Be mindful that <code>collect()</code> brings all data into the driver's memory, so use it with caution on large DataFrames.</p>
                <pre class="codehilite"><code class="language-python"># Optional: Limit the DataFrame to a smaller size before collecting
df_limited = df.limit(10)
# Show preview as JSON (WARNING: in-memory operation)
print(json.dumps([row.asDict(recursive=True) for row in df_limited.collect()], indent=2))
</code></pre>

                <h2>Limiting DataFrame Rows</h2>
                <p>You can limit the number of rows in a DataFrame. Note that <code>limit()</code> itself is a transformation and doesn't trigger computation until an action is performed. Applying it multiple times can be non-deterministic in terms of which specific rows are kept if no ordering is applied.</p>
                <pre class="codehilite"><code class="language-python"># Limit the DataFrame to a specific number of rows
df = df.limit(5)
</code></pre>

                <h2>Inspecting DataFrame Structure</h2>
                <p>Understanding the schema and columns of your DataFrame is key to effective data processing.</p>
                <pre class="codehilite"><code class="language-python"># Get the list of column names
df.columns

# Get column names and their data types
df.dtypes

# Get the detailed schema of the DataFrame
df.schema
</code></pre>

                <h2>Counting Rows and Columns</h2>
                <p>Determine the size of your DataFrame by counting its rows and columns.</p>
                <pre class="codehilite"><code class="language-python"># Get the total number of rows in the DataFrame
df.count()

# Get the number of columns in the DataFrame
len(df.columns)
</code></pre>

                <h2>Writing DataFrame Output</h2>
                <p>Persist your processed DataFrame to disk in various formats. CSV is a common choice for interoperability.</p>
                <pre class="codehilite"><code class="language-python"># Write the DataFrame output to disk in CSV format
df.write.csv('/path/to/your/output/file')
</code></pre>

                <h2>Retrieving Data into Driver Memory</h2>
                <p>These operations bring data from the distributed Spark environment to the driver program. Use them judiciously, especially with large datasets, to avoid memory issues.</p>
                <pre class="codehilite"><code class="language-python"># Get results as a list of PySpark Rows (WARNING: in-memory)
results_rows = df.collect()

# Get results as a list of Python dictionaries (WARNING: in-memory)
results_dicts = [row.asDict(recursive=True) for row in df.collect()]
</code></pre>

                <h2>Converting to Pandas DataFrame</h2>
                <p>For tasks that require libraries like Pandas or for final analysis on smaller datasets, you can convert a PySpark DataFrame to a Pandas DataFrame.</p>
                <pre class="codehilite"><code class="language-python"># Convert the PySpark DataFrame to a Pandas DataFrame (WARNING: in-memory)
pandas_df = df.toPandas()
</code></pre>

                <p>These basic operations form the building blocks for more complex data transformations and analyses in PySpark. Familiarity with these methods will significantly speed up your development process.</p>
            </div>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
</body>
</html>