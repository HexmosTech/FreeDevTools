{
  "category": "multimodal-input-processing",
  "categoryDisplay": "Multimodal Input Processing",
  "description": "",
  "totalRepositories": 4,
  "repositories": {
    "8beeeaaat--touchdesigner-mcp": {
      "owner": "8beeeaaat",
      "name": "touchdesigner-mcp",
      "url": "https://github.com/8beeeaaat/touchdesigner-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/8beeeaaat.webp",
      "description": "The TouchDesigner MCP Server allows AI agents to interact with TouchDesigner projects by creating, modifying, and deleting project elements, as well as executing Python scripts to automate tasks.",
      "stars": 94,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:04:27Z",
      "readme_content": "# TouchDesigner MCP\n\nThis is an implementation of an MCP (Model Context Protocol) server for TouchDesigner. Its goal is to enable AI agents to control and operate TouchDesigner projects.\n\n[English](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/README.md) / [日本語](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/README.ja.md)\n\n## Overview\n\n[![demo clip](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/particle_on_youtube.png)](https://youtu.be/V2znaqGU7f4?si=6HDFbcBHCFPdttkM&t=635)\n\nTouchDesigner MCP acts as a bridge between AI models and the TouchDesigner WebServer DAT, enabling AI agents to:\n- Create, modify, and delete nodes\n- Query node properties and project structure\n- Programmatically control TouchDesigner via Python scripts\n\n## Usage\n\n<details>\n  <summary>Method 1: Using Claude Desktop and Desktop Extensions (Recommended)</summary>\n\n### 1. Download Files\nDownload the following from the [releases page](https://github.com/8beeeaaat/touchdesigner-mcp/releases/latest):\n- **TouchDesigner Components**: `touchdesigner-mcp-td.zip`\n- **Desktop Extension (.dxt)**: `touchdesigner-mcp.dxt`\n\n### 2. Set up TouchDesigner Components\n1. Extract the TouchDesigner components from `touchdesigner-mcp-td.zip`.\n2. Import `mcp_webserver_base.tox` into your TouchDesigner project.\n3. Place it at `/project1/mcp_webserver_base`.\n\nhttps://github.com/user-attachments/assets/215fb343-6ed8-421c-b948-2f45fb819ff4\n\n  You can check the startup logs by opening the Textport from the TouchDesigner menu.\n\n  ![import](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/textport.png)\n\n### 3. Install the Desktop Extension\nDouble-click the `touchdesigner-mcp.dxt` file to install the extension in Claude Desktop.\n\nhttps://github.com/user-attachments/assets/0786d244-8b82-4387-bbe4-9da048212854\n\n### 4. Connect to the Server\nThe extension will automatically handle the connection to the TouchDesigner server.\n\n**⚠️ Important:** The directory structure must be preserved exactly as extracted. The `mcp_webserver_base.tox` component references relative paths to the `modules/` directory and other files.\n\n</details>\n\n<details>\n  <summary>Method 2: Using npx</summary>\n\n*Requires Node.js to be installed.*\n\n### 1. Set up TouchDesigner Components\n1. Download and extract the TouchDesigner components from `touchdesigner-mcp-td.zip` ([releases page](https://github.com/8beeeaaat/touchdesigner-mcp/releases/latest)).\n2. Import `mcp_webserver_base.tox` into your TouchDesigner project.\n3. Place it at `/project1/mcp_webserver_base`.\n\nhttps://github.com/user-attachments/assets/215fb343-6ed8-421c-b948-2f45fb819ff4\n\n  You can check the startup logs by opening the Textport from the TouchDesigner menu.\n\n  ![import](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/textport.png)\n\n### 2. Set up the MCP Server Configuration\n\n*Example for Claude Desktop:*\n```json\n{\n  \"mcpServers\": {\n    \"touchdesigner\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"touchdesigner-mcp-server@latest\", \"--stdio\"]\n    }\n  }\n}\n```\n\n**Customization:** You can customize the TouchDesigner server connection by adding `--host` and `--port` arguments:\n```json\n\"args\": [\n  \"-y\",\n  \"touchdesigner-mcp-server@latest\",\n  \"--stdio\",\n  \"--host=http://custom_host\",\n  \"--port=9982\"\n]\n```\n</details>\n\n<details>\n  <summary>Method 3: Using a Docker Image</summary>\n\n  [![tutorial](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/tutorial_docker.png)](https://www.youtube.com/watch?v=BRWoIEVb0TU)\n\n  ### 1. Clone the repository\n  ```bash\n  git clone https://github.com/8beeeaaat/touchdesigner-mcp.git\n  cd touchdesigner-mcp\n  ```\n\n  ### 2. Build the Docker image\n  ```bash\n  make build\n  ```\n\n  ### 3. Install the API Server in Your TouchDesigner Project\n\n  Start TouchDesigner and import the `td/mcp_webserver_base.tox` component into the project you want to control.\n  Example: Place it at `/project1/mcp_webserver_base`.\n\n  Importing the `.tox` file will trigger the `td/import_modules.py` script, which loads the necessary modules for the API server.\n\nhttps://github.com/user-attachments/assets/215fb343-6ed8-421c-b948-2f45fb819ff4\n\n  You can check the startup logs by opening the Textport from the TouchDesigner menu.\n\n  ![import](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/textport.png)\n\n  ### 4. Start the MCP server container\n\n  ```bash\n  docker-compose up -d\n  ```\n\n  ### 5. Configure your AI agent to use the Docker container\n\n  *Example for Claude Desktop:*\n  ```json\n  {\n    \"mcpServers\": {\n      \"touchdesigner\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"compose\",\n          \"-f\",\n          \"/path/to/your/touchdesigner-mcp/docker-compose.yml\",\n          \"exec\",\n          \"-i\",\n          \"touchdesigner-mcp-server\",\n          \"node\",\n          \"dist/cli.js\",\n          \"--stdio\",\n          \"--host=http://host.docker.internal\"\n        ]\n      }\n    }\n  }\n```\n\n  *On Windows systems, include the drive letter, e.g., `C:\\path\\to\\your\\touchdesigner-mcp\\docker-compose.yml`.*\n\n**Note:** You can customize the TouchDesigner server connection by adding `--host` and `--port` arguments:\n  ```json\n\"args\": [\n  ...,\n  \"--stdio\",\n  \"--host=http://host.docker.internal\",\n  \"--port=9982\"\n]\n  ```\n</details>\n\n\n## Verify Connection\n\nIf the MCP server is recognized, the setup is complete.\nIf it's not recognized, try restarting your AI agent.\nIf you see an error at startup, try launching the agent again after starting TouchDesigner.\nWhen the API server is running properly in TouchDesigner, the agent can use the provided tools to operate it.\n\n### Directory Structure Requirements\n\n**Critical:** When using any method, you must maintain the original directory structure:\n\n```\ntd/\n├── import_modules.py          # Module loader script\n├── mcp_webserver_base.tox     # Main TouchDesigner component\n└── modules/                   # Python modules directory\n    ├── mcp/                   # MCP core logic\n    ├── utils/                 # Shared utilities\n    └── td_server/             # Generated API server code\n```\n\nThe `mcp_webserver_base.tox` component uses relative paths to locate Python modules. Moving or reorganizing these files will cause import errors in TouchDesigner.\n\n![demo](https://github.com/8beeeaaat/touchdesigner-mcp/blob/main/assets/nodes_list.png)\n\n\n## MCP Server Features\n\nThis server enables AI agents to perform operations in TouchDesigner using the Model Context Protocol (MCP).\n\n### Tools\n\nTools allow AI agents to perform actions in TouchDesigner.\n\n| Tool Name                | Description                                                        |\n| :---------------------- | :----------------------------------------------------------------- |\n| `create_td_node`        | Creates a new node.                                                |\n| `delete_td_node`        | Deletes an existing node.                                          |\n| `exec_node_method`      | Calls a Python method on a node.                                   |\n| `execute_python_script` | Executes an arbitrary Python script in TouchDesigner.              |\n| `get_td_class_details`  | Gets details of a TouchDesigner Python class or module.            |\n| `get_td_classes`        | Gets a list of TouchDesigner Python classes.                       |\n| `get_td_info`           | Gets information about the TouchDesigner server environment.       |\n| `get_td_node_parameters`| Gets the parameters of a specific node.                            |\n| `get_td_nodes`          | Gets nodes under a parent path, with optional filtering.           |\n| `update_td_node_parameters` | Updates the parameters of a specific node.                     |\n\n### Prompts\n\nPrompts provide instructions for AI agents to perform specific actions in TouchDesigner.\n\n| Prompt Name         | Description                                                                 |\n| :------------------| :-------------------------------------------------------------------------- |\n| `Search node`      | Fuzzy searches for nodes and retrieves information based on name, family, or type. |\n| `Node connection`  | Provides instructions to connect nodes within TouchDesigner.                |\n| `Check node errors`| Checks for errors on a specified node, and recursively for its children.    |\n\n### Resources\n\nNot implemented.\n\n\n## For Developers\n\n### Quick Start for Development\n\n1. **Set up your environment:**\n   ```bash\n   # Clone and install dependencies\n   git clone https://github.com/8beeeaaat/touchdesigner-mcp.git\n   cd touchdesigner-mcp\n   npm install\n   ```\n\n2. **Build the project:**\n   ```bash\n   make build        # Docker-based build (recommended)\n   # OR\n   npm run build     # Node.js-based build\n   ```\n\n3. **Available commands:**\n   ```bash\n   npm run test      # Run unit and integration tests\n   npm run dev       # Launch the MCP inspector for debugging\n   ```\n\n**Note:** When you update the code, you must restart both the MCP server and TouchDesigner to apply the changes.\n\n### Project Structure Overview\n\n```\n├── src/                       # MCP server source code\n│   ├── api/                  # OpenAPI spec for the TouchDesigner WebServer\n│   ├── core/                 # Core utilities (logger, error handling)\n│   ├── features/             # MCP feature implementations\n│   │   ├── prompts/         # Prompt handlers\n│   │   ├── resources/       # Resource handlers\n│   │   └── tools/           # Tool handlers (e.g., tdTools.ts)\n│   ├── gen/                  # Code generated from the OpenAPI schema for the MCP server\n│   ├── server/               # MCP server logic (connections, main server class)\n│   ├── tdClient/             # TouchDesigner connection API client\n│   ├── index.ts              # Main entry point for the Node.js server\n│   └── ...\n├── td/                        # TouchDesigner-related files\n│   ├── modules/              # Python modules for TouchDesigner\n│   │   ├── mcp/              # Core logic for handling MCP requests in TouchDesigner\n│   │   │   ├── controllers/ # API request controllers (api_controller.py, generated_handlers.py)\n│   │   │   └── services/    # Business logic (api_service.py)\n│   │   ├── td_server/        # Python model code generated from the OpenAPI schema\n│   │   └── utils/            # Shared Python utilities\n│   ├── templates/             # Mustache templates for Python code generation\n│   ├── genHandlers.js         # Node.js script for generating generated_handlers.py\n│   ├── import_modules.py      # Helper script to import API server modules into TouchDesigner\n│   └── mcp_webserver_base.tox # Main TouchDesigner component\n├── tests/                      # Test code\n│   ├── integration/\n│   └── unit/\n└── orval.config.ts             # Orval config (TypeScript client generation)\n```\n\n\n### API Code Generation Workflow\n\nThis project uses OpenAPI-based code generation tools (Orval and openapi-generator-cli).\n\n**API Definition:** The API contract between the Node.js MCP server and the Python server running inside TouchDesigner is defined in `src/api/index.yml`.\n\n1.  **Python server generation (`npm run gen:webserver`):**\n    *   Uses `openapi-generator-cli` via Docker.\n    *   Reads `src/api/index.yml`.\n    *   Generates a Python server skeleton (`td/modules/td_server/`) based on the API definition. This code runs inside TouchDesigner's WebServer DAT.\n    *   **Requires Docker to be installed and running.**\n2.  **Python handler generation (`npm run gen:handlers`):**\n    *   Uses a custom Node.js script (`td/genHandlers.js`) and Mustache templates (`td/templates/`).\n    *   Reads the generated Python server code or OpenAPI spec.\n    *   Generates handler implementations (`td/modules/mcp/controllers/generated_handlers.py`) that connect to the business logic in `td/modules/mcp/services/api_service.py`.\n3.  **TypeScript client generation (`npm run gen:mcp`):**\n    *   Uses `Orval` to generate an API client and Zod schemas for tool validation from the schema YAML, which is bundled by `openapi-generator-cli`.\n    *   Generates a typed TypeScript client (`src/tdClient/`) used by the Node.js server to make requests to the WebServer DAT.\n\nThe build process (`npm run build`) runs all necessary generation steps (`npm run gen`), followed by TypeScript compilation (`tsc`).\n\n## Contributing\n\nWe welcome your contributions!\n\n1. Fork the repository.\n2. Create a feature branch (`git checkout -b feature/amazing-feature`).\n3. Make your changes.\n4. Add tests and ensure everything works (`npm test`).\n5. Commit your changes (`git commit -m 'Add some amazing feature'`).\n6. Push to your branch (`git push origin feature/amazing-feature`).\n7. Open a pull request.\n\nPlease always include appropriate tests when making implementation changes.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "touchdesigner",
        "multimodal",
        "interact",
        "mcp touchdesigner",
        "touchdesigner mcp",
        "touchdesigner projects"
      ],
      "category": "multimodal-input-processing"
    },
    "pinkpixel-dev--MCPollinations": {
      "owner": "pinkpixel-dev",
      "name": "MCPollinations",
      "url": "https://github.com/pinkpixel-dev/MCPollinations",
      "imageUrl": "/freedevtools/mcp/pfp/pinkpixel-dev.webp",
      "description": "Generates images, text, and audio from prompts using the Pollinations APIs. It supports returning images as base64-encoded data and allows listing available models for image and text generation.",
      "stars": 34,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T03:37:33Z",
      "readme_content": "# MCPollinations Multimodal MCP Server\nA Model Context Protocol (MCP) server that enables AI assistants to generate images, text, and audio through the Pollinations APIs\n\n[![smithery badge](https://smithery.ai/badge/@pinkpixel-dev/mcpollinations)](https://smithery.ai/server/@pinkpixel-dev/mcpollinations) [![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/8448e4ec-c863-476a-8adb-aed3cf16ea2b)\n\n## Features\n\n- Generate image URLs from text prompts\n- Generate images and return them as base64-encoded data AND save as png, jpeg, jpg, or webp (default: png)\n- Generate text responses from text prompts\n- Generate audio responses from text prompts\n- List available image and text generation models\n- No authentication required\n- Simple and lightweight\n- Compatible with the Model Context Protocol (MCP)\n\n## System Requirements\n\n- **Node.js**: Version 14.0.0 or higher\n  - For best performance, we recommend Node.js 16.0.0 or higher\n  - Node.js versions below 16 use an AbortController polyfill\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install mcpollinations for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/mcpollinations):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/mcpollinations --client claude\n```\n\nThe easiest way to use the MCP server:\n\n```bash\n# Run directly with npx (no installation required)\nnpx @pinkpixel/mcpollinations\n```\n\nIf you prefer to install it globally:\n\n```bash\n# Install globally\nnpm install -g @pinkpixel/mcpollinations\n\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n\n```\n\nOr clone the repository:\n\n```bash\n# Clone the git repository\ngit clone https://github.com/pinkpixel-dev/mcpollinations.git\n# Run the server\nmcpollinations\n# or\nnpx @pinkpixel/mcpollinations\n# or run directly\nnode /path/to/MCPollinations/pollinations-mcp-server.js\n\n```\n\n## MCP Integration\n\nTo integrate the server with applications that support the Model Context Protocol (MCP):\n\n1. Generate an MCP configuration file:\n\n```bash\n# If installed globally\nnpx @pinkpixel/mcpollinations generate-config\n\n# Or run directly\nnode /path/to/MCPollinations/generate-mcp-config.js\n```\n\n### Quick MCP Config (env)\nIf you prefer to skip the generator, copy this into your MCP client config:\n\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"YOUR_TOKEN_OPTIONAL\",\n      \"referrer\": \"your-app-or-domain-optional\",\n      \"IMAGE_MODEL\": \"flux\",\n      \"IMAGE_WIDTH\": \"1024\",\n      \"IMAGE_HEIGHT\": \"1024\",\n      \"IMAGE_ENHANCE\": \"true\",\n      \"IMAGE_SAFE\": \"false\",\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"\",\n      \"AUDIO_VOICE\": \"alloy\",\n      \"OUTPUT_DIR\": \"./mcpollinations-output\"\n    }\n  }\n}\n```\n\n2. Follow the prompts to customize your configuration or use the defaults.\n   - Set an output directory (relative paths recommended for portability)\n     - **Windows users**: Consider using absolute paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations`) for more reliable file saving\n   - Configure optional authentication (token, referrer) under `env`\n   - Configure default parameters for image generation (with a list of available models, dimensions, etc.)\n   - Configure default parameters for text generation (with a list of available models)\n   - Configure default parameters for audio generation (voice)\n\n\n3. Copy the generated `mcp.json` file to your application's MCP settings .json file.\n4. Restart your application.\n\nAfter integration, you can use commands like:\n\n\"Generate an image of a sunset over the ocean using MCPollinations\"\n\n## Authentication (Optional)\n\nMCPollinations supports optional authentication to provide access to more models and better rate limits. The server works perfectly without authentication (free tier), but users with API tokens can get enhanced access.\n\n### Configuration Methods\n\n**Method 1: Environment Variables (Recommended for security)**\n```bash\n# Set environment variables before running the server\nexport POLLINATIONS_TOKEN=\"your-api-token\"\nexport POLLINATIONS_REFERRER=\"https://your-domain.com\"\n\n# Then run the server\nnpx @pinkpixel/mcpollinations\n```\n\n**Method 2: MCP Configuration File (env)**\nWhen generating your MCP configuration, place auth inside `env` so your MCP client passes them as environment variables to the server process:\n```json\n{\n  \"mcpollinations\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@pinkpixel/mcpollinations\"],\n    \"env\": {\n      \"token\": \"your-api-token\",\n      \"referrer\": \"your-app-or-domain\"\n    }\n  }\n}\n```\n\nYou can also provide `POLLINATIONS_TOKEN` and `POLLINATIONS_REFERRER` instead; the server recognizes both forms. Using `token` and `referrer` inside `env` is recommended for MCP configs.\n\n### Authentication Parameters\n\n- **`token`** (optional): Your Pollinations API token for enhanced access\n- **`referrer`** (optional): Your domain/application referrer URL\n\nBoth parameters are completely optional. Leave them empty or unset to use the free tier.\n\n## Using Your Configuration Settings\n\nMCPollinations respects your MCP configuration settings placed in `env` as defaults. When you ask an AI assistant to generate content:\n\n- **Your configured models, output directories, and parameters are used automatically**\n- **To override**: Specifically instruct the AI to use different settings\n  - \"Generate an image using the kontext model\"\n  - \"Save this image to my Desktop folder\"\n  - \"Use a temperature of 1.2 for this text generation\"\n\n**Example Instructions:**\n- ✅ \"Generate a sunset image\" → Uses your configured model and output directory\n- ✅ \"Generate a sunset image with the flux model\" → Overrides model only\n- ✅ \"Generate a sunset image and save it to C:\\Pictures\" → Overrides output path only\n\nThis ensures your preferences are always respected unless you specifically want different settings for a particular request.\n\n## Troubleshooting\n\n### \"AbortController is not defined\" Error\n\nIf you encounter this error when running the MCP server:\n\n```\nReferenceError: AbortController is not defined\n```\n\nThis is usually caused by running on an older version of Node.js (below version 16.0.0). Try one of these solutions:\n\n1. **Update Node.js** (recommended):\n   - Update to Node.js 16.0.0 or newer\n\n2. **Use Global Installation**\n   - Update to the latest version of the package:\n   ```bash\n   npm install -g @pinkpixel/mcpollinations\n   # Run with npx\n   npx @pinkpixel/mcpollinations\n   ```\n\n3. **Install AbortController manually**:\n   - If for some reason the polyfill doesn't work:\n   ```bash\n   npm install node-abort-controller\n   ```\n\n### Check Your Node.js Version\n\nTo check your current Node.js version:\n\n```bash\nnode --version\n```\n\nIf it shows a version lower than 16.0.0, consider upgrading for best compatibility.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n### **Image Generation Tools**\n1. `generateImageUrl` - Generates an image URL from a text prompt\n2. `generateImage` - Generates an image, returns it as base64-encoded data, and saves it to a file by default (PNG format)\n3. `editImage` - **NEW!** Edit or modify existing images based on text prompts\n4. `generateImageFromReference` - **NEW!** Generate new images using existing images as reference\n5. `listImageModels` - Lists available models for image generation\n\n### **Text & Audio Tools**\n6. `respondText` - Responds with text to a prompt using text models (customizable parameters)\n7. `respondAudio` - Generates an audio response to a text prompt (customizable voice parameter)\n8. `listTextModels` - Lists available models for text generation\n9. `listAudioVoices` - Lists all available voices for audio generation\n\n## Text Generation Details\n\n### Available Parameters\n\nThe `respondText` tool supports several parameters for fine-tuning text generation:\n\n- **`model`**: Choose from available text models (use `listTextModels` to see current options)\n- **`temperature`** (0.0-2.0): Controls randomness in the output\n  - Lower values (0.1-0.7) = more focused and deterministic\n  - Higher values (0.8-2.0) = more creative and random\n- **`top_p`** (0.0-1.0): Controls diversity via nucleus sampling\n  - Lower values = more focused on likely tokens\n  - Higher values = considers more token possibilities\n- **`system`**: System prompt to guide the model's behavior and personality\n\n### Customizing Text Generation\n\n```javascript\n// Example options for respondText\nconst options = {\n  model: \"openai\",           // Model selection\n  temperature: 0.7,          // Balanced creativity\n  top_p: 0.9,               // High diversity\n  system: \"You are a helpful assistant that explains things clearly and concisely.\"\n};\n```\n\n### Configuration Examples\n\nIn your MCP configuration, set defaults under `env` so the server uses them automatically:\n\n```json\n{\n  \"mcpollinations\": {\n    \"env\": {\n      \"TEXT_MODEL\": \"openai\",\n      \"TEXT_TEMPERATURE\": \"0.7\",\n      \"TEXT_TOP_P\": \"0.9\",\n      \"TEXT_SYSTEM\": \"You are a helpful coding assistant.\"\n    }\n  }\n}\n```\n\n## Image-to-Image Generation (NEW!)\n\nMCPollinations now supports powerful image-to-image generation with two specialized tools:\n\n### **editImage Tool**\nPerfect for modifying existing images:\n- **Remove objects**: \"remove the cat from this image\"\n- **Add elements**: \"add a dog to this scene\"\n- **Change backgrounds**: \"replace the background with mountains\"\n- **Style modifications**: \"make the lighting more dramatic\"\n\n### **generateImageFromReference Tool**\nPerfect for creating variations and new styles:\n- **Style transfer**: \"make this photo look like a painting\"\n- **Format changes**: \"convert this to a cartoon style\"\n- **Creative variations**: \"create a futuristic version of this\"\n- **Artistic interpretations**: \"make this look like a sketch\"\n\n### **Supported Models**\n- **`kontext`**: Specialized model optimized for image-to-image tasks\n- **`nanobanana`**: New Google model supporting both text-to-image and image-to-image generation\n- **`seedream`**: New ByteDance model supporting both text-to-image and image-to-image generation\n\nMulti-reference images: `editImage` and `generateImageFromReference` accept `imageUrl` as a single URL or an array of URLs. The server encodes arrays as the comma-separated `image` parameter used by the API. Ordering matters; kontext uses only the first image, nanobanana is safe up to ~4 refs, and seedream supports up to 10.\n\nImportant: URLs only. The image-to-image tools require publicly accessible HTTP(S) URLs. Local file paths, file uploads, and base64/data URLs are not supported by this MCP server (it does not upload files). If you need to work from a local image, host it somewhere accessible (e.g., a temporary file host, object storage, or a raw link in a repo) and pass the URL.\n\n### **Example Usage**\n```javascript\n// Edit an existing image\nconst editResult = await editImage(\n  \"change the background to a sunset beach\",\n  \"https://example.com/photo.jpg\",\n  \"nanobanana\"  // or \"kontext\", \"seedream\"\n);\n\n// Generate from reference\nconst referenceResult = await generateImageFromReference(\n  \"make this into a watercolor painting\",\n  \"https://example.com/photo.jpg\",\n  \"seedream\"  // or \"kontext\", \"nanobanana\"\n);\n```\n\n## Image Generation Details\n\n### Default Behavior\n\nWhen using the `generateImage` tool:\n\n- Images are saved to disk by default as PNG files\n- The default save location is the current working directory where the MCP server is running\n- The 'flux' model is used by default\n- A random seed is generated by default for each image (ensuring variety)\n- Base64-encoded image data is always returned, regardless of whether the image is saved to a file\n\n### Customizing Image Generation\n\n```javascript\n// Example options for generateImage\nconst options = {\n  // Model selection (defaults to 'flux')\n  // Available models: \"flux\", \"turbo\", \"kontext\", \"nanobanana\", \"seedream\"\n  model: \"flux\",\n\n  // Image dimensions\n  width: 1024,\n  height: 1024,\n\n  // Generation options\n  seed: 12345,  // Specific seed for reproducibility (defaults to random)\n  enhance: true,  // Enhance the prompt using an LLM before generating (defaults to true)\n  safe: false,  // Content filtering (defaults to false)\n\n  // File saving options\n  saveToFile: true,  // Set to false to skip saving to disk\n  outputPath: \"/path/to/save/directory\",  // Custom save location\n  fileName: \"my_custom_name\",  // Without extension\n  format: \"png\"  // png, jpeg, jpg, or webp\n};\n```\n\n### Where Images Are Saved\n\nWhen using Claude or another application with the MCP server:\n\n1. **Images are saved in the current working directory of where the MCP server is running**, not where Claude or the client application is installed.\n\n2. If you start the MCP server manually from a specific directory, images will be saved there by default.\n\n3. If Claude Desktop launches the MCP server automatically, images will be saved in Claude Desktop's working directory (typically in an application data folder).\n\n**💡 Windows Users**: For reliable file saving on Windows, use absolute paths in your MCP configuration instead of relative paths (e.g., `C:\\Users\\YourName\\Pictures\\MCPollinations` instead of `./mcpollinations-output`). Relative paths may not resolve as expected depending on the working directory context.\n\n### Finding Your Generated Images\n\n- The response from Claude after generating an image includes the full file path where the image was saved\n- You can specify a familiar location using the `outputPath` parameter\n- Best practice: Ask Claude to save images to an easily accessible folder like your Pictures or Downloads directory\n\n### Unique Filenames\n\nThe MCP server ensures that generated images always have unique filenames and will never overwrite existing files:\n\n1. **Default filenames** include:\n   - A sanitized version of the prompt (first 20 characters)\n   - A timestamp\n   - A random suffix\n\n2. **Custom filenames** are also protected:\n   - If you specify a filename and a file with that name already exists, a numeric suffix will be added automatically\n   - For example: `sunset.png`, `sunset_1.png`, `sunset_2.png`, etc.\n\nThis means you can safely generate multiple images with the same prompt or filename without worrying about overwriting previous images.\n\n### Accessing Base64 Data\n\nEven when saving to a file, the base64-encoded image data is always returned and can be used for:\n\n- Embedding in web pages (`<img src=\"data:image/png;base64,...\" />`)\n- Passing to other services or APIs\n- Processing in memory without filesystem operations\n- Displaying in applications that support data URIs\n\n## For Developers\n\nIf you want to use the package in your own projects:\n\n```bash\n# Install as a dependency\nnpm install @pinkpixel/mcpollinations\n\n# Import in your code\nimport { generateImageUrl, generateImage, repsondText, respondAudio, listTextModels, listImageModels, listAudioVoices } from '@pinkpixel/mcpollinations';\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pinkpixel",
        "multimodal",
        "images",
        "pinkpixel dev",
        "processing pinkpixel",
        "generates images"
      ],
      "category": "multimodal-input-processing"
    },
    "stabgan--openrouter-mcp-multimodal": {
      "owner": "stabgan",
      "name": "openrouter-mcp-multimodal",
      "url": "https://github.com/stabgan/openrouter-mcp-multimodal",
      "imageUrl": "/freedevtools/mcp/pfp/stabgan.webp",
      "description": "Combines text chat and image analysis capabilities to conduct multimodal conversations and handle custom queries seamlessly. Optimizes workflows with intelligent model selection and performance improvements.",
      "stars": 10,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-05T07:28:32Z",
      "readme_content": "# OpenRouter MCP Multimodal Server\n\n[![Build Status](https://github.com/stabgan/openrouter-mcp-multimodal/actions/workflows/publish.yml/badge.svg)](https://github.com/stabgan/openrouter-mcp-multimodal/actions/workflows/publish.yml)\n[![npm version](https://img.shields.io/npm/v/@stabgan/openrouter-mcp-multimodal.svg)](https://www.npmjs.com/package/@stabgan/openrouter-mcp-multimodal)\n[![Docker Pulls](https://img.shields.io/docker/pulls/stabgandocker/openrouter-mcp-multimodal.svg)](https://hub.docker.com/r/stabgandocker/openrouter-mcp-multimodal)\n\nAn MCP (Model Context Protocol) server that provides chat and image analysis capabilities through OpenRouter.ai's diverse model ecosystem. This server combines text chat functionality with powerful image analysis capabilities.\n\n## Features\n\n- **Text Chat:**\n  - Direct access to all OpenRouter.ai chat models\n  - Support for simple text and multimodal conversations\n  - Configurable temperature and other parameters\n\n- **Image Analysis:**\n  - Analyze single images with custom questions\n  - Process multiple images simultaneously \n  - Automatic image resizing and optimization\n  - Support for various image sources (local files, URLs, data URLs)\n\n- **Model Selection:**\n  - Search and filter available models\n  - Validate model IDs\n  - Get detailed model information\n  - Support for default model configuration\n\n- **Performance Optimization:**\n  - Smart model information caching\n  - Exponential backoff for retries\n  - Automatic rate limit handling\n\n## What's New in 1.5.0\n\n- **Improved OS Compatibility:**\n  - Enhanced path handling for Windows, macOS, and Linux\n  - Better support for Windows-style paths with drive letters\n  - Normalized path processing for consistent behavior across platforms\n\n- **MCP Configuration Support:**\n  - Cursor MCP integration without requiring environment variables\n  - Direct configuration via MCP parameters\n  - Flexible API key and model specification options\n\n- **Robust Error Handling:**\n  - Improved fallback mechanisms for image processing\n  - Better error reporting with specific diagnostics\n  - Multiple backup strategies for file reading\n\n- **Image Processing Enhancements:**\n  - More reliable base64 encoding for all image types\n  - Fallback options when Sharp module is unavailable\n  - Better handling of large images with automatic optimization\n\n## Installation\n\n### Option 1: Install via npm\n\n```bash\nnpm install -g @stabgan/openrouter-mcp-multimodal\n```\n\n### Option 2: Run via Docker\n\n```bash\ndocker run -i -e OPENROUTER_API_KEY=your-api-key-here stabgandocker/openrouter-mcp-multimodal:latest\n```\n\n## Quick Start Configuration\n\n### Prerequisites\n\n1. Get your OpenRouter API key from [OpenRouter Keys](https://openrouter.ai/keys)\n2. Choose a default model (optional)\n\n### MCP Configuration Options\n\nAdd one of the following configurations to your MCP settings file (e.g., `cline_mcp_settings.json` or `claude_desktop_config.json`):\n\n#### Option 1: Using npx (Node.js)\n\n```json\n{\n  \"mcpServers\": {\n    \"openrouter\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@stabgan/openrouter-mcp-multimodal\"\n      ],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\",\n        \"DEFAULT_MODEL\": \"qwen/qwen2.5-vl-32b-instruct:free\"\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Using uv (Python Package Manager)\n\n```json\n{\n  \"mcpServers\": {\n    \"openrouter\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"-m\",\n        \"openrouter_mcp_multimodal\"\n      ],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\",\n        \"DEFAULT_MODEL\": \"qwen/qwen2.5-vl-32b-instruct:free\"\n      }\n    }\n  }\n}\n```\n\n#### Option 3: Using Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"openrouter\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENROUTER_API_KEY=your-api-key-here\",\n        \"-e\", \"DEFAULT_MODEL=qwen/qwen2.5-vl-32b-instruct:free\",\n        \"stabgandocker/openrouter-mcp-multimodal:latest\"\n      ]\n    }\n  }\n}\n```\n\n#### Option 4: Using Smithery (recommended)\n\n```json\n{\n  \"mcpServers\": {\n    \"openrouter\": {\n      \"command\": \"smithery\",\n      \"args\": [\n        \"run\",\n        \"stabgan/openrouter-mcp-multimodal\"\n      ],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\",\n        \"DEFAULT_MODEL\": \"qwen/qwen2.5-vl-32b-instruct:free\"\n      }\n    }\n  }\n}\n```\n\n## Examples\n\nFor comprehensive examples of how to use this MCP server, check out the [examples directory](./examples/). We provide:\n\n- JavaScript examples for Node.js applications\n- Python examples with interactive chat capabilities\n- Code snippets for integrating with various applications\n\nEach example comes with clear documentation and step-by-step instructions.\n\n## Dependencies\n\nThis project uses the following key dependencies:\n\n- `@modelcontextprotocol/sdk`: ^1.8.0 - Latest MCP SDK for tool implementation\n- `openai`: ^4.89.1 - OpenAI-compatible API client for OpenRouter\n- `sharp`: ^0.33.5 - Fast image processing library\n- `axios`: ^1.8.4 - HTTP client for API requests\n- `node-fetch`: ^3.3.2 - Modern fetch implementation\n\nNode.js 18 or later is required. All dependencies are regularly updated to ensure compatibility and security.\n\n## Available Tools\n\n### mcp_openrouter_chat_completion\n\nSend text or multimodal messages to OpenRouter models:\n\n```javascript\nuse_mcp_tool({\n  server_name: \"openrouter\",\n  tool_name: \"mcp_openrouter_chat_completion\",\n  arguments: {\n    model: \"google/gemini-2.5-pro-exp-03-25:free\", // Optional if default is set\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\"\n      },\n      {\n        role: \"user\",\n        content: \"What is the capital of France?\"\n      }\n    ],\n    temperature: 0.7 // Optional, defaults to 1.0\n  }\n});\n```\n\nFor multimodal messages with images:\n\n```javascript\nuse_mcp_tool({\n  server_name: \"openrouter\",\n  tool_name: \"mcp_openrouter_chat_completion\",\n  arguments: {\n    model: \"anthropic/claude-3.5-sonnet\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          {\n            type: \"text\",\n            text: \"What's in this image?\"\n          },\n          {\n            type: \"image_url\",\n            image_url: {\n              url: \"https://example.com/image.jpg\"\n            }\n          }\n        ]\n      }\n    ]\n  }\n});\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "multimodal",
        "conversations",
        "chat",
        "multimodal conversations",
        "mcp multimodal",
        "multimodal input"
      ],
      "category": "multimodal-input-processing"
    },
    "zakahan--vedit-mcp": {
      "owner": "zakahan",
      "name": "vedit-mcp",
      "url": "https://github.com/zakahan/vedit-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zakahan.webp",
      "description": "Enables video editing through natural language commands for basic editing operations. Integrates with projects to automate video processing tasks using ffmpeg.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-02T16:43:03Z",
      "readme_content": "## Vedit-MCP\nThis is an MCP service for `video editing`, which can achieve basic editing operations with just one sentence.\n\nEnglish | [中文](README_CN.md)\n## Quick Start\n\n### 1. Install Dependencies\n\n#### 1.1 Clone this project or directly download the zip package\n\n#### 1.2 Configure the Python environment\n\n1. It is recommended to use uv for installation\n```bash\ncd vedit-mcp\nuv pip install -r requirements.txt\n```\n2. Or install directly using pip\n```bash\npip install -r requirements.txt\n```\n\n#### 1.3 Configure ffmpeg\n\n`vedit-mcp.py` relies on `ffmpeg` for implementation. Therefore, please configure ffmpeg.\n\n```bash\n# For Mac\nbrew install ffmpeg\n# For Ubuntu\nsudo apt update\nsudo apt install ffmpeg\n``` \n\n### 2. Start the Service\n\n#### 2.1. It is recommended to use `google-adk` to build your own project\n\n- Please refer to [adk-sample](sample/adk_sample.py)\n\n##### Before executing this sample script\n\n1. Please ensure that the path format is at least as follows\n\n> - sample\n>     - kb\n>         - raw/test.mp4   // This is the original video you need to process\n>     - adk_sample.py\n> - vedit_mcp.py\n\n2. Please install the following two dependencies\n```python\n# # adk-sample pip install requirements\n# google-adk==0.3.0\n# litellm==1.67.2\n```\n3. Please set the api-key and api-base\n\nCurrently, this script uses the API of the [`Volcano Ark Platform`](https://www.volcengine.com/product/ark), and you can go there to configure it by yourself.\n\nAfter obtaining the API_KEY, please configure the API_KEY as an environment variable.\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\n4. Execute the script\n\n```bash\ncd sample\npython adk_sample.py\n```\n\n5. End of execution\n\nAfter this script is executed correctly and ends, a video result file will be generated in kb/result, and a log file will be generated and the result will be output.\n\nIf you need secondary development, you can choose to add `vedit_mcp.py` to your project for use.\n\n#### 2.2 Or build using `cline`\n\n\nFirstly, please ensure that your Python environment and ffmpeg configuration are correct\nConfigure cline_mcp_settings. json as follows\n```json\n{\n  \"mcpServers\": {\n    \"vedit-mcp\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"vedit_mcp.py\",\n        \"--kb_dir\",\n        \"your-kb-dir-here\"\n      ]\n    }\n  }\n}\n```\n\n#### 2.3. Execute using the stramlit web interface\n\nTo be supplemented \n\n\n### 3. precautions\n\n1. It is recommended to use the `thinking model` to handle this type of task. Currently, it seems that the `thinking model` performs better in handling this type of task? But no further testing has been conducted, it's just an intuitive feeling.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ffmpeg",
        "multimodal",
        "editing",
        "video editing",
        "automate video",
        "multimodal input"
      ],
      "category": "multimodal-input-processing"
    }
  }
}
