{
  "category": "aggregators",
  "categoryDisplay": "Aggregators",
  "description": "Servers for accessing many apps and tools through a single MCP server.",
  "repositories": {
    "1mcp--agent": {
      "category": "aggregators",
      "description": "A unified Model Context Protocol server implementation that aggregates multiple MCP servers into one.",
      "forks": 0,
      "imageUrl": "",
      "keywords": [
        "mcp",
        "aggregators",
        "servers",
        "mcp servers",
        "mcp server",
        "aggregators servers"
      ],
      "language": "Unknown",
      "license": "Unknown",
      "name": "agent",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "1mcp",
      "readme_content": "",
      "stars": 0,
      "updated_at": "",
      "url": "https://github.com/1mcp-app/agent"
    },
    "Data-Everything--mcp-server-templates": {
      "category": "aggregators",
      "description": "One server. All tools. A unified MCP platform that connects many apps, tools, and services behind one powerful interface‚Äîideal for local devs or production agents.",
      "forks": 2,
      "imageUrl": "",
      "keywords": [
        "aggregators",
        "servers",
        "mcp",
        "aggregators servers",
        "mcp server",
        "mcp platform"
      ],
      "language": "Python",
      "license": "Other",
      "name": "mcp-server-templates",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "Data-Everything",
      "readme_content": "# üöÄ This Project Has Moved!\n\n\u003e ## ‚ö†Ô∏è **IMPORTANT: This repository has been renamed and moved to [MCP Platform](https://github.com/Data-Everything/MCP-Platform)**\n\u003e\n\u003e **What changed:**\n\u003e - **New Repository**: [`Data-Everything/MCP-Platform`](https://github.com/Data-Everything/MCP-Platform)\n\u003e - **New Package**: `pip install mcp-platform` (replaces `mcp-templates`)\n\u003e - **New CLI**: `mcpp` command (replaces `mcpt`)\n\u003e - **Enhanced Features**: Improved architecture and expanded capabilities\n\u003e\n\u003e **Migration is easy:**\n\u003e ```bash\n\u003e # Uninstall old package\n\u003e pip uninstall mcp-templates\n\u003e\n\u003e # Install new package\n\u003e pip install mcp-platform\n\u003e\n\u003e # Use new command (all your configs work the same!)\n\u003e mcpp deploy demo  # instead of mcpt deploy demo\n\u003e ```\n\u003e\n\u003e **üìö [Complete Migration Guide](https://github.com/Data-Everything/MCP-Platform#migration-from-mcp-templates)** | **üÜï [New Documentation](https://data-everything.github.io/MCP-Platform/)**\n\n---\n\n# MCP Server Templates (Legacy)\n\n\u003e **‚ö†Ô∏è This version is in maintenance mode. Please migrate to [MCP Platform](https://github.com/Data-Everything/MCP-Platform) for latest features and updates.**\n\n[![Version](https://img.shields.io/pypi/v/mcp-templates.svg)](https://pypi.org/project/mcp-templates/)\n[![Python Versions](https://img.shields.io/pypi/pyversions/mcp-templates.svg)](https://pypi.org/project/mcp-templates/)\n[![License](https://img.shields.io/badge/License-Elastic%202.0-blue.svg)](LICENSE)\n[![Discord](https://img.shields.io/discord/XXXXX?color=7289da\u0026logo=discord\u0026logoColor=white)](https://discord.gg/55Cfxe9gnr)\n\n\u003cdiv align=\"center\"\u003e\n\n**ÔøΩ [Migrate to MCP Platform](https://github.com/Data-Everything/MCP-Platform)** ‚Ä¢ **[üí¨ Discord Community](https://discord.gg/55Cfxe9gnr)** ‚Ä¢ **[ÔøΩ Legacy Docs](#-quick-start)**\n\n\u003c/div\u003e\n\n\u003e **Deploy Model Context Protocol (MCP) servers in seconds, not hours.**\n\nZero-configuration deployment of production-ready MCP servers with Docker containers, comprehensive CLI tools, and intelligent caching. Focus on AI integration, not infrastructure setup.\n\n---\n\n## üöÄ Quick Start\n\n```bash\n# Install MCP Templates\npip install mcp-templates\n\n# List available templates\nmcpt list\n\n# Deploy instantly\nmcpt deploy demo\n\n# View deployment\nmcpt logs demo\n```\n\n**That's it!** Your MCP server is running at `http://localhost:8080`\n\n---\n\n## ‚ö° Why MCP Templates?\n\n| Traditional MCP Setup | With MCP Templates |\n|----------------------|-------------------|\n| ‚ùå Complex configuration | ‚úÖ One-command deployment |\n| ‚ùå Docker expertise required | ‚úÖ Zero configuration needed |\n| ‚ùå Manual tool discovery | ‚úÖ Automatic detection |\n| ‚ùå Environment setup headaches | ‚úÖ Pre-built containers |\n\n**Perfect for:** AI developers, data scientists, DevOps teams building with MCP.\n\n---\n\n## üåü Key Features\n\n### üñ±Ô∏è **One-Click Deployment**\nDeploy MCP servers instantly with pre-built templates‚Äîno Docker knowledge required.\n\n### üîç **Smart Tool Discovery**\nAutomatically finds and showcases every tool your server offers.\n\n### üß† **Intelligent Caching**\n6-hour template caching with automatic invalidation for lightning-fast operations.\n\n### üíª **Powerful CLI**\nComprehensive command-line interface for deployment, management, and tool execution.\n\n### üõ†Ô∏è **Flexible Configuration**\nConfigure via JSON, YAML, environment variables, CLI options, or override parameters.\n\n### üì¶ **Growing Template Library**\nReady-to-use templates for common use cases: filesystem, databases, APIs, and more.\n\n---\n\n## üìö Installation\n\n### PyPI (Recommended)\n```bash\npip install mcp-templates\n```\n\n### Docker\n```bash\ndocker run --privileged -it dataeverything/mcp-server-templates:latest deploy demo\n```\n\n### From Source\n```bash\ngit clone https://github.com/DataEverything/mcp-server-templates.git\ncd mcp-server-templates\npip install -r requirements.txt\n```\n\n---\n\n## üéØ Common Use Cases\n\n### Deploy with Custom Configuration\n```bash\n# Basic deployment\nmcpt deploy filesystem --config allowed_dirs=\"/path/to/data\"\n\n# Advanced overrides\nmcpt deploy demo --override metadata__version=2.0 --transport http\n```\n\n### Manage Deployments\n```bash\n# List all deployments\nmcpt list --deployed\n\n# Stop a deployment\nmcpt stop demo\n\n# View logs\nmcpt logs demo --follow\n```\n\n### Template Development\n```bash\n# Create new template\nmcpt create my-template\n\n# Test locally\nmcpt deploy my-template --backend mock\n```\n\n---\n\n## üèóÔ∏è Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  CLI Tool   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ DeploymentManager ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Backend (Docker)    ‚îÇ\n‚îÇ  (mcpt)     ‚îÇ    ‚îÇ                   ‚îÇ    ‚îÇ                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ                      ‚îÇ                        ‚îÇ\n       ‚ñº                      ‚ñº                        ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Template    ‚îÇ    ‚îÇ CacheManager      ‚îÇ    ‚îÇ Container Instance  ‚îÇ\n‚îÇ Discovery   ‚îÇ    ‚îÇ (6hr TTL)         ‚îÇ    ‚îÇ                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Configuration Flow:** Template Defaults ‚Üí Config File ‚Üí CLI Options ‚Üí Environment Variables\n\n---\n\n## üì¶ Available Templates\n\n| Template | Description | Transport | Use Case |\n|----------|-------------|-----------|----------|\n| **demo** | Hello world MCP server | HTTP, stdio | Testing \u0026 learning |\n| **filesystem** | Secure file operations | stdio | File management |\n| **gitlab** | GitLab API integration | stdio | CI/CD workflows |\n| **github** | GitHub API integration | stdio | Development workflows |\n| **zendesk** | Customer support tools | HTTP, stdio | Support automation |\n\n[View all templates ‚Üí](https://data-everything.github.io/mcp-server-templates/server-templates/)\n\n---\n\n## üõ†Ô∏è Configuration Examples\n\n### Basic Configuration\n```bash\nmcpt deploy filesystem --config allowed_dirs=\"/home/user/data\"\n```\n\n### Advanced Configuration\n```bash\nmcpt deploy gitlab \\\n  --config gitlab_token=\"$GITLAB_TOKEN\" \\\n  --config read_only_mode=true \\\n  --override metadata__version=1.2.0 \\\n  --transport stdio\n```\n\n### Configuration File\n```json\n{\n  \"allowed_dirs\": \"/home/user/projects\",\n  \"log_level\": \"DEBUG\",\n  \"security\": {\n    \"read_only\": false,\n    \"max_file_size\": \"100MB\"\n  }\n}\n```\n\n```bash\nmcpt deploy filesystem --config-file myconfig.json\n```\n\n---\n\n## üîß Template Development\n\n### Creating Templates\n\n1. **Use the generator**:\n   ```bash\n   mcpt create my-template\n   ```\n\n2. **Define template.json**:\n   ```json\n   {\n     \"name\": \"My Template\",\n     \"description\": \"Custom MCP server\",\n     \"docker_image\": \"my-org/my-mcp-server\",\n     \"transport\": {\n       \"default\": \"stdio\",\n       \"supported\": [\"stdio\", \"http\"]\n     },\n     \"config_schema\": {\n       \"type\": \"object\",\n       \"properties\": {\n         \"api_key\": {\n           \"type\": \"string\",\n           \"env_mapping\": \"API_KEY\",\n           \"sensitive\": true\n         }\n       }\n     }\n   }\n   ```\n\n3. **Test and deploy**:\n   ```bash\n   mcpt deploy my-template --backend mock\n   ```\n\n[Full template development guide ‚Üí](https://data-everything.github.io/mcp-server-templates/templates/creating/)\n\n---\n\n## ÔøΩ Migration to MCP Platform\n\n**This repository has evolved into MCP Platform with enhanced features and better architecture.**\n\n### Why We Moved\n\n1. **Better Naming**: \"MCP Platform\" better reflects the comprehensive nature of the project\n2. **Enhanced Architecture**: Improved codebase structure and performance\n3. **Expanded Features**: More deployment options, better tooling, enhanced templates\n4. **Future Growth**: Better positioned for upcoming MCP ecosystem developments\n\n### What Stays the Same\n\n- ‚úÖ All your existing configurations work unchanged\n- ‚úÖ Same Docker images and templates\n- ‚úÖ Same deployment workflows\n- ‚úÖ Full backward compatibility during transition\n\n### Migration Steps\n\n1. **Install new package:**\n   ```bash\n   pip uninstall mcp-templates\n   pip install mcp-platform\n   ```\n\n2. **Update commands:**\n   ```bash\n   # Old command\n   mcpt deploy demo\n\n   # New command (everything else identical)\n   mcpp deploy demo\n   ```\n\n3. **Update documentation bookmarks:**\n   - New docs: https://data-everything.github.io/MCP-Platform/\n   - New repository: https://github.com/Data-Everything/MCP-Platform\n\n### Support Timeline\n\n- **Current (Legacy) Package**: Security updates only through 2025\n- **New Platform**: Active development, new features, full support\n- **Migration Support**: Available through Discord and GitHub issues\n\n**üöÄ [Start your migration now ‚Üí](https://github.com/Data-Everything/MCP-Platform)**\n\n---\n\n## ÔøΩüìñ Documentation (Legacy)\n\n- **[Getting Started](https://data-everything.github.io/mcp-server-templates/getting-started/)** - Installation and first deployment\n- **[CLI Reference](https://data-everything.github.io/mcp-server-templates/cli/)** - Complete command documentation\n- **[Template Guide](https://data-everything.github.io/mcp-server-templates/templates/)** - Creating and configuring templates\n- **[User Guide](https://data-everything.github.io/mcp-server-templates/user-guide/)** - Advanced usage and best practices\n\n---\n\n## ü§ù Community\n\n- **[Discord Server](https://discord.gg/55Cfxe9gnr)** - Get help and discuss features\n- **[GitHub Issues](https://github.com/DataEverything/mcp-server-templates/issues)** - Report bugs and request features\n- **[Discussions](https://github.com/DataEverything/mcp-server-templates/discussions)** - Share templates and use cases\n\n---\n\n## üìù License\n\nThis project is licensed under the [Elastic License 2.0](LICENSE).\n\n---\n\n## üôè Acknowledgments\n\nBuilt with ‚ù§Ô∏è for the MCP community. Thanks to all contributors and template creators!\n",
      "stars": 7,
      "updated_at": "2025-09-30T17:49:33Z",
      "url": "https://github.com/Data-Everything/mcp-server-templates"
    },
    "PipedreamHQ--pipedream": {
      "category": "aggregators",
      "description": "A developer-centric platform enabling connectivity to over 2,500 external services via 8,000+ pre-built components, while offering infrastructure for embedding server management within your proprietary applications.",
      "forks": 5504,
      "imageUrl": "",
      "keywords": [
        "pipedreamhq",
        "pipedream",
        "aggregators",
        "server pipedreamhq",
        "pipedream connect",
        "pipedreamhq pipedream"
      ],
      "language": "JavaScript",
      "license": "Other",
      "name": "Pipedream Integration Fabric",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "PipedreamHQ",
      "readme_content": "![pipedream](https://i.ibb.co/LPhXtH1/logo.png)\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://pipedream.com/community\"\u003e\u003cimg alt=\"community\" src=\"https://img.shields.io/badge/discourse-forum-brightgreen.svg?style=flat-square\u0026link=https%3A%2F%2Fpipedream.com%2Fcommunity\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://pipedream.com/support\"\u003e\u003cimg alt=\"support\" src=\"https://img.shields.io/badge/-Join%20us%20on%20Slack-green?logo=slack\u0026logoColor=34d28B\u0026labelColor=150d11\u0026color=34d28B\u0026logoWidth=18\u0026link=https%3A%2F%2Fpipedream.com%2Fsupport\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fpublish.twitter.com%2F%3FbuttonType%3DFollowButton%26query%3Dhttps%253A%252F%252Ftwitter.com%252Fpipedream%26widget%3DButton\u0026ref_src=twsrc%5Etfw\u0026region=follow_link\u0026screen_name=pipedream\u0026tw_p=followbutton\"\u003e\u003cimg alt=\"pipedream_label_Follow_pipedream_style_social\" src=\"https://img.shields.io/twitter/follow/pipedream?label=Follow%20%40pipedream\u0026style=social\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://wellfound.com/company/pipedreamhq/jobs\"\u003e\u003cimg alt=\"We_re_hiring_Join_us_brightgreen\" src=\"https://img.shields.io/badge/%F0%9F%91%8B%F0%9F%8F%BC%20We're%20hiring!-Join%20us-brightgreen\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\nPipedream serves as a robust integration platform tailored specifically for software developers.\n\nWe offer a complimentary, managed environment designed for architecting event-driven automation and linking disparate applications. This ecosystem boasts more than 1,000 pre-configured application connectors, enabling rapid deployment of functionalities such as dispatching notifications to Slack or inserting records into Google Sheets using ready-made modules. Furthermore, users retain the flexibility to execute arbitrary code written in Node.js, Python, Golang, or Bash when bespoke logic is required. Pipedream maintains SOC 2 compliance, and the Type 2 report is obtainable upon formalized request via support@pipedream.com.\n\n\u003cp align=\"center\"\u003e\n  \u003cbr /\u003e\n  \n  \u003cbr /\u003e\n\u003c/p\u003e\n\nThis repository furnishes:\n\n- [The source code for all readily available integration components](https://github.com/PipedreamHQ/pipedream/tree/master/components)\n- [The official product development roadmap](https://github.com/PipedreamHQ/pipedream/issues)\n- [The comprehensive Pipedream documentation suite](https://github.com/PipedreamHQ/pipedream/tree/master/docs)\n- Various other source artifacts pertaining to the Pipedream service.\n\nThis `README` provides an overview of the platform's core capabilities and guides for initial setup.\n\nFor technical assistance, kindly navigate to [https://pipedream.com/support](https://pipedream.com/support).\n\n## Core Capabilities\n\n- [Workflows](#workflows) - These define automation sequences. A workflow consists of discrete steps‚Äîeither leveraging pre-built actions or executing custom code in [Node.js](https://pipedream.com/docs/code/nodejs/), [Python](https://pipedream.com/docs/code/python/), [Golang](https://pipedream.com/docs/code/go/), or [Bash](https://pipedream.com/docs/code/bash/)‚Äîall initiated by a triggering event (e.g., an HTTP invocation, a scheduled timer, or a new entry in a connected service like Google Sheets).\n- [Event Sources](#event-sources) - These components initiate workflows by emitting data events originating from external services such as GitHub, Slack, Airtable, RSS feeds, and [many others](https://pipedream.com/apps). If a third-party service generates data you wish to act upon, an event source is employed.\n- [Actions](#actions) - These are encapsulated, pre-written code units designed for executing conventional operations across Pipedream's extensive library of over 1,000 API integrations. Examples include dispatching electronic mail or programmatically updating rows within Google Sheets.\n- [Custom Code Execution](#code) - When intricate logic is mandated, custom scripting provides the optimal solution. Pipedream facilitates the execution of virtually any [Node.js](https://pipedream.com/docs/code/nodejs/), [Python](https://pipedream.com/docs/code/python/), [Golang](https://pipedream.com/docs/code/go/), or [Bash](https://pipedream.com/docs/code/bash/) code within a workflow. You can readily import external packages via respective language package managers, interact with any Pipedream-integrated application, and more. Pipedream embodies \"low-code\" principles advantageously: routine tasks utilize standardized modules, while complex requirements are addressed via expressive custom scripting.\n- [Destinations](#destinations) - These abstract the complexities of asynchronous data transmission, facilitating the routing of events to common endpoints such as Amazon S3 storage, Snowflake data warehousing, arbitrary HTTP endpoints, or email.\n- [Pricing Model](#pricing) - The platform features a highly accommodating tier for individual developers (refer to [limits](https://docs.pipedream.com/limits/) for specifics), ensuring no initial expenditure.\n\n## Demonstration\n\nAccess a concise video demonstration via YouTube by clicking the image below.\n\n\u003cp align=\"center\"\u003e\n  \u003cbr /\u003e\n  \u003ca href=\"https://bit.ly/3ytGgyR\"\u003e\n    \n  \u003c/a\u003e\n\u003c/p\u003e\n\n### Workflows in Detail\n\nWorkflows represent a series of sequential operations, or [steps](https://pipedream.com/docs/workflows/steps), triggered by an initiating event (e.g., an incoming HTTP payload or a new record in a spreadsheet). Users can rapidly engineer sophisticated automation sequences by assembling workflows and integrating with the 1,000+ supported applications.\n\nReview our [workflow quickstart guide](https://pipedream.com/docs/quickstart/) to begin building.\n\n### Event Sources Explained\n\n[Event Sources](https://pipedream.com/docs/sources/) actively monitor services such as GitHub, Slack, Airtable, or RSS feeds for novel data occurrences. Upon detection of a relevant event, the source emits a signal that activates any associated workflows.\n\nYou also possess the capability to ingest events emitted by sources directly through [Pipedream's REST API](https://pipedream.com/docs/api/rest/) or a dedicated, low-latency [SSE stream](https://pipedream.com/docs/api/sse/).\n\nShould a suitable pre-built source not exist, you are empowered to [construct your own](https://pipedream.com/docs/components/quickstart/nodejs/sources/). The simplest custom event source exposes an HTTP endpoint designed to accept any incoming request and log its contents upon execution:\n\njavascript\nexport default {\n  name: \"http\",\n  version: \"0.0.1\",\n  props: {\n    http: \"$.interface.http\",\n  },\n  run(event) {\n    console.log(event); // event contains the method, payload, etc.\n  },\n};\n\n\n\u003ca href=\"https://pipedream.com/sources/new?app=http\"\u003e\u003cimg alt=\"deploy_clean\" src=\"https://i.ibb.co/m0bBsSL/deploy-clean.png\" height=\"35\"\u003e\u003c/a\u003e\n\nThe codebase for all standard event sources resides within [the `components` directory](https://github.com/PipedreamHQ/pipedream/tree/master/components). For defect reporting or feature contribution, consult [our contribution guidelines](https://github.com/PipedreamHQ/pipedream/blob/master/docs/components/guidelines.md#process).\n\n### Actions Detail\n\n[Actions](https://pipedream.com/docs/components/actions/) function as modular, pre-written execution steps within a workflow, facilitating standard operations across Pipedream's expansive catalog of over 500 integrated APIs. For instance, they can be used to dispatch emails or append data to a Google Sheet, [among other capabilities](https://pipedream.com/apps).\n\nUsers can [author custom actions](https://pipedream.com/docs/components/quickstart/nodejs/actions/) for internal reuse across multiple workflows, or elect to [publish them to the wider Pipedream community](https://pipedream.com/docs/components/guidelines/) for collective benefit.\n\nThis example action accepts a 'name' input and outputs it to the workflow execution log:\n\njavascript\nexport default {\n  name: \"Action Demo\",\n  description: \"This is a demo action\",\n  key: \"action_demo\",\n  version: \"0.0.1\",\n  type: \"action\",\n  props: {\n    name: {\n      type: \"string\",\n      label: \"Name\",\n    },\n  },\n  async run() {\n    return `hello ${this.name}!`;\n  },\n};\n\n\nAll official action code is located in [the `components` directory](https://github.com/PipedreamHQ/pipedream/tree/master/components). Refer to [our contribution process documentation](https://github.com/PipedreamHQ/pipedream/blob/master/docs/components/guidelines.md#process) for reporting issues or submitting enhancements.\n\n### Extensibility via Custom Code\n\nComplex integration scenarios frequently necessitate bespoke logic. Pipedream supports embedding custom code execution directly into workflows using the following runtimes:\n\n\u003ctable align=\"center\"\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\n      \u003ca href=\"https://pipedream.com/docs/code/nodejs/\"\u003e\n        \u003cimg alt=\"Node.js\" src=\"https://res.cloudinary.com/pipedreamin/image/upload/v1646761316/docs/icons/icons8-nodejs_aax6wn.svg\" width=\"100\"\u003e\n      \u003c/a\u003e\n    \u003c/td\u003e\n    \u003ctd\u003e\n      \u003ca href=\"https://pipedream.com/docs/code/python/\"\u003e\n        \u003cimg alt=\"Python\" src=\"https://res.cloudinary.com/pipedreamin/image/upload/v1647356607/docs/icons/python-logo-generic_k3o5w2.svg\" width=\"100\"\u003e\n      \u003c/a\u003e\n    \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tr\u003e\n    \u003ctd\u003e\n      \u003ca href=\"https://pipedream.com/docs/code/go/\"\u003e\n        \u003cimg alt=\"Go\" src=\"https://res.cloudinary.com/pipedreamin/image/upload/v1646763751/docs/icons/Go-Logo_Blue_zhkchv.svg\" width=\"100\"\u003e\n      \u003c/a\u003e\n    \u003c/td\u003e\n    \u003ctd\u003e\n      \u003ca href=\"https://pipedream.com/docs/code/bash/\"\u003e\n        \u003cimg alt=\"Bash\" src=\"https://res.cloudinary.com/pipedreamin/image/upload/v1647356698/docs/icons/full_colored_dark_1_-svg_vyfnv7.svg\" width=\"100\"\u003e\n      \u003c/a\u003e\n    \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nYou can pull in any dependency from the language's standard package manager simply by declaring the import statements directly within your script; Pipedream handles the dependency resolution and downloading.\n\njavascript\n// Node.js\nimport axios from \"axios\";\n\n\npython\n# Python\nimport pandas as pd\n\n\ngolang\n// Go\nimport (\n    \"fmt\"\n    pd \"github.com/PipedreamHQ/pipedream-go\"\n)\n\n\nFurthermore, custom code steps permit seamless interaction with any configured Pipedream application credentials. For example, authenticating with Slack to post a message:\n\njavascript\nimport { WebClient } from \"@slack/web-api\";\n\nexport default defineComponent({\n  props: {\n    // Establishes an app connection reference named \"slack\".\n    slack: {\n      type: \"app\",\n      app: \"slack\",\n    },\n  },\n  async run({ steps, $ }) {\n    const web = new WebClient(this.slack.$auth.oauth_access_token);\n\n    return await web.chat.postMessage({\n      text: \"Hello, world!\",\n      channel: \"#general\",\n    });\n  },\n});\n\n\n### Data Delivery Destinations\n\n[Destinations](https://pipedream.com/docs/destinations/), analogous to Actions, abstract away the complex logic associated with connection management, data batching, and reliable transmission to external systems like Amazon S3 or custom HTTP endpoints.\n\nFor instance, sending data securely to an Amazon S3 bucket is achieved via a straightforward function call, `$send.s3()`:\n\njavascript\n$send.s3({\n  bucket: \"your-bucket-here\",\n  prefix: \"your-prefix/\",\n  payload: event.body,\n});\n\n\nPipedream currently furnishes integrations for the following delivery targets:\n\n- [Amazon S3](https://docs.pipedream.com/destinations/s3/)\n- [Snowflake](https://docs.pipedream.com/destinations/snowflake/)\n- [HTTP](https://docs.pipedream.com/destinations/http/)\n- [Email](https://docs.pipedream.com/destinations/email/)\n- [SSE](https://docs.pipedream.com/destinations/sse/)\n\n## Contributors\n\nOur sincere gratitude to every individual who has enhanced the Pipedream source repository. Your contributions are highly valued!\n\n\u003ca href=\"https://github.com/PipedreamHQ/pipedream/graphs/contributors\"\u003e\n  \u003cimg alt=\"pipedream\" src=\"https://contrib.rocks/image?repo=PipedreamHQ/pipedream\" /\u003e\n\u003c/a\u003e\n\n## Financial Structure\n\nPipedream offers a [robust complimentary tier](https://pipedream.com/docs/pricing/#developer-tier). Users can execute workflows and sources without charge, provided they remain within the specified free-tier operational constraints. Should these limits be exceeded, users may transition to one of our [subscription-based tiers](https://pipedream.com/docs/pricing/).\n\n## Operational Constraints\n\nThe Pipedream execution environment imposes certain runtime limitations on both sources and workflows. Detailed information regarding these constraints is available in [our official documentation](https://pipedream.com/docs/limits/).\n\n## Reporting Defects or Suggesting Enhancements\n\nBefore submitting a new issue, please thoroughly search [the existing issue tracker](https://github.com/PipedreamHQ/pipedream/issues) or [contact our support team](https://pipedream.com/support/) to confirm if a similar request has already been documented.\n\nIf a matching issue is found, we request that you either [add a supportive reaction](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-conversations-on-github) or post a comment elaborating on your unique scenario.\n\nIf no prior record exists, please utilize [the defined issue templates](https://github.com/PipedreamHQ/pipedream/issues/new/choose) when filing a new report.\n\n## Security Posture\n\nInformation regarding our platform's security protocols and data privacy standards can be accessed [via this document](https://pipedream.com/docs/privacy-and-security/).\n\nTo confidentially disclose a potential vulnerability or security concern, or for any security-related inquiries, please communicate directly with our dedicated security team at **security@pipedream.com**.",
      "stars": 10652,
      "updated_at": "2025-10-04T11:29:59Z",
      "url": "https://github.com/PipedreamHQ/pipedream/tree/master/modelcontextprotocol"
    },
    "SureScaleAI--openai-gpt-image-mcp": {
      "category": "aggregators",
      "description": "OpenAI GPT image generation/editing MCP server.",
      "forks": 23,
      "imageUrl": "",
      "keywords": [
        "openai",
        "mcp",
        "gpt",
        "openai gpt",
        "mcp openai",
        "mcp server"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "openai-gpt-image-mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "SureScaleAI",
      "readme_content": "# openai-gpt-image-mcp\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://www.npmjs.com/package/@modelcontextprotocol/sdk\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/@modelcontextprotocol/sdk?label=MCP%20SDK\u0026color=blue\" alt=\"MCP SDK\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://www.npmjs.com/package/openai\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/openai?label=OpenAI%20SDK\u0026color=blueviolet\" alt=\"OpenAI SDK\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/blob/main/LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/github/license/SureScaleAI/openai-gpt-image-mcp?color=brightgreen\" alt=\"License\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/stargazers\"\u003e\u003cimg src=\"https://img.shields.io/github/stars/SureScaleAI/openai-gpt-image-mcp?style=social\" alt=\"GitHub stars\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/SureScaleAI/openai-gpt-image-mcp/actions\"\u003e\u003cimg src=\"https://img.shields.io/github/actions/workflow/status/SureScaleAI/openai-gpt-image-mcp/main.yml?label=build\u0026logo=github\" alt=\"Build Status\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n---\n\nA Model Context Protocol (MCP) tool server for OpenAI's GPT-4o/gpt-image-1 image generation and editing APIs.\n\n- **Generate images** from text prompts using OpenAI's latest models.\n- **Edit images** (inpainting, outpainting, compositing) with advanced prompt control.\n- **Supports**: Claude Desktop, Cursor, VSCode, Windsurf, and any MCP-compatible client.\n\n---\n\n## ‚ú® Features\n\n- **create-image**: Generate images from a prompt, with advanced options (size, quality, background, etc).\n- **edit-image**: Edit or extend images using a prompt and optional mask, supporting both file paths and base64 input.\n- **File output**: Save generated images directly to disk, or receive as base64.\n\n---\n\n## üöÄ Installation\n\n```sh\ngit clone https://github.com/SureScaleAI/openai-gpt-image-mcp.git\ncd openai-gpt-image-mcp\nyarn install\nyarn build\n```\n\n---\n\n## üîë Configuration\n\nAdd to Claude Desktop or VSCode (including Cursor/Windsurf) config:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\"],\n      \"env\": { \"OPENAI_API_KEY\": \"sk-...\" }\n    }\n  }\n}\n```\n\nAlso supports Azure deployments:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\"],\n      \"env\": { \n        \"AZURE_OPENAI_API_KEY\": \"sk-...\",\n        \"AZURE_OPENAI_ENDPOINT\": \"my.endpoint.com\",\n        \"OPENAI_API_VERSION\": \"2024-12-01-preview\"\n      }\n    }\n  }\n}\n```\n\nAlso supports supplying an environment files:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-gpt-image-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\", \"--env-file\", \"./deployment/.env\"]\n    }\n  }\n}\n```\n\n---\n\n## ‚ö° Advanced\n\n- For `create-image`, set `n` to generate up to 10 images at once.\n- For `edit-image`, provide a mask image (file path or base64) to control where edits are applied.\n- Provide an environment file with `--env-file path/to/file/.env`\n- See `src/index.ts` for all options.\n\n---\n\n## üßë‚Äçüíª Development\n\n- TypeScript source: `src/index.ts`\n- Build: `yarn build`\n- Run: `node dist/index.js`\n\n---\n\n## üìù License\n\nMIT\n\n---\n\n## ü©∫ Troubleshooting\n\n- Make sure your `OPENAI_API_KEY` is valid and has image API access.\n- You must have a [verified OpenAI organization](https://platform.openai.com/account/organization). After verifying, it can take 15‚Äì20 minutes for image API access to activate.\n- File paths must be absolute.\n  - **Unix/macOS/Linux**: Starting with `/` (e.g., `/path/to/image.png`)\n  - **Windows**: Drive letter followed by `:` (e.g., `C:/path/to/image.png` or `C:\\path\\to\\image.png`)\n- For file output, ensure the directory is writable.\n- If you see errors about file types, check your image file extensions and formats.\n\n---\n\n## ‚ö†Ô∏è Limitations \u0026 Large File Handling\n\n- **1MB Payload Limit:** MCP clients (including Claude Desktop) have a hard 1MB limit for tool responses. Large images (especially high-res or multiple images) can easily exceed this limit if returned as base64.\n- **Auto-Switch to File Output:** If the total image size exceeds 1MB, the tool will automatically save images to disk and return the file path(s) instead of base64. This ensures compatibility and prevents errors like `result exceeds maximum length of 1048576`.\n- **Default File Location:** If you do not specify a `file_output` path, images will be saved to `/tmp` (or the directory set by the `MCP_HF_WORK_DIR` environment variable) with a unique filename.\n- **Environment Variable:**\n  - `MCP_HF_WORK_DIR`: Set this to control where large images and file outputs are saved. Example: `export MCP_HF_WORK_DIR=/your/desired/dir`\n- **Best Practice:** For large or production images, always use file output and ensure your client is configured to handle file paths.\n\n---\n\n## üìö References\n\n- [OpenAI Images API Documentation](https://platform.openai.com/docs/api-reference/images)\n\n---\n\n## üôè Credits\n\n- Built with [@modelcontextprotocol/sdk](https://www.npmjs.com/package/@modelcontextprotocol/sdk)\n- Uses [openai](https://www.npmjs.com/package/openai) Node.js SDK \n- Built by [SureScale.ai](https://surescale.ai)\n- Contributions from [Axle Research and Technology](https://axleinfo.com/)",
      "stars": 74,
      "updated_at": "2025-10-03T22:32:34Z",
      "url": "https://github.com/SureScaleAI/openai-gpt-image-mcp"
    },
    "TheLunarCompany--lunar": {
      "category": "aggregators",
      "description": "MCPX is a production-ready, open-source gateway to manage MCP servers at scale‚Äîcentralize tool discovery, access controls, call prioritization, and usage tracking to simplify agent workflows.",
      "forks": 19,
      "imageUrl": "",
      "keywords": [
        "mcpx",
        "mcp",
        "aggregators",
        "mcp servers",
        "mcp server",
        "manage mcp"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "lunar",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "TheLunarCompany",
      "readme_content": "",
      "stars": 297,
      "updated_at": "2025-10-04T07:29:33Z",
      "url": "https://github.com/TheLunarCompany/lunar/tree/main/mcpx"
    },
    "VeriTeknik--pluggedin-mcp-proxy": {
      "category": "aggregators",
      "description": "A comprehensive proxy server that combines multiple MCP servers into a single interface with extensive visibility features. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.",
      "forks": 18,
      "imageUrl": "",
      "keywords": [
        "proxy",
        "aggregators",
        "servers",
        "mcp proxy",
        "comprehensive proxy",
        "mcp servers"
      ],
      "language": "TypeScript",
      "license": "Apache License 2.0",
      "name": "pluggedin-mcp-proxy",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "VeriTeknik",
      "readme_content": "# plugged.in MCP Proxy Server\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://plugged.in/_next/image?url=%2Fpluggedin-wl.png\u0026w=256\u0026q=75\" alt=\"plugged.in Logo\" width=\"256\" height=\"75\"\u003e\n  \u003ch3\u003eThe Crossroads for AI Data Exchanges\u003c/h3\u003e\n  \u003cp\u003eA unified interface for managing all your MCP servers with built-in playground for testing on any AI model\u003c/p\u003e\n\n  [![Version](https://img.shields.io/badge/version-1.9.0-blue?style=for-the-badge)](https://github.com/VeriTeknik/pluggedin-mcp/releases)\n  [![GitHub Stars](https://img.shields.io/github/stars/VeriTeknik/pluggedin-mcp?style=for-the-badge)](https://github.com/VeriTeknik/pluggedin-mcp/stargazers)\n  [![License](https://img.shields.io/github/license/VeriTeknik/pluggedin-mcp?style=for-the-badge)](LICENSE)\n  [![TypeScript](https://img.shields.io/badge/TypeScript-4.9+-blue?style=for-the-badge\u0026logo=typescript)](https://www.typescriptlang.org/)\n  [![MCP](https://img.shields.io/badge/MCP-Compatible-green?style=for-the-badge)](https://modelcontextprotocol.io/)\n\u003c/div\u003e\n\n## üìã Overview\n\nThe plugged.in MCP Proxy Server is a powerful middleware that aggregates multiple Model Context Protocol (MCP) servers into a single unified interface. It fetches tool, prompt, and resource configurations from the [plugged.in App](https://github.com/VeriTeknik/pluggedin-app) and intelligently routes requests to the appropriate underlying MCP servers.\n\nThis proxy enables seamless integration with any MCP client (Claude, Cline, Cursor, etc.) while providing advanced management capabilities through the plugged.in ecosystem.\n\n\u003e ‚≠ê **If you find this project useful, please consider giving it a star on GitHub!** It helps us reach more developers and motivates us to keep improving.\n\n## ‚ú® Key Features\n\n### üöÄ Core Capabilities\n- **Built-in AI Playground**: Test your MCPs instantly with Claude, Gemini, OpenAI, and xAI without any client setup\n- **Universal MCP Compatibility**: Works with any MCP client including Claude Desktop, Cline, and Cursor\n- **Multi-Server Support**: Connect to STDIO, SSE, and Streamable HTTP MCP servers\n- **Dual Transport Modes**: Run proxy as STDIO (default) or Streamable HTTP server\n- **Unified Document Search**: Search across all connected servers with built-in RAG capabilities\n- **AI Document Exchange (RAG v2)**: MCP servers can create and manage documents in your library with full attribution\n- **Notifications from Any Model**: Receive real-time notifications with optional email delivery\n- **Multi-Workspace Layer**: Switch between different sets of MCP configurations with one click\n- **API-Driven Proxy**: Fetches capabilities from plugged.in App APIs rather than direct discovery\n- **Full MCP Support**: Handles tools, resources, resource templates, and prompts\n- **Custom Instructions**: Supports server-specific instructions formatted as MCP prompts\n\n### üéØ New in v1.5.0 (RAG v2 - AI Document Exchange)\n\n- **AI Document Creation**: MCP servers can now create documents directly in your library\n  - Full model attribution tracking (which AI created/updated the document)\n  - Version history with change tracking\n  - Content deduplication via SHA-256 hashing\n  - Support for multiple formats: MD, TXT, JSON, HTML, PDF, and more\n- **Advanced Document Search**: Enhanced RAG queries with AI filtering\n  - Filter by AI model, provider, date range, tags, and source type\n  - Semantic search with relevance scoring\n  - Automatic snippet generation with keyword highlighting\n  - Support for filtering: `ai_generated`, `upload`, or `api` sources\n- **Document Management via MCP**: \n  - Set document visibility: private, workspace, or public\n  - Parent-child relationships for document versions\n  - Profile-based organization alongside project-based scoping\n  - Real-time progress tracking for document processing\n\n### üéØ Features from v1.4.0 (Registry v2 Support)\n\n- **OAuth Token Management**: Seamless OAuth authentication handling for Streamable HTTP MCP servers\n  - Automatic token retrieval from plugged.in App\n  - Secure token storage and refresh mechanisms\n  - No client-side authentication needed\n- **Enhanced Notification System**: Bidirectional notification support\n  - Send notifications to plugged.in App\n  - Receive notifications from MCP servers\n  - Mark notifications as read/unread\n  - Delete notifications programmatically\n- **Trending Analytics**: Real-time activity tracking\n  - Every tool call is logged and tracked\n  - Contributes to trending server calculations\n  - Usage metrics and popularity insights\n- **Registry Integration**: Full support for Registry v2 features\n  - Automatic server discovery from registry\n  - Installation tracking and metrics\n  - Community server support\n\n### üì¶ Features from v1.1.0\n\n- **Streamable HTTP Support**: Full support for downstream MCP servers using Streamable HTTP transport\n- **HTTP Server Mode**: Run the proxy as an HTTP server with configurable ports\n- **Flexible Authentication**: Optional Bearer token authentication for HTTP endpoints\n- **Session Management**: Choose between stateful (session-based) or stateless operation modes\n\n### üéØ Core Features from v1.0.0\n\n- **Real-Time Notifications**: Track all MCP activities with comprehensive notification support\n- **RAG Integration**: Support for document-enhanced queries through the plugged.in App\n- **Inspector Scripts**: Automated testing tools for debugging and development\n- **Health Monitoring**: Built-in ping endpoint for connection monitoring\n\n## üîß Tool Categories\n\nThe proxy provides two distinct categories of tools:\n\n### üîß Static Built-in Tools (Always Available)\nThese tools are built into the proxy and work without any server configuration:\n- **`pluggedin_discover_tools`** - Smart discovery with caching for instant results\n- **`pluggedin_rag_query`** - RAG v2 search across your documents with AI filtering capabilities\n- **`pluggedin_send_notification`** - Send notifications with optional email delivery\n- **`pluggedin_create_document`** - (Coming Soon) Create AI-generated documents in your library\n\n### ‚ö° Dynamic MCP Tools (From Connected Servers)\nThese tools come from your configured MCP servers and can be turned on/off:\n- Database tools (PostgreSQL, SQLite, etc.)\n- File system tools\n- API integration tools\n- Custom tools from any MCP server\n\nThe discovery tool intelligently shows both categories, giving AI models immediate access to all available capabilities.\n\n### üöÄ Discovery Tool Usage\n\n```bash\n# Quick discovery - returns cached data instantly\npluggedin_discover_tools()\n\n# Force refresh - shows current tools + runs background discovery  \npluggedin_discover_tools({\"force_refresh\": true})\n\n# Discover specific server\npluggedin_discover_tools({\"server_uuid\": \"uuid-here\"})\n```\n\n**Example Response:**\n```\n## üîß Static Built-in Tools (Always Available):\n1. **pluggedin_discover_tools** - Smart discovery with caching\n2. **pluggedin_rag_query** - RAG v2 search across documents with AI filtering  \n3. **pluggedin_send_notification** - Send notifications\n4. **pluggedin_create_document** - (Coming Soon) Create AI-generated documents\n\n## ‚ö° Dynamic MCP Tools (8) - From Connected Servers:\n1. **query** - Run read-only SQL queries\n2. **generate_random_integer** - Generate secure random integers\n...\n```\n\n### üìö RAG v2 Usage Examples\n\nThe enhanced RAG v2 system allows MCP servers to create and search documents with full AI attribution:\n\n```bash\n# Search for documents created by specific AI models\npluggedin_rag_query({\n  \"query\": \"system architecture\",\n  \"filters\": {\n    \"modelName\": \"Claude 3 Opus\",\n    \"source\": \"ai_generated\",\n    \"tags\": [\"technical\"]\n  }\n})\n\n# Search across all document sources\npluggedin_rag_query({\n  \"query\": \"deployment guide\",\n  \"filters\": {\n    \"dateFrom\": \"2024-01-01\",\n    \"visibility\": \"workspace\"\n  }\n})\n\n# Future: Create AI-generated documents (Coming Soon)\npluggedin_create_document({\n  \"title\": \"Analysis Report\",\n  \"content\": \"# Market Analysis\\n\\nDetailed findings...\",\n  \"format\": \"md\",\n  \"tags\": [\"analysis\", \"market\"],\n  \"metadata\": {\n    \"model\": {\n      \"name\": \"Claude 3 Opus\",\n      \"provider\": \"Anthropic\"\n    }\n  }\n})\n```\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Node.js 18+ (recommended v20+)\n- An API key from the plugged.in App (get one at [plugged.in/api-keys](https://plugged.in/api-keys))\n\n### Installation\n\n```bash\n# Install and run with npx (latest v1.0.0)\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --pluggedin-api-key YOUR_API_KEY\n```\n\n### üîÑ Upgrading to v1.0.0\n\nFor existing installations, see our [Migration Guide](./MIGRATION_GUIDE_v1.0.0.md) for detailed upgrade instructions.\n\n```bash\n# Quick upgrade\nnpx -y @pluggedin/pluggedin-mcp-proxy@1.0.0 --pluggedin-api-key YOUR_API_KEY\n```\n\n### Configuration for MCP Clients\n\n#### Claude Desktop\n\nAdd the following to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pluggedin\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@pluggedin/pluggedin-mcp-proxy@latest\"],\n      \"env\": {\n        \"PLUGGEDIN_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cline\n\nAdd the following to your Cline configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pluggedin\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@pluggedin/pluggedin-mcp-proxy@latest\"],\n      \"env\": {\n        \"PLUGGEDIN_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nFor Cursor, you can use command-line arguments instead of environment variables:\n\n```bash\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --pluggedin-api-key YOUR_API_KEY\n```\n\n## ‚öôÔ∏è Configuration Options\n\n### Environment Variables\n\n| Variable | Description | Required | Default |\n|----------|-------------|----------|---------|\n| `PLUGGEDIN_API_KEY` | API key from plugged.in App | Yes | - |\n| `PLUGGEDIN_API_BASE_URL` | Base URL for plugged.in App | No | `https://plugged.in` |\n\n### Command Line Arguments\n\nCommand line arguments take precedence over environment variables:\n\n```bash\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --pluggedin-api-key YOUR_API_KEY --pluggedin-api-base-url https://your-custom-url.com\n```\n\n#### Transport Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--transport \u003ctype\u003e` | Transport type: `stdio` or `streamable-http` | `stdio` |\n| `--port \u003cnumber\u003e` | Port for Streamable HTTP server | `12006` |\n| `--stateless` | Enable stateless mode for Streamable HTTP | `false` |\n| `--require-api-auth` | Require API key for Streamable HTTP requests | `false` |\n\nFor a complete list of options:\n\n```bash\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --help\n```\n\n## üåê Streamable HTTP Mode\n\nThe proxy can run as an HTTP server instead of STDIO, enabling web-based access and remote connections.\n\n### Basic Usage\n\n```bash\n# Run as HTTP server on default port (12006)\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --transport streamable-http --pluggedin-api-key YOUR_API_KEY\n\n# Custom port\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --transport streamable-http --port 8080 --pluggedin-api-key YOUR_API_KEY\n\n# With authentication required\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --transport streamable-http --require-api-auth --pluggedin-api-key YOUR_API_KEY\n\n# Stateless mode (new session per request)\nnpx -y @pluggedin/pluggedin-mcp-proxy@latest --transport streamable-http --stateless --pluggedin-api-key YOUR_API_KEY\n```\n\n### HTTP Endpoints\n\n- `POST /mcp` - Send MCP messages\n- `GET /mcp` - Server-sent events stream (optional)\n- `DELETE /mcp` - Terminate session\n- `GET /health` - Health check endpoint\n\n### Session Management\n\nIn stateful mode (default), use the `mcp-session-id` header to maintain sessions:\n\n```bash\n# First request creates a session\ncurl -X POST http://localhost:12006/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}'\n\n# Subsequent requests use the same session\ncurl -X POST http://localhost:12006/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"mcp-session-id: YOUR_SESSION_ID\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{\"name\":\"tool_name\"},\"id\":2}'\n```\n\n### Authentication\n\nWhen using `--require-api-auth`, include your API key as a Bearer token:\n\n```bash\ncurl -X POST http://localhost:12006/mcp \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"ping\",\"id\":1}'\n```\n\n## üê≥ Docker Usage\n\nYou can also build and run the proxy server using Docker.\n\n### Building the Image\n\nEnsure you have Docker installed and running. Navigate to the `pluggedin-mcp` directory and run:\n\n```bash\ndocker build -t pluggedin-mcp-proxy:latest .\n```\n\nA `.dockerignore` file is included to optimize the build context.\n\n### Running the Container\n\n#### STDIO Mode (Default)\n\nRun the container in STDIO mode for MCP Inspector testing:\n\n```bash\ndocker run -it --rm \\\n  -e PLUGGEDIN_API_KEY=\"YOUR_API_KEY\" \\\n  -e PLUGGEDIN_API_BASE_URL=\"YOUR_API_BASE_URL\" \\\n  --name pluggedin-mcp-container \\\n  pluggedin-mcp-proxy:latest\n```\n\n#### Streamable HTTP Mode\n\nRun the container as an HTTP server:\n\n```bash\ndocker run -d --rm \\\n  -e PLUGGEDIN_API_KEY=\"YOUR_API_KEY\" \\\n  -e PLUGGEDIN_API_BASE_URL=\"YOUR_API_BASE_URL\" \\\n  -p 12006:12006 \\\n  --name pluggedin-mcp-http \\\n  pluggedin-mcp-proxy:latest \\\n  --transport streamable-http --port 12006\n```\n\nReplace `YOUR_API_KEY` and `YOUR_API_BASE_URL` (if not using the default `https://plugged.in`).\n\n### Testing with MCP Inspector\n\nWhile the container is running, you can connect to it using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector docker://pluggedin-mcp-container\n```\n\nThis will connect to the standard input/output of the running container.\n\n### Stopping the Container\n\nPress `Ctrl+C` in the terminal where `docker run` is executing. The `--rm` flag ensures the container is removed automatically upon stopping.\n\n## üèóÔ∏è System Architecture\n\nThe plugged.in MCP Proxy Server acts as a bridge between MCP clients and multiple underlying MCP servers:\n\n```mermaid\nsequenceDiagram\n    participant MCPClient as MCP Client (e.g. Claude Desktop)\n    participant PluggedinMCP as plugged.in MCP Proxy\n    participant PluggedinApp as plugged.in App\n    participant MCPServers as Underlying MCP Servers\n\n    MCPClient -\u003e\u003e PluggedinMCP: Request list tools/resources/prompts\n    PluggedinMCP -\u003e\u003e PluggedinApp: Get capabilities via API\n    PluggedinApp -\u003e\u003e PluggedinMCP: Return capabilities (prefixed)\n\n    MCPClient -\u003e\u003e PluggedinMCP: Call tool/read resource/get prompt\n    alt Standard capability\n        PluggedinMCP -\u003e\u003e PluggedinApp: Resolve capability to server\n        PluggedinApp -\u003e\u003e PluggedinMCP: Return server details\n        PluggedinMCP -\u003e\u003e MCPServers: Forward request to target server\n        MCPServers -\u003e\u003e PluggedinMCP: Return response\n    else Custom instruction\n        PluggedinMCP -\u003e\u003e PluggedinApp: Get custom instruction\n        PluggedinApp -\u003e\u003e PluggedinMCP: Return formatted messages\n    end\n    PluggedinMCP -\u003e\u003e MCPClient: Return response\n\n    alt Discovery tool (Smart Caching)\n        MCPClient -\u003e\u003e PluggedinMCP: Call pluggedin_discover_tools\n        alt Cached data available\n            PluggedinMCP -\u003e\u003e PluggedinApp: Check cached capabilities\n            PluggedinApp -\u003e\u003e PluggedinMCP: Return cached tools/resources/prompts\n            PluggedinMCP -\u003e\u003e MCPClient: Return instant results (static + dynamic)\n        else Force refresh or no cache\n            PluggedinMCP -\u003e\u003e PluggedinApp: Trigger background discovery\n            PluggedinMCP -\u003e\u003e MCPClient: Return current tools + \"discovery running\"\n            PluggedinApp -\u003e\u003e MCPServers: Connect and discover capabilities (background)\n            MCPServers -\u003e\u003e PluggedinApp: Return fresh capabilities\n        end\n    end\n```\n\n## üîÑ Workflow\n\n1. **Configuration**: The proxy fetches server configurations from the plugged.in App\n2. **Smart Discovery** (`pluggedin_discover_tools`):\n   - **Cache Check**: First checks for existing cached data (\u003c 1 second)\n   - **Instant Response**: Returns static tools + cached dynamic tools immediately\n   - **Background Refresh**: For `force_refresh=true`, runs discovery in background while showing current tools\n   - **Fresh Discovery**: Only runs full discovery if no cached data exists\n3. **Capability Listing**: The proxy fetches discovered capabilities from plugged.in App APIs\n   - `tools/list`: Fetches from `/api/tools` (includes static + dynamic tools)\n   - `resources/list`: Fetches from `/api/resources`\n   - `resource-templates/list`: Fetches from `/api/resource-templates`\n   - `prompts/list`: Fetches from `/api/prompts` and `/api/custom-instructions`, merges results\n4. **Capability Resolution**: The proxy resolves capabilities to target servers\n   - `tools/call`: Parses prefix from tool name, looks up server in internal map\n   - `resources/read`: Calls `/api/resolve/resource?uri=...` to get server details\n   - `prompts/get`: Checks for custom instruction prefix or calls `/api/resolve/prompt?name=...`\n5. **Request Routing**: Requests are routed to the appropriate underlying MCP server\n6. **Response Handling**: Responses from the underlying servers are returned to the client\n\n## üîí Security Features\n\nThe plugged.in MCP Proxy implements comprehensive security measures to protect your system and data:\n\n### Input Validation \u0026 Sanitization\n\n- **Command Injection Prevention**: All commands and arguments are validated against allowlists before execution\n- **Environment Variable Security**: Secure parsing of `.env` files with proper handling of quotes and multiline values\n- **Token Validation**: Strong regex patterns for API keys and authentication tokens (32-64 hex characters)\n\n### Network Security\n\n- **SSRF Protection**: URL validation blocks access to:\n  - Localhost and loopback addresses (127.0.0.1, ::1)\n  - Private IP ranges (10.x, 172.16-31.x, 192.168.x)\n  - Link-local addresses (169.254.x)\n  - Multicast and reserved ranges\n  - Common internal service ports (SSH, databases, etc.)\n- **Header Validation**: Protection against header injection with:\n  - Dangerous header blocking\n  - RFC 7230 compliant header name validation\n  - Control character detection\n  - Header size limits (8KB max)\n- **Rate Limiting**: \n  - Tool calls: 60 requests per minute\n  - API calls: 100 requests per minute\n- **Error Sanitization**: Prevents information disclosure by sanitizing error messages\n\n### Process Security\n\n- **Safe Command Execution**: Uses `execFile()` instead of `exec()` to prevent shell injection\n- **Command Allowlist**: Only permits execution of:\n  - `node`, `npx` - Node.js commands\n  - `python`, `python3` - Python commands\n  - `uv`, `uvx`, `uvenv` - UV Python tools\n- **Argument Sanitization**: Removes shell metacharacters and control characters from all arguments\n- **Environment Variable Validation**: Only allows alphanumeric keys with underscores\n\n### Streamable HTTP Security\n\n- **Lazy Authentication**: Tool discovery doesn't require authentication, improving compatibility\n- **Session Security**: Cryptographically secure session ID generation\n- **CORS Protection**: Configurable CORS headers for web access\n- **Request Size Limits**: Prevents DoS through large payloads\n\n### Security Utilities\n\nA dedicated `security-utils.ts` module provides:\n- Bearer token validation\n- URL validation with SSRF protection\n- Command argument sanitization\n- Environment variable validation\n- Rate limiting implementation\n- Error message sanitization\n\nFor detailed security implementation, see [SECURITY.md](SECURITY.md).\n\n## üß© Integration with plugged.in App\n\nThe plugged.in MCP Proxy Server is designed to work seamlessly with the [plugged.in App](https://github.com/VeriTeknik/pluggedin-app), which provides:\n\n- A web-based interface for managing MCP server configurations\n- Centralized capability discovery (Tools, Resources, Templates, Prompts)\n- **RAG v2 Document Library**: Upload documents and enable AI-generated content with full attribution\n- Custom instructions management\n- Multi-workspace support for different configuration sets\n- An interactive playground for testing MCP tools with any AI model\n- User authentication and API key management\n- **AI Document Exchange**: Create, search, and manage documents with model attribution tracking\n\n## üìö Related Resources\n\n- [plugged.in App Repository](https://github.com/VeriTeknik/pluggedin-app)\n- [Model Context Protocol (MCP) Specification](https://modelcontextprotocol.io/)\n- [Claude Desktop Documentation](https://docs.anthropic.com/claude/docs/claude-desktop)\n- [Cline Documentation](https://docs.cline.bot/)\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìù Recent Updates\n\n### Version 1.9.0 (September 2025) - Security Enhancements\n\n#### üîí Enhanced HTML Sanitization\n- **Industry-Standard Sanitization**: Replaced custom regex-based HTML sanitization with `sanitize-html` library\n- **XSS Prevention**: Comprehensive protection against cross-site scripting attacks\n- **HTML Attribute Security**: Enhanced sanitization for HTML attribute contexts (quotes, ampersands)\n- **Format String Injection**: Fixed format string injection vulnerabilities in logging\n- **Security Testing**: Comprehensive test coverage for all sanitization functions\n\n#### üõ°Ô∏è Security Improvements\n- **CodeQL Compliance**: Resolved all security vulnerabilities identified by GitHub CodeQL analysis\n- **Input Validation**: Strengthened input validation and sanitization across all functions\n- **Dependency Updates**: Added `sanitize-html` for robust HTML content filtering\n- **Test Coverage**: Enhanced security test suite with XSS attack prevention verification\n\n### Version 1.5.0 (January 2025) - RAG v2\n\n#### ü§ñ AI Document Exchange\n- **AI-Generated Documents**: MCP servers can now create documents in your library with full AI attribution\n- **Model Attribution Tracking**: Complete history of which AI models created or updated each document\n- **Advanced Document Search**: Filter by AI model, provider, date, tags, and source type\n- **Document Versioning**: Track changes and maintain version history for AI-generated content\n- **Multi-Source Support**: Documents from uploads, AI generation, or API integrations\n\n#### üîç Enhanced RAG Capabilities\n- **Semantic Search**: Improved relevance scoring with PostgreSQL full-text search\n- **Smart Filtering**: Filter results by visibility, model attribution, and document source\n- **Snippet Generation**: Automatic snippet extraction with keyword highlighting\n- **Performance Optimization**: Faster queries with optimized indexing\n\n### Version 1.2.0 (January 2025)\n\n#### üîí Security Enhancements\n\n- **URL Validation**: Comprehensive SSRF protection blocking private IPs, localhost, and dangerous ports\n- **Command Allowlisting**: Only approved commands (node, npx, python, etc.) can be executed\n- **Header Sanitization**: Protection against header injection attacks\n- **Lazy Authentication**: Improved Smithery compatibility with auth-free tool discovery\n\n#### üöÄ Performance Improvements\n\n- **Optimized Docker Builds**: Multi-stage builds for minimal container footprint\n- **Production Dependencies Only**: Test files and dev dependencies excluded from Docker images\n- **Resource Efficiency**: Designed for deployment in resource-constrained environments\n\n#### üîß Technical Improvements\n\n- Enhanced error handling in Streamable HTTP transport\n- Better session cleanup and memory management\n- Improved TypeScript types and code organization\n\n### Version 1.1.0 (December 2024)\n\n#### üöÄ New Features\n\n- **Streamable HTTP Support**: Connect to downstream MCP servers using the modern Streamable HTTP transport\n- **HTTP Server Mode**: Run the proxy as an HTTP server for web-based access\n- **Flexible Session Management**: Choose between stateless or stateful modes\n- **Authentication Options**: Optional Bearer token authentication for HTTP endpoints\n- **Health Monitoring**: `/health` endpoint for service monitoring\n\n#### üîß Technical Improvements\n\n- Updated MCP SDK to v1.13.1 for latest protocol support\n- Added Express.js integration for HTTP server functionality\n- Enhanced TypeScript types for better developer experience\n\n### Version 1.0.0 (June 2025)\n\n#### üéØ Major Features\n- **Real-Time Notification System**: Track all MCP activities with comprehensive notification support\n- **RAG Integration**: Support for document-enhanced queries through the plugged.in App\n- **Inspector Scripts**: New automated testing tools for debugging and development\n- **Health Monitoring**: Built-in ping endpoint for connection monitoring\n\n#### üîí Security Enhancements\n- **Input Validation**: Industry-standard validation and sanitization for all inputs\n- **URL Security**: Enhanced URL validation with SSRF protection\n- **Environment Security**: Secure parsing of environment variables with dotenv\n- **Error Sanitization**: Prevents information disclosure in error responses\n\n#### üêõ Bug Fixes\n- Fixed JSON-RPC protocol interference (stdout vs stderr separation)\n- Resolved localhost URL validation for development environments\n- Fixed API key handling in inspector scripts\n- Improved connection stability and memory management\n\n#### üîß Developer Tools\n- New inspector scripts for automated testing\n- Improved error messages and debugging capabilities\n- Structured logging with proper stderr usage\n- Enhanced TypeScript type safety\n\nSee [Release Notes](./RELEASE_NOTES_v1.0.0.md) for complete details.\n\n## üß™ Testing and Development\n\n### Local Development\nTests are included for development purposes but are excluded from Docker builds to minimize the container footprint.\n\n```bash\n# Run tests locally\nnpm test\n# or\n./scripts/test-local.sh\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Run tests with UI\nnpm run test:ui\n```\n\n### Lightweight Docker Builds\nThe Docker image is optimized for minimal footprint:\n- Multi-stage build process\n- Only production dependencies in final image\n- Test files and dev dependencies excluded\n- Optimized for resource-constrained environments\n\n```bash\n# Build optimized Docker image\ndocker build -t pluggedin-mcp .\n\n# Check image size\ndocker images pluggedin-mcp\n```\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgements\n\n- Inspired by the [MCP Proxy Server](https://github.com/adamwattis/mcp-proxy-server/)\n- Built on the [Model Context Protocol](https://modelcontextprotocol.io/)\n",
      "stars": 95,
      "updated_at": "2025-10-03T04:50:11Z",
      "url": "https://github.com/VeriTeknik/pluggedin-mcp-proxy"
    },
    "WayStation-ai--mcp": {
      "category": "aggregators",
      "description": "Seamlessly and securely connect Claude Desktop and other MCP hosts to your favorite apps (Notion, Slack, Monday, Airtable, etc.). Takes less than 90 secs.",
      "forks": 7,
      "imageUrl": "",
      "keywords": [
        "mcp",
        "servers",
        "aggregators",
        "mcp server",
        "mcp seamlessly",
        "desktop mcp"
      ],
      "language": "JavaScript",
      "license": "No License",
      "name": "mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "WayStation-ai",
      "readme_content": "# What is WayStation\n\u003cimg alt=\"logo\" src=\"https://waystation.ai/images/logo.svg\" width=\"50\" align=\"left\"/\u003e [WayStation](https://waystation.ai) connects Claude Desktop, ChatGPT and any MCP host with the productivity tools you use daily such as Notion, Monday, Airtable, Jira etc. through a no-code, secure integration hub. \n\n***The original local WayStation MCP server has been deprecated in favor of the new remote MCP server hosted at https://waystation.ai/mcp. Please refer to the new WayStation MCP server documentation here***\n\n## Overview\nWayStation MCP server is a universal remote MCP server that seamlessly connects Claude (and other clients) to a broad range of productivity tools, including Notion, Monday, AirTable, etc.\n\n- WayStation MCP supports both Streamable HTTPS and SSE transports\n- The default endpoint is https://waystation.ai/mcp. It does transport negotiation and authorization if necessary\n- WayStation also provides preauthenticated individual endpoints like https://waystation.ai/mcp/Iddq66dIdkfARDNb3K. Any registered user can get one in their dashboard at https://waystation.ai/dashboard\n\n## Supported providers\n- WayStation supports the following productivity apps: [Notion](https://waystation.ai/connect/notion), [Monday](https://waystation.ai/connect/monday), [Asana](https://waystation.ai/connect/asana), [Linear](https://waystation.ai/connect/linear), [Atlassian JIRA/Confluence](https://waystation.ai/connect/atlassian), [Slack](https://waystation.ai/connect/slack), [Teams](https://waystation.ai/connect/teams), [Google Drive](https://waystation.ai/connect/gdrive) (including Docs and Sheets), [Office 365](https://waystation.ai/connect/office), [Airtable](https://waystation.ai/connect/airtable), [Miro](https://waystation.ai/connect/miro), [Intercom](https://waystation.ai/connect/intercom), [PayPal](https://waystation.ai/connect/paypal).\n- Users can browse available integrations/providers in the [Integrations Marketplace](https://waystation.ai/marketplace)\n- New integrations are added regularly based on customer requests or community contributions. If you have an integration request, please contact us at support@waystation.ai.\n- Users can connect their apps in the [dashboard](https://waystation.ai/dashboard). The connection process may vary by app but generally involves OAuth2 authentication flow with some additional steps for certain apps.\n\n## Supported AI apps\n- WayStation remote MCP was tested with Claude, Cursor, Cline, WindSurf, and MCP-remote STDIO proxy provider\n- For Claude, user should go into their Settings, then Integrations and click \"Add Integration\". Then enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cline, user should simply go into the MCP Server screen, switch to the Remote Servers tab, enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cursor, user should go to the Cursor Settings, MCP tab and click \"Add new global MCP server\". In mcp.json file user should add the entry for WayStation as following:\n```json\n\"WayStation\": {\n      \"url\": \"https://waystation.ai/mcp/\u003cuser_unique_id\u003e\"\n}\n```\n\n## Use Cases\nWayStation supports a variety of productivity and automation use cases listed below:\n- [Project Management](https://waystation.ai/ai/project-management)\n- [Task Automation](https://waystation.ai/ai/task-automation)\n- [Meeting Summaries \u0026 Action Items](https://waystation.ai/ai/meeting-summaries)\n- [Workflow Automation \u0026 Process Optimization](https://waystation.ai/ai/workflow-automation)\n- [Resource \u0026 Capacity Planning](https://waystation.ai/ai/resource-capacity-planning)\n- [Risk \u0026 Issue Management](https://waystation.ai/ai/risk-issue-management)\n- [Reporting \u0026 Insights](https://waystation.ai/ai/reporting-insights)\n- [Portfolio Management](https://waystation.ai/ai/portfolio-management)\n- [Team Collaboration Assistant](https://waystation.ai/ai/team-collaboration-assistant)\n- [Creative Production Management](https://waystation.ai/ai/creative-production-management)\n- [Campaign Management](https://waystation.ai/ai/campaign-management)\n- [Product Management \u0026 Roadmapping](https://waystation.ai/ai/product-management-roadmapping)\n- [Product Launch Coordination](https://waystation.ai/ai/product-launch-coordination)\n- [Operations Management](https://waystation.ai/ai/operations-management)\n- [IT Project Coordination](https://waystation.ai/ai/it-project-coordination)\n- [Project Intake \u0026 Triage](https://waystation.ai/ai/project-intake-triage)\n- [Knowledge Management Integration](https://waystation.ai/ai/knowledge-management-integration)\n- [Goal Tracking \u0026 OKR Alignment](https://waystation.ai/ai/goal-tracking-okr-alignment)\n- [Compliance \u0026 Audit Trail Management](https://waystation.ai/ai/compliance-audit-trail)\n- [Timeline \u0026 Deadline Optimization](https://waystation.ai/ai/timeline-deadline-optimization)\n",
      "stars": 32,
      "updated_at": "2025-09-26T16:29:33Z",
      "url": "https://github.com/waystation-ai/mcp"
    },
    "YangLiangwei--PersonalizationMCP": {
      "category": "aggregators",
      "description": "Comprehensive personal data aggregation MCP server with Steam, YouTube, Bilibili, Spotify, Reddit and other platforms integrations. Features OAuth2 authentication, automatic token management, and 90+ tools for gaming, music, video, and social platform data access.",
      "forks": 2,
      "imageUrl": "",
      "keywords": [
        "aggregators",
        "personalizationmcp",
        "servers",
        "aggregators servers",
        "yangliangwei personalizationmcp",
        "mcp server"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "PersonalizationMCP",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "YangLiangwei",
      "readme_content": "# üéØ PersonalizationMCP\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n[![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n\nA unified personal data hub built on MCP (Model Context Protocol) that allows AI assistants to access your digital life from multiple platforms, providing truly personalized and contextual interactions.\n\n\u003e üìñ **‰∏≠ÊñáÊñáÊ°£**: [README_zh.md](README_zh.md)\n\n## üöÄ Quick Start\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/YangLiangwei/PersonalizationMCP.git\n   cd PersonalizationMCP\n   ```\n\n2. **Install dependencies**\n   \n   \u003e üìñ **See detailed installation instructions: [Installation and Setup](#-installation-and-setup)**\n\n3. **Configure your API keys**\n   ```bash\n   cp config.example config\n   # Edit config file with your actual API keys\n   ```\n\n4. **Add to Cursor settings**\n   \n   \u003e üìñ **See detailed MCP configuration: [Cursor Configuration](#-cursor-configuration)**\n\n## üåü Features\n\n### üéÆ Steam Integration\n- Get your game library with detailed statistics and playtime\n- View recent gaming activity and currently playing games\n- Get detailed game information and achievements\n- Compare games with friends and get recommendations\n- Analyze gaming habits and preferences\n\n### üé• YouTube Integration\n- Search YouTube videos and get detailed video information\n- Get channel information and trending videos\n- Access personal data with OAuth2 (subscriptions, playlists, liked videos)\n- Get personalized recommendations based on your viewing history\n- üîÑ **Smart Token Management** - Automatically detect and refresh expired OAuth2 tokens\n- üõ°Ô∏è **Maintenance-Free Configuration** - Prioritize token files, no need to manually update MCP configuration\n\n### üì∫ Bilibili Integration\n- Get user profile information and statistics\n- Search videos and get detailed video information\n- Access personal data (watch history, favorites, liked videos, coin history)\n- Get following list and user-uploaded videos\n- Browse \"to view later\" list and personal collections\n\n### üéµ Spotify Integration\n- Complete OAuth2 authentication with automatic token management\n- Get user profile and music library data\n- Access top artists, tracks, and recently played music\n- Social features: follow/unfollow artists and playlists\n- Library management: saved tracks, albums, shows, episodes, audiobooks\n- Playlist operations: view and manage personal playlists\n\n### üí¨ Reddit Integration\n- Complete OAuth2 authentication with automatic token management\n- Access user account information, karma breakdown, and preferences\n- Get submitted posts, comments, and user activity overview\n- View saved content, hidden posts, and voting history\n- Explore subscribed communities and moderation permissions\n- Message system access (inbox, unread, sent messages)\n\n## üì¶ Installation and Setup\n\n### 1. Install Dependencies\n\nDue to the complexity of bilibili-api dependencies (especially lxml compilation issues), installation requires specific steps. Choose one of the methods below:\n\n#### **Option A: Using conda (Recommended)**\n```bash\n# 1. Create conda environment\nconda create -n personalhub python=3.12\nconda activate personalhub\n\n# 2. Install lxml via conda (avoids compilation issues)\nconda install lxml\n\n# 3. Install remaining packages\npip install bilibili-api --no-deps\npip install -r requirements.txt\n```\n\n#### **Option B: Using uv**\n```bash\n# 1. Install uv if not already installed\n# Visit: https://docs.astral.sh/uv/getting-started/installation/\n\n# 2. Create environment and install core dependencies\nuv venv\nuv sync\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# 3. Install bilibili-api and its dependencies separately (due to version conflicts)\nuv pip install lxml  # Install lxml first (uses precompiled wheel)\nuv pip install bilibili-api --no-deps  # Install bilibili-api without dependencies\nuv pip install aiohttp beautifulsoup4 colorama PyYAML brotli urllib3  # Install required dependencies\n```\n\n#### **Option C: Using pip (Manual Multi-Step Installation)**\n```bash\n# 1. Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# 2. Install packages in specific order to avoid compilation issues\npip install lxml  # Install lxml first (uses precompiled wheel)\npip install bilibili-api --no-deps  # Install bilibili-api without dependencies\npip install -r requirements.txt  # Install all other dependencies\n```\n\n\u003e **‚ö†Ô∏è Important**: The bilibili-api package has complex dependency requirements that can cause compilation failures on some systems. The multi-step installation approach ensures compatibility by installing lxml first, then bilibili-api without its conflicting dependencies, and finally all other required packages.\n\n### 2. Configuration Setup\n\nCopy the example configuration file and fill in your credentials:\n```bash\ncp config.example config\n```\n\nThen edit the `config` file with your actual API keys and tokens.\n\n## üîß Platform Configuration\n\n### üéÆ Steam API Setup\n\n\u003e üìñ **Detailed setup guide**: [platforms/steam/README.md](platforms/steam/README.md) | [‰∏≠ÊñáÊåáÂçó](platforms/steam/README_zh.md)\n\n**Quick summary**: Get Steam API key and User ID, then configure:\n```bash\nSTEAM_API_KEY=your_steam_api_key_here\nSTEAM_USER_ID=your_steam_user_id_here\n```\n\n### üé• YouTube API Setup\n\n\u003e üìñ **Detailed setup guide**: [platforms/youtube/README.md](platforms/youtube/README.md) | [‰∏≠ÊñáÊåáÂçó](platforms/youtube/README_zh.md)\n\n**Quick summary**: \n1. Get YouTube API key from Google Cloud Console\n2. For personal data access, set up OAuth2 with \"TV and Limited Input device\" type\n3. Use MCP tools for easy authentication\n\n**Configuration:**\n```bash\nYOUTUBE_API_KEY=your_youtube_api_key_here\n# OAuth2 tokens are managed automatically after setup\n```\n\n### üì∫ Bilibili Setup\n\n\u003e üìñ **Detailed setup guide**: [platforms/bilibili/README.md](platforms/bilibili/README.md) | [‰∏≠ÊñáÊåáÂçó](platforms/bilibili/README_zh.md)\n\n**Quick summary**: Extract cookies from your browser after logging into Bilibili\n\n**Configuration:**\n```bash\nBILIBILI_SESSDATA=your_bilibili_sessdata_cookie\nBILIBILI_BILI_JCT=your_bilibili_bili_jct_cookie\nBILIBILI_BUVID3=your_bilibili_buvid3_cookie\n```\n\n### üéµ Spotify API Setup\n\n\u003e üìñ **Detailed setup guide**: [platforms/spotify/README.md](platforms/spotify/README.md) | [‰∏≠ÊñáÊåáÂçó](platforms/spotify/README_zh.md)\n\n**Quick summary**: \n1. Create a Spotify app in [Spotify Developer Dashboard](https://developer.spotify.com/dashboard)\n2. Configure redirect URIs in your app settings\n3. Use MCP tools for OAuth2 authentication with automatic token management\n\n**Configuration:**\n```bash\nSPOTIFY_CLIENT_ID=your_spotify_client_id_here\nSPOTIFY_CLIENT_SECRET=your_spotify_client_secret_here\nSPOTIFY_REDIRECT_URI=https://example.com/callback\n# OAuth2 tokens are managed automatically after authentication\n```\n\n### üí¨ Reddit API Setup\n\n\u003e üìñ **Detailed setup guide**: [platforms/reddit/README.md](platforms/reddit/README.md) | [‰∏≠ÊñáÊåáÂçó](platforms/reddit/README_zh.md)\n\n**Quick summary**: \n1. Create a Reddit app in [Reddit Apps](https://www.reddit.com/prefs/apps)\n2. Configure as \"web app\" with redirect URI\n3. Use MCP tools for OAuth2 authentication with automatic token management\n\n**Configuration:**\n```bash\nREDDIT_CLIENT_ID=your_reddit_client_id_here\nREDDIT_CLIENT_SECRET=your_reddit_client_secret_here\nREDDIT_REDIRECT_URI=http://localhost:8888/callback\n# OAuth2 tokens are managed automatically after authentication\n```\n\n## üñ•Ô∏è Cursor Configuration\n\nAdd the MCP server to your Cursor settings:\n\n**If using conda:**\n```json\n{\n  \"mcpServers\": {\n    \"personalhub\": {\n      \"command\": \"/path/to/your/conda/envs/personalhub/bin/python\",\n      \"args\": [\"/absolute/path/to/your/project/server.py\"],\n      \"env\": {\n        \"STEAM_API_KEY\": \"your_steam_api_key\",\n        \"STEAM_USER_ID\": \"your_steam_user_id\",\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\",\n        \"BILIBILI_SESSDATA\": \"your_bilibili_sessdata\",\n        \"BILIBILI_BILI_JCT\": \"your_bilibili_bili_jct\",\n        \"BILIBILI_BUVID3\": \"your_bilibili_buvid3\",\n        \"REDDIT_CLIENT_ID\": \"your_reddit_client_id\",\n        \"REDDIT_CLIENT_SECRET\": \"your_reddit_client_secret\"\n      }\n    }\n  }\n}\n```\n\n**If using uv:**\n```json\n{\n  \"mcpServers\": {\n    \"personalhub\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"/absolute/path/to/your/project/server.py\"],\n      \"env\": {\n        \"STEAM_API_KEY\": \"your_steam_api_key\",\n        \"STEAM_USER_ID\": \"your_steam_user_id\",\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\",\n        \"BILIBILI_SESSDATA\": \"your_bilibili_sessdata\",\n        \"BILIBILI_BILI_JCT\": \"your_bilibili_bili_jct\",\n        \"BILIBILI_BUVID3\": \"your_bilibili_buvid3\",\n        \"REDDIT_CLIENT_ID\": \"your_reddit_client_id\",\n        \"REDDIT_CLIENT_SECRET\": \"your_reddit_client_secret\"\n      }\n    }\n  }\n}\n```\n\n**If using pip with virtual environment:**\n```json\n{\n  \"mcpServers\": {\n    \"personalhub\": {\n      \"command\": \"/absolute/path/to/your/project/venv/bin/python\",\n      \"args\": [\"/absolute/path/to/your/project/server.py\"],\n      \"env\": {\n        \"STEAM_API_KEY\": \"your_steam_api_key\",\n        \"STEAM_USER_ID\": \"your_steam_user_id\",\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\",\n        \"BILIBILI_SESSDATA\": \"your_bilibili_sessdata\",\n        \"BILIBILI_BILI_JCT\": \"your_bilibili_bili_jct\",\n        \"BILIBILI_BUVID3\": \"your_bilibili_buvid3\",\n        \"REDDIT_CLIENT_ID\": \"your_reddit_client_id\",\n        \"REDDIT_CLIENT_SECRET\": \"your_reddit_client_secret\"\n      }\n    }\n  }\n}\n```\n\n**Note**: For YouTube OAuth2 tokens, we recommend using automatic token management. No need to add `YOUTUBE_ACCESS_TOKEN` in the above configuration. The system will automatically read and refresh tokens from the `youtube_tokens.json` file.\n\n## üîÑ YouTube Smart Token Management\n\nThis system implements intelligent YouTube OAuth2 token management with the following features:\n\n### ‚ú® Core Features\n- **Automatic Expiration Detection**: System automatically detects tokens expiring within 5 minutes\n- **Auto-Refresh**: No manual intervention needed, system automatically refreshes expired tokens\n- **Smart Priority**: Prioritizes token files, with environment variables as backup\n- **Maintenance-Free Configuration**: No need to manually update tokens in MCP configuration files\n\n### üîß Token Priority\n1. **Explicitly passed access_token parameter** (Highest priority)\n2. **Auto-refresh tokens from token file** (Recommended method)\n3. **Tokens from environment variables** (Backup method)\n\n\nThe system automatically handles all token management - no manual maintenance required!\n\n## üõ†Ô∏è Available Tools\n\n### üéÆ Steam Tools\n- `get_steam_library()` - Get your game library with statistics\n- `get_steam_recent_activity()` - Get recent gaming activity\n- `get_steam_friends()` - Get your Steam friends list\n- `get_steam_profile()` - Get Steam profile information\n- `get_player_achievements(app_id)` - Get achievements for a specific game\n- `get_user_game_stats(app_id)` - Get detailed game statistics\n- `get_friends_current_games()` - See what games your friends are playing\n- `compare_games_with_friend(friend_steamid)` - Compare game libraries\n- `get_friend_game_recommendations(friend_steamid)` - Get game recommendations\n\n### üé• YouTube Tools\n- `search_youtube_videos(query)` - Search for videos\n- `get_video_details(video_id)` - Get detailed video information\n- `get_channel_info(channel_id)` - Get channel information\n- `get_trending_videos()` - Get trending videos\n- `get_youtube_subscriptions()` - Get your subscriptions (OAuth2 required)\n- `get_youtube_playlists()` - Get your playlists (OAuth2 required)\n- `get_youtube_liked_videos()` - Get your liked videos (OAuth2 required)\n- `refresh_youtube_token()` - Manually refresh OAuth2 token\n- `get_youtube_token_status()` - Check OAuth2 token status\n\n### üì∫ Bilibili Tools\n- `get_bilibili_user_info(uid)` - Get user profile information\n- `get_my_bilibili_profile()` - Get your own profile\n- `search_bilibili_videos(keyword)` - Search for videos\n- `get_bilibili_video_info(bvid)` - Get detailed video information\n- `get_bilibili_user_videos(uid)` - Get videos uploaded by a user\n- `get_bilibili_following_list()` - Get your following list\n- `get_bilibili_watch_history()` - Get your watch history\n- `get_bilibili_favorites()` - Get your favorite videos\n- `get_bilibili_liked_videos()` - Get your liked videos\n- `get_bilibili_coin_videos()` - Get videos you've given coins to\n- `get_bilibili_toview_list()` - Get your \"to view later\" list\n\n### üéµ Spotify Tools (17 Total)\n\n**Authentication \u0026 Configuration (7 tools):**\n- `test_spotify_credentials()` - Test API credentials\n- `setup_spotify_oauth()` - Initialize OAuth flow\n- `complete_spotify_oauth()` - Complete OAuth authentication\n- `get_spotify_token_status()` - Get token status\n- `refresh_spotify_token()` - Manual token refresh\n\n**Music Discovery \u0026 Social (9 tools):**\n- `get_current_user_profile()` - Get your Spotify profile\n- `get_user_top_items()` - Get top artists/tracks\n- `get_user_recently_played()` - Get recently played music\n- `get_followed_artists()` - Get followed artists\n- `follow_artists_or_users()` / `unfollow_artists_or_users()` - Social features\n\n**Library \u0026 Playlists (6 tools):**\n- `get_user_saved_tracks()` / `get_user_saved_albums()` - Library management\n- `get_user_saved_shows()` / `get_user_saved_episodes()` - Podcast content\n- `get_current_user_playlists()` / `get_playlist_items()` - Playlist operations\n\n### üí¨ Reddit Tools (25 Total)\n\n**Authentication \u0026 Configuration (6 tools):**\n- `test_reddit_credentials()` - Test API credentials\n- `setup_reddit_oauth()` - Initialize OAuth flow\n- `complete_reddit_oauth()` - Complete OAuth authentication\n- `get_reddit_token_status()` - Get token status\n- `refresh_reddit_token()` - Manual token refresh\n- `auto_refresh_reddit_token_if_needed()` - Auto token management\n\n**Account Information (6 tools):**\n- `get_user_subreddits()` - Get subscribed communities\n- `get_user_trophies()` - Get Reddit trophies and achievements\n- `get_user_preferences()` - Get account settings\n- `get_user_karma_breakdown()` - Get karma distribution\n- `get_moderated_subreddits()` - Get moderated communities\n- `get_contributor_subreddits()` - Get contributor permissions\n\n**Content \u0026 Activity (10 tools):**\n- `get_user_submitted_posts()` - Get submitted posts\n- `get_user_comments()` - Get comment history\n- `get_user_overview()` - Get mixed activity timeline\n- `get_saved_content()` - Get saved posts/comments\n- `get_hidden_posts()` - Get hidden content\n- `get_upvoted_content()` - Get upvoted content\n- `get_downvoted_content()` - Get downvoted content\n\n**Messaging (3 tools):**\n- `get_inbox_messages()` - Get inbox messages\n- `get_unread_messages()` - Get unread messages\n- `get_sent_messages()` - Get sent messages\n\n### üîß System Tools\n- `test_connection()` - Test if MCP server is working\n- `get_personalization_status()` - Get overall platform status\n- `test_steam_credentials()` - Test Steam API configuration\n- `test_youtube_credentials()` - Test YouTube API configuration\n- `test_bilibili_credentials()` - Test Bilibili configuration\n- `test_spotify_credentials()` - Test Spotify API configuration\n- `test_reddit_credentials()` - Test Reddit API configuration\n\n## üí¨ Usage Examples\n\n### Gaming Analysis\n- \"What games have I been playing recently?\"\n- \"Show me my most played Steam games\"\n- \"What games do my friends recommend?\"\n- \"Compare my game library with my friend's\"\n\n### Video Content Discovery\n- \"Find YouTube videos about machine learning\"\n- \"What are the trending videos on YouTube today?\"\n- \"Show me my YouTube liked videos\"\n- \"Find popular Bilibili videos about programming\"\n\n### Personal Data Insights\n- \"Analyze my gaming habits and preferences\"\n- \"What type of YouTube content do I watch most?\"\n- \"Show me my Bilibili favorites and liked videos\"\n\n### Music \u0026 Audio Analysis\n- \"What artists have I been listening to most lately on Spotify?\"\n- \"Show me my recently played music and find patterns\"\n- \"What are my top tracks from the past month?\"\n- \"Find new music recommendations based on my Spotify data\"\n\n### Reddit Activity Analysis\n- \"What communities am I most active in on Reddit?\"\n- \"Show me my recent Reddit posts and comments\"\n- \"What's my karma breakdown across different subreddits?\"\n- \"Find my saved Reddit content and analyze my interests\"\n\n## üöÄ Development\n\n### Running the Server\n\n**If using conda:**\n```bash\nconda activate personalhub\npython server.py\n```\n\n**If using uv:**\n```bash\nuv run python server.py\n```\n\n**If using pip with virtual environment:**\n```bash\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npython server.py\n```\n\n### Testing Configuration\nUse these tools to test your setup:\n```python\n# Test individual platforms\ntest_steam_credentials()\ntest_youtube_credentials()\ntest_bilibili_credentials()\ntest_reddit_credentials()\n\n# Check overall status\nget_personalization_status()\n```\n\n### Adding New Platforms\n1. Create a new `platform_mcp.py` file\n2. Implement the platform-specific tools using `@mcp.tool()` decorator\n3. Add setup function to `server.py`\n4. Update configuration files and documentation\n\n## üîí Privacy and Security\n\n- **Local Storage**: All API keys and tokens are stored locally on your machine\n- **No Data Transmission**: Your personal data is never transmitted to third parties\n- **Direct API Calls**: All API calls are made directly from your machine to the respective platforms\n- **Secure Configuration**: Use environment variables or local config files\n- **Regular Updates**: Rotate API keys and tokens regularly for security\n\n### Security Best Practices\n1. **Don't commit sensitive files**: Ensure `config`, `.env`, `myinfo.json`, and `youtube_tokens.json` are in `.gitignore`\n2. **Update cookies regularly**: Bilibili cookies expire and need periodic updates\n3. **Use environment variables**: In production, use system environment variables\n4. **File permissions**: Ensure config files are only readable by you\n5. **YouTube token security**: The system automatically manages OAuth2 tokens securely in local files\n6. **Gradual configuration**: You can configure platforms incrementally - missing credentials won't cause errors\n\n## üÜò Troubleshooting\n\n### Common Issues\n\n**Q: Bilibili cookies not working?**\nA: Cookies expire regularly. Re-extract them from your browser and update your config.\n\n**Q: Steam API rate limits?**\nA: Steam API has rate limits. Avoid frequent calls and implement reasonable delays.\n\n**Q: YouTube API quota exceeded?**\nA: YouTube API has daily quotas. You can request quota increases or optimize your usage.\n\n**Q: YouTube OAuth2 token expired?**\nA: The system automatically refreshes expired tokens. If manual refresh is needed, use `refresh_youtube_token()`.\n\n**Q: Can I use only some platforms?**\nA: Yes! You can configure only the platforms you want to use. Missing credentials won't cause errors.\n\n**Q: How to verify my configuration?**\nA: Use the test tools or call `get_personalization_status()` to check all platforms.\n\n### Getting Help\n1. Check configuration file format\n2. Verify API keys and cookies are valid\n3. Review MCP server logs\n4. Use test tools to validate each platform configuration\n\n## ü§ù Contributing\n\nContributions are welcome! Here's how you can help:\n\n1. **Fork the repository**\n2. **Create a feature branch**: `git checkout -b feature/amazing-feature`\n3. **Make your changes** and add tests if applicable\n4. **Commit your changes**: `git commit -m 'Add amazing feature'`\n5. **Push to the branch**: `git push origin feature/amazing-feature`\n6. **Open a Pull Request**\n\n### Adding New Platforms\n\nWant to add support for a new platform? Follow these steps:\n\n1. Create a new `platform_mcp.py` file (e.g., `spotify_mcp.py`)\n2. Implement platform-specific tools using the `@mcp.tool()` decorator\n3. Add a setup function and integrate it in `server.py`\n4. Update configuration files and documentation\n5. Add tests and examples\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) for the amazing protocol\n- [Anthropic](https://www.anthropic.com/) for Claude and MCP development\n- All the platform APIs that make this integration possible\n\n## ‚≠ê Star History\n\nIf you find this project useful, please consider giving it a star on GitHub!\n\n---\n\n**Made with ‚ù§Ô∏è for connecting your digital life with AI**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "stars": 6,
      "updated_at": "2025-09-26T23:38:24Z",
      "url": "https://github.com/YangLiangwei/PersonalizationMCP"
    },
    "duaraghav8--MCPJungle": {
      "category": "aggregators",
      "description": "Centralized repository solution for self-managed Model Context Protocol (MCP) endpoints tailored for organizational AI ecosystems.",
      "forks": 72,
      "imageUrl": "",
      "keywords": [
        "aggregators",
        "agents",
        "enterprise",
        "aggregators servers",
        "enterprise ai",
        "mcp server"
      ],
      "language": "Go",
      "license": "Mozilla Public License 2.0",
      "name": "JungleHub-Registry",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "duaraghav8",
      "readme_content": "\u003ch1 align=\"center\"\u003e\n  :deciduous_tree: JungleHub-Registry :deciduous_tree:\n\u003c/h1\u003e\n\u003cp align=\"center\"\u003e\n  On-premise gateway infrastructure for unified access to your proprietary AI tools via MCP\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://discord.gg/CapV4Z3krk\" style=\"text-decoration: none;\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Community-JungleHub-5865F2?style=flat-square\u0026logo=discord\u0026logoColor=white\" alt=\"Discord Support\" style=\"max-width: 100%;\"\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\nJungleHub-Registry serves as the authoritative, single point of reference for all [Model Context Protocol](https://modelcontextprotocol.io/introduction) service instances operating within your corporate boundary.\n\nüßë‚Äçüíª Development teams leverage this platform to provision, administer, and maintain configurations for their MCP services and the functionalities they expose.\n\nü§ñ AI Agent instances (MCP Clients) utilize this centralized proxy to enumerate and invoke the aggregated capabilities from a singular, harmonized entry point.\n\n\n\n\u003cp align=\"center\"\u003eJungleHub-Registry is the definitive convergence point for all your organizational AI agents' tool interactions!\u003c/p\u003e\n\n# Target Audience\n1. **Engineers** integrating AI tools (e.g., those utilizing Claude or Cursor) via MCP Client interfaces.\n2. **Engineers** deploying resilient, production-grade AI systems requiring inherent security, data governance, and granular authorization for tool access.\n3. **Enterprises** seeking comprehensive visibility and governance over all communication flows between MCP Clients and Services, securely hosted within their private infrastructure üîí\n\n# üìã Index of Contents\n\n- [Expedited Setup Guide](#expedited-setup-guide)\n- [Deployment Procedures](#deployment-procedures)\n- [Operational Usage](#operational-usage)\n  - [Service Layer](#service-layer)\n    - [Containerized Service Launch](#containerized-service-launch)\n    - [Direct Host Execution of the Service](#direct-host-execution-of-the-service)\n  - [Client Interaction](#client-interaction)\n    - [Integrating HTTP-Based MCP Endpoints](#integrating-http-based-mcp-endpoints)\n    - [Integrating Local Process (STDIO) Based Services](#integrating-local-process-stdio-based-services)\n    - [Decommissioning Registered Services](#decommissioning-registered-services)\n  - [Connecting Claude Instances](#connecting-claude-instances)\n  - [Connecting Cursor Instances](#connecting-cursor-instances)\n  - [Global Feature State Management (Enable/Disable)](#global-feature-state-management-enabledisable)\n  - [Functionality Groupings (Tool Groups)](#functionality-groupings-tool-groups)\n  - [Credential Verification Mechanisms](#credential-verification-mechanisms)\n  - [Advanced Organizational Features](#advanced-organizational-features-)\n    - [Authorization Controls](#authorization-controls)\n    - [Telemetry and Tracing (OpenTelemetry)](#telemetry-and-tracing-opentelemetry)\n- [Current Known Constraints](#current-known-constraints-)\n- [Community Involvement](#community-involvement-)\n\n# Expedited Setup Guide\nThis guide demonstrates the rapid deployment of the JungleHub-Registry service locally:\n1. Initiate the service using `docker compose` for a local deployment.\n2. Enroll a prototype MCP service into the registry.\n3. Configure your local Claude instance to communicate through JungleHub-Registry to utilize your enrolled tools.\n\n## Service Initialization\nbash\ncurl -O https://raw.githubusercontent.com/mcpjungle/MCPJungle/refs/heads/main/docker-compose.yaml\ndocker compose up -d\n\n\n## Service Registration\nAcquire the `junglehub` command-line utility on your workstation via Homebrew or by directly downloading the executable from the [Software Releases Repository](https://github.com/mcpjungle/MCPJungle/releases).\nbash\nbrew install mcpjungle/mcpjungle/junglehub\n\n\nThe CLI facilitates all administrative tasks within JungleHub-Registry.\n\nNext, let's attach an existing MCP endpoint (e.g., [context7](https://context7.com/)) to JungleHub-Registry using the CLI.\nbash\njunglehub register --alias context7 --description \"External Math Provider\" --endpoint https://mcp.context7.com/mcp\n\n\n## Client Connection\nIntegrate JungleHub-Registry by setting the following configuration within your Claude tool configuration file:\n\n{\n  \"mcpServers\": {\n    \"junglehub-proxy\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://localhost:8080/mcp\",\n        \"--allow-http\"\n      ]\n    }\n  }\n}\n\n\nOnce JungleHub-Registry is established as an MCP source within Claude, prompt it with the following instruction:\ntext\nAccess the context7 service to fetch the documentation reference for the utility '/lodash/lodash'\n\n\nClaude will then attempt to invoke the `context7__get-library-docs` function via the JungleHub-Registry proxy, which retrieves the specified library documentation.\n\n\u003cp align=\"center\"\u003e\n  \n\u003c/p\u003e\n\nCongratulations! You have successfully provisioned a remote MCP endpoint via JungleHub-Registry and executed one of its functions through an AI agent.\n\nYou are now encouraged to explore the system's documentation and the CLI for more advanced configurations.\n\n# Deployment Procedures\nJungleHub-Registry is distributed as a self-contained executable binary.\n\nInstallation options include downloading from the [Releases Archive](https://github.com/mcpjungle/MCPJungle/releases) or utilizing [Homebrew](https://brew.sh/):\n\nbash\nbrew install mcpjungle/mcpjungle/junglehub\n\n\nVerify correct installation status:\n\nbash\njunglehub version\n\n\n\u003e [!IMPORTANT]\n\u003e On macOS systems, Homebrew is the mandated installation route as the statically compiled binary lacks necessary Apple Notarization certification ([Reference](https://developer.apple.com/documentation/security/notarizing-macos-software-before-distribution)).\n\nJungleHub-Registry furnishes a Docker artifact, ideal for running the centralized registry component.\n\nbash\ndocker pull mcpjungle/mcpjungle\n\n\n# Operational Usage\nJungleHub-Registry operates using a Client-Server topology; the binary supports execution of both roles.\n\n## Service Layer\nThe JungleHub-Registry service component manages the catalog of registered MCP endpoints and furnishes a unified intermediary for AI Agents to discover and invoke capabilities exposed by these registered endpoints.\n\nThe gateway interface communicates via a streamable HTTP transport mechanism, exposed at the `/mcp` URI path.\n\n### Containerized Service Launch\nDocker Compose is the preferred methodology for initializing the JungleHub-Registry service locally:\nshell\n# docker-compose.yaml is optimized for individual, local utility deployment.\n# JungleHub-Registry defaults to 'development' operational mode.\ncurl -O https://raw.githubusercontent.com/mcpjungle/MCPJungle/refs/heads/main/docker-compose.yaml\n\ndocker compose up -d\n\n# docker-compose.prod.yaml is configured for organizational deployment on remote infrastructure for multi-user access.\n# JungleHub-Registry defaults to 'enterprise' operational mode, activating advanced features.\ncurl -O https://raw.githubusercontent.com/mcpjungle/MCPJungle/refs/heads/main/docker-compose.prod.yaml\n\ndocker compose -f docker-compose.prod.yaml up -d\n\n\n\u003e [!NOTE]\n\u003e The 'enterprise' operational mode supersedes the former 'production' designation. Configuration parameters remain unchanged.\n\nThis action launches the JungleHub-Registry service alongside a containerized, persistent PostgreSQL database instance.\n\nConfirm service availability by querying the health check endpoint:\nbash\ncurl http://localhost:8080/health\n\n\nIf you intend to catalog local process (stdio) based MCP services requiring executables like `npx` or `uvx`, you must utilize the `stdio` tagged Docker image for JungleHub-Registry:\nbash\nMCPJUNGLE_IMAGE_TAG=latest-stdio docker compose up -d\n\n\n\u003e [!NOTE]\n\u003e If employing `docker-compose.yaml`, this image tag is the default. Only specify the stdio tag when using `docker-compose.prod.yaml`.\n\nThis specific image carries a larger footprint but offers superior convenience when relying on stdio-driven MCP providers.\n\nFor instance, if your usage is restricted to remote endpoints like context7 and deepwiki, the standard (minimal) image suffices.\n\nHowever, for incorporating stdio providers such as `filesystem`, `time`, or `github`, the `stdio`-tagged variant is essential.\n\n\u003e [!NOTE]\n\u003e If your stdio providers depend on execution tools other than `npx` or `uvx`, you must construct a bespoke Docker image incorporating those dependencies alongside the JungleHub-Registry binary.\n\n**Production Deployment Strategy**\n\nThe default [JungleHub-Registry Docker Image](https://hub.docker.com/r/mcpjungle/mcpjungle) is highly streamlined, containing merely a minimal base environment and the `junglehub` executable.\n\nThis minimalism renders it the recommended choice for large-scale production rollouts.\n\nFor the data persistence layer, deploying a dedicated Postgres database cluster and supplying its connection string to JungleHub-Registry is advised (refer to the [Database Configuration](#database-configuration) section).\n\nYou can review the structure of the [standard Dockerfile](./Dockerfile) and the [stdio Dockerfile](./stdio.Dockerfile).\n\n### Direct Host Execution of the Service\nThe service engine can be initiated directly utilizing the compiled binary:\n\nbash\njunglehub start\n\n\nThis command launches the primary registry broker and MCP conduit, exposed by default on TCP port `8080`.\n\n\n\n### Database Configuration\nThe JungleHub-Registry service mandates a functional data store; by default, it provisions an embedded SQLite database within the execution directory.\n\nThis default is acceptable for initial local validation.\nAlternatively, you can configure an external PostgreSQL data source using a Data Source Name (DSN):\n\nbash\nexport DATABASE_URL=postgres://admin:root@localhost:5432/junglehub_db\n\n# Execution via container\ndocker run mcpjungle/mcpjungle:latest\n\n# Or direct execution\njunglehub start\n\n\n## Client Interaction\nOnce the central service is operational, the `junglehub` CLI tool becomes the interface for system management.\n\nJungleHub-Registry supports MCP services utilizing both [stdio](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#stdio) and the [Streamable HTTP](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) communication protocols.\n\nLet's examine the registration process.\n\n### Integrating HTTP-Based MCP Endpoints\nSuppose you operate a streamable HTTP MCP service locally, accessible at `http://127.0.0.1:8000/mcp`, offering foundational arithmetic functions such as `add`, `subtract`, etc.\n\nYou can register this service within JungleHub-Registry:\nbash\njunglehub register --alias calculator --description \"Offers fundamental arithmetic operations\" --endpoint http://127.0.0.1:8000/mcp\n\n\nIf the service was launched using Docker Compose, and your operating system is not Linux, substitute the loopback address with the Docker gateway address (`host.docker.internal`):\nbash\njunglehub register --alias calculator --description \"Offers fundamental arithmetic operations\" --endpoint http://host.docker.internal:8000/mcp\n\n\nThe registry will then monitor this service and ingest its exposed capabilities.\n\n\n\nRegistration can alternatively be executed via a manifest file:\nbash\ncat ./calculator.json\n{\n  \"alias\": \"calculator\",\n  \"transport\": \"streamable_http\",\n  \"description\": \"Provides some basic math tools\",\n  \"endpoint\": \"http://127.0.0.1:8000/mcp\"\n}\n\njunglehub register -c ./calculator.json\n\n\nAll functions sourced from this service are now discoverable via JungleHub-Registry:\n\nbash\njunglehub list functions\n\n# Inspect function invocation signature\njunglehub usage calculator__multiply\n\n# Execute a function\njunglehub invoke calculator__multiply --payload '{\"a\": 100, \"b\": 50}'\n\n\n\n\n\u003e [!NOTE]\n\u003e A function within JungleHub-Registry must be referenced by its standardized identifier, which adheres to the template `\u003cmcp-service-alias\u003e__\u003cfunction-name\u003e`.\n\u003e The separator between the service alias and the function name is a double underscore `__`.\n\u003e \n\u003e E.g., If you register an MCP service alias `codebase` exposing a function named `find_dependencies`, invocation within JungleHub-Registry uses the canonical name `codebase__find_dependencies`.\n\u003e \n\u003e Your MCP client must employ this canonical nomenclature when invoking the function through JungleHub-Registry.\n\nThe manifest file schema for enrolling a Streamable HTTP-based MCP service is:\n\n{\n  \"alias\": \"\u003cname of your mcp service\u003e\",\n  \"transport\": \"streamable_http\",\n  \"description\": \"\u003cdescription\u003e\",\n  \"endpoint\": \"\u003curl of the mcp server\u003e\",\n  \"bearer_token\": \"\u003coptional token for authorization\u003e\"\n}\n\n\n### Integrating Local Process (STDIO) Based Services\n\nHere is a sample configuration manifest (e.g., `filesystem.json`) for an MCP service utilizing the STDIO transport protocol:\n\n\n{\n  \"alias\": \"filesystem\",\n  \"transport\": \"stdio\",\n  \"description\": \"Local filesystem access mcp service\",\n  \"executor\": \"npx\",\n  \"arguments\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]\n}\n\n\nYou enroll this MCP service into JungleHub-Registry by supplying its configuration file:\nbash\n# Save the JSON configuration to a file (e.g., filesystem.json)\njunglehub register -c ./filesystem.json\n\n\nThe manifest file schema for enrolling an STDIO-based MCP service is:\n\n\n{\n  \"alias\": \"\u003cname of your mcp service\u003e\",\n  \"transport\": \"stdio\",\n  \"description\": \"\u003cdescription\u003e\",\n  \"executor\": \"\u003ccommand to initiate the mcp service, e.g., 'npx', 'uvx'\u003e\",\n  \"arguments\": [\"arguments\", \"to\", \"pass\", \"to\", \"the\", \"command\"],\n  \"environment_vars\": {\n    \"KEY\": \"value\"\n  }\n}\n\n\nYou can watch a brief tutorial on [How to enroll an STDIO-based MCP service](https://youtu.be/YqHiuexR5fw).\n\n\u003e [!TIP]\n\u003e If your STDIO service encounters initialization failures or runtime exceptions, examine the JungleHub-Registry service logs to review its standard error stream output.\n\n**Constraint** üöß\n\nJungleHub-Registry establishes a fresh communication channel upon every function invocation. This necessitates spawning a new child process for an stdio-based service per call.\n\nWhile this introduces minor latency overhead, it guarantees complete isolation and prevents resource accumulation or memory exhaustion issues. Consequently, JungleHub-Registry currently lacks support for persistent, stateful sessions with registered MCP services.\n\nWe solicit community input via issue creation, discussions, or direct Discord contact to refine this execution model in upcoming iterations.\n\n\n**Crucial Consideration** ‚ö†Ô∏è\n\nWhen JungleHub-Registry executes within a Docker environment, specific preparatory steps are required to facilitate access to host-level filesystem-interacting MCP services.\n\nBy default, the containerized JungleHub-Registry lacks inherent visibility into your host machine's file structure.\n\nTherefore, you must:\n- Map the desired host directory into the container as a volume mount.\n- Specify this newly mounted path as the directory argument within the filesystem MCP service's execution command.\n\nThe default `docker-compose.yaml` provided maps the current working directory into the container at the `/host` location.\n\nThus, the filesystem MCP service configuration should utilize this mapping:\n\n\n{\n  \"alias\": \"filesystem\",\n  \"transport\": \"stdio\",\n  \"executor\": \"npx\",\n  \"arguments\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/host\"]\n}\n\n\nThis configuration grants the MCP service read/write access to the host machine's current working directory, accessible within the container at `/host`.\n\nConsult [DEVELOPMENT.md](./DEVELOPMENT.md#docker-filesystem-access) for exhaustive details on Docker volume interaction.\n\n\n### Decommissioning Registered Services\nYou possess the capability to excise a registered MCP service from the JungleHub-Registry catalog.\n\nbash\njunglehub deregister calculator\njunglehub deregister filesystem\n\n\nUpon removal, the associated service and all its functions become inaccessible to JungleHub-Registry and any connected MCP clients.\n\n## Integration with other MCP Clients\nAssuming JungleHub-Registry is operating on `http://localhost:8080`, use the corresponding connection configurations below:\n\n### Claude\n\n{\n  \"mcpServers\": {\n    \"junglehub-proxy\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://localhost:8080/mcp\",\n        \"--allow-http\"\n      ]\n    }\n  }\n}\n\n\n### Cursor\n\n{\n  \"mcpServers\": {\n    \"junglehub-proxy\": {\n      \"url\": \"http://localhost:8080/mcp\"\n    }\n  }\n}\n\n\nYou can view a brief instructional video on [Connecting Cursor to JungleHub-Registry](https://youtu.be/SaUqj-eLPnw).\n\n## Global Feature State Management (Enable/Disable)\nYou maintain control over the availability of individual functions or the entire suite of functions provided by any registered MCP Service.\nIf a function is marked as disabled, it is intentionally blocked by the JungleHub-Registry Proxy; thus, no connected MCP client can enumerate or invoke it.\n\nbash\n# disable the `get-library-docs` function provided by the `context7` service\njunglehub disable context7__get-library-docs\n\n# re-enable the function\njunglehub enable context7__get-library-docs\n\n# disable all functions offered by the `context7` service\njunglehub disable context7\n\n# re-enable all functions belonging to `context7`\njunglehub enable context7\n\n\nA disabled function remains accessible via JungleHub-Registry's direct HTTP interface, allowing administrative oversight via the CLI or other HTTP clients.\n\n\u003e [!NOTE]\n\u003e Upon successful enrollment of a new service into JungleHub-Registry, all its associated functions are **enabled** by default.\n\n## Functionality Groupings (Tool Groups)\nAs you assimilate more MCP services into JungleHub-Registry, the volume of available functions through the Gateway can escalate dramatically.\nIf an MCP client is presented with hundreds of functions via the Gateway MCP, its operational efficiency may suffer degradation.\n\nJungleHub-Registry facilitates the exposure of a precisely curated subset of the total functions to your MCP clients via **Functionality Groups**.\n\nYou define a new group and specify precisely which functions are permitted entry into that group.\n\nUpon group creation, JungleHub-Registry issues a unique endpoint URL specific to that group.\n\nYour MCP client can then be configured to utilize this group-scoped endpoint instead of the primary, all-inclusive gateway address.\n\n### Creating a Functionality Group\nA new functionality group is instantiated by submitting a JSON manifest file to the `create group` command.\n\nYou must declare a unique `name` for the group and specify the included functions using one or more of the following criteria fields:\n- **`included_functions`**: An explicit list of function identifiers to include (e.g., `[\"filesystem__read_file\", \"time__get_current_time\"]`)\n- **`included_services`**: Designate ALL functions sourced from specific MCP service aliases (e.g., `[\"time\", \"deepwiki\"]`)\n- **`excluded_functions`**: Specify functions to omit (particularly useful when incorporating entire services)\n\n#### Example 1: Selective Function Inclusion\nConsider this function group configuration file (`claude-functions-group.json`):\n\n{\n  \"name\": \"claude-functions\",\n  \"description\": \"This group isolates functions intended for use by Claude Desktop\",\n  \"included_functions\": [\n    \"filesystem__read_file\",\n    \"deepwiki__read_wiki_contents\",\n    \"time__get_current_time\"\n  ]\n}\n\n\nThis configuration exposes only three carefully selected functions, excluding the vast majority of those registered in JungleHub-Registry.\n\n#### Example 2: Including Entire Services with Exemptions\nIt is possible to include all functions from designated services while optionally excluding specific ones:\n\n{\n  \"name\": \"claude-functions\",\n  \"description\": \"All functions from time and deepwiki services, excluding time__convert_time\",\n  \"included_services\": [\"time\", \"deepwiki\"],\n  \"excluded_functions\": [\"time__convert_time\"]\n}\n\n\nThis incorporates every function from the `time` and `deepwiki` services, with the sole exception of `time__convert_time`.\n\n#### Example 3: Composite Inclusion Logic\nAll three criteria can be combined for maximal configuration specificity:\n\n{\n  \"name\": \"comprehensive-functions\",\n  \"description\": \"A blend of manually selected functions, bulk service inclusion, and specific exclusions\",\n  \"included_functions\": [\"filesystem__read_file\"],\n  \"included_services\": [\"time\"],\n  \"excluded_functions\": [\"time__convert_time\"]\n}\n\n\nThis set includes `filesystem__read_file` plus every function from the `time` service, except for `time__convert_time`.\n\nYou create this group within JungleHub-Registry:\nbash\n$ junglehub create group -c ./claude-functions-group.json\n\nFunctionality Group claude-functions successfully provisioned\nIt is reachable via the following streamable http endpoint:\n\n    http://127.0.0.1:8080/v0/groups/claude-functions/mcp\n\n\n\nSubsequently, configure Claude (or any other MCP client) to route its requests to this group-specific address to access the restricted function set.\n\nThe client will exclusively perceive and be capable of invoking only these three functions, remaining oblivious to the broader set cataloged in JungleHub-Registry.\n\n\u003e [!TIP]\n\u003e Execute `junglehub list functions` to review the complete catalog of available identifiers and select those appropriate for your group definitions.\n\nYou can also watch a [Video Tutorial on Utilizing Functionality Groups](https://youtu.be/A21rfGgo38A).\n\n\u003e [!NOTE]\n\u003e Exclusion logic is applied as the final filtering step. If a function is explicitly listed in `included_functions` but also appears in `excluded_functions`, it will ultimately be omitted from the resultant group definition.\n\n### Managing Functionality Groups\nAdministrative actions supported include listing all defined groups, retrieving detailed information for a specific group, and permanently removing a group.\n\nbash\n# list all defined functionality groups\njunglehub list groups\n\n# retrieve metadata for a specific group\njunglehub get group claude-functions\n\n# delete a group\njunglehub delete group claude-functions\n\n\n### Working with Functions within Groups\nFunctions can be listed and invoked scoped specifically to a group using the `--group` parameter:\n\nbash\n# list functions confined to a specific group\njunglehub list functions --group claude-functions\n\n# invoke a function within the specific group context\njunglehub invoke filesystem__read_file --group claude-functions --payload '{\"path\": \"README.md\"}'\n\n\nThese group-scoped commands aid in validating function accessibility and ensuring correct operational context when interacting with filtered sets.\n\n\u003e [!NOTE]\n\u003e If a function designated for inclusion in a group is subsequently deactivated globally or the defining service is removed, that function ceases to be available via the group's MCP interface.\n\u003e However, should the function be reactivated or its service re-enrolled later, its availability within the group is instantaneously restored.\n\n**Constraints** üöß\n1. Updating the definition of an existing functionality group is not currently supported; deletion followed by recreation with the revised configuration file is required.\n2. In the `enterprise` operational mode, group creation is presently restricted to administrative accounts. We are developing mechanisms to permit standard users to provision their own specialized groups.\n\n## Credential Verification Mechanisms\nJungleHub-Registry incorporates support for authentication when connecting to Streamable HTTP MCP services that rely on static bearer tokens for securing access.\n\nThis is particularly beneficial for integrating external, cloud-hosted MCP services such as those provided by HuggingFace or Stripe, which necessitate an API key for authorization.\n\nThe credential can be supplied during the service registration process:\nbash\n# Supplying the `--bearer-token` argument instructs JungleHub-Registry to inject the `Authorization: Bearer \u003ctoken\u003e` header into all outbound communications to this specific service endpoint.\njunglehub register --alias huggingface --description \"HuggingFace MCP Broker\" --endpoint https://huggingface.co/mcp --bearer-token \u003cyour-hf-api-token\u003e\n\n\nAlternatively, integrate the token within the service registration manifest:\nbash\n{\n  \"alias\": \"huggingface\",\n  \"transport\": \"streamable_http\",\n  \"endpoint\": \"https://huggingface.co/mcp\",\n  \"description\": \"hugging face mcp server\",\n  \"bearer_token\": \"\u003cyour-hf-api-token\u003e\"\n}\n\n\nSupport for full OAuth authorization flows is scheduled for a subsequent release!\n\n## Advanced Organizational Features üîí\nWhen deploying JungleHub-Registry within an organizational setting, utilizing the `enterprise` operational mode is strongly recommended:\nbash\n# Activate advanced organizational features by starting in enterprise mode\njunglehub start --enterprise\n\n# Alternatively, set the operational mode via environment variable (valid values: `development` and `enterprise`)\nexport SERVER_MODE=enterprise\njunglehub start\n\n# Or utilize the enterprise Docker Compose manifest detailed previously\ndocker compose -f docker-compose.prod.yaml up -d\n\n\nBy default, the service runs in `development` mode, which is optimized for local, individual use.\n\nIn Enterprise mode, the server enforces stricter security protocols and unlocks advanced capabilities including external credential handling, Access Control Lists (ACLs), and advanced observability tooling.\n\nAfter initiating the service in enterprise mode, you must perform an initial server setup from a client machine:\nbash\njunglehub init-server\n\n\nThis command establishes a primary administrative identity on the server and securely caches its corresponding API access token in your local configuration file (`~/.mcpjungle.conf`).\n\nThis token then permits authenticated interactions with the server via the `junglehub` CLI.\n\n### Authorization Controls\nIn `development` mode, every connected MCP client is granted unrestricted access to every registered MCP service via the JungleHub-Registry Proxy.\n\n`enterprise` mode introduces granular control, enabling you to dictate precisely which MCP clients can interface with specific MCP services.\n\nAssume you have registered two services, `calculator` and `codebase`, in enterprise mode.\nBy default, no MCP client possesses access rights to these services. **You must explicitly define MCP Clients within JungleHub-Registry and grant them explicit permissions to the desired services.**\n\nbash\n# Provision a new MCP client intended for use by the Cursor IDE; grant it access to calculator and codebase services\njunglehub create mcp-client cursor-local --allow \"calculator, codebase\"\n\nMCP client 'cursor-local' instantiated successfully!\nPermitted Services: calculator,codebase\n\nAccess Token: 1YHf2LwE1LXtp5lW_vM-gmdYHlPHdqwnILitBhXE4Aw\nThis token must be transmitted in the `Authorization: Bearer {token}` HTTP header for subsequent requests.\n\n\nJungleHub-Registry generates a unique access token for the client. This token must be supplied by the client configuration when making requests to the JungleHub-Registry proxy, typically in the `Authorization` header.\n\nFor instance, configuring Cursor to connect securely to JungleHub-Registry might involve this addition:\n\n\n{\n  \"mcpServers\": {\n    \"junglehub-proxy\": {\n      \"url\": \"http://localhost:8080/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer 1YHf2LwE1LXtp5lW_vM-gmdYHlPHdqwnILitBhXE4Aw\"\n      }\n    }\n  }\n}\n\n\nA client successfully authenticated against a specific service via this token gains full visibility and invocation rights for all functions belonging to that authorized service.\n\n\u003e [!NOTE]\n\u003e If the `--allow` flag is omitted during client creation, the new MCP client will be denied access to all registered MCP services by default.\n\n### Telemetry and Tracing (OpenTelemetry)\nJungleHub-Registry incorporates Prometheus-compatible OpenTelemetry Metrics support for comprehensive system diagnostics.\n\n- In `enterprise` mode, OpenTelemetry telemetry collection is activated automatically.\n- In `development` mode, telemetry is dormant by default. Activation requires setting the `OTEL_ENABLED` environment variable to `true` prior to service startup:\n\nbash\n# Activate OpenTelemetry metrics collection\nexport OTEL_ENABLED=true\n\n# Optionally, append supplementary metadata to every metric point generated\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment.name=enterprise\n\n# Initiate the service\njunglehub start\n\n\nOnce the JungleHub-Registry service is running, telemetry data streams are available on the `/metrics` endpoint.\n\n# Current Known Constraints üöß\nWe are continuously refining the platform, though certain limitations persist:\n\n### 1. JungleHub-Registry does not sustain persistent network links to registered MCP Services\nWhen invoking a function from a Streamable HTTP service, JungleHub-Registry establishes a transient connection to handle the request.\n\nSimilarly, invoking a function from a STDIO service mandates the initiation of a new communication channel and the launch of a new subprocess to execute that service.\n\nUpon request servicing, this temporary subprocess is terminated.\n\nTherefore, a new stdio process is initiated for every single function call.\n\nThis pattern introduces moderate execution overhead but guarantees system stability by precluding resource leakage. It also means that stateful interactions with your MCP providers are currently unsupported.\n\nWe intend to enhance this connection methodology in upcoming releases and welcome community suggestions on ideal state management approaches!\n\n### 2. Lack of Integrated OAuth Flow Support for Service Authentication.\nSupport for standardized OAuth grant flows is actively under development.\n\nWe are gathering data on diverse organizational requirements for OAuth integration with external MCP services; please contribute your specific workflow via a Discussion thread or issue report to inform our implementation strategy.\n\n# Community Involvement üíª\nContributions from the wider community are highly valued!\n\n- **For contribution protocols and style mandates**, consult [CONTRIBUTION.md](./CONTRIBUTION.md)\n- **For development environment setup and architectural insights**, see [DEVELOPMENT.md](./DEVELOPMENT.md)\n\nJoin our [Discord community](https://discord.gg/CapV4Z3krk) to network with other developers and project maintainers.\n\n\n\n== See also ==\nRating site\nReview site\nTestFreaks, product review aggregator company\n\n\n== References ==\n\n\n=== Bibliography ===\nNeedleman, Rafe (20 September 2006). \"Wize: tallies user feedback\". cnet.com. CBS Interactive. Archived from the original on 17 August 2010. Retrieved 18 July 2010.\nNeedleman, Rafe (19 October 2006). \"Still more reviews aggregators: Retrevo, DigitalAdvisor, and TheFind\". cnet.com. CBS Interactive. Archived from the original on 16 August 2010. Retrieved 18 July 2010.\n",
      "stars": 571,
      "updated_at": "2025-10-04T09:50:29Z",
      "url": "https://github.com/duaraghav8/MCPJungle"
    },
    "glenngillen--mcpmcp-server": {
      "category": "aggregators",
      "description": "A list of MCP servers so you can ask your client which servers you can use to improve your daily workflow.",
      "forks": 8,
      "imageUrl": "",
      "keywords": [
        "mcpmcp",
        "mcp",
        "servers",
        "mcp servers",
        "mcpmcp server",
        "mcp server"
      ],
      "language": "",
      "license": "Apache License 2.0",
      "name": "mcpmcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "glenngillen",
      "readme_content": "# mcpmcp-server\n\nDiscover, setup, and integrate MCP servers with your favorite clients. Unlock the full potential of AI in your daily workflow.\n\n## Installation/usage:\n\nUpdate the configuration of your MCP client to the following: \n\n```json\n{\n  \"mcpServers\": {\n    \"mcpmcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://mcpmcp.io/mcp\"]\n    }\n  }\n}\n```\n\n(**note:** this config definitely works for Claude Desktop on macOS. If you need variations for other apps or platforms check the [homepage](https://mcpmcp.io/#install)\n",
      "stars": 20,
      "updated_at": "2025-09-30T09:22:38Z",
      "url": "https://github.com/glenngillen/mcpmcp-server"
    },
    "hamflx--imagen3-mcp": {
      "category": "aggregators",
      "description": "A robust image synthesis engine leveraging Google's cutting-edge Imagen 3.0 architecture via the Model Control Protocol (MCP) interface. Facilitates the creation of visually stunning imagery from textual descriptions, offering granular command over photographic aesthetics, artistic styles, and photorealism.",
      "forks": 7,
      "imageUrl": "",
      "keywords": [
        "imagen3",
        "imagen",
        "images",
        "imagen3 mcp",
        "imagen api",
        "google imagen"
      ],
      "language": "Rust",
      "license": "No License",
      "name": "Imagen3-Service-Facilitator",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "hamflx",
      "readme_content": "# Imagen3-Service-Facilitator (MCP Adapter)\n\n[English Version](#imagen3-service-facilitator-english)\n\nÂà©Áî® Google ÊúÄÊñ∞ÁöÑ Imagen 3.0 Ê®°ÂûãÁöÑÂõæÂÉèÁîüÊàêÊúçÂä°Êé•Âè£ÔºåÈÄöËøá MCPÔºàÊ®°ÂûãÊéßÂà∂ÂçèËÆÆÔºâËøõË°åÂ∞ÅË£ÖÂíåË∞ÉÁî®„ÄÇ\n\n## Á§∫‰æãËæìÂá∫\n\nÁîüÊàê‰∏ÄÂè™Ê≠£Âú®ÁñæÈ©∞ÁöÑÊù∞ÂÖãÁΩóÁ¥†Ê¢óÁä¨ÔºåÈááÁî®ÈïøÁÑ¶ÈïúÂ§¥Ê®°ÊãüÔºåÂÖâÁ∫øÁ©øËøáÂÖ∂ÁöÆÊØõÁöÑÁªÜËäÇÔºåËææÂà∞ÁÖßÁâáÁ∫ßÈÄºÁúüÂ∫¶„ÄÇ\n\nÁªòÂà∂‰∏Ä‰∏™ÂÖ∑ÊúâÊú™Êù•‰∏ª‰πâÁßëÊäÄÁæéÂ≠¶ÁöÑËãπÊûúÂΩ¢Ë±°„ÄÇ\n\n## ÂâçÁΩÆÊù°‰ª∂\n\n* ÂøÖÈúÄÂÖ∑Â§áÊúâÊïàÁöÑ [Google Gemini API ËÆøÈóÆÂá≠ËØÅ](https://aistudio.google.com/apikey)\n\n## ÈÖçÁΩÆÊåáÂçó‚Äî‚ÄîCherry Studio ÁéØÂ¢É\n\n1. ‰ªé [GitHub Releases](https://github.com/hamflx/imagen3-mcp/releases) È°µÈù¢Ëé∑ÂèñÊúÄÊñ∞ÁºñËØëÂ•ΩÁöÑ‰∫åËøõÂà∂Êñá‰ª∂„ÄÇ\n2. Â∞ÜÊ≠§ÂèØÊâßË°åÊñá‰ª∂ÊîæÁΩÆ‰∫éÁ≥ªÁªüË∑ØÂæÑ‰∏ãÁöÑ‰ªªÊÑèÂÆâÂÖ®‰ΩçÁΩÆÔºå‰æãÂ¶Ç `C:\\toolchain\\imagen3-mcp.exe`„ÄÇ\n3. Âú® Cherry Studio ÁïåÈù¢‰∏≠ËøõË°å‰ª•‰∏ãËÆæÁΩÆÔºö\n   - **Command Â≠óÊÆµ**: Â°´ÂÖ•‰∏ä‰∏ÄÊ≠•ÊîæÁΩÆÁöÑÂèØÊâßË°åÊñá‰ª∂ÂÆåÊï¥Ë∑ØÂæÑÔºå‰æãÂ¶Ç `C:\\toolchain\\imagen3-mcp.exe`„ÄÇ\n   - **ÁéØÂ¢ÉÂèòÈáè `GEMINI_API_KEY`**: ËæìÂÖ•ÊÇ®ÁöÑÊéàÊùÉ API ÂØÜÈí•„ÄÇ\n   - **[ÂèØÈÄâ] ÁéØÂ¢ÉÂèòÈáè `BASE_URL`**: ÈÖçÁΩÆËØ∑Ê±Ç‰ª£ÁêÜÂú∞ÂùÄÔºàÂ¶ÇÈúÄË¶ÅËßÑÈÅøÂú∞ÂüüÈôêÂà∂ÔºâÔºå‰æãÂ¶Ç `https://proxy.your-service.net/api/provider/google`„ÄÇ\n   - **[ÂèØÈÄâ] ÁéØÂ¢ÉÂèòÈáè `SERVER_LISTEN_ADDR`**: ÂÆö‰πâÂêéÁ´ØÊúçÂä°ÁõëÂê¨ÁöÑÁΩëÁªúÊé•Âè£Âú∞ÂùÄÔºàÈªòËÆ§ÁªëÂÆö `127.0.0.1`Ôºâ„ÄÇ\n   - **[ÂèØÈÄâ] ÁéØÂ¢ÉÂèòÈáè `SERVER_PORT`**: ÈÖçÁΩÆÊúçÂä°ÁõëÂê¨Á´ØÂè£ÔºåËØ•Á´ØÂè£‰πüÂ∞ÜÁî®‰∫éÁîüÊàêÂõæÁâáÁöÑ URL ÂºïÁî®ÔºàÈªòËÆ§ÂÄº‰∏∫ `9981`Ôºâ„ÄÇ\n   - **[ÂèØÈÄâ] ÁéØÂ¢ÉÂèòÈáè `IMAGE_RESOURCE_SERVER_ADDR`**: Áî®‰∫éÊûÑÈÄ†ÂõæÁâáËÆøÈóÆÈìæÊé•ÁöÑÂü∫Á°ÄÂú∞ÂùÄÔºàÈªòËÆ§ `127.0.0.1`Ôºâ„ÄÇÊ≠§È°πÂú®ÊúçÂä°ÈÉ®ÁΩ≤‰∫éÂÆπÂô®ÊàñËøúÁ®ãÊúçÂä°Âô®Êó∂Â∞§ÂÖ∂ÂÖ≥ÈîÆ„ÄÇ\n\n\n\n## ÈÖçÁΩÆÊåáÂçó‚Äî‚ÄîCursor IDE ÁéØÂ¢É\n\n\n{\n  \"mcpServers\": {\n    \"imagen3_generator\": {\n      \"command\": \"C:\\\\toolchain\\\\imagen3-mcp.exe\",\n      \"env\": {\n        \"GEMINI_API_KEY\": \"\u003cYOUR_GEMINI_SECRET\u003e\"\n        // ÈôÑÂä†ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆÁ§∫‰æã:\n        // \"BASE_URL\": \"\u003cPROXY_ENDPOINT\u003e\",\n        // \"SERVER_LISTEN_ADDR\": \"0.0.0.0\", // ÁõëÂê¨ÊâÄÊúâÂèØÁî®Êé•Âè£\n        // \"SERVER_PORT\": \"9981\",\n        // \"IMAGE_RESOURCE_SERVER_ADDR\": \"img.myhost.com\" // ‰ΩøÁî®ÂüüÂêç‰Ωú‰∏∫ËµÑÊ∫êÂú∞ÂùÄ\n      }\n    }\n  }\n}\n\n\n## Áü•ËØÜ‰∫ßÊùÉÂ£∞Êòé\n\nÈÅµÂæ™ MIT ÂçèËÆÆ„ÄÇ\n\n---\n\n# Imagen3-Service-Facilitator (English)\n\nAn infrastructure component built around Google's state-of-the-art Imagen 3.0 for image synthesis, exposed via the Model Control Protocol (MCP).\n\n## Operational Examples\n\nA depiction of a rapidly moving Jack Russell Terrier, utilizing a telephoto lens perspective, emphasizing sunlight interaction with the coat texture, rendered with photographic veracity.\n\nAn abstract representation of an apple infused with advanced technological motifs.\n\n## Prerequisites\n\n- A valid, authenticated [Google Gemini API credential](https://aistudio.google.com/apikey)\n\n## Deployment Instructions‚ÄîCherry Studio Workbench\n\n1. Obtain the most recent compiled binary package from the [GitHub Releases](https://github.com/hamflx/imagen3-mcp/releases) repository.\n2. Install the binary file in a persistent, accessible location on your machine, such as `C:\\toolchain\\imagen3-mcp.exe`.\n3. Configure the integration within the Cherry Studio environment:\n   - **Command Field**: Specify the absolute path to the executable, e.g., `C:\\toolchain\\imagen3-mcp.exe`.\n   - **Environment Variable `GEMINI_API_KEY`**: Input your granted API key.\n   - **[Optional] Environment Variable `BASE_URL`**: Set a network transit point if needed, e.g., `https://gateway.external-proxy.net`.\n   - **[Optional] Environment Variable `SERVER_LISTEN_ADDR`**: Dictates the network interface IP the internal web server binds to (default is `127.0.0.1`).\n   - **[Optional] Environment Variable `SERVER_PORT`**: Defines the communication port for the service and the port embedded in generated image URLs (default is `9981`).\n   - **[Optional] Environment Variable `IMAGE_RESOURCE_SERVER_ADDR`**: The host address used when constructing URLs pointing back to the generated media (default `127.0.0.1`). This is critical for externally accessible deployments.\n\n\n\n## Deployment Instructions‚ÄîCursor IDE Integration\n\n\n{\n  \"mcpServers\": {\n    \"imagen3_generator\": {\n      \"command\": \"C:\\\\toolchain\\\\imagen3-mcp.exe\",\n      \"env\": {\n        \"GEMINI_API_KEY\": \"\u003cYOUR_GEMINI_SECRET\u003e\"\n        // Supplementary configuration variables:\n        // \"BASE_URL\": \"\u003cPROXY_ENDPOINT\u003e\",\n        // \"SERVER_LISTEN_ADDR\": \"0.0.0.0\", // Bind to all network interfaces\n        // \"SERVER_PORT\": \"9981\",\n        // \"IMAGE_RESOURCE_SERVER_ADDR\": \"media.myplatform.io\" // Use a public domain for access\n      }\n    }\n  }\n}\n\n\n## Licensing\n\nMIT License\n",
      "stars": 43,
      "updated_at": "2025-09-20T19:36:59Z",
      "url": "https://github.com/hamflx/imagen3-mcp"
    },
    "julien040--anyquery": {
      "category": "aggregators",
      "description": "Query more than 40 apps with one binary using SQL. It can also connect to your PostgreSQL, MySQL, or SQLite compatible database. Local-first and private by design.",
      "forks": 82,
      "imageUrl": "",
      "keywords": [
        "anyquery",
        "aggregators",
        "servers",
        "aggregators servers",
        "anyquery query",
        "mcp server"
      ],
      "language": "Go",
      "license": "Other",
      "name": "anyquery",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "julien040",
      "readme_content": "# Anyquery\n\n\u003cimg src=\"https://anyquery.dev/images/logo-shadow.png\" alt=\"Anyquery logo\" width=\"96\"\u003e\u003c/img\u003e\n\n![GitHub Downloads (all assets, all releases)](https://img.shields.io/github/downloads/julien040/anyquery/total)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/m/julien040/anyquery)\n[![Documentation](https://img.shields.io/badge/documentation-blue)](https://anyquery.dev)\n[![GitHub issues](https://img.shields.io/github/issues/julien040/anyquery)](https://github.com/julien040/anyquery/issues)\n[![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fregistry.anyquery.dev%2Fv0%2Fregistry%2F\u0026query=%24.plugins_count\u0026label=Integrations%20count\u0026cacheSeconds=3600)](https://anyquery.dev/integrations/)\n[![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fregistry.anyquery.dev%2Fv0%2Fquery%2F\u0026query=%24.queries_count\u0026style=flat\u0026label=Queries%20from%20the%20hub\u0026cacheSeconds=3600\u0026link=https%3A%2F%2Fanyquery.dev%2Fqueries)](https://anyquery.dev/queries)\n[![Go Reference](https://pkg.go.dev/badge/github.com/julien040/anyquery@v0.1.3/namespace.svg)](https://pkg.go.dev/github.com/julien040/anyquery/namespace)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/julien040/anyquery)](https://archestra.ai/mcp-catalog/julien040__anyquery)\n\nAnyquery is a SQL query engine that allows you to run SQL queries on pretty much anything. It supports querying [files](https://anyquery.dev/docs/usage/querying-files/), [databases](https://anyquery.dev/docs/database), and [apps](https://anyquery.dev/integrations) (e.g. Apple Notes, Notion, Chrome, Todoist, etc.). It's built on top of [SQLite](https://www.sqlite.org) and uses [plugins](https://anyquery.dev/integrations) to extend its functionality.\n\nIt can also connect to [LLMs](https://anyquery.dev/llm) (e.g. ChatGPT, Claude, Cursor, TypingMind, etc.) to allow them to access your data.\n\nFinally, it can act as a [MySQL server](https://anyquery.dev/docs/usage/mysql-server/), allowing you to run SQL queries from your favorite MySQL-compatible client (e.g. [TablePlus](https://anyquery.dev/connection-guide/tableplus/), [Metabase](https://anyquery.dev/connection-guide/metabase/), etc.).\n\n![Anyquery header](https://anyquery.dev/images/release-header.png)\n\n## Usage\n\n### Connecting LLM\n\nLLMs can connect to Anyquery using the [Model Context Protocol (MCP)](https://anyquery.dev/docs/reference/commands/anyquery_mcp). This protocol provides context for LLMs that support it. You can start the MCP server with the following command:\n\n```bash\n# To be started by the LLM client\nanyquery mcp --stdio\n# To connect using an HTTP and SSE tunnel\nanyquery mcp --host 127.0.0.1 --port 8070\n```\n\nYou can also connect to clients that supports function calling (e.g. ChatGPT, TypingMind). Refer to each [connection guide](https://anyquery.dev/integrations#llm) in the documentation for more information.\n\n```bash\n# Copy the ID returned by the command, and paste it in the LLM client (e.g. ChatGPT, TypingMind)\nanyquery gpt\n```\n\n![5ire example](https://anyquery.dev/images/docs/llm/5ire-final.png)\n\n### Running SQL queries\n\nThe [documentation](https://anyquery.dev/docs/usage/running-queries) provides detailed instructions on how to run queries with Anyquery.\nBut let's see a quick example. Type `anyquery` in your terminal to open the shell mode. Then, run the following query:\n\n![Anyquery SQL examples](https://anyquery.dev/images/anyquery_examples.sql.png)\n\nYou can also launch the MySQL server with `anyquery server` and connect to it with your favorite MySQL-compatible client.\n\n```bash\nanyquery server \u0026\nmysql -u root -h 127.0.0.1 -P 8070\n```\n\n## Installation\n\nThe [documentation](https://anyquery.dev/docs/#installation) provides detailed instructions on how to install Anyquery on your system. You can install anyquery from Homebrew, APT, YUM/DNF, Scoop, Winget and Chocolatey. You can also download the binary from the [releases page](https://github.com/julien040/anyquery/releases).\n\n### Homebrew\n\n```zsh\nbrew install anyquery\n```\n\u003c!-- \n### Snap\n\n```bash\nsudo snap install anyquery\n``` --\u003e\n\n### APT\n\n```bash\necho \"deb [trusted=yes] https://apt.julienc.me/ /\" | sudo tee /etc/apt/sources.list.d/anyquery.list\nsudo apt update\nsudo apt install anyquery\n```\n\n### YUM/DNF\n\n```bash\necho \"[anyquery]\nname=Anyquery\nbaseurl=https://yum.julienc.me/\nenabled=1\ngpgcheck=0\" | sudo tee /etc/yum.repos.d/anyquery.repo\nsudo dnf install anyquery\n```\n\n### Scoop\n\n```powershell\nscoop bucket add anyquery https://github.com/julien040/anyquery-scoop\nscoop install anyquery\n```\n\n### Winget\n\n```powershell\nwinget install JulienCagniart.anyquery\n```\n\n### Chocolatey\n\n```powershell\nchoco install anyquery\n```\n\n## Plugins\n\nAnyquery is plugin-based, and you can install plugins to extend its functionality. You can install plugins from the [official registry](https://anyquery.dev/integrations) or create your own. Anyquery can also [load any SQLite extension](https://anyquery.dev/docs/usage/plugins#using-sqlite-extensions).\n\n![Integrations](https://anyquery.dev/images/integrations_logo.png)\n\n## License\n\nAnyquery is licensed under the AGPLv3 license for the core engine. The RPC library is licensed under the MIT license so that anyone can reuse plugins in different projects.\n\nThe plugins are not subject to the AGPL license. Each plugins has its own license and the copyright is owned by the plugin author.\nSee the [LICENSE](https://github.com/julien040/anquery/blob/main/LICENSE.md) file for more information.\n\n## Contributing\n\nIf you want to contribute to Anyquery, please read the [contributing guidelines](https://anyquery.dev/docs/developers/project/contributing). I currently only accept minor contributions, but I'm open to any suggestions or feedback.\n\nYou can have a brief overview of the project in the [architecture](https://anyquery.dev/docs/developers/project/architecture/) documentation.\n",
      "stars": 1360,
      "updated_at": "2025-10-04T11:11:01Z",
      "url": "https://github.com/julien040/anyquery"
    },
    "metatool-ai--metatool-app": {
      "category": "aggregators",
      "description": "MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.",
      "forks": 200,
      "imageUrl": "",
      "keywords": [
        "metamcp",
        "middleware",
        "mcp",
        "middleware mcp",
        "mcp server",
        "server metatool"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "metatool-app",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "metatool-ai",
      "readme_content": "# üöÄ MetaMCP (MCP Aggregator, Orchestrator, Middleware, Gateway in one docker)\n\n\u003cdiv align=\"center\"\u003e\n\n\u003cdiv align=\"center\"\u003e\n  \u003ca href=\"https://discord.gg/mNsyat7mFX\" style=\"text-decoration: none;\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Discord-MetaMCP-5865F2?style=flat-square\u0026logo=discord\u0026logoColor=white\" alt=\"Discord\" style=\"max-width: 100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://docs.metamcp.com\" style=\"text-decoration: none;\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Documentation-docs.metamcp.com-blue?style=flat-square\u0026logo=book\" alt=\"Documentation\" style=\"max-width: 100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://opensource.org/licenses/MIT\" style=\"text-decoration: none;\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square\" alt=\"MIT License\" style=\"max-width: 100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/metatool-ai/metamcp/pkgs/container/metamcp\" style=\"text-decoration: none;\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/GHCR-available-green.svg?style=flat-square\u0026logo=github\" alt=\"GHCR\" style=\"max-width: 100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://deepwiki.com/metatool-ai/metamcp\"\u003e\u003cimg src=\"https://img.shields.io/badge/DeepWiki-metatool--ai%2Fmetamcp-blue.svg?style=flat-square\u0026logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==\" alt=\"DeepWiki: MetaMCP\"\u003e\u003c/a\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e\n\n**MetaMCP** is a MCP proxy that lets you dynamically aggregate MCP servers into a unified MCP server, and apply middlewares. MetaMCP itself is a MCP server so it can be easily plugged into **ANY** MCP clients.\n\n\n\n---\n\nFor more details, consider visiting our documentation site: https://docs.metamcp.com\n\nEnglish | [‰∏≠Êñá](./README_cn.md)\n\n## üìã Table of Contents\n\n- [üéØ Use Cases](#-use-cases)\n- [üìñ Concepts](#-concepts)\n  - [üñ•Ô∏è MCP Server](#Ô∏è-mcp-server)\n  - [üè∑Ô∏è MetaMCP Namespace](#Ô∏è-metamcp-namespace)\n  - [üåê MetaMCP Endpoint](#-metamcp-endpoint)\n  - [‚öôÔ∏è Middleware](#Ô∏è-middleware)\n  - [üîç Inspector](#-inspector)\n- [üöÄ Quick Start](#-quick-start)\n  - [üê≥ Run with Docker Compose (Recommended)](#-run-with-docker-compose-recommended)\n  - [üíª Local Development](#-local-development)\n- [üîå MCP Protocol Compatibility](#-mcp-protocol-compatibility)\n- [üîó Connect to MetaMCP](#-connect-to-metamcp)\n  - [üìù E.g., Cursor via mcp.json](#-eg-cursor-via-mcpjson)\n  - [üñ•Ô∏è Connecting Claude Desktop and Other STDIO-only Clients](#Ô∏è-connecting-claude-desktop-and-other-stdio-only-clients)\n  - [üîß API Key Auth Troubleshooting](#-api-key-auth-troubleshooting)\n- [‚ùÑÔ∏è Cold Start Problem and Custom Dockerfile](#Ô∏è-cold-start-problem-and-custom-dockerfile)\n- [üîê Authentication](#-authentication)\n- [üîó OpenID Connect (OIDC) Provider Support](#-openid-connect-oidc-provider-support)\n  - [üõ†Ô∏è Configuration](#Ô∏è-configuration)\n  - [üè¢ Supported Providers](#-supported-providers)\n  - [üîí Security Features](#-security-features)\n  - [üì± Usage](#-usage)\n- [üåê Custom Deployment and SSE conf for Nginx](#-custom-deployment-and-sse-conf-for-nginx)\n- [üèóÔ∏è Architecture](#Ô∏è-architecture)\n  - [üìä Sequence Diagram](#-sequence-diagram)\n- [üó∫Ô∏è Roadmap](#Ô∏è-roadmap)\n- [üåê i18n](#-i18n)\n- [ü§ù Contributing](#-contributing)\n- [üìÑ License](#-license)\n- [üôè Credits](#-credits)\n\n\n## üéØ Use Cases\n- üè∑Ô∏è **Group MCP servers into namespaces, host them as meta-MCPs, and assign public endpoints** (SSE or Streamable HTTP), with auth. One-click to switch a namespace for an endpoint.\n-  üéØ **Pick tools you only need when remixing MCP servers.** Apply other **pluggable middleware** around observability, security, etc. (coming soon)\n-  üîç **Use as enhanced MCP inspector** with saved server configs, and inspect your MetaMCP endpoints in house to see if it works or not.\n-  üîç **Use as Elasticsearch for MCP tool selection** (coming soon)\n\nGenerally developers can use MetaMCP as **infrastructure** to host dynamically composed MCP servers through a unified endpoint, and build agents on top of it.\n\nQuick demo video: https://youtu.be/Cf6jVd2saAs\n\n\n\n## üìñ Concepts\n\n### üñ•Ô∏è **MCP Server**\nA MCP server configuration that tells MetaMCP how to start a MCP server.\n\n```json\n\"HackerNews\": {\n  \"type\": \"STDIO\",\n  \"command\": \"uvx\",\n  \"args\": [\"mcp-hn\"]\n}\n```\n\n#### üîê **Environment Variables \u0026 Secrets (STDIO MCP Servers)**\n\nFor **STDIO MCP servers**, MetaMCP supports three ways to handle environment variables and secrets:\n\n**1. Raw Values** - Direct string values (not recommended for secrets):\n```\nAPI_KEY=your-actual-api-key-here\nDEBUG=true\n```\n\n**2. Environment Variable References** - Use `${ENV_VAR_NAME}` syntax:\n```\nAPI_KEY=${OPENAI_API_KEY}\nDATABASE_URL=${DB_CONNECTION_STRING}\n```\n\n**3. Auto-matching** - If the expected environment variable name in your tool matches the container's environment variable, you can omit it entirely. MetaMCP will automatically pass through matching environment variables.\n\n\u003e **üîí Security Note**: Environment variable references (`${VAR_NAME}`) are resolved from the MetaMCP container's environment at runtime. This keeps actual secret values out of your configuration and git repository.\n\n\u003e **‚öôÔ∏è Development Note**: For local development with `pnpm run dev:docker`, ensure your environment variables are listed in `turbo.json` under `globalEnv` to be passed to the development processes. This is not required for production Docker deployments.\n\n### üè∑Ô∏è **MetaMCP Namespace**\n- Group one or more MCP servers into a namespace\n- Enable/disable MCP servers or at tool level\n- Apply middlewares to MCP requests and responses\n\n### üåê **MetaMCP Endpoint**\n- Create endpoints and assign namespace to endpoints\n- Multiple MCP servers in the namespace will be aggregated and emitted as a MetaMCP endpoint\n- Choose between API-Key Auth (in header or query param) or standard OAuth in MCP Spec 2025-06-18\n- Host through **SSE** or **Streamable HTTP** transports in MCP and **OpenAPI** endpoints for clients like [Open WebUI](https://github.com/open-webui/open-webui)\n\n### ‚öôÔ∏è **Middleware**\n- Intercepts and transforms MCP requests and responses at namespace level\n- **Built-in example**: \"Filter inactive tools\" - optimizes tool context for LLMs\n- **Future ideas**: tool logging, error traces, validation, scanning\n\n### üîç **Inspector**\nSimilar to the official MCP inspector, but with **saved server configs** - MetaMCP automatically creates configurations so you can debug MetaMCP endpoints immediately.\n\n## üöÄ Quick Start\n\n### **üê≥ Run with Docker Compose (Recommended)**\n\nClone repo, prepare `.env`, and start with docker compose:\n\n```bash\ngit clone https://github.com/metatool-ai/metamcp.git\ncd metamcp\ncp example.env .env\ndocker compose up -d\n```\n\nIf you modify APP_URL env vars, make sure you only access from the APP_URL, because MetaMCP enforces CORS policy on the URL, so no other URL is accessible.\n\nNote that the pg volume name may collide with your other pg dockers, which is global, consider rename it in `docker-compose.yml`:\n\n```\nvolumes:\n  metamcp_postgres_data:\n    driver: local\n```\n\n### **üíª Local Development**\n\nStill recommend running postgres through docker for easy setup:\n\n```bash\npnpm install\npnpm dev\n```\n\n## üîå MCP Protocol Compatibility\n\n- ‚úÖ **Tools, Resources, and Prompts** supported\n- ‚úÖ **OAuth-enabled MCP servers** tested for 03-26 version\n\nIf you have questions, feel free to leave **GitHub issues** or **PRs**.\n\n## üîó Connect to MetaMCP\n\n### üìù E.g., Cursor via mcp.json\n\nExample `mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"MetaMCP\": {\n      \"url\": \"http://localhost:12008/metamcp/\u003cYOUR_ENDPOINT_NAME\u003e/sse\"\n    }\n  }\n}\n```\n\n### üñ•Ô∏è Connecting Claude Desktop and Other STDIO-only Clients\n\nSince MetaMCP endpoints are remote only (SSE, Streamable HTTP, OpenAPI), clients that only support stdio servers (like Claude Desktop) need a local proxy to connect.\n\n**Note:** While `mcp-remote` is sometimes suggested for this purpose, it's designed for OAuth-based authentication and doesn't work with MetaMCP's API key authentication. Based on testing, `mcp-proxy` is the recommended solution.\n\nHere's a working configuration for Claude Desktop using `mcp-proxy`:\n\nUsing Streamable HTTP\n\n```json\n{\n  \"mcpServers\": {\n    \"MetaMCP\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-proxy\",\n        \"--transport\",\n        \"streamablehttp\",\n        \"http://localhost:12008/metamcp/\u003cYOUR_ENDPOINT_NAME\u003e/mcp\"\n      ],\n      \"env\": {\n        \"API_ACCESS_TOKEN\": \"\u003cYOUR_API_KEY_HERE\u003e\"\n      }\n    }\n  }\n}\n```\n\nUsing SSE\n\n```json\n{\n  \"mcpServers\": {\n    \"ehn\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-proxy\",\n        \"http://localhost:12008/metamcp/\u003cYOUR_ENDPOINT_NAME\u003e/sse\"\n      ],\n      \"env\": {\n        \"API_ACCESS_TOKEN\": \"\u003cYOUR_API_KEY_HERE\u003e\"\n      }\n    }\n  }\n}\n```\n\n**Important notes:**\n- Replace `\u003cYOUR_ENDPOINT_NAME\u003e` with your actual endpoint name\n- Replace `\u003cYOUR_API_KEY_HERE\u003e` with your MetaMCP API key (format: `sk_mt_...`)\n\nFor more details and alternative approaches, see [issue #76](https://github.com/metatool-ai/metamcp/issues/76#issuecomment-3046707532).\n\n### üîß API Key Auth Troubleshooting\n\n- `?api_key=` param api key auth doesn't work for SSE. It only works for Streamable HTTP and OpenAPI.\n- Best practice is to use the API key in `Authorization: Bearer \u003cAPI_KEY\u003e` header.\n- Try disable auth temporarily when you face connection issues to see if it is an auth issue.\n\n## ‚ùÑÔ∏è Cold Start Problem and Custom Dockerfile\n\n- MetaMCP pre-allocate idle sessions for each configured MCP servers and MetaMCPs. The default idle session for each is 1 and that can help reduce cold start time.\n- If your MCP requires dependencies other than `uvx` or `npx`, you need to customize the Dockerfile to install dependencies on your own.\n- Check [invalidation.md](invalidation.md) for a seq diagram about how idle session invalidates during updates.\n\nüõ†Ô∏è **Solution**: Customize the Dockerfile to add dependencies or pre-install packages to reduce cold start time.\n\n## üîê Authentication\n\n- üõ°Ô∏è **Better Auth** for frontend \u0026 backend (TRPC procedures)\n- üç™ **Session cookies** enforce secure internal MCP proxy connections\n- üîë **API key authentication** for external access via `Authorization: Bearer \u003capi-key\u003e` header\n- ü™™ **MCP OAuth**: Exposed endpoints have options to use standard OAuth in MCP Spec 2025-06-18, easy to connect.\n- üè¢ **Multi-tenancy**: Designed for organizations to deploy on their own machines. Supports both private and public access scopes. Users can create MCPs, namespaces, endpoints, and API keys for themselves or for everyone. Public API keys cannot access private MetaMCPs.\n- ‚öôÔ∏è **Separate Registration Controls**: Administrators can independently control UI registration and SSO/OAuth registration through the settings page, allowing for flexible enterprise deployment scenarios.\n\n## üîó OpenID Connect (OIDC) Provider Support\n\nMetaMCP supports **OpenID Connect authentication** for enterprise SSO integration. This allows organizations to use their existing identity providers (Auth0, Keycloak, Azure AD, etc.) for authentication.\n\n### üõ†Ô∏è **Configuration**\n\nAdd the following environment variables to your `.env` file:\n\n```bash\n# Required\nOIDC_CLIENT_ID=your-oidc-client-id\nOIDC_CLIENT_SECRET=your-oidc-client-secret\nOIDC_DISCOVERY_URL=https://your-provider.com/.well-known/openid-configuration\n\n# Optional customization\nOIDC_PROVIDER_ID=oidc\nOIDC_SCOPES=openid email profile\nOIDC_PKCE=true\n```\n\n### üè¢ **Supported Providers**\n\nMetaMCP has been tested with popular OIDC providers:\n\n- **Auth0**: `https://your-domain.auth0.com/.well-known/openid-configuration`\n- **Keycloak**: `https://your-keycloak.com/realms/your-realm/.well-known/openid-configuration`\n- **Azure AD**: `https://login.microsoftonline.com/your-tenant-id/v2.0/.well-known/openid-configuration`\n- **Google**: `https://accounts.google.com/.well-known/openid-configuration`\n- **Okta**: `https://your-domain.okta.com/.well-known/openid-configuration`\n\n### üîí **Security Features**\n\n- üîê **PKCE (Proof Key for Code Exchange)** enabled by default\n- üõ°Ô∏è **Authorization Code Flow** with automatic user creation\n- üîÑ **Auto-discovery** of OIDC endpoints\n- üç™ **Seamless session management** with existing auth system\n\n### üì± **Usage**\n\nOnce configured, users will see a **\"Sign in with OIDC\"** button on the login page alongside the email/password form. The authentication flow automatically creates new users on first login.\n\nFor more detailed configuration examples and troubleshooting, see **[CONTRIBUTING.md](CONTRIBUTING.md#openid-connect-oidc-provider-setup)**.\n\n## ‚öôÔ∏è Registration Controls\n\nMetaMCP provides **separate controls** for different registration methods, allowing administrators to fine-tune user access policies for enterprise deployments.\n\n### üéõÔ∏è **Available Controls**\n\n- **UI Registration**: Controls whether users can create accounts via the registration form\n- **SSO Registration**: Controls whether users can create accounts via SSO/OAuth providers (OIDC, etc.)\n\n### üè¢ **Enterprise Use Cases**\n\nThis separation enables common enterprise scenarios:\n\n- **Block UI registration, allow SSO**: Prevent manual signups while allowing corporate SSO users\n- **Block SSO registration, allow UI**: Allow manual signups while restricting SSO access\n- **Block both**: Completely disable new user registration\n- **Allow both**: Default behavior for open deployments\n\n### üõ†Ô∏è **Configuration**\n\nAccess the **Settings** page in the MetaMCP admin interface to configure these controls:\n\n1. Navigate to **Settings** ‚Üí **Authentication Settings**\n2. Toggle **\"Disable UI Registration\"** to control form-based signups\n3. Toggle **\"Disable SSO Registration\"** to control OAuth/OIDC signups\n\nBoth controls work independently, giving you full flexibility over your registration policy.\n\n## üåê Custom Deployment and SSE conf for Nginx\n\nIf you want to deploy it to a online service or a VPS, a instance of at least 2GB-4GB of memory is required. And the larger size, the better performance.\n\nSince MCP leverages SSE for long connection, if you are using reverse proxy like nginx, please refer to an example setup [nginx.conf.example](nginx.conf.example)\n\n## üèóÔ∏è Architecture\n\n- **Frontend**: Next.js\n- **Backend**: Express.js with tRPC, hosting MCPs through TS SDK and internal proxy\n- **Auth**: Better Auth\n- **Structure**: Standalone monorepo with Turborepo and Docker publishing\n\n### üìä Sequence Diagram\n\n*Note: Prompts and resources follow similar patterns to tools.*\n\n```mermaid\nsequenceDiagram\n    participant MCPClient as MCP Client (e.g., Claude Desktop)\n    participant MetaMCP as MetaMCP Server\n    participant MCPServers as Installed MCP Servers\n\n    MCPClient -\u003e\u003e MetaMCP: Request list tools\n\n    loop For each listed MCP Server\n        MetaMCP -\u003e\u003e MCPServers: Request list_tools\n        MCPServers -\u003e\u003e MetaMCP: Return list of tools\n    end\n\n    MetaMCP -\u003e\u003e MetaMCP: Aggregate tool lists \u0026 apply middleware\n    MetaMCP -\u003e\u003e MCPClient: Return aggregated list of tools\n\n    MCPClient -\u003e\u003e MetaMCP: Call tool\n    MetaMCP -\u003e\u003e MCPServers: call_tool to target MCP Server\n    MCPServers -\u003e\u003e MetaMCP: Return tool response\n    MetaMCP -\u003e\u003e MCPClient: Return tool response\n```\n\n## üó∫Ô∏è Roadmap\n\n**Potential next steps:**\n\n- [ ] üîå Headless Admin API access\n- [ ] üîç Dynamically apply search rules on MetaMCP endpoints\n- [ ] üõ†Ô∏è More middlewares\n- [ ] üí¨ Chat/Agent Playground\n- [ ] üß™ Testing \u0026 Evaluation for MCP tool selection optimization\n- [ ] ‚ö° Dynamically generate MCP servers\n\n## üåê i18n\n\nSee [README-i18n.md](README-i18n.md)\n\nCurrently en and zh locale are supported, but welcome contributions.\n\n## ü§ù Contributing\n\nWe welcome contributions! See details at **[CONTRIBUTING.md](CONTRIBUTING.md)**\n\n## üìÑ License\n\n**MIT**\n\nWould appreciate if you mentioned with back links if your projects use the code.\n\n## üôè Credits\n\nSome code inspired by:\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n- [MCP Proxy Server](https://github.com/adamwattis/mcp-proxy-server)\n\nNot directly used the code by took ideas from\n- https://github.com/open-webui/openapi-servers\n- https://github.com/open-webui/mcpo",
      "stars": 1416,
      "updated_at": "2025-10-04T03:33:25Z",
      "url": "https://github.com/metatool-ai/metatool-app"
    },
    "mindsdb--mindsdb": {
      "category": "aggregators",
      "description": "Establish a unified operational data layer via MindsDB, functioning as a central management point protocol (MCP) server to seamlessly integrate and query diverse data sources and databases.",
      "forks": 5823,
      "imageUrl": "",
      "keywords": [
        "mindsdb",
        "databases",
        "servers",
        "server mindsdb",
        "mindsdb connect",
        "databases mindsdb"
      ],
      "language": "Python",
      "license": "Other",
      "name": "mindsdb-unified-data-gateway",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "mindsdb",
      "readme_content": "\u003ca name=\"readme-top\"\u003e\u003c/a\u003e\n\n\u003cdiv align=\"center\"\u003e\n\t\u003ca href=\"https://pypi.org/project/MindsDB/\" target=\"_blank\"\u003e\u003cimg src=\"https://badge.fury.io/py/MindsDB.svg\" alt=\"MindsDB Release\"\u003e\u003c/a\u003e\n\t\u003ca href=\"https://www.python.org/downloads/\" target=\"_blank\"\u003e\u003cimg src=\"https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg\" alt=\"Python supported\"\u003e\u003c/a\u003e\n\t\u003ca href=\"https://hub.docker.com/u/mindsdb\" target=\"_blank\"\u003e\u003cimg src=\"https://img.shields.io/docker/pulls/mindsdb/mindsdb\" alt=\"Docker pulls\"\u003e\u003c/a\u003e\n\n  \u003cbr /\u003e\n  \u003cbr /\u003e\n\n  \u003ca href=\"https://trendshift.io/repositories/3068\" target=\"_blank\"\u003e\u003cimg src=\"https://trendshift.io/api/badge/repositories/3068\" alt=\"mindsdb%2Fmindsdb | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/\u003e\u003c/a\u003e\n\n  \u003ca href=\"https://github.com/mindsdb/mindsdb\"\u003e\n    \n  \u003c/a\u003e\n\n  \u003cp align=\"center\"\u003e\n    \u003cbr /\u003e\n    \u003ca href=\"https://www.mindsdb.com?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo\"\u003eOfficial Site\u003c/a\u003e\n    ¬∑\n    \u003ca href=\"https://docs.mindsdb.com?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo\"\u003eTechnical Documentation\u003c/a\u003e\n    ¬∑\n    \u003ca href=\"https://mindsdb.com/contact\"\u003eRequest a Demonstration\u003c/a\u003e\n    ¬∑\n    \u003ca href=\"https://mindsdb.com/joincommunity?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo\"\u003eCommunity Channel (Slack)\u003c/a\u003e\n  \u003c/p\u003e\n\u003c/div\u003e\n\n----------------------------------------\n\n\nMindsDB empowers users, intelligent agents, and sophisticated applications to derive highly precise insights from extensive collections of distributed data assets.\n\n\u003ca href=\"https://www.youtube.com/watch?v=MX3OKpnsoLM\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064\" alt=\"MindsDB Demo\"\u003e\n\t\n\u003c/a\u003e\n\n\n## Server Deployment\n\nMindsDB functions as an open-source orchestration engine, deployable across any environment, from local machines to scalable cloud infrastructure, with extensive customization capabilities.\n\n  * [Containerized Setup via Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the quickest, recommended pathway to initial operation.\n  * [Standard Docker Deployment](https://docs.mindsdb.com/setup/self-hosted/docker). Provides greater control for advanced server configurations.\n\n[The integrated MCP server within MindsDB](https://docs.mindsdb.com/mcp/overview) facilitates secure connectivity, data harmonization, and intelligent query resolution across federated datasets, including transactional databases, data warehouses, and SaaS platforms.\n \n----------------------------------------\n\n# Foundational Principles: Connect, Harmonize, Produce Output\n\nMindsDB's core architecture rests upon three primary functional pillars:\n\n## 1. Establish Connectivity (Integrate Data Sources) [See Integrations](https://docs.mindsdb.com/integrations/data-overview)\n\nIntegration support extends to hundreds of commercial and proprietary [data repositories](https://docs.mindsdb.com/integrations/data-overview). These connectors enable MindsDB to access data regardless of its physical location, forming the necessary infrastructure for subsequent operations.\n\n## 2. Data Harmonization and Synthesis [MindsDB SQL Overview](https://docs.mindsdb.com/mindsdb_sql/overview)\n\nEffective data synthesis often precedes meaningful output generation. MindsDB SQL provides constructs like knowledge bases and views designed for indexing and organizing both structured and unstructured information as if it resided within a singular, centralized system.\n\n* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) ‚Äì Mechanism for indexing and querying complex, unstructured data efficiently.\n* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) ‚Äì Abstract complexity by generating unified access points across disparate data origins (eliminating the need for traditional ETL).\n\nData consolidation workflows can be automated using scheduled operations:\n\n* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) ‚Äì Mechanisms for scheduling periodic data synchronization and transformation routines for near real-time data status.\n\n## 3. Insight Generation (Data Response) [Agents \u0026 MCP](https://docs.mindsdb.com/mindsdb_sql/agents/agent)\n\nInteract directly with your integrated data assets to derive answers:\n\n* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) ‚Äì Pre-configured, specialized AI entities designed to respond to complex inquiries based on your connected and synthesized data.\n* [**MCP INTERFACE**](https://docs.mindsdb.com/mcp/overview) ‚Äì Utilize the Model Context Protocol for standardized communication and interaction with the MindsDB engine.\n\n----------------------------------------\n\n## ü§ù Community Collaboration\n\nInterested in enhancing MindsDB? Please refer to our [development setup guide](https://docs.mindsdb.com/contribute/install?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo) to get started.\n\nYou can review our comprehensive [contribution guidelines here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo).\n\nWe value all suggestions! Open a new issue to present your concepts, and we will provide guidance. \n\nThis project strictly adheres to the established [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md).\n\nExplore our [community incentive programs](https://mindsdb.com/community?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo) for recognition.\n\n## ü§ç Support Channels\n\nIf you discover a defect, please file a [formal issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).\n\nFor community assistance, utilize these platforms:\n\n* Inquire in our [MindsDB Slack workspace](https://mindsdb.com/joincommunity?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo).\n* Engage in [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).\n* Post technical queries on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) using the designated tag.\n\nFor enterprise-grade technical assistance, please [reach out to the MindsDB team](https://mindsdb.com/contact?utm_medium=community\u0026utm_source=github\u0026utm_campaign=mindsdb%20repo).\n\n## üíö Active Contributors\n\n\u003ca href=\"https://github.com/mindsdb/mindsdb/graphs/contributors\"\u003e\n  \u003cimg alt=\"mindsdb\" src=\"https://contributors-img.web.app/image?repo=mindsdb/mindsdb\" /\u003e\n\u003c/a\u003e\n\nGenerated with [contributors-img](https://contributors-img.web.app).\n\n## üîî Stay Informed\n\nJoin our [Slack workspace](https://mindsdb.com/joincommunity) for latest updates.\n\nWIKIPEDIA: A review aggregator is a system that collects reviews and ratings of products and services, such as films, books, video games, music, software, hardware, or cars. This system then stores the reviews to be used for supporting a website where users can view the reviews, sells information to third parties about consumer tendencies, and creates databases for companies to learn about their actual and potential customers. The system enables users to easily compare many different reviews of the same work. Many of these systems calculate an approximate average assessment, usually based on assigning a numeric value to each review related to its degree of positive rating of the work.\nReview aggregation sites have begun to have economic effects on the companies that create or manufacture items under review, especially in certain categories such as electronic games, which are expensive to purchase. Some companies have tied royalty payment rates and employee bonuses to aggregate scores, and stock prices have been seen to reflect ratings, as related to potential sales. It is widely accepted in the literature that there is a strong correlation between sales and aggregated scores.\nDue to the influence, manufacturers are often interested in measuring these reviews for their own products. This is often done using a business-facing product review aggregator. In the film industry, according to Reuters, big studios pay attention to aggregators but \"they don't always like to assign much importance to them\". Movie Review Intelligence was a review aggregator website, which collated and analyzed movie reviews.\n\n\n== See also ==\nRating site\nReview site\nTestFreaks, product review aggregator company\n\n\n== References ==\n\n\n=== Bibliography ===\nNeedleman, Rafe (20 September 2006). \"Wize: tallies user feedback\". cnet.com. CBS Interactive. Archived from the original on 17 August 2010. Retrieved 18 July 2010.\nNeedleman, Rafe (19 October 2006). \"Still more reviews aggregators: Retrevo, DigitalAdvisor, and TheFind\". cnet.com. CBS Interactive. Archived from the original on 16 August 2010. Retrieved 18 July 2010.",
      "stars": 36251,
      "updated_at": "2025-10-04T12:39:17Z",
      "url": "https://github.com/mindsdb/mindsdb"
    },
    "particlefuture--MCPDiscovery": {
      "category": "aggregators",
      "description": "The Meta-Coordination Platform's definitive nexus. Functions as the primary repository for locating diverse MCP deployments, furnishing comprehensive guidance on their setup and operational utilization.",
      "forks": 0,
      "imageUrl": "",
      "keywords": [
        "mcps",
        "mcp",
        "aggregators",
        "mcp servers",
        "mcp server",
        "server particlefuture"
      ],
      "language": "Unknown",
      "license": "Unknown",
      "name": "CentralizedMetaCoordinationPlatform",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "particlefuture",
      "readme_content": "WIKIPEDIA: A system designed to consolidate appraisals and evaluations for various commodities and services, encompassing media such as motion pictures, literature, interactive entertainment software, musical compositions, digital applications, physical apparatus, or motor vehicles. This infrastructure subsequently archives these critiques to power a web portal where patrons can peruse the assessments, disseminates consumption pattern data to external entities, and constructs informational repositories assisting corporations in comprehending their extant and prospective clientele. The utility empowers end-users to effortlessly juxtapose numerous independent critiques pertaining to the identical work. A substantial number of these infrastructures compute an approximate consensus metric, typically derived by assigning a quantifiable value to each critique reflecting the intensity of its favorable judgment regarding the subject matter.\n\nThe confluence of critical assessments is beginning to exert measurable financial implications upon the enterprises responsible for originating or fabricating the evaluated articles, particularly within specific sectors like electronic gaming systems, which necessitate considerable capital expenditure. Certain manufacturers have linked the remittance schedules for royalties and the payout structure for employee incentives directly to the consolidated evaluation figures, and publicly traded valuations have demonstrably reacted to these ratings in correlation with projected sales volumes. Scholarly discourse widely affirms a robust interdependence between transactional volume and synthesized appraisal metrics.\n\nGiven this substantial leverage, manufacturers frequently dedicate resources to quantifying these appraisals concerning their proprietary offerings. This monitoring is often executed through a dedicated, business-oriented critique aggregation service. Within the cinematic domain, reports from Reuters indicate that major production houses pay heed to aggregators, though 'they are not uniformly inclined to grant them significant weight.' Movie Review Intelligence served as an example of such a platform, collecting and systematically analyzing film critiques.\n\n== Further Reading ==\nAssessment Venue\nCritique Portal\nTestFreaks, a commercial entity specializing in product appraisal aggregation\n\n== Citations ==\n\n=== Bibliographic Entries ===\nNeedleman, Rafe (September 20, 2006). \"Wize: tallies user feedback\". cnet.com. CBS Interactive. Archived from the original on August 17, 2010. Retrieved July 18, 2010.\nNeedleman, Rafe (October 19, 2006). \"Still more reviews aggregators: Retrevo, DigitalAdvisor, and TheFind\". cnet.com. CBS Interactive. Archived from the original on August 16, 2010. Retrieved July 18, 2010.",
      "stars": 0,
      "updated_at": "",
      "url": "https://github.com/particlefuture/MCPDiscovery"
    },
    "sitbon--magg": {
      "category": "aggregators",
      "description": "Magg: A meta-MCP server that acts as a universal hub, allowing LLMs to autonomously discover, install, and orchestrate multiple MCP servers - essentially giving AI assistants the power to extend their own capabilities on-demand.",
      "forks": 16,
      "imageUrl": "",
      "keywords": [
        "magg",
        "aggregators",
        "mcp",
        "mcp servers",
        "mcp server",
        "magg meta"
      ],
      "language": "Python",
      "license": "GNU Affero General Public License v3.0",
      "name": "magg",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "sitbon",
      "readme_content": "# üß≤ **Magg** - *The MCP Aggregator*\n\n[//]: # ([![Tests]\u0026#40;https://img.shields.io/github/actions/workflow/status/sitbon/magg/test.yml?style=flat-square\u0026label=tests\u0026#41;]\u0026#40;https://github.com/sitbon/magg/actions/workflows/test.yml\u0026#41;)\n[![Python Version](https://img.shields.io/pypi/pyversions/magg?style=flat-square\u0026logo=python\u0026logoColor=white)](https://pypi.org/project/magg/)\n[![PyPI Version](https://img.shields.io/pypi/v/magg?style=flat-square\u0026logo=pypi\u0026logoColor=white)](https://pypi.org/project/magg/)\n[![GitHub Release](https://img.shields.io/github/v/release/sitbon/magg?style=flat-square\u0026logo=github)](https://github.com/sitbon/magg/releases)\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-sitbon%2Fmagg-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/sitbon/magg)\n[![Downloads](https://img.shields.io/pypi/dm/magg?style=flat-square)](https://pypistats.org/packages/magg)\n\u003c!-- DeepWiki badge generated by https://deepwiki.ryoppippi.com/ --\u003e\n\n[![Tests](https://github.com/sitbon/magg/actions/workflows/test.yml/badge.svg)](https://github.com/sitbon/magg/actions/workflows/test.yml)\n[![Docker](https://github.com/sitbon/magg/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/sitbon/magg/actions/workflows/docker-publish.yml)\n\nA *[Model Context Protocol](https://modelcontextprotocol.io/)* server that manages, aggregates, and proxies other MCP servers, enabling LLMs to dynamically extend their own capabilities.\n\n## What is Magg?\n\nMagg is a meta-MCP server that acts as a central hub for managing multiple MCP servers. It provides tools that allow LLMs to:\n\n- Search for new MCP servers and discover setup instructions\n- Add and configure MCP servers dynamically\n- Enable/disable servers on demand\n- Aggregate tools from multiple servers under unified prefixes\n- Persist configurations across sessions\n\nThink of Magg as a \"package manager for LLM tools\" - it lets AI assistants install and manage their own capabilities at runtime.\n\n## Features\n\n- **Self-Service Tool Management**: LLMs can search for and add new MCP servers without human intervention.\n- **Dynamic Configuration Reloading**: Automatically detects and applies config changes without restarting.\n- **Automatic Tool Proxying**: Tools from added servers are automatically exposed with configurable prefixes.\n- **ProxyMCP Tool**: A built-in tool that proxies the MCP protocol to itself, for clients that don't support notifications or dynamic tool updates (which is most of them currently).\n- **Smart Configuration**: Uses MCP sampling to intelligently configure servers from just a URL.\n- **Persistent Configuration**: Maintains server configurations in `.magg/config.json`.\n- **Multiple Transport Support**: Works with stdio, HTTP, and in-memory transports.\n- **Bearer Token Authentication**: Optional RSA-based JWT authentication for secure HTTP access.\n- **Docker Support**: Pre-built images for production, staging, and development workflows.\n- **Health Monitoring**: Built-in `magg_status` and `magg_check` tools for server health checks.\n- **Real-time Messaging**: Full support for MCP notifications and messages - receive tool/resource updates and progress notifications from backend servers.\n- **Python 3.12+ Support**: Fully compatible with Python 3.12 and 3.13.\n- **Kit Management**: Bundle related MCP servers into kits for easy loading/unloading as a group.\n- **MBro CLI**: Included [MCP Browser](docs/mbro.md) for interactive exploration and management of MCP servers, with script support for automation.\n\n## Installation\n\n### Prerequisites\n\n- Python 3.12 or higher (3.13+ recommended)\n- `uv` (recommended) - Install from [astral.sh/uv](https://astral.sh/uv)\n\n### Quick Install (Recommended)\n\nThe easiest way to install Magg is as a tool using `uv`:\n\n```bash\n# Install Magg as a tool\nuv tool install magg\n\n# Run with stdio transport (for Claude Desktop, Cline, etc.)\nmagg serve\n\n# Run with HTTP transport (for system-wide access)\nmagg serve --http\n```\n\n### Alternative: Run Directly from GitHub\n\nYou can also run Magg directly from GitHub without installing:\n\n```bash\n# Run with stdio transport\nuvx --from git+https://github.com/sitbon/magg.git magg\n\n# Run with HTTP transport\nuvx --from git+https://github.com/sitbon/magg.git magg serve --http\n```\n\n### Local Development\n\nFor development, clone the repository and install in editable mode:\n\n```bash\n# Clone the repository\ngit clone https://github.com/sitbon/magg.git\ncd magg\n\n# Install in development mode with dev dependencies\nuv sync --dev\n\n# Or with poetry\npoetry install --with dev\n\n# Run the CLI\nmagg --help\n```\n\n### Docker\n\nMagg is available as pre-built Docker images from GitHub Container Registry:\n\n```bash\n# Run production image (WARNING log level)\ndocker run -p 8000:8000 ghcr.io/sitbon/magg:latest\n\n# Run with authentication (mount or set private key)\ndocker run -p 8000:8000 \\\n  -v ~/.ssh/magg:/home/magg/.ssh/magg:ro \\\n  ghcr.io/sitbon/magg:latest\n\n# Or with environment variable\ndocker run -p 8000:8000 \\\n  -e MAGG_PRIVATE_KEY=\"$(cat ~/.ssh/magg/magg.key)\" \\\n  ghcr.io/sitbon/magg:latest\n\n# Run beta image (INFO log level)\ndocker run -p 8000:8000 ghcr.io/sitbon/magg:beta\n\n# Run with custom config directory\ndocker run -p 8000:8000 \\\n  -v /path/to/config:/home/magg/.magg \\\n  ghcr.io/sitbon/magg:latest\n```\n\n#### Docker Image Strategy\n\nMagg uses a multi-stage Docker build with three target stages:\n\n- **`pro` (Production)**: Minimal image with WARNING log level, suitable for production deployments\n- **`pre` (Pre-production)**: Same as production but with INFO log level for staging/testing (available but not published)\n- **`dev` (Development)**: Includes development dependencies and DEBUG logging for troubleshooting\n\nImages are automatically published to GitHub Container Registry with the following tags:\n\n- **Version tags** (from main branch): `1.2.3`, `1.2`, `dev`, `1.2-dev`, `1.2-dev-py3.12`, etc.\n- **Branch tags** (from beta branch): `beta`, `beta-dev`\n- **Python-specific dev tags**: `beta-dev-py3.12`, `beta-dev-py3.13`, etc.\n\n#### Docker Compose\n\nFor easier management, use Docker Compose:\n\n```bash\n# Clone the repository\ngit clone https://github.com/sitbon/magg.git\ncd magg\n\n# Run production version\ndocker compose up magg\n\n# Run staging version (on port 8001)\ndocker compose up magg-beta\n\n# Run development version (on port 8008)\n# This uses ./.magg/config.json for configuration\ndocker compose up magg-dev\n\n# Build and run with custom registry\nREGISTRY=my.registry.com docker compose build\nREGISTRY=my.registry.com docker compose push\n```\n\nSee `compose.yaml` and `.env.example` for configuration options.\n\n## Usage\n\n### Running Magg\n\nMagg can run in three modes:\n\n1. **Stdio Mode** (default) - For integration with Claude Desktop, Cline, Cursor, etc.:\n   ```bash\n   magg serve\n   ```\n\n2. **HTTP Mode** - For system-wide access or web integrations:\n   ```bash\n   magg serve --http --port 8000\n   ```\n\n3. **Hybrid Mode** - Both stdio and HTTP simultaneously:\n   ```bash\n   magg serve --hybrid\n   magg serve --hybrid --port 8080  # Custom port\n   ```\n   \n   This is particularly useful when you want to use Magg through an MCP client while also allowing HTTP access. For example:\n   \n   **With Claude Code:**\n   ```bash\n   # Configure Claude Code to use Magg in hybrid mode\n   claude mcp add magg -- magg serve --hybrid --port 42000\n   ```\n   \n   **With mbro:**\n   ```bash\n   # mbro hosts Magg and connects via stdio\n   mbro connect magg \"magg serve --hybrid --port 8080\"\n   \n   # Other mbro instances can connect via HTTP\n   mbro connect magg http://localhost:8080\n   ```\n\n### Available Tools\n\nOnce Magg is running, it exposes the following tools to LLMs:\n\n- `magg_list_servers` - List all configured MCP servers\n- `magg_add_server` - Add a new MCP server\n- `magg_remove_server` - Remove a server\n- `magg_enable_server` / `magg_disable_server` - Toggle server availability\n- `magg_search_servers` - Search for MCP servers online\n- `magg_list_tools` - List all available tools from all servers\n- `magg_smart_configure` - Intelligently configure a server from a URL\n- `magg_analyze_servers` - Analyze configured servers and suggest improvements\n- `magg_status` - Get server and tool statistics\n- `magg_check` - Health check servers with repair actions (report/remount/unmount/disable)\n- `magg_reload_config` - Reload configuration from disk and apply changes\n- `magg_load_kit` - Load a kit and its servers into the configuration\n- `magg_unload_kit` - Unload a kit and optionally its servers from the configuration\n- `magg_list_kits` - List all available kits with their status\n- `magg_kit_info` - Get detailed information about a specific kit\n\n### Quick Inspection with MBro\n\nMagg includes the `mbro` (MCP Browser) CLI tool for interactive exploration. A unique feature is the ability to connect to Magg in stdio mode for quick inspection:\n\n```bash\n# Connect mbro to a Magg instance via stdio (no HTTP server needed)\nmbro connect local-magg magg serve\n\n# Now inspect your Magg setup from the MCP client perspective\nmbro:local-magg\u003e call magg_status\nmbro:local-magg\u003e call magg_list_servers\n```\n\nMBro also supports:\n- **Scripts**: Create `.mbro` files with commands for automation\n- **Shell-style arguments**: Use `key=value` syntax instead of JSON\n- **Tab completion**: Rich parameter hints after connecting\n\nSee the [MBro Documentation](docs/mbro.md) for details.\n\n### Authentication\n\nMagg supports optional bearer token authentication to secure access:\n\n#### Quick Start\n\n1. **Initialize authentication** (creates RSA keypair):\n   ```bash\n   magg auth init\n   ```\n\n2. **Generate a JWT token** for clients:\n   ```bash\n   # Generate token (displays on screen)\n   magg auth token\n   \n   # Export as environment variable\n   export MAGG_JWT=$(magg auth token -q)\n   ```\n\n3. **Connect with authentication**:\n   - Using `MaggClient` (auto-loads from MAGG_JWT):\n     ```python\n     from magg.client import MaggClient\n     \n     async def main():\n         async with MaggClient(\"http://localhost:8000/mcp\") as client:\n             tools = await client.list_tools()\n     ```\n   - Using FastMCP with explicit token:\n     ```python\n     from fastmcp import Client\n     from fastmcp.client import BearerAuth\n     \n     jwt_token = \"your-jwt-token-here\"\n     async with Client(\"http://localhost:8000/mcp\", auth=BearerAuth(jwt_token)) as client:\n         tools = await client.list_tools()\n     ```\n\n#### Key Management\n\n- Keys are stored in `~/.ssh/magg/` by default\n- Private key can be set via `MAGG_PRIVATE_KEY` environment variable\n- To disable auth, remove keys or set non-existent `key_path` in `.magg/auth.json`\n\n#### Authentication Commands\n\n- `magg auth init` - Initialize authentication (generates RSA keypair)\n- `magg auth status` - Check authentication configuration\n- `magg auth token` - Generate JWT token\n- `magg auth public-key` - Display public key (for verification)\n- `magg auth private-key` - Display private key (for backup)\n\nSee [examples/authentication.py](examples/authentication.py) for more usage patterns.\n\n### Configuration\n\nMagg stores its configuration in `.magg/config.json` in your current working directory. This allows for project-specific tool configurations.\n\n#### Dynamic Configuration Reloading\n\nMagg supports automatic configuration reloading without requiring a restart:\n\n- **Automatic file watching**: Detects changes to `config.json` and reloads automatically (uses watchdog when available)\n- **SIGHUP signal**: Send `kill -HUP \u003cpid\u003e` to trigger immediate reload (Unix-like systems)\n- **MCP tool**: Use `magg_reload_config` tool from any MCP client\n- **Smart transitions**: Only affected servers are restarted during reload\n\nConfiguration reload is enabled by default. You can control it with:\n- `MAGG_AUTO_RELOAD=false` - Disable automatic reloading\n- `MAGG_RELOAD_POLL_INTERVAL=5.0` - Set polling interval in seconds (when watchdog unavailable)\n\nSee [Configuration Reload Documentation](docs/config-reload.md) for detailed information.\n\n#### Environment Variables\n\nMagg supports several environment variables for configuration:\n- `MAGG_CONFIG_PATH` - Path to config file (default: `.magg/config.json`)\n- `MAGG_LOG_LEVEL` - Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)\n- `MAGG_STDERR_SHOW=1` - Show stderr output from subprocess MCP servers (default: suppressed)\n- `MAGG_AUTO_RELOAD` - Enable/disable config auto-reload (default: true)\n- `MAGG_RELOAD_POLL_INTERVAL` - Config polling interval in seconds (default: 1.0)\n- `MAGG_READ_ONLY=true` - Run in read-only mode\n- `MAGG_SELF_PREFIX` - Prefix for Magg tools (default: \"magg\"). Tools will be named as `{prefix}{sep}{tool}` (e.g., `magg_list_servers`)\n- `MAGG_PREFIX_SEP` - Separator between prefix and tool name (default: \"_\")\n\nExample configuration:\n```json\n{\n  \"servers\": {\n    \"calculator\": {\n      \"name\": \"calculator\",\n      \"source\": \"https://github.com/executeautomation/calculator-mcp\",\n      \"command\": \"npx @executeautomation/calculator-mcp\",\n      \"prefix\": \"calc\",\n      \"enabled\": true\n    }\n  }\n}\n```\n\n### Adding Servers\n\nServers can be added in several ways:\n\n1. **Using the LLM** (recommended):\n   ```\n   \"Add the Playwright MCP server\"\n   \"Search for and add a calculator tool\"\n   ```\n\n2. **Manual configuration** via `magg_add_server`:\n   ```\n   name: playwright\n   url: https://github.com/microsoft/playwright-mcp\n   command: npx @playwright/mcp@latest\n   prefix: pw\n   ```\n\n3. **Direct config editing**: Edit `.magg/config.json` directly\n\n### Real-time Notifications with MaggClient\n\nThe `MaggClient` now supports real-time notifications from backend MCP servers:\n\n```python\nfrom magg import MaggClient, MaggMessageHandler\n\n# Using callbacks\nhandler = MaggMessageHandler(\n    on_tool_list_changed=lambda n: print(\"Tools changed!\"),\n    on_progress=lambda n: print(f\"Progress: {n.params.progress}\")\n)\n\nasync with MaggClient(\"http://localhost:8000/mcp\", message_handler=handler) as client:\n    # Client will receive notifications while connected\n    tools = await client.list_tools()\n```\n\nSee [Messaging Documentation](docs/messaging.md) for advanced usage including custom message handlers.\n\n### Kit Management\n\nMagg supports organizing related MCP servers into \"kits\" - bundles that can be loaded and unloaded as a group:\n\n```bash\n# List available kits\nmagg kit list\n\n# Load a kit (adds all its servers)\nmagg kit load web-tools\n\n# Unload a kit (removes servers only in that kit)\nmagg kit unload web-tools\n\n# Get information about a kit\nmagg kit info web-tools\n```\n\nYou can also manage kits programmatically through Magg's tools when connected via an MCP client:\n- `magg_list_kits` - List all available kits\n- `magg_load_kit` - Load a kit and its servers\n- `magg_unload_kit` - Unload a kit\n- `magg_kit_info` - Get detailed kit information\n\nKits are JSON files stored in `~/.magg/kit.d/` or `.magg/kit.d/` that define a collection of related servers. See [Kit Documentation](docs/kits.md) for details on creating and managing kits.\n\n### MBro Scripts\n\nAutomate common workflows with MBro scripts:\n\n```bash\n# Create a setup script\ncat \u003e setup.mbro \u003c\u003cEOF\n# Connect to Magg and check status\nconnect magg magg serve\ncall magg_status\ncall magg_list_servers\n\n# Add a new server if needed\ncall magg_add_server name=calculator source=\"npx -y @modelcontextprotocol/server-calculator\"\nEOF\n\n# Run the script\nmbro -x setup.mbro\n```\n\n## Documentation\n\nFor more documentation, see [docs/](docs/index.md).\n\n## Appearances\n\nMagg appears in multiple locations. Please feel free to submit a PR to add more appearances below in alphabetical order.\n\n### Listing, Index, and other MCP Sites\n\n* [DeepWiki](https://deepwiki.com/sitbon/magg) - AI-generated documentation\n* [Glama.ai](https://glama.ai/mcp/servers/@sitbon/magg) - MCP server listing and hosting\n\n### Awesome GitHub MCP Lists\n\n* [@modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n* [@punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)\n* [@wong2/awesome-mcp-servers](https://github.com/wong2/awesome-mcp-servers)\n",
      "stars": 84,
      "updated_at": "2025-10-03T22:32:49Z",
      "url": "https://github.com/sitbon/magg"
    },
    "sxhxliang--mcp-access-point": {
      "category": "aggregators",
      "description": "Instantly transform any standard web API into an MCP server interface without requiring source code modifications.",
      "forks": 24,
      "imageUrl": "",
      "keywords": [
        "mcp",
        "aggregators",
        "servers",
        "mcp server",
        "mcp access",
        "service mcp"
      ],
      "language": "Rust",
      "license": "MIT License",
      "name": "mcp-gateway-adapter",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "sxhxliang",
      "readme_content": "# MCP Protocol Bridging Component\n\nThis utility, named `MCP Gateway Adapter`, functions as a high-efficiency protocol transformation proxy. Its primary purpose is to establish a functional communication channel between conventional `HTTP` endpoints and clients adhering to the `MCP` (Model Context Protocol) specification. It achieves this by enabling direct interaction for MCP consumers with established HTTP backends, completely bypassing the need for any alteration to the existing server logic.\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"./README.md\"\u003e\u003cimg alt=\"English Documentation\" src=\"https://img.shields.io/badge/English-4578DA\"\u003e\u003c/a\u003e\n  \u003ca href=\"./README_CN.md\"\u003e\u003cimg alt=\"Chinese Documentation\" src=\"https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-F40002\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://deepwiki.com/sxhxliang/mcp-access-point\"\u003e\u003cimg src=\"https://deepwiki.com/badge.svg\" alt=\"Query DeepWiki\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://zread.ai/sxhxliang/mcp-access-point\"\u003e\u003cimg alt=\"Chinese Manual\" src=\"https://img.shields.io/badge/‰∏≠ÊñáÊñáÊ°£-4578DA\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n\n\n## Core Overview\nThis solution is engineered utilizing `Pingora`, an exceptionally performant gateway proxy library. Pingora is architected to manage massive volumes of request proxy traffic, having served as the infrastructure underpinning core Cloudflare services, consistently processing beyond 40 million requests per second globally for many years. It represents the technological foundation for a substantial fraction of worldwide internet traffic.\n\n## HTTP Translation to MCP\nThis operational mode facilitates communication between clients such as `Cursor Desktop` and remote HTTP systems using `SSE` (Server-Sent Events), even when the target servers natively lack SSE support.\n\n- Illustrative Deployment Topology:\n  - Endpoint A is situated locally, accessible at `127.0.0.1:8090`\n  - Endpoint B resides externally at `api.example.com`\n- By employing the `MCP Gateway Adapter`, both service endpoints are effectively converted into MCP-compatible services without any requisite codebase adjustments.\n- Clients interface with Endpoint A and Endpoint B exclusively via the MCP framework. The Adapter intelligently determines MCP requests and routes them to the correct backend infrastructure.\n\nmermaid\ngraph LR\n   A[\"Cursor Desktop Client\"] \u003c--\u003e |SSE Stream| B[\"MCP Gateway Adapter\"]\n   A2[\"Alternative Client\"] \u003c--\u003e |Streamable Http| B[\"MCP Gateway Adapter\"]\n   B \u003c--\u003e |HTTP 127.0.0.1:8090| C1[\"Existing Web Service\"]\n   B \u003c--\u003e |HTTPS api.example.com| C2[\"External Web Service\"]\n  \n   style A2 fill:#ffe6f9,stroke:#333,color:black,stroke-width:2px\n   style A fill:#ffe6f9,stroke:#333,color:black,stroke-width:2px\n   style B fill:#e6e6af,stroke:#333,color:black,stroke-width:2px\n   style C1 fill:#e6ffe6,stroke:#333,color:black,stroke-width:2px\n   style C2 fill:#e6ffd6,stroke:#333,color:black,stroke-width:2px\n\n\n### Supported Communication Formats (Specification)\nPresently supports the following transport mechanisms: `SSE` and `Streamable HTTP` protocols:\n- ‚úÖ Streamable HTTP (Stateless) Activated: 2025-03-26\n  - Universal access: `ip:port/mcp`\n  - Dedicated service access: `ip:port/api/{service_identifier}/mcp`\n  \n- ‚úÖ Server-Sent Events (SSE) Activated: 2024-11-05\n  - Universal access: `ip:port/sse`\n  - Dedicated service access: `ip:port/api/{service_identifier}/sse`\n\nUtilize the path `IP:PORT/sse` for the SSE transport mode.\nUtilize the path `IP:PORT/mcp` for the Streamable HTTP transport mode.\n\n### Compatible MCP Consumer Applications\n- ‚úÖ [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n- ‚úÖ [Cursor Desktop](https://docs.cursor.com/context/model-context-protocol)\n- ‚úÖ [Windsurf](https://docs.windsurf.com/plugins/cascade/mcp#model-context-protocol-mcp)\n- ‚úÖ [VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers)\n- ‚úÖ [Trae](https://docs.trae.ai/ide/model-context-protocol)\n\n## Key Capabilities\n- **Protocol Interoperability**: Effortlessly translates data flows between HTTP standards and MCP requirements.\n- **Non-Invasive Deployment**: Achieves full compatibility with pre-existing HTTP service infrastructures.\n- **Client Enablement**: Grants MCP-native clients direct invocation capability over standard HTTP resources.\n- **Low-Footprint Proxy**: Minimalist structural design ensures highly efficient protocol mediation.\n- **Multi-Tenant Isolation**: Supports distinct configuration settings and endpoint segregation per registered tenant.\n- **Dynamic State Modification**: Allows configuration parameters to be adjusted while the service is operational.\n- **Management Interface**: Provides a RESTful administrative API for live configuration control.\n\n## Initial Setup Guide\n\n### Deployment Steps\nbash\n# Obtain source code\ngit clone https://github.com/sxhxliang/mcp-access-point.git\ncd mcp-access-point\n# Start service using configuration file\ncargo run -- -c config.yaml\n\n# Debugging with Inspector (start service first)\nnpx @modelcontextprotocol/inspector node build/index.js\n# Connect to http://127.0.0.1:6274/\n# Choose \"SSE\" and input 0.0.0.0:8080/sse, then initiate connection\n# Alternatively, select \"Streamable HTTP\" and input 0.0.0.0:8080/mcp\n\n\n### Multi-Tenant Access Scheme\nThe MCP Access Gateway facilitates multi-tenancy, where each distinct client configuration exposes services via segregated paths:\n- `/api/{mcp-service-id}/sse` (For SSE traffic)\n- `/api/{mcp-service-id}/mcp` (For Streamable HTTP traffic)\n\nConfiguration Example:\nyaml\n# config.yaml snippet for service definition\nmcps:\n  - id: primary-api # Accessible via /api/primary-api/sse or /api/primary-api/mcp\n    ...\n  - id: secondary-data # Accessible via /api/secondary-data/sse or /api/secondary-data/mcp\n    ...\n\n\nTo reach all registered services concurrently, utilize the root paths:\n- `0.0.0.0:8080/mcp` (Streamable HTTP aggregate)\n- `0.0.0.0:8080/sse` (SSE aggregate)\n\n### Configuration Parameterization\n1. **`-c config.yaml`**\n   - The `-c` (or `--config`) flag dictates the location of the YAML configuration document.\n   - This document itemizes all upstream APIs intended for proxying and protocol conversion by the Adapter.\n\n### config.yaml Detailed Structure\nThe configuration file enables granular control over multiple distinct MCP services, defining upstream targets and routing logic independently. Key configuration sections include:\n\n1. **mcps** - Inventory of defined MCP services\n   - `id`: A unique identifier serving as the prefix for access URLs.\n   - `upstream_id`: Reference key linking to an entry in the `upstreams` section.\n   - `path`: Pointer to the OpenAPI documentation schema. Accepts local filesystem paths (e.g., `schema/spec.json`) or remote URLs (e.g., `https://api.docs.org/v3/swagger.json`). Supports both JSON and YAML schema formats.\n   - `routes`: Specific, customized routing rules (Optional).\n   - `upstream`: Service-specific parameters for the backend connection (Optional).\n\n2. **upstreams** - Definitions for backend destinations\n   - `id`: The unique identifier referenced by the `mcps` section.\n   - `nodes`: A list of target backend addresses along with their associated load-balancing weights.\n   - `type`: Load balancing methodology (e.g., `roundrobin`, `random`, `ip_hash`).\n   - `scheme`: The protocol used to connect to the backend (`http`/`https`).\n   - `pass_host`: Policy for relaying the HTTP Host header.\n   - `upstream_host`: Value to substitute into the Host header for backend requests.\n\nComprehensive Configuration Blueprint:\nyaml\n# Configuration document example\nmcps:\n  - id: data-source-one \n    upstream_id: backend_A\n    path: ./schemas/api_v1.json \n\n  - id: external-catalog \n    upstream_id: backend_B\n    path: https://catalog.vendor.com/openapi.yaml \n\n  - id: telemetry-ingest \n    upstream_id: backend_C\n    routes: # Custom path definitions\n      - id: get_metric_data\n        operation_id: fetch_sensor_reading\n        uri: /readings/{sensor_id}\n        method: GET\n        meta: {}\n\nupstreams: \n  - id: backend_A\n    headers: \n      X-Auth-Token: \"secret-key-alpha\"\n    nodes: \n      \"service.internal:9000\": 5 \n\n  - id: backend_B\n    nodes:\n      \"remote.host.net:443\": 1\n    type: random \n    scheme: https\n    pass_host: preserve\n\n\nTo launch the MCP Protocol Bridging Component using the configuration file:\nbash\ncargo run -- -c config.yaml\n\n\n## Containerized Deployment (Docker)\n\n### Rapid Local Launch\nbash\n# Substitute /path/to/your/config.yaml with the actual host path\ndocker run -d --name mcp-adapter --rm \\\n  -p 8080:8080 \\\n  -e port=8080 \\\n  -v /path/to/your/config.yaml:/app/config/config.yaml \\\n  ghcr.io/sxhxliang/mcp-access-point:main\n\n\n### Building Custom Docker Image (Optional)\n- Ensure Docker is installed.\n- Obtain repository and build the image locally:\nbash\n# Clone repository\ngit clone https://github.com/sxhxliang/mcp-access-point.git\ncd mcp-access-point\n\n# Build image (using custom tag)\ndocker build -t myregistry/mcp-adapter:latest .\n\n\n- Execute Container:\nbash\n# Use environment variables for configuration path mapping\ndocker run -d --name mcp-adapter-runtime --rm \\\n  -p 8080:8080 \\\n  -e port=8080 \\\n  -v /path/to/your/config.yaml:/app/config/config.yaml \\\n  myregistry/mcp-adapter:latest\n\n\n### Environmental Settings\n- `port`: Defines the network interface port where the Adapter listens (Default: 8080).\n\n## Common Application Scenarios\n\n- **Incremental System Modernization**: Facilitates a phased transition of services from pure HTTP to MCP utilization.\n- **Mixed Infrastructure Support**: Enables legacy HTTP assets to participate seamlessly within the MCP operational environment.\n- **Protocol Coexistence**: Allows the creation of systems that natively manage both communication standards concurrently.\n\n**Use Case Example**:\nWhen modern, AI-centric clients leveraging MCP require data from older HTTP-based microservices, the MCP Gateway Adapter sits intermediately, executing flawless protocol transformation.\n\nAcknowledgement to [@limcheekin](https://github.com/limcheekin) for a practical implementation case study: https://limcheekin.medium.com/building-your-first-no-code-mcp-server-the-fabric-integration-story-90da58cdbe1f\n\n## Live Configuration Management via Admin Interface\n\nThe Adapter now integrates a RESTful Administration API, enabling modification of operational parameters without service interruption.\n\n### Admin API Capabilities\n\n- **Instant Configuration Updates**: Modify service definitions, upstream pools, routing maps, and other settings dynamically.\n- **Dependency Checking**: Automated validation ensures structural integrity before applying configuration changes.\n- **Atomic Batches**: Execute sequences of updates as a single, guaranteed operation.\n- **Pre-flight Validation**: Test proposed changes using a 'dry-run' mode.\n- **State Monitoring**: Access metrics related to the current configuration layout.\n\n### Admin API Setup\nIncorporate this block into `config.yaml` to activate the Admin Endpoint:\n\nyaml\naccess_point:\n  admin:\n    address: \"127.0.0.1:8081\"  # Interface for administrative commands\n    api_key: \"secure-key-456\"    # Optional security token\n\n\n### Admin API Path Definitions\n\n#### Resource Manipulation\n- `GET /admin/resources` - Overview of all managed resources and basic metrics.\n- `GET /admin/resources/{type}` - List all entries of a specific resource category.\n- `GET /admin/resources/{type}/{id}` - Retrieve the details of one specific resource.\n- `POST /admin/resources/{type}/{id}` - Provision a new resource instance.\n- `PUT /admin/resources/{type}/{id}` - Overwrite/update an existing resource.\n- `DELETE /admin/resources/{type}/{id}` - Terminate a resource.\n\n#### Advanced Control Flows\n- `POST /admin/validate/{type}/{id}` - Validate the syntax and logic of a specific resource configuration.\n- `POST /admin/batch` - Execute a series of operations together.\n- `POST /admin/reload/{type}` - Force a refresh of all instances within a specific resource type.\n- `POST /admin/reload/config` - Re-read the entire configuration file (defaults to `config.yaml`). Accepts an optional JSON body: `{ \"config_path\": \"new/path/config.yaml\" }`\n\n#### Managed Resource Categories\n- `upstreams` - Definitions for backend server groups.\n- `services` - High-level service mappings.\n- `routes` - Specific request handling rules.\n- `global_rules` - System-wide operational policies.\n- `mcp_services` - Definitions specific to MCP exposure.\n- `ssls` - SSL/TLS certificate definitions.\n\n### Admin API Request Examples\n\n#### Updating an Upstream Group\nbash\ncurl -X POST http://localhost:8081/admin/resources/upstreams/new-backend-pool \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: secure-key-456\" \\\n  -d '{\n    \"id\": \"new-backend-pool\",\n    \"type\": \"RoundRobin\",\n    \"nodes\": [\"10.0.0.5:9000\"],\n    \"timeout\": {\"connect\": 3, \"read\": 8}\n  }'\n\n\n#### Executing a Batch Operation\nbash\ncurl -X POST http://localhost:8081/admin/batch \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: secure-key-456\" \\\n  -d '{\n    \"dry_run\": false,\n    \"operations\": [\n      {\n        \"operation_type\": \"update\",\n        \"resource_type\": \"upstreams\",\n        \"resource_id\": \"backend_A\",\n        \"data\": {\"timeout\": {\"send\": 20}}\n      }\n    ]\n  }'\n\n\n### Accessing the Web Management Interface\n\n- Access URL: `GET /admin` serves an integrated administrative dashboard (`static/admin_dashboard.html`).\n- The UI visualizes status for: 1) `mcp_services`, 2) `ssls`, 3) `global_rules`, 4) `routes`, 5) `upstreams`, 6) `services`.\n- Each dashboard element displays the current `count` and the timestamp of the `last_updated` configuration retrieved via the API.\n\n### Triggering File Configuration Reload\nbash\n# Reload using the primary configuration file\ncurl -X POST http://localhost:8081/admin/reload/config \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: secure-key-456\"\n\n# Force load a different file path\ncurl -X POST http://localhost:8081/admin/reload/config \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: secure-key-456\" \\\n  -d '{\"config_path\": \"./production_settings.yaml\"}'\n\n\nFor comprehensive reference on the Admin API functionality, consult [RUNTIME_CONFIG_API.md](./RUNTIME_CONFIG_API.md).\n\n## Developer Contributions\n1. Create a fork of this repository.\n2. Establish a new feature branch.\n3. Implement and commit your intended modifications.\n4. Submit a Pull Request for review and potential integration.\n5. Ensure all new code adheres to established Rust idiomatic practices.\n\nREFERENCES: The concept of collecting and synthesizing external appraisals into a singular metric (aggregation) is widespread. For instance, in entertainment, review aggregators synthesize various critical opinions on media like films or software, impacting market perception and sometimes even financial outcomes. This tool performs an analogous function by gathering disparate HTTP requests and presenting them uniformly through the MCP lens.",
      "stars": 130,
      "updated_at": "2025-10-01T15:37:24Z",
      "url": "https://github.com/sxhxliang/mcp-access-point"
    },
    "tigranbs--mcgravity": {
      "category": "aggregators",
      "description": "A foundational utility designed to coalesce disparate Model Context Protocol (MCP) backends into a singular, cohesive access point. Facilitate robust scaling of generative AI services through intelligent request distribution across multiple underlying MCP engines, analogous to a reverse proxy architecture like HAProxy for network traffic.",
      "forks": 4,
      "imageUrl": "",
      "keywords": [
        "aggregators",
        "mcp",
        "mcgravity",
        "mcp servers",
        "mcgravity proxy",
        "mcp server"
      ],
      "language": "TypeScript",
      "license": "Apache License 2.0",
      "name": "ContextFabricator",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "tigranbs",
      "readme_content": "# ContextFabricator\n\n\u003cdiv align=\"center\"\u003e\n  \n\u003c/div\u003e\n\n## Overview\n\nContextFabricator functions as a sophisticated intermediary layer, aggregating several independent Model Context Protocol (MCP) service instances beneath a unified interface. This architecture enables leveraging a single client-facing address while achieving near-limitless horizontal scaling of the backend MCP infrastructure.\n\nInitially structured as a command-line interface (CLI) utility, ContextFabricator is targeted for evolution into a comprehensive proxy solution for advanced AI endpoints‚Äîserving the role for modern LLMs that Nginx plays for traditional web applications.\n\n## Rationale for ContextFabricator\n\n\nWithout ContextFabricator:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Consumer‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMCP Node A‚îÇ\n‚îÇ Client  ‚îÇ     ‚îÇ Server  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚îÇ\n    ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMCP Node B‚îÇ\n                ‚îÇ Server  ‚îÇ\n                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nWith ContextFabricator:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Consumer‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇContextFab ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMCP Node A‚îÇ\n‚îÇ Client  ‚îÇ     ‚îÇ           ‚îÇ     ‚îÇ Server  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMCP Node B‚îÇ\n                                ‚îÇ Server  ‚îÇ\n                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\nContextFabricator addresses critical operational needs:\n\n- Unifying access to multiple MCP endpoints via a single gateway address.\n- Implementing intelligent traffic distribution (load balancing) among backend servers.\n- Establishing a consolidated ingress point for consuming applications.\n\n## Deployment Instructions\n\nbash\n# Obtain necessary dependencies\nbun install\n\n# Compile the source code into a standalone binary\nbun build src/index.ts --compile --outfile contextfabricator\n\n\n## Containerization\n\nContextFabricator images are published on Docker Hub: [your-repo/contextfabricator](https://hub.docker.com/r/your-repo/contextfabricator).\n\nbash\ndocker pull your-repo/contextfabricator\n\n# Standard operational mode\ndocker run -p 3001:3001 your-repo/contextfabricator http://mcp-a.internal http://mcp-b.internal\n\n# Specifying binding interface and port\ndocker run -p 4000:4000 your-repo/contextfabricator --listen-host 0.0.0.0 --listen-port 4000 http://mcp-a.internal\n\n\n## Operational Usage\n\nStandard invocation:\n\nbash\n./contextfabricator \u003cbackend-mcp-uri-1\u003e \u003cbackend-mcp-uri-2\u003e ...\n\n\nWith configuration flags:\n\nbash\n./contextfabricator --listen-host 127.0.0.1 --listen-port 8080 http://backend1.svc http://backend2.svc\n\n\nUtilizing a configuration manifest:\n\nbash\n./contextfabricator --manifest settings.json\n\n\n### Parameterization\n\n- `--listen-host \u003caddress\u003e`: The network interface IP address for the proxy listener (Default: localhost).\n- `--listen-port \u003cnumber\u003e`: The TCP port the proxy listens on (Default: 3001).\n- `--manifest \u003cpath\u003e`: Path to the primary configuration file (Default: settings.json).\n- `--protocol-version \u003cversion\u003e`: The mandated MCP version negotiated with backends (Default: 1.0.0).\n- `--service-label \u003cmoniker\u003e`: Internal identifier for the aggregated service (Default: contextfabricator).\n- `--help`: Display command-line assistance.\n\n### Configuration Structure\n\nContextFabricator accepts configuration via a JSON manifest file. Consult `settings.example.json` for the schema:\n\n\n{\n  \"service_name\": \"contextfabricator\",\n  \"version\": \"1.0.0\",\n  \"description\": \"An aggregated MCP gateway.\",\n  \"backends\": {\n    \"test-streamer\": {\n      \"uri\": \"http://10.0.0.5:3000/stream\",\n      \"label\": \"streamer-node\",\n      \"schema_version\": \"1.0.0\",\n      \"notes\": \"Primary high-throughput backend\"\n    }\n  }\n}\n\n\nAn included demonstration backend server can be launched for verification:\n\nbash\n# Activate the example backend server first\nbun run start:demo-server\n\n# Then launch ContextFabricator targeting it via the manifest\n./contextfabricator --manifest settings.json\n\n\n## Operational Scenarios\n\nInitiate ContextFabricator using only positional backend addresses:\n\nbash\n./contextfabricator http://remote-mcp-node-a http://remote-mcp-node-b\n\n\nBind to a specific interface and port while listing backends:\n\nbash\n./contextfabricator --listen-host 0.0.0.0 --listen-port 9000 http://mcp-pool-1 http://mcp-pool-2\n\n\n## Validation Suite Execution\n\nTo execute the full test suite:\n\nbash\nbun test\n\n\nTo isolate and run only end-to-end validation tests:\n\nbash\nbun run test:e2e\n\n\n### End-to-End Checks\n\nThe E2E suite confirms that ContextFabricator successfully:\n\n1. Establishes connectivity to the target MCP instance(s).\n2. Transparently forwards capability declarations received from the backend.\n3. Routes client requests correctly to the designated backend server(s) and returns the resulting data stream.\n\nDetailed specifications of the testing harness are available in [test/MANIFESTO.md].\n\nContinuous Integration pipelines automatically invoke these validation steps upon repository events.\n\n## Roadmap for Expansion\n\nFuture enhancements include:\n\n- Dashboard interface for real-time operational visibility.\n- Sophisticated, configurable routing algorithms beyond basic round-robin.\n- Automated status probing and failure detection for backend nodes.\n- Integration of authorization layers and access governance.\n- A modular extension framework for custom data transformations.\n\n## Development Environment\n\n### Tooling and Style Adherence\n\nThis repository leverages:\n\n- TypeScript language compiled via the Bun runtime environment.\n- ESLint, configured with strict rules tailored for TypeScript projects, ensuring code quality.\n- Prettier for consistent code aesthetics and automatic formatting.\n\nThe project configuration is finely tuned for optimal performance within the Bun ecosystem.\n\nExecute these commands for development workflow management:\n\nbash\n# Apply formatting rules globally\nbun run fmt\n\n# Verify adherence to formatting standards without modification\nbun run fmt:check\n\n# Run static analysis via ESLint\nbun run check:style\n\n# Attempt automated correction of style violations\nbun run fix:style\n\n\nVS Code integration is established to trigger Prettier on save events and display immediate ESLint feedback upon installation of recommended extensions.\n\n## Community Contributions\n\nWe welcome external input! Please submit detailed issues or propose enhancements via pull requests.",
      "stars": 71,
      "updated_at": "2025-09-30T02:46:46Z",
      "url": "https://github.com/tigranbs/mcgravity"
    },
    "wegotdocs--open-mcp": {
      "category": "aggregators",
      "description": "Turn a web API into an MCP server in 10 seconds and add it to the open source registry: https://open-mcp.org",
      "forks": 30,
      "imageUrl": "",
      "keywords": [
        "aggregators",
        "mcp",
        "api",
        "mcp server",
        "api mcp",
        "aggregators servers"
      ],
      "language": "TypeScript",
      "license": "No License",
      "name": "open-mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "wegotdocs",
      "readme_content": "# OpenMCP\n\nhttps://www.open-mcp.org\n\nOpenMCP is both:\n\n1. a standard for converting web APIs into MCP servers\n2. an open source registry of servers which follow the standard\n\nEach OpenMCP server gives MCP clients the ability to make requests to a particular web API in a token-efficient way. Together the servers in the registry represent a broad range of services, empowering the underlying client LLMs to fetch data and perform actions on behalf of their users across many domains.\n\n## Contents\n\n- [Creating a server](#creating-a-server)\n- [Adding OpenMCP servers to MCP clients](#adding-openmcp-servers-to-mcp-clients)\n- [Converting web API -\u003e OpenMCP](#converting-web-api---openmcp)\n\n## Creating a server\n\nhttps://www.open-mcp.org/servers/creating-a-server\n\n## Adding OpenMCP servers to MCP clients\n\n### Remote hosting\n\n...\n\n### Local hosting\n\n\u003cdiv\u003e\n  \u003ca href=\"https://www.loom.com/share/aa26fed41f084ff1bd115436f9d799dd\"\u003e\n    \u003cp\u003eLocal hosting demo - watch video\u003c/p\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://www.loom.com/share/aa26fed41f084ff1bd115436f9d799dd\"\u003e\n    \u003cimg alt=\"aa26fed41f084ff1bd115436f9d799dd_9815ccb91b155b9d_full_play\" style=\"max-width:300px;\" src=\"https://cdn.loom.com/sessions/thumbnails/aa26fed41f084ff1bd115436f9d799dd-9815ccb91b155b9d-full-play.gif\"\u003e\n  \u003c/a\u003e\n\u003c/div\u003e\n\n#### Requirements:\n\n- Node.js v18 or later (includes npx and npm)\n\n#### Claude desktop\n\n```bash\nnpx @open-mcp/config add {server-id} \\\n  ~/Library/Application\\ Support/Claude/claude_desktop_config.json \\\n  --ENV_VAR=abc123\n```\n\nNow restart Claude desktop to load the tools.\n\n#### Cursor\n\nRun this from the root of your project directory or, to add to all cursor projects, run it from your home directory `~`.\n\n```bash\nnpx @open-mcp/config add {server-id} \\\n  .cursor/mcp.json \\\n  --ENV_VAR=abc123\n```\n\nNow go to `Cursor \u003e Settings \u003e Cursor Settings` then click `MCP` to ensure the server is enabled.\n\n#### Other clients\n\n```bash\nnpx @open-mcp/config add {server-id} \\\n  /path/to/config.json \\\n  --ENV_VAR=abc123\n```\n\n#### Alternatives\n\nIf you don't want to use the CLI you can use `npm` to install the package manually, then add a `node` command to your client config with an absolute path to `dist/index.js`. See the individual server READMEs for more details.\n\n## Converting web API -\u003e OpenMCP\n\n### REST `openapi.yaml` / `openapi.json`\n\n...\n\n### gRPC `service.proto`\n\n...\n\n### JSON-RPC `openrpc.json`\n\n...\n\n### GraphQL `schema.gql`\n\n...\n\n### SOAP `service.wsdl`\n\n...\n\n### PostgREST `schema.sql`\n\n...\n",
      "stars": 267,
      "updated_at": "2025-10-02T06:44:03Z",
      "url": "https://github.com/wegotdocs/open-mcp"
    }
  },
  "totalRepositories": 19
}