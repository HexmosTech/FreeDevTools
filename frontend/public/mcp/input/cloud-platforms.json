{
  "category": "cloud-platforms",
  "categoryDisplay": "Cloud Platforms",
  "description": "Cloud platform service integration. Enables management and interaction with cloud infrastructure and services.",
  "repositories": {
    "4everland--4everland-hosting-mcp": {
      "category": "cloud-platforms",
      "description": "An MCP server implementation for 4EVERLAND Hosting enabling instant deployment of AI-generated code to decentralized storage networks like Greenfield, IPFS, and Arweave.",
      "forks": 3,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "platform",
        "platforms",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "TypeScript",
      "license": "No License",
      "name": "4everland-hosting-mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "4everland",
      "readme_content": "# 4EVERLAND Hosting MCP Server\n\n[![Version](https://img.shields.io/ repository\ngit clone https://github.com/4everland/4everland-hosting-mcp.git\ncd 4everland-hosting-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server locally\nnpm run serve\n```\n\n## License\n\nThis project is licensed under the MIT License.",
      "stars": 1,
      "updated_at": "2025-09-06T14:18:37Z",
      "url": "https://github.com/4everland/4everland-hosting-mcp"
    },
    "Nebula-Block-Data--nebulablock-mcp-server": {
      "category": "cloud-platforms",
      "description": "Exposes the complete feature set of the NebulaBlock API via accessible tools by utilizing the high-speed 'fastmcp' integration library.",
      "forks": 2,
      "imageUrl": "",
      "keywords": [
        "nebulablock",
        "nebula",
        "cloud",
        "nebulablock api",
        "services nebula",
        "data nebulablock"
      ],
      "language": "Python",
      "license": "No License",
      "name": "nebulablock-mcp-gateway",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "Nebula-Block-Data",
      "readme_content": "# NebulaBlock API Model Context Protocol (MCP) Interface\n\nThis repository contains the official server implementation for accessing the NebulaBlock API through the Model Context Protocol (MCP). By leveraging the optimized `fastmcp` library, this service translates all available NebulaBlock API capabilities into consumable tools, ensuring smooth and effective integration within any environment supporting the MCP standard.\n\n## Directory Organization\n\nThe project layout is organized as follows:\n\n\n.\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îú‚îÄ‚îÄ main.py\n‚îÇ   ‚îú‚îÄ‚îÄ tools.py\n‚îÇ   ‚îî‚îÄ‚îÄ mcp_project.egg-info/\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_main.py\n‚îú‚îÄ‚îÄ scripts/\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ .env.example\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ uv.lock\n\n\n*   `src/`: Core application logic, housing system configuration and tool definitions.\n*   `tests/`: Contains validation scripts, including unit and integration checks.\n*   `scripts/`: Designated area for auxiliary operational scripts (e.g., initial setup).\n*   `docs/`: Location for supplementary documentation materials.\n*   `.env.example`: A template file illustrating required environment variables.\n*   `.gitignore`: Specifies file patterns that Git should omit from version control.\n*   `pyproject.toml`: Defines project metadata, build parameters, dependency lists, and overall project specifications.\n*   `README.md`: This current descriptive file.\n*   `uv.lock`: The dependency resolution lock file generated by the `uv` package manager.\n\n## Setup and Deployment Procedures\n\nFollow these sequential steps to establish the development environment:\n\n1.  **Source Code Retrieval (if necessary):**\n    bash\n    git clone https://github.com/Nebula-Block-Data/api-mcp\n    cd mcp-project\n    \n\n2.  **Virtual Environment Creation:**\n    A segregated environment is strongly advised for dependency isolation.\n    bash\n    python3 -m venv .venv\n    \n\n3.  **Environment Activation:**\n    *   **Linux/macOS Users:**\nbash\nsource .venv/bin/activate\n\n\n4.  **Dependency Installation:**\n    Dependencies are managed via `pyproject.toml`. Install `setuptools` and then install the package locally in editable mode.\n    bash\nuv pip install -e .\n    \n    This action resolves and installs `fastmcp` along with all other specified prerequisites.\n\n## Initiating the NebulaBlock API MCP Server\n\nThe server is launched using the following command:\n\nbash\nuv run -m src.main\n\n\nUpon successful startup, the console should display a log similar to: `[05/29/25 17:32:58] INFO     Starting MCP server 'FastMCP' with transport 'stdio'`\n\n### API Credential Configuration\n\nThe prerequisite NebulaBlock API access token can be supplied through two distinct mechanisms:\n\n1.  **Via Command Line Flag:**\n    The key can be injected directly during application launch:\n    bash\n    python -m src.main --api-key your_nebula_block_api_key\n    \n    This method takes precedence over any setting found in configuration files.\n\n2.  **Via Environment File:**\n    Create a file named `.env` in the root project folder and define the key there:\n    \n    NEBULA_BLOCK_API_KEY=your_nebula_block_api_key\n    \n    The application automatically loads credentials from this file if the `--api-key` argument is omitted.\n\n## Executing System Verification Routines\n\nTo confirm system integrity through testing, ensure your virtual environment is active and `pytest` is installed (it's included upon editable installation):\n\nbash\npytest\n\n\nSuccessful execution will report the completion of all defined tests.\n\n## Connecting with an MCP Client Application\n\nTo leverage the functionality provided by this NebulaBlock API MCP gateway, the client application (e.g., an MCP plugin within an IDE) must be configured to establish a connection. The following JSON snippet illustrates a typical configuration for a client's `settings.json`:\n\n\n{\n  \"mcpServers\": {\n    \"nebula\": {\n      \"command\": \"~/path/to/uv\",\n      \"args\": [\n        \"--directory\",\n        \"~/path/to/nebulablock_mcp\",\n        \"run\",\n        \"-m\",\n        \"src.main\",\n        \"--api-key=YOUR_API_KEY\"\n      ]\n    }\n  }\n}\n\n\n*   Adjust `~/path/to/uv` to point to the location of your installed `uv` binary.\n*   Set `~/path/to/nebulablock_mcp` to the absolute location of this project directory.\n*   Substitute `YOUR_API_KEY` with your actual authorization credential.\n\n## Licensing Information\n\nDistribution of this software is governed by the terms of the MIT License. Refer to the `LICENSE` file (if present) for complete particulars.",
      "stars": 1,
      "updated_at": "2025-07-25T07:01:11Z",
      "url": "https://github.com/Nebula-Block-Data/nebulablock-mcp-server"
    },
    "StacklokLabs--mkp": {
      "category": "cloud-platforms",
      "description": "A specialized Model Context Protocol (MCP) serving agent engineered for seamless, LLM-driven interaction with active Kubernetes environments. This utility exposes essential Kubernetes API functionalities‚Äîsuch as resource introspection, state retrieval, and declarative configuration deployment‚Äîvia the standardized MCP layer.",
      "forks": 5,
      "imageUrl": "",
      "keywords": [
        "mkp",
        "kubernetes",
        "mcp",
        "stackloklabs mkp",
        "mkp mkp",
        "mkp model"
      ],
      "language": "Go",
      "license": "Apache License 2.0",
      "name": "k8s-mcp-interface",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "StacklokLabs",
      "readme_content": "# K8S-MCP-INTERFACE: Kubernetes Abstraction Layer for Generative Models\n\nThis component, referred to as K8S-MCP-INTERFACE, functions as a robust Model Context Protocol (MCP) service endpoint designed specifically to bridge Large Language Models (LLMs) with the operational mechanics of Kubernetes clusters. It facilitates querying cluster state and orchestrating resource manipulation using the structured MCP messaging paradigm.\n\n## Core Capabilities\n\n*   Enumeration of discoverable resources within the target Kubernetes API domain.\n*   Listing of cluster-scoped resources.\n*   Listing of resources confined to specific namespaces.\n*   Fetching detailed resource manifests, including access to derived views such as resource status, scaling metrics, and container output streams (logs).\n*   Provisioning or updating (idempotent application) of both cluster-wide and namespaced resources.\n*   Initiating remote command execution within running containers, complete with mechanisms for enforcing execution deadlines.\n*   Implementation rooted in the Kubernetes API Machinery's unstructured client, ensuring adaptability and broad compatibility.\n*   Integrated traffic throttling to safeguard the API server against overly aggressive client interactions.\n\n## Rationale for Selection\n\nK8S-MCP-INTERFACE offers distinct advantages when serving as the Kubernetes interface for AI agents:\n\n### Native Implementation Synergy\n\n*   Developed in Go, aligning with Kubernetes' primary language stack.\n*   Optimized performance profile suitable for high-throughput server tasks.\n*   Leverages Go's intrinsic safety features for type checking and concurrent operations.\n*   Direct interoperability with core Kubernetes client libraries.\n\n### Direct API Conduit\n\n*   Bypasses reliance on external tools like `kubectl` or Helm for core interactions.\n*   Communicates directly against the Kubernetes Control Plane endpoint.\n*   Minimizes operational latency and enhances system robustness.\n\n### Comprehensive Resource Adaptability\n\n*   Utilizes the unstructured client to support *any* registered Kubernetes object, including custom resources (CRDs).\n*   Eliminates the need to pre-define schemas for every supported resource type.\n*   Automatically incorporates support for future Kubernetes API additions.\n\n### Design Philosophy\n\n*   Singular focus on essential Kubernetes interaction primitives.\n*   Codebase emphasizes clarity, maintainability, and clear separation of functional units.\n*   Minimal external library footprint for lightweight deployment.\n\n### Production Readiness\n\n*   Engineered for stability and responsiveness in demanding operational settings.\n*   Robust error propagation and resource handling protocols.\n*   Built-in defense against denial-of-service via request throttling.\n*   Architected for comprehensive unit testing coverage.\n\n## Operational Requirements\n\n*   Go runtime version 1.24 or newer.\n*   Access credentials for a target Kubernetes cluster (kubeconfig file).\n*   The `task` runner utility (v3+) for development workflows.\n\n## Deployment Procedure\n\n1.  Obtain the source repository:\n\n    bash\n    git clone https://github.com/StacklokLabs/mkp.git\n    cd mkp\n    \n\n2.  Fetch necessary project dependencies:\n\n    bash\n    task install\n    \n\n3.  Compile the executable server binary:\n\n    bash\n    task build\n    \n\n## Execution Guide\n\n### Launching the Service Endpoint\n\nTo initiate the service utilizing the default credentials path:\n\nbash\ntask run\n\n\nTo specify an alternative kubeconfig location:\n\nbash\nKUBECONFIG=/path/to/custom/config task run-with-kubeconfig\n\n\nTo bind the service to a non-default network port:\n\nbash\nMCP_PORT=9091 task run\n\n\n### Available MCP Interaction Tools\n\nThe K8S-MCP-INTERFACE exposes the following structured methods for agent interaction:\n\n#### `get_k8s_entity`\n\nRetrieves a specified Kubernetes object or one of its dynamic sub-paths.\n\nParameters:\n\n- `scope` (required): Classification of the resource scope (\"cluster\" or \"namespaced\").\n- `api_group`: The API grouping identifier (e.g., \"apps\", \"networking.k8s.io\").\n- `api_version` (required): The version segment of the API (e.g., \"v1\", \"v1beta1\").\n- `resource_kind` (required): The plural name of the resource collection (e.g., \"deployments\").\n- `target_ns`: The containing namespace (mandatory for namespaced entities).\n- `entity_name` (required): The unique identifier of the resource instance.\n- `sub_path`: The specific subresource endpoint to query (e.g., \"status\", \"scale\", \"logs\").\n- `query_params`: Supplementary parameters appended to the API request (see examples).\n\nExample (Fetching Deployment Status):\n\n\n{\n  \"name\": \"get_k8s_entity\",\n  \"arguments\": {\n    \"scope\": \"namespaced\",\n    \"api_group\": \"apps\",\n    \"api_version\": \"v1\",\n    \"resource_kind\": \"deployments\",\n    \"target_ns\": \"production\",\n    \"entity_name\": \"web-frontend\",\n    \"sub_path\": \"status\"\n  }\n}\n\n\nExample (Retrieving Container Logs with Filters):\n\n\n{\n  \"name\": \"get_k8s_entity\",\n  \"arguments\": {\n    \"scope\": \"namespaced\",\n    \"api_group\": \"\",\n    \"api_version\": \"v1\",\n    \"resource_kind\": \"pods\",\n    \"target_ns\": \"logging\",\n    \"entity_name\": \"data-processor-xyz\",\n    \"sub_path\": \"logs\",\n    \"query_params\": {\n      \"container\": \"processor-main\",\n      \"sinceSeconds\": \"7200\",\n      \"timestamps\": \"true\"\n    }\n  }\n}\n\n\nPod Log Parameter Specifics:\n\n*   `container`: Designates the target container within the Pod.\n*   `previous`: Boolean flag to retrieve logs from a terminated container instance.\n*   `sinceSeconds`: Filters output to logs generated after the specified duration in seconds.\n*   `sinceTime`: Filters logs based on an RFC3339 timestamp.\n*   `timestamps`: Instructs the API to prefix each line with a timestamp.\n*   `limitBytes`: Imposes a ceiling on the total bytes returned.\n*   `tailLines`: Limits output to the final N lines of the log stream.\n\nDefault log constraints (100 lines, 32KB) are enforced to optimize LLM context utilization, modifiable via `query_params`.\n\nRegular Resource Query Parameter:\n\n*   `resourceVersion`: Allows fetching the object state as it existed at a specific version token.\n\n#### `scan_k8s_collection`\n\nPerforms a bulk listing of resources belonging to a specified collection.\n\nParameters:\n\n- `scope` (required): Resource scope (\"cluster\" or \"namespaced\").\n- `api_group`: API group identifier.\n- `api_version` (required): API version identifier.\n- `resource_kind` (required): Plural resource collection name.\n- `target_ns`: Namespace (mandatory for namespaced collections).\n- `label_filter`: Standard Kubernetes label selector string for filtering.\n- `detail_annotations`: Inclusion flag for metadata annotations (default: true).\n- `omit_annotation_keys`: List of annotation keys (supports globbing with `*`) to remove from results.\n- `include_only_annotation_keys`: If set, only these specified annotations will be retained.\n\n##### Annotation Exclusion Logic\n\nThis tool features advanced metadata trimming to mitigate context bloat from extensive annotations (e.g., large GPU device metadata).\n\n**Excluding Specific Annotations (Example: NVIDIA metadata):**\n\n\n{\n  \"name\": \"scan_k8s_collection\",\n  \"arguments\": {\n    \"scope\": \"clustered\",\n    \"api_group\": \"\",\n    \"api_version\": \"v1\",\n    \"resource_kind\": \"nodes\",\n    \"omit_annotation_keys\": [\n      \"nvidia.com/*\",\n      \"kubectl.kubernetes.io/last-applied-configuration\"\n    ]\n  }\n}\n\n\nAnnotation Rule Summary:\n\n-   `kubectl.kubernetes.io/last-applied-configuration` is excluded by default.\n-   `omit_annotation_keys` recognizes trailing wildcards (`*`).\n-   If `include_only_annotation_keys` is active, it overrides all other inclusion/exclusion rules.\n-   Setting `detail_annotations: false` suppresses all metadata annotations.\n\n#### `deploy_k8s_entity`\n\nApplies a new or modified declarative manifest to the cluster.\n\nParameters:\n\n- `scope` (required): Resource scope (\"cluster\" or \"namespaced\").\n- `api_group`: API group identifier.\n- `api_version` (required): API version identifier.\n- `resource_kind` (required): Plural resource collection name.\n- `target_ns`: Namespace (required for namespaced entities).\n- `payload_manifest` (required): The YAML/JSON structure representing the desired state.\n\nExample (Deploying a new Deployment):\n\n\n{\n  \"name\": \"deploy_k8s_entity\",\n  \"arguments\": {\n    \"scope\": \"namespaced\",\n    \"api_group\": \"apps\",\n    \"api_version\": \"v1\",\n    \"resource_kind\": \"deployments\",\n    \"target_ns\": \"default\",\n    \"payload_manifest\": {\n      \"apiVersion\": \"apps/v1\",\n      \"kind\": \"Deployment\",\n      \"metadata\": {\n        \"name\": \"nginx-proxy\",\n        \"labels\": {\"agent\":\"mkp\"}\n      },\n      \"spec\": {\n        \"replicas\": 1,\n        \"selector\": {\"matchLabels\": {\"app\": \"nginx\"}},\n        \"template\": {\n          \"metadata\": {\"labels\": {\"app\": \"nginx\"}},\n          \"spec\": {\n            \"containers\": [\n              {\n                \"name\": \"web\",\n                \"image\": \"nginx:stable-alpine\",\n                \"ports\": [{\"containerPort\": 80}]\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n#### `interact_k8s_endpoint`\n\nSubmits a POST request to a resource or subresource endpoint, primarily used for actions like remote execution.\n\nParameters:\n\n- `scope` (required): Resource scope (\"cluster\" or \"namespaced\").\n- `api_group`: API group identifier.\n- `api_version` (required): API version identifier.\n- `resource_kind` (required): Plural resource collection name.\n- `target_ns`: Namespace (required for namespaced resources).\n- `entity_name` (required): Name of the target object (e.g., Pod name).\n- `action_path`: The subresource endpoint target (e.g., \"exec\").\n- `request_payload` (required): The body content for the POST operation.\n- `request_options`: Supplementary parameters for the POST request.\n\nExample (Executing a command via Pod exec):\n\n\n{\n  \"name\": \"interact_k8s_endpoint\",\n  \"arguments\": {\n    \"scope\": \"namespaced\",\n    \"api_group\": \"\",\n    \"api_version\": \"v1\",\n    \"resource_kind\": \"pods\",\n    \"target_ns\": \"dev\",\n    \"entity_name\": \"app-shell-01\",\n    \"action_path\": \"exec\",\n    \"request_payload\": {\n      \"command\": [\"sh\", \"-c\", \"echo Hello from LLM agent\"],\n      \"container\": \"main-app\",\n      \"timeout\": 45\n    }\n  }\n}\n\n\nPod Execution Payload Directives:\n\n*   `command`: The shell instruction(s) to run, specified as a list of strings.\n*   `container`: Specifies the container within a multi-container Pod.\n*   `timeout`: Duration limit for execution in seconds (capped at 60s; default is 15s).\n\nThe response structure contains collected output streams (`stdout`, `stderr`) alongside any system error indications.\n\n### MCP Resource Mapping\n\nK8S-MCP-INTERFACE exposes Kubernetes objects via standardized MCP URIs:\n\n*   Cluster-Scoped: `k8s://cluster/{group}/{version}/{resource}/{name}`\n*   Namespaced: `k8s://namespace/{namespace}/{group}/{version}/{resource}/{name}`\n\n### System Configuration Parameters\n\n#### Communication Transport Layer\n\nMKP supports two transport methodologies for MCP communication:\n\n*   **SSE (Server-Sent Events)**: The established default transmission mechanism.\n*   **Streaming HTTP**: A unified transport layer accommodating standard HTTP replies and SSE feeds, advantageous for specific orchestration frameworks like ToolHive.\n\nConfiguration is managed via CLI flags or the `MCP_TRANSPORT` environment variable:\n\nbash\n# Force Streaming HTTP transport\n./build/mkp-server --transport=streaming-http\n\n\n#### Controlling Resource Schema Advertising\n\nBy default, the server advertises the complete set of discoverable Kubernetes API resources to provide maximum context to consuming LLMs. For deployments in high-scale clusters, this introspection can consume valuable context tokens. Disable this advertising using the `--serve-resources` flag set to `false`:\n\nbash\n# Launching without advertising API schemas\n./build/mkp-server --serve-resources=false\n\n\nCrucially, disabling schema advertising does *not* deactivate the explicit MCP tools (`get_k8s_entity`, `scan_k8s_collection`, etc.); they remain fully operational for direct cluster interaction.\n\n#### Enabling Write/Mutation Permissions\n\nBy default, the service operates in a defensive, read-only mode, blocking tools that modify state (`deploy_k8s_entity`, etc.). To permit write operations, activate the `--read-write` directive:\n\nbash\n# Activate full read/write capabilities\n./build/mkp-server --read-write=true\n\n\n### Request Rate Governance\n\nTo ensure cluster stability when handling requests from autonomous agents, internal rate limiting is active by default (Token Bucket algorithm):\n\n*   Read Actions (`scan_k8s_collection`, `get_k8s_entity`): Capped at 120 transactions per minute per client session.\n*   Write Actions (`deploy_k8s_entity`, etc.): Capped at 30 transactions per minute per client session.\n*   Default Operations: Capped at 60 transactions per minute.\n\nRate control can be deactivated via the `--enable-rate-limiting=false` flag.\n\n## Integration with ToolHive Orchestrator\n\nK8S-MCP-INTERFACE is designed for streamlined deployment via the ToolHive ecosystem.\n\n### ToolHive Execution (Recommended)\n\n1.  Ensure ToolHive CLI is installed and configured (`thv client setup`).\n2.  Execute the packaged Kubernetes client (aliased as 'k8s' in the registry), mounting local kubeconfig credentials:\n\n    bash\n    thv run --volume $HOME/.kube:/home/nonroot/.kube:ro k8s\n    \n\n### Advanced Container Launch\n\nFor environments requiring explicit configuration:\n\nbash\n# Specify SSE transport and run against the latest published image\nthv run --name k8s-agent --transport sse --target-port 8080 --volume $HOME/.kube:/home/nonroot/.kube:ro ghcr.io/stackloklabs/mkp/server:latest\n\n\n## Contribution \u0026 Licensing\n\nContributions, bug reports, and feature proposals are highly encouraged via [Issues](https://github.com/StacklokLabs/mkp/issues). For direct engagement, join the community on the Stacklok [Discord server](https://discord.gg/stacklok) in the `#mcp-servers` channel.\n\nThis software is distributed under the terms of the Apache License, Version 2.0 (see LICENSE file).",
      "stars": 52,
      "updated_at": "2025-10-03T23:35:15Z",
      "url": "https://github.com/StacklokLabs/mkp"
    },
    "StacklokLabs--ocireg-mcp": {
      "category": "cloud-platforms",
      "description": "An SSE-conformant MCP intermediary for interacting with OCI artifact repositories, designed to expose container image metadata and structure to large language models.",
      "forks": 2,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "platform",
        "platforms",
        "cloud platforms",
        "platforms cloud",
        "cloud platform"
      ],
      "language": "Go",
      "license": "Apache License 2.0",
      "name": "oci-registry-mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "StacklokLabs",
      "readme_content": "# OCI Repository Model Context Protocol Server\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/StacklokLabs/ocireg-mcp)](https://archestra.ai/mcp-catalog/stackloklabs__ocireg-mcp)\nThis implementation establishes an MCP (Model Context Protocol) service endpoint facilitating queries against OCI (Open Container Initiative) artifact registries and image references.\n\n## Core Functionality\n\nThis project furnishes an SSE (Server-Sent Events) driven MCP gateway enabling language model-backed agents to interrogate OCI registries. It offers specialized endpoints for extracting container image specifics, enumerating available tags, and fetching detailed image specifications.\n\n## Capabilities\n\n*   Fetch metadata for specified OCI images\n*   Retrieve a complete listing of tags associated with a repository\n*   Acquire the raw image manifest document\n*   Download the image configuration blob\n\n## Exposed MCP Utilities\n\nThe server exposes the following specialized functions for LLM consumption:\n\n### get_image_info\n\nRetrieves comprehensive data pertaining to a single OCI image identifier.\n\n**Input Parameters:**\n\n- `image_ref`: The fully qualified image locator (e.g., \"docker.io/library/alpine:latest\")\n\n**Return Value:**\n\n- Structured data including the image digest hash, byte size, platform architecture, operating system, creation timestamp, and layer count.\n\n### list_tags\n\nGenerates a sequence of all known tags for a designated container repository.\n\n**Input Parameters:**\n\n- `repository`: The repository's canonical path (e.g., \"docker.io/library/alpine\")\n\n**Return Value:**\n\n- A collection (list) of string tags.\n\n### get_image_manifest\n\nReturns the raw OCI image manifest associated with a specific image reference.\n\n**Input Parameters:**\n\n- `image_ref`: The identifier for the image (e.g., \"docker.io/library/alpine:latest\")\n\n**Return Value:**\n\n- The raw image manifest content.\n\n### get_image_config\n\nFetches the configuration document embedded within an OCI image manifest.\n\n**Input Parameters:**\n\n- `image_ref`: The target image reference (e.g., \"docker.io/library/alpine:latest\")\n\n**Return Value:**\n\n- The image's configuration object.\n\n## Deployment Strategy\n\n### Integration with ToolHive (Recommended Pathway)\n\nThe simplest method for operationalizing the OCI Registry MCP service involves utilizing the [ToolHive](https://github.com/stacklok/toolhive) orchestration layer, which facilitates secure, containerized hosting of MCP intermediaries:\n\n```bash\n# Provision ToolHive if absent\n# Refer to: https://docs.stacklok.com/toolhive/guides-cli/install\n\n# Configure necessary client integrations for environment setup\nthv client setup\n# Deploy and launch the OCI Registry MCP server (packaged under 'oci-registry' in ToolHive)\nthv run oci-registry\n\n# Review currently active servers\nthv list\n\n# Query operational details for the deployed server instance\nthv registry info oci-registry\n```\n\nThe service will then be accessible to MCP-aware clients for querying OCI registry data.\n\n#### Secure Credential Handling via ToolHive\n\nFor access to private registries, authentication material can be injected using ToolHive's built-in secret management:\n\n```bash\n# For token-based authentication\nthv secret set oci-token\n# Input your bearer token when prompted\n\nthv run --secret oci-token,target=OCI_TOKEN oci-registry\n\n# For standard user/pass authentication\nthv secret set oci-username\nthv secret set oci-password\n# Input credentials upon request\n\nthv run --secret oci-username,target=OCI_USERNAME --secret oci-password,target=OCI_PASSWORD oci-registry\n```\n\n## Development Guidelines\n\n### Prerequisites for Building\n\n- Go language compiler version 1.21 or newer\n- Network access to target OCI registries\n\n### Registry Authentication Mechanisms\n\nThe server is engineered to support several methods for authenticating against restricted OCI repositories:\n\n1. **Bearer Token**: Defined via the `OCI_TOKEN` environment variable.\n\n   - `OCI_TOKEN`: The requisite bearer token for registry access\n\n2. **Username and Password Credentials**: Specified through environment variables.\n\n   - `OCI_USERNAME`: The credential for user identity\n   - `OCI_PASSWORD`: The associated secret credential\n\n3. **Docker Configuration File**: If neither token nor U/P is configured, the service defaults to utilizing the standard Docker credential helper chain, reading from `~/.docker/config.json`.\n\nExample Environment Variable Setup:\n\n```bash\n# Token Example\nexport OCI_TOKEN=your-secret-token-here\n\n# User/Password Example\nexport OCI_USERNAME=registry_user\nexport OCI_PASSWORD=registry_password\n```\n\n### Network Port Specification\n\nThe listening interface port can be dictated using one of two primary configuration sources:\n\n1. **Environment Variable**: \n\n   - `MCP_PORT`: Specifies the numeric listening port (must be in the range 0-65535).\n   - Default is 8080 if unset or invalid.\n\n2. **Command-line Argument**: \n   - `-port`: Takes precedence over the environment variable (must be a valid port number).\n   - Invalid argument values revert to the 8080 default.\n   - Execution illustration: `./ocireg-mcp -port 9090`\n\n### Quality Assurance Testing\n\nRun the integrated test suite:\n\n```bash\ngo test ./...\n```\n\n### Code Linting\n\nExecute the static analysis tool:\n\n```bash\ngolangci-lint run\n```\n\n## Collaboration\n\nWe encourage community involvement in the maintenance and enhancement of this MCP service! Please consult the [CONTRIBUTING guide](./CONTRIBUTING.md) for contribution instructions.\n\nShould you discover a fault or propose a new capability, kindly [report an issue](https://github.com/StacklokLabs/ocireg-mcp/issues) on the repository or engage with our team in the `#mcp-servers` channel on our [community Discord server](https://discord.gg/stacklok).\n\n## Licensing\n\nThis software is distributed under the terms of the Apache v2 License; review the LICENSE file for complete terms.",
      "stars": 10,
      "updated_at": "2025-09-28T06:14:55Z",
      "url": "https://github.com/StacklokLabs/ocireg-mcp"
    },
    "aashari--mcp-server-aws-sso": {
      "category": "cloud-platforms",
      "description": "A robust mechanism to securely bridge AI agents with Amazon Web Services (AWS) environments utilizing AWS IAM Identity Center (SSO). This tool facilitates credential retrieval, account/role enumeration, and secured execution of arbitrary AWS CLI operations via temporary session tokens.",
      "forks": 8,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "sso",
        "aws",
        "aws sso",
        "sso aws",
        "cloud platforms"
      ],
      "language": "TypeScript",
      "license": "No License",
      "name": "mcp-authenticator-aws-iam-identity-center",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "aashari",
      "readme_content": "# Bridging Conversational AI with Your Amazon Cloud Infrastructure\n\nRevolutionize your interaction model with AWS resources by establishing a direct, secure conduit from large language models (like Claude or Cursor) to your cloud accounts, authenticated via AWS Single Sign-On (IAM Identity Center). Obtain immediate access to inventory data, dispatch operational commands, and manage infrastructure components using natural language instructions.\n\n[![NPM Version](https://img.shields.io/npm/v/@aashari/mcp-server-aws-sso)](https://www.npmjs.com/package/@aashari/mcp-server-aws-sso)\n\n## Core Capabilities for AI Interaction\n\n‚úÖ **Inventory Inquiry**: *\"Enumerate all accessible AWS organizational units and the associated permission sets.\"*\n‚úÖ **CLI Command Dispatch**: *\"Execute 'aws ec2 describe-vpcs --output json' within my primary development account.\"*\n‚úÖ **Instance Management**: *\"Query the system uptime metrics for EC2 identifier i-123456789.\"*\n‚úÖ **Cross-Account Context Switching**: *\"Transition context to the production workspace and retrieve database configuration details.\"*\n‚úÖ **Resource Health Check**: *\"Report the current operational status for all active virtual machines.\"*\n‚úÖ **Remote Shell Execution**: *\"Fire the command 'yum update -y' against my staging server fleet utilizing SSM Agent.\"*\n\n## Optimal Use Cases\n\n- **Infrastructure Automation Specialists**: Streamlining operations across sprawling, multi-account AWS organizational structures.\n- **Cloud Solution Architects**: Rapid, on-demand retrieval of configuration metadata spanning disparate cloud workspaces.\n- **Software Developers**: Interfacing with AWS APIs and CLI tooling directly through conversational prompts.\n- **Site Reliability Engineers (SRE)**: Proactive diagnostics and incident response leveraging natural language interaction with cloud telemetry.\n- **System Administrators**: Secure remote execution and lifecycle management of EC2 virtual machines.\n- **Any User**: Seeking an intuitive, natural-language interface for interacting with the AWS ecosystem.\n\n## Rapid Deployment Guide\n\nAchieve operational status in under 120 seconds:\n\n### 1. AWS IAM Identity Center Prerequisite\n\nEnsure your AWS Identity Center environment is configured:\n1. **Activate AWS IAM Identity Center** within the target management account.\n2. **Integrate your authoritative identity provider** (e.g., Azure AD, Okta, or native AWS Directory Service).\n3. **Define granular permission sets** and map users/groups to specific AWS accounts.\n4. **Capture the AWS SSO Portal Start URL** (e.g., `https://your-organization.awsapps.com/sso/start`).\n\n### 2. Immediate Operational Testing\n\nbash\n# Configure necessary environmental variables pointing to your SSO instance\nexport AWS_SSO_START_URL=\"https://your-organization.awsapps.com/start\"\nexport AWS_REGION=\"us-west-2\"\n\n# Initiate the secure authentication handshake\nnpx -y @aashari/mcp-server-aws-sso login\n\n# Query for accessible cloud accounts and assumed roles\nnpx -y @aashari/mcp-server-aws-sso ls-accounts\n\n# Invoke an AWS operational command\nnpx -y @aashari/mcp-server-aws-sso exec-command \\\n  --account-id 987654321098 \\\n  --role-name SecurityAuditor \\\n  --command \"aws sts get-caller-identity\"\n\n\n## Integration with AI Frontends\n\n### For Claude Desktop Integrations\n\nAugment your local configuration file (`~/.claude/claude_desktop_config.json`):\n\n\n{\n  \"mcpServers\": {\n    \"aws-sso-identity\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@aashari/mcp-server-aws-sso\"],\n      \"env\": {\n        \"AWS_SSO_START_URL\": \"https://your-organization.awsapps.com/start\",\n        \"AWS_REGION\": \"us-west-2\"\n      }\n    }\n  }\n}\n\n\nUpon restarting the Claude client, a new communication endpoint, \"üîó aws-sso-identity,\" will appear in the control panel.\n\n### For Other MCP-Compliant Platforms\n\nGlobal installation is recommended for broadest utility:\n\nbash\nnpm install -g @aashari/mcp-server-aws-sso\n\n\nConfigure your chosen AI interface to utilize the server via its standard input/output (STDIO) communication channel.\n\n### System-Wide Configuration File\n\nAlternatively, define settings globally via `~/.mcp/configs.json`:\n\n\n{\n  \"aws-sso-identity\": {\n    \"environments\": {\n      \"AWS_SSO_START_URL\": \"https://your-organization.awsapps.com/start\",\n      \"AWS_REGION\": \"us-west-2\",\n      \"DEBUG_LOGGING\": \"false\"\n    }\n  }\n}\n\n\n**Alias Note**: The configuration key can also reference the package name directly (`\"@aashari/mcp-server-aws-sso\"`).\n\n## Real-World Interaction Scenarios\n\n### üîê Session Initialization and Context Verification\n\nPrompt your AI agent with:\n- *\"Initiate the AWS SSO authentication sequence and report the current session metadata.\"*\n- *\"Display a comprehensive list of all obtainable AWS accounts and their assumed roles.\"*\n- *\"Confirm the validity and expiry of the current authentication context.\"*\n- *\"Provide a catalog of all cloud accounts visible through the current Identity Center configuration.\"*\n\n### üõ†Ô∏è Remote AWS API Execution\n\nRequest actions like:\n- *\"Audit all S3 storage buckets in the production workspace utilizing the AuditorRole.\"*\n- *\"Retrieve the detailed configuration of all running EC2 virtual machines within the eu-central-1 zone.\"*\n- *\"Outline the Virtual Private Cloud (VPC) topology in the designated staging account.\"*\n- *\"Fetch the current operational status for all managed RDS database instances under account 987654321098.\"*\n\n### üñ•Ô∏è EC2 System-Level Interactions\n\nInstruct the AI to execute on specific instances:\n- *\"Determine the current filesystem utilization statistics for instance i-0a69e80761897dcce.\"*\n- *\"Fire the command 'systemctl restart httpd' on my primary web node via SSM Run Command.\"*\n- *\"Execute 'cat /proc/cpuinfo' on instance i-abc123 in the development environment.\"*\n- *\"Aggregate memory utilization metrics across the entire fleet of application servers.\"*\n\n### üîç Cloud Resource Monitoring\n\nAsk for status updates:\n- *\"List every deployed AWS Lambda function across my development accounts.\"*\n- *\"Fetch the status of all active CloudFormation stacks deployed in us-east-1.\"*\n- *\"Assess the operational health of all associated Elastic Load Balancers.\"*\n- *\"Retrieve the most recent CloudWatch alarms currently reporting an ALARM state.\"*\n\n### üîÑ Multi-Workspace Operations\n\nOrchestrate across boundaries:\n- *\"Switch context to account 112233445566 using the GlobalAdminRole and list all associated security groups.\"*\n- *\"Perform a comparison of running compute instance counts between staging and production tenants.\"*\n- *\"Audit the established backup policies across the entire set of connected AWS tenants.\"*\n- *\"Review and report on all IAM users within the dedicated security operations account.\"*\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eProtocol Method Signatures (JSON Invocation Examples)\u003c/b\u003e\u003c/summary\u003e\n\n### `aws_sso_login` (Authentication Initiation)\n\n**Default Invocation:**\n\n{}\n\n\n**Disabling Browser Launch:**\n\n{\n  \"launchBrowser\": false\n}\n\n\n### `aws_sso_status` (Context Verification)\n\n**Check Authentication State:**\n\n{}\n\n\n### `aws_sso_list_workspaces` (Inventory Retrieval)\n\n**Fetch Account/Role Inventory:**\n\n{}\n\n\n### `aws_sso_invoke_cli` (General AWS API Call)\n\n**Listing S3 Assets:**\n\n{\n  \"accountId\": \"123456789012\", \n  \"roleName\": \"ReadOnly\",\n  \"command\": \"aws s3 ls\"\n}\n\n\n**Region-Scoped EC2 Query:**\n\n{\n  \"accountId\": \"123456789012\",\n  \"roleName\": \"Administrator\",\n  \"command\": \"aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,InstanceType]' --output table\",\n  \"region\": \"eu-west-1\"\n}\n\n\n### `aws_sso_run_ssm_command` (Remote Instance Operations)\n\n**System Diagnostics:**\n\n{\n  \"instanceId\": \"i-0a69e80761897dcce\",\n  \"accountId\": \"123456789012\",\n  \"roleName\": \"OpsEngineer\",\n  \"command\": \"uptime \u0026\u0026 df -h \u0026\u0026 free -m\"\n}\n\n\n\u003c/details\u003e\n\n## Communication Transport Mechanisms\n\nThis server supports dual modes for integration flexibility:\n\n### STDIO Transport (Standard for MCP Clients)\n- Utilizes conventional pipe-based communication via standard input/output streams.\n- Optimal for tightly coupled, local agent execution environments (e.g., Claude Desktop).\n\nbash\n# Explicitly force STDIO mode for local execution\nTRANSPORT_MODE=stdio npx @aashari/mcp-server-aws-sso\n\n# Using installed package script\nnpm run mcp:stdio\n\n\n### HTTP Transport (Default for Standalone Server Mode)\n- Employs modern HTTP/HTTPS protocols, leveraging Server-Sent Events (SSE) for push capabilities.\n- Designed for asynchronous interactions, supporting multiple concurrent client sessions.\n- Default listening port is 3000 (adjustable via `PORT` environment variable).\n- Primary API Endpoint: `http://localhost:3000/mcp`\n- System Health Check: `http://localhost:3000/`\n\nbash\n# Launch as a persistent HTTP service\nTRANSPORT_MODE=http npx @aashari/mcp-server-aws-sso\n\n# Using npm scripts\nnpm run mcp:http\n\n# Utility for inspecting HTTP traffic\nnpm run mcp:inspect\n\n\n### Essential Environment Variables\n\n**Transport Settings:**\n- `TRANSPORT_MODE`: Set to `stdio` or `http` (Default: `http` for server launch, `stdio` for client context).\n- `PORT`: Defines the network port for HTTP operation (Default: 3000).\n- `DEBUG`: Toggles verbose diagnostic output (Boolean).\n\n**AWS Authentication Context:**\n- `AWS_SSO_START_URL`: Mandatory entry point URL for Identity Center.\n- `AWS_SSO_REGION`: The AWS region where your Identity Center is provisioned.\n- `AWS_PROFILE`: Optional override for specific AWS CLI profile configuration.\n- `AWS_REGION`: Default region for subsequent AWS CLI operations if not specified in the command JSON.\n\n## Command Line Interface (CLI) Reference\n\nCLI operations adhere to `kebab-case` conventions. Use the `--help` flag for method-specific parameter details (e.g., `mcp-aws-sso login --help`).\n\n- **login**: Triggers SSO authentication sequence (`--no-launch-browser` suppresses browser opening). Usage: `mcp-aws-sso login`.\n- **status**: Reports current token validity (No arguments required). Usage: `mcp-aws-sso status`.\n- **ls-accounts**: Enumerates accessible AWS workspaces and roles (No arguments required). Usage: `mcp-aws-sso ls-accounts`.\n- **exec-command**: Executes arbitrary AWS CLI statements (`--account-id`, `--role-name`, `--command`, `--region` are key parameters). Usage example: `mcp-aws-sso exec-command --account-id 123456789012 --role-name Auditor --command \"aws sts get-caller-identity\"`.\n- **ec2-exec-command**: Executes OS-level commands on target instances via SSM (`--instance-id`, `--account-id`, `--role-name`, `--command`, `--region`). Usage example: `mcp-aws-sso ec2-exec-command --instance-id i-0a69e80761897dcce --account-id 123456789012 --role-name SysAdmin --command \"uptime\"`.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eCLI Command Walkthroughs\u003c/b\u003e\u003c/summary\u003e\n\n### Session Login\n\n**Standard Browser-Assisted Login:**\nbash\nmcp-aws-sso login\n\n\n**Non-Interactive Login Attempt:**\nbash\nmcp-aws-sso login --no-launch-browser\n\n\n### AWS API Command Invocation\n\n**S3 Bucket Listing:**\nbash\nmcp-aws-sso exec-command \\\n  --account-id 123456789012 \\\n  --role-name ReadOnly \\\n  --command \"aws s3 ls\"\n\n\n**Region-Specific EC2 Inventory:**\nbash\nmcp-aws-sso exec-command \\\n  --account-id 123456789012 \\\n  --role-name PowerUser \\\n  --region ap-southeast-2 \\\n  --command \"aws ec2 describe-instances --output table\"\n\n\n### SSM Instance Command Execution\n\n**System Resource Check:**\nbash\nmcp-aws-sso ec2-exec-command \\\n  --instance-id i-0a69e80761897dcce \\\n  --account-id 123456789012 \\\n  --role-name InfraOps \\\n  --command \"uptime \u0026\u0026 df -h \u0026\u0026 free -m\"\n\n\n\u003c/details\u003e\n\n## Diagnostic and Error Resolution\n\n### ‚ö†Ô∏è Encountering \"Authentication Failure\" or \"Token Invalidation\"\n\n1. **Re-establish Session**: Force a fresh authentication pass:\n   bash\n   npx -y @aashari/mcp-server-aws-sso login\n   \n\n2. **Validate Configuration Inputs**:\n   - Confirm the `AWS_SSO_START_URL` precisely matches your organization's Identity Center access portal.\n   - Ensure `AWS_SSO_REGION` aligns with the region hosting your Identity Center configuration.\n\n3. **Browser Verification**:\n   - Manually navigate to the SSO Start URL to confirm accessibility and valid user session status.\n\n### ‚ö†Ô∏è \"Target Account/Role Not Resolvable\"\n\n1. **Review Accessible Entities**:\n   bash\n   npx -y @aashari/mcp-server-aws-sso ls-accounts\n   \n\n2. **ID Format Confirmation**:\n   - Account identifiers must strictly adhere to the 12-digit standard shown in the listing output.\n\n3. **Permission Set Validation**:\n   - Verify that the assigned permission set grants the ability to assume the role name specified in the request.\n\n### ‚ö†Ô∏è \"AWS CLI Command Execution Errors\"\n\n1. **Prerequisite Check: AWS CLI v2 Installation**:\n   - Installation is mandatory for API interaction tooling. Consult the [Official AWS CLI Installation Guide](https://aws.amazon.com/cli/).\n   - Confirm that the `aws` executable is reachable system-wide (i.e., present in the PATH).\n\n2. **Independent CLI Verification**:\n   bash\n   aws --version\n   aws sts get-caller-identity\n   \n\n### ‚ö†Ô∏è \"SSM Command Delivery Failure on EC2\"\n\n1. **Instance Prerequisites**:\n   - The target EC2 instance must have the SSM Agent installed and confirmed to be running.\n   - The instance IAM profile requires the `AmazonSSMManagedInstanceCore` managed policy attached.\n\n2. **Assumed Role Permissions**:\n   - The role used for execution must possess the `ssm:SendCommand` and `ssm:GetCommandInvocation` permissions.\n   - Confirm the EC2 instance is in a 'running' state.\n\n3. **SSM Connectivity Test**:\n   bash\n   # Verify the instance can communicate with the SSM service endpoint\n   npx -y @aashari/mcp-server-aws-sso exec-command \\\n     --account-id YOUR_ACCOUNT \\\n     --role-name YOUR_ROLE \\\n     --command \"aws ssm get-parameters-by-path --path /\"\n   \n\n### Claude Desktop Configuration Problems\n\n1. Always perform a **complete restart** of the Claude application after modifying the configuration structure.\n2. Visually confirm the presence of the connection indicator: **\"üîó aws-sso-identity\"**.\n3. Double-check the configuration file path for your operating system:\n   - macOS: `~/.claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n### Seeking Advanced Support\n\nIf standard troubleshooting fails:\n1. Execute a minimal, verifiable command to isolate the failure point.\n2. Search existing resolutions on the [Official GitHub Repository Issues Tracker](https://github.com/aashari/mcp-server-aws-sso/issues).\n3. Submit a detailed new issue, including environment specifications and full error tracebacks.\n\n## Frequently Encountered Queries\n\n### What credential access permissions are mandatory?\n\n**For SSO Environment Setup:**\n- An active AWS SSO account linked to IAM Identity Center.\n- Administrator clearance to define and assign permission sets.\n- Necessary permissions granted by your IAM administrator to access target AWS environments.\n\n**For SSM Remote Operations:**\n- The assumed role requires fine-grained permissions for SSM interaction (`ssm:SendCommand`, etc.).\n- Target EC2 instances must possess the `AmazonSSMManagedInstanceCore` IAM policy.\n\n### Can this utility manage credentials across multiple distinct AWS Organizations?\n\nPresently, the persistent configuration maps to a singular AWS SSO Start URL. Managing multiple independent organizations necessitates either reconfiguring the environment variables between sessions or leveraging distinct deployment instances of the server for each organization.\n\n### What is the standard validity period for retrieved SSO session tokens?\n\nAWS SSO session lifetimes are generally set between 8 to 12 hours. The temporary credentials generated for specific account/role assumptions usually expire after 60 minutes. The MCP server actively manages token refresh and secure credential caching to ensure seamless operation.\n\n### Which AI frameworks are compatible with this protocol implementation?\n\nThis server adheres strictly to the Model Context Protocol (MCP) specification, supporting:\n- Claude Desktop Client (Primary Target)\n- Cursor AI Development Environment\n- Continue.dev\n- Numerous other MCP-enabled artificial intelligence platforms.\n\n### How is data privacy and security maintained?\n\nSecurity is paramount:\n- **Local Execution**: All authentication processes and command relay occur exclusively on your host machine.\n- **Credential Ownership**: Leverages your existing, protected AWS SSO credentials.\n- **No Data Exfiltration**: Sensitive session data is never transmitted to external servers.\n- **Principle of Least Privilege**: Only accesses resources explicitly permitted by your assumed IAM roles.\n- **Ephemeral Credentials**: Utilizes short-lived session tokens that automatically become invalid.\n\n### Is prior installation of the AWS CLI mandatory?\n\nWhile the core authentication and inventory fetching tools operate independently, the `aws_sso_exec_command` functionality critically relies on having **AWS CLI v2** installed and accessible in the execution path for constructing and parsing API calls.\n\n### Does this tool interact with standard AWS CLI profile files (~/.aws/credentials)?\n\nNo. This server establishes and maintains its own secure, temporary credential store derived directly from the SSO login flow, bypassing reliance on static profiles defined in the standard AWS CLI configuration directories.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eProtocol Output Schema Examples\u003c/b\u003e\u003c/summary\u003e\n\n### MCP Tool Success Output (`aws_sso_invoke_cli`)\n\nmarkdown\n# AWS SSO: Command Execution Report\n\n**Context**: 987654321098/SecurityAuditor\n**Region**: ap-northeast-1 (Defaulted: us-east-1)\n\n## Executed Operation\n\n\taws s3 ls\n\n## Standard Output Stream\n\n\t2023-07-01 10:00:00 global-config-bucket\n\t2024-01-20 15:30:11 compliance-data-archive\n\t2025-04-11 09:12:45 log-retention-store\n\n*Invocation Time: 2025-05-19 06:21:49 UTC*\n\n\n### MCP Tool Failure Report Example\n\nmarkdown\n# üõë AWS SSO: Operation Failure Notice\n\n**Context**: 123456789012/ReadOnly\n**Region**: us-east-1 (Defaulted: ap-southeast-1)\n\n## Executed Operation\n\t\n\taws s3api delete-object --bucket sensitive-data-store --key old_backup.zip\n\n## Error Classification: Authorization Violation\nAttempted operation is prohibited for the currently assumed role (`ReadOnly`).\n\n## Detailed API Error Message\n\n\tAn error occurred (AccessDenied) when calling the DeleteObject operation: Access Denied to resource 'sensitive-data-store'.\n\n### Remediation Suggestions\n\n#### Identified Permitted Roles\n- IAMFullAccess\n- S3Admin\n- SecurityReadOnly\n\nRe-submit the request using a role possessing explicit write/delete permissions for S3 resources.\n\n*Invocation Time: 2025-05-19 06:17:49 UTC*\n\n\n\u003c/details\u003e\n\n## Developer Contribution Guide\n\nbash\n# Obtain the source repository\ngit clone https://github.com/aashari/mcp-server-aws-sso.git\ncd mcp-server-aws-sso\n\n# Install requisite packages for development\nnpm install\n\n# Initiate development watch-mode execution\nnpm run dev:server\n\n# Execute comprehensive unit and integration test suite\nnpm test\n\n\n## Support Channels\n\nIf you require assistance beyond the self-help documentation:\n\n1. **Review Diagnostics Above**: Check the comprehensive troubleshooting matrix first.\n2. **Visit the Source Repository**: Comprehensive documentation and roadmap details are maintained at: [github.com/aashari/mcp-server-aws-sso](https://github.com/aashari/mcp-server-aws-sso)\n3. **File a Bug Report**: Use the official tracker for reproducible errors: [GitHub Issues](https://github.com/aashari/mcp-server-aws-sso/issues)\n4. **General Inquiries**: Open a discussion thread for feature suggestions or general architecture questions.\n\n--- \n\n*Engineered with precision for enterprise DevOps teams seeking to seamlessly integrate conversational AI into their AWS operational framework.*\n",
      "stars": 6,
      "updated_at": "2025-10-02T10:59:59Z",
      "url": "https://github.com/aashari/mcp-server-aws-sso"
    },
    "alexbakers--mcp-ipfs": {
      "category": "cloud-platforms",
      "description": "upload and manipulation of IPFS storage",
      "forks": 11,
      "imageUrl": "",
      "keywords": [
        "ipfs",
        "cloud",
        "platform",
        "platforms cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "mcp-ipfs",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "alexbakers",
      "readme_content": "# ü™ê MCP IPFS Server (storacha.network) üõ∞Ô∏è\n\n![Screenshot](https://raw.githubusercontent.com/alexbakers/mcp-ipfs/refs/heads/main/mcp-ipfs.png?neon-game)\n\n[![Publish Docker](https://github.com/alexbakers/mcp-ipfs/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/alexbakers/mcp-ipfs/actions/workflows/publish-docker.yml) [![Publish NPM](https://github.com/alexbakers/mcp-ipfs/actions/workflows/publish-npm.yml/badge.svg)](https://github.com/alexbakers/mcp-ipfs/actions/workflows/publish-npm.yml) [![npm version](https://badge.fury.io/js/mcp-ipfs.svg)](https://badge.fury.io/js/mcp-ipfs)\n[![smithery badge](https://smithery.ai/badge/@alexbakers/mcp-ipfs)](https://smithery.ai/server/@alexbakers/mcp-ipfs)\n\nA Node.js server implementing the [Model Context Protocol (MCP)](https://github.com/ModelContextProtocol/specification) for interacting with the [storacha.network](https://storacha.network/) platform via the `w3` command-line interface (`@web3-storage/w3cli`).\n\nThis server empowers language models ü§ñ and other MCP clients to manage storacha.network spaces, upload/download data, manage delegations, and perform various other tasks by seamlessly wrapping `w3` commands.\n\n## ‚ú® Features\n\n- Wraps the `w3` CLI for native integration with storacha.network.\n- Provides MCP tools covering a wide range of `w3` functionality:\n  - üîë **Authentication \u0026 Agent:** `w3_login`, `w3_reset`, `w3_account_ls` (for checking authorization)\n  - üì¶ **Space Management:** `w3_space_ls`, `w3_space_use`, `w3_space_info`, `w3_space_add`, `w3_space_provision` (Note: `w3_space_create` must be run manually due to interactive prompts)\n  - üíæ **Data Management:** `w3_up`, `w3_ls`, `w3_rm`\n  - üîó **Sharing:** `w3_open` (generates w3s.link URL)\n  - ü§ù **Delegations \u0026 Proofs:** `w3_delegation_create`, `w3_delegation_ls`, `w3_delegation_revoke`, `w3_proof_add`, `w3_proof_ls`\n  - üîê **Keys \u0026 Tokens:** `w3_key_create`, `w3_bridge_generate_tokens`\n  - ‚öôÔ∏è **Advanced Storage (`w3 can ...`):** Blob, CAR, Upload, Index, Access Claim, Filecoin Info management\n  - üí≥ **Account \u0026 Billing:** `w3_plan_get`, `w3_coupon_create`, `w3_usage_report`\n\n## üõ†Ô∏è Prerequisites\n\n- **Node.js:** Version 22.0.0 or higher (`node -v`).\n- **`w3` CLI:** The server executes `w3` commands directly. Ensure `@web3-storage/w3cli` is installed globally and configured:\n  ```bash\n  npm install -g @web3-storage/w3cli\n  w3 login \u003cyour-email@example.com\u003e\n  # Follow email verification steps\n  ```\n- **Environment Variable:** The `w3_login` tool requires the `W3_LOGIN_EMAIL` environment variable to be set to the same email used for `w3 login`.\n\n## üèóÔ∏è Project Structure\n\nThe codebase is organized as follows:\n\n```\nsrc/\n‚îú‚îÄ‚îÄ index.ts          # Main server entry point, MCP setup, request routing\n‚îú‚îÄ‚îÄ schemas.ts        # Zod schemas defining input arguments for each tool\n‚îú‚îÄ‚îÄ tool_handlers.ts  # Implementation logic for each MCP tool\n‚îú‚îÄ‚îÄ utils.ts          # Helper functions (e.g., running w3 commands, parsing JSON)\n‚îî‚îÄ‚îÄ utils/\n    ‚îî‚îÄ‚îÄ logger.ts     # Basic logger configuration\n```\n\n## üöÄ Usage with MCP Clients\n\nThis server can be used with any MCP-compatible client. You need to configure your client to connect to this server.\n\n### Example: NPX (Recommended for simple local use)\n\nThis assumes `npm` and the prerequisites are met.\n\n```json\n{\n  \"mcpServers\": {\n    \"ipfs\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-ipfs\"],\n      \"env\": {\n        \"W3_LOGIN_EMAIL\": \"your-email@example.com\"\n      }\n    }\n  }\n}\n```\n\n### Example: Docker\n\nBuild the image first (see Build section) or use the pre-built image `alexbakers/mcp-ipfs`.\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-ipfs\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/your/project:/path/to/your/project\",\n        \"-e\",\n        \"W3_LOGIN_EMAIL\",\n        \"alexbakers/mcp-ipfs\"\n      ],\n      \"env\": {\n        \"W3_LOGIN_EMAIL\": \"your-email@example.com\"\n      }\n    }\n  }\n}\n```\n\n#### üìù Note on Paths:\n\nSeveral `w3` commands require **absolute filesystem paths** (e.g., `w3_up`, `w3_delegation_create --output`, `w3_proof_add`, `w3_can_blob_add`, `w3_can_store_add`).\n\n- **NPX:** Provide absolute paths from your host machine.\n- **Docker:** Provide absolute paths _inside the container_. If interacting with files from your host (e.g., uploading), you **must** mount the relevant host directory into the container using the `-v` flag (e.g., `-v /Users/me/project:/Users/me/project`) and then use the _container path_ (e.g., `/Users/me/project/my_file.txt`) in the tool arguments.\n\n## üì¶ Build\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/alexbakers/mcp-ipfs.git\ncd mcp-ipfs\nnpm install\n```\n\nBuild the TypeScript code:\n\n```bash\nnpm run build\n```\n\nYou can then run the server directly:\n\n```bash\n# Ensure W3_LOGIN_EMAIL is set in your environment\nexport W3_LOGIN_EMAIL=\"your-email@example.com\"\nnode dist/index.js\n```\n\nOr publish it (if you have the rights):\n\n```bash\nnpm publish\n```\n\n### üê≥ Docker Build\n\nBuild the Docker image:\n\n```bash\n# Build locally (replace with your username/repo and desired tag)\ndocker build -t alexbakers/mcp-ipfs .\n```\n\n## üìú License\n\nThis MCP server is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "stars": 11,
      "updated_at": "2025-08-14T02:33:10Z",
      "url": "https://github.com/alexbakers/mcp-ipfs"
    },
    "alexei-led--aws-mcp-server": {
      "category": "cloud-platforms",
      "description": "A highly optimized, secure daemon facilitating secure execution of Google Cloud SDK (gcloud) operations via the Model Context Protocol (MCP), operating within isolated, multi-arch container environments.",
      "forks": 27,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "aws",
        "docker",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "gcp-mcp-service-bridge",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "alexei-led",
      "readme_content": "# GCP Model Context Protocol (MCP) Orchestrator\n\n[![Build Status](https://img.shields.io/badge/Build-Passing-success)](link-to-ci)\n[![Coverage](https://codecov.io/gh/user/gcp-mcp-bridge/branch/main/badge)](link-to-codecov)\n[![Architecture Support](https://img.shields.io/badge/Arch-AMD64%20%7C%20ARM64-blue)](link-to-arch-support)\n[![License](https://img.shields.io/badge/License-Apache--2.0-orange.svg)](LICENSE)\n\nA robust intermediary service engineered to interface MCP-enabled Artificial Intelligence agents (such as advanced LLM IDEs) with the comprehensive functionality of the Google Cloud Command Line Interface (`gcloud`). This service ensures secure command dispatch, output sanitization, and context awareness within a containerized deployment model.\n\n## Core Functionality\n\nThis orchestrator acts as a secure proxy, enabling AI systems to systematically interact with GCP resources by providing two primary capabilities:\n\n1. **Schema Retrieval (`gcloud_documentation`)**: Instantly fetches up-to-date official documentation and usage parameters for any `gcloud` component.\n2. **Command Execution (`gcloud_pipeline`)**: Executes complex `gcloud` sequences, incorporating standard Unix command piping (`|`) and filtering for optimized, AI-consumable results.\n\nmermaid\nflowchart TD\n    AI[Intelligent Agent (MCP Client)] \u003c--\u003e|Protocol Interaction| Bridge[GCP MCP Orchestrator]\n    Bridge \u003c--\u003e|Subprocess Execution| GCLOUD[gcloud CLI]\n    GCLOUD \u003c--\u003e|Service Calls| GCP_API[Google Cloud Platform Services]\n\n\n## Key Attributes\n\n*   **Secure Execution Context**: All operations are sandboxed within a Docker container, drastically limiting potential host impact.\n*   **Multi-Architecture**: Built natively for x86_64 (Intel/AMD) and AArch64 (ARM/Graviton) systems.\n*   **Unix Pipeline Support**: Allows chaining commands (e.g., `gcloud compute instances list | jq .`) for advanced data manipulation.\n*   **Contextual Awareness (MCP Resources)**: Provides the agent with readily available GCP credentials context, active project IDs, and region configurations.\n*   **Templated Workflows**: Supports parameterized prompt templates for complex, repeatable GCP management tasks.\n*   **Credential Agnostic**: Seamlessly utilizes existing, host-provided GCP authentication methods.\n\n## Deployment Modalities\n\nDeployment is achieved primarily via containerization, which is the **mandated best practice** for security and operational consistency.\n\n### Option 1: Container Orchestration (Recommended)\n\nLeverage Docker Compose for straightforward setup:\n\nbash\ngit clone https://github.com/user/gcp-mcp-bridge.git\ncd gcp-mcp-bridge\n\n# Deploy in detached mode\ndocker compose up -d\n\n\nImages are available on the container registry, supporting multi-arch pulls:\n\nbash\n# Latest stable build\ndocker pull ghcr.io/user/gcp-mcp-bridge:stable\n\n# Version pinned (e.g., v1.5.0)\ndocker pull ghcr.io/user/gcp-mcp-bridge:v1.5.0\n\n\n### Option 2: Native Python Execution (Use with Caution)\n\nFor environments where containerization is infeasible, native setup is possible. This path requires manual dependency management and inherently carries higher risk:\n\nbash\n# Setup Python environment\npython3 -m venv venv \u0026\u0026 source venv/bin/activate\n\npip install -e .[gcp-native]\n\n# Initiate the service\npython -m gcp_mcp_bridge.server\n\n\n## Configuration Parameters\n\nConfiguration utilizes standard environment variables to dictate operational parameters:\n\n| Variable Name | Purpose | Default Value |\n| :--- | :--- | :--- |\n| `GCP_MCP_SESSION_TTL` | Maximum allowable command duration (seconds) | 360 |\n| `GCP_MCP_MAX_RESPONSE_BYTES` | Cap on output payload size | 150000 |\n| `GCP_MCP_INTERFACE` | Communication mechanism (\"stdio\" or \"http_sse\") | stdio |\n| `CLOUDSDK_CORE_PROJECT` | Default GCP Project ID to utilize | (Inherited from environment) |\n| `CLOUDSDK_COMPUTE_ZONE` | Default Compute Zone | us-central1-a |\n| `GCP_MCP_LOCKDOWN_LEVEL` | Security enforcement mode (\"enforce\" or \"audit\") | enforce |\n\n## Security Mandates and Isolation\n\n**Security is the foundational design principle.** The architecture emphasizes defense-in-depth, primarily relying on container isolation and stringent command filtering.\n\n### 1. Containerization: The Primary Barrier\n\nExecuting within Docker is non-negotiable for secure operation. This isolates the execution context, preventing malicious or erroneous shell commands from achieving persistence or modifying the host operating system's filesystem or processes.\n\n### 2. IAM Principle of Least Privilege (Critical)\n\nThe service inherits the credentials provided to it (via `gcloud auth application-default login` or mounted configuration). **It is imperative that these credentials** belong to an identity granted the absolute minimum required IAM permissions (Least Privilege). This policy configuration forms the ultimate control boundary, as no executed command can exceed the permissions granted to the underlying service account or user.\n\n### 3. Command Validation Layer\n\nThe orchestrator employs a rigorous, multi-stage inspection process before executing any command:\n\n*   **Syntax Check**: Initial verification of command structure (must start with `gcloud`).\n*   **Keyword Filtering**: Explicitly blocks high-risk verbs associated with credential manipulation, logging disruption, or resource destruction.\n*   **Pipe Sanitization**: Scrutinizes any chained Unix utilities (e.g., `grep`, `awk`, `xargs`) against a predefined safe allowlist to prevent command injection.\n\n### High-Risk Command Blocklist (Examples)\n\nCommands related to IAM modification, logging cessation, or resource destruction are typically forbidden:\n\n*   **IAM Tampering**: `gcloud iam service-accounts create`, `gcloud iam policies set-iam-policy`, `gcloud iam keys create`.\n*   **Audit Disruption**: `gcloud logging sinks delete`, `gcloud audit-logs disable`.\n*   **Destructive Actions**: `gcloud compute instances delete`, `gcloud storage buckets delete`.\n\n**Allowed Operations**: All standard read operations (all commands prefixed with `describe-`, `list-`, `get-`) are permitted by default, provided they do not violate other filtering rules.\n\n### Security Modes\n\n*   `enforce` (Default): Blocks any detected risky operation and reports an error.\n*   `audit`: Executes the command but logs a high-severity warning if a potentially risky pattern is matched, offering visibility without immediate termination.\n\n## Available Service Templates\n\nThe service exposes numerous pre-configured operational templates to guide the AI agent toward secure and efficient GCP interactions.\n\n| Template Name | Functionality Description | Required Arguments |\n| :--- | :--- | :--- |\n| `gcp_deploy_vm` | Generates idempotent commands for spinning up Compute Engine instances. | `instance_name`, `zone`, `machine_type` |\n| `gcp_inventory_gcs` | Outputs a detailed, filtered inventory of all Cloud Storage buckets. | `project_id` (optional) |\n| `gcp_iam_reader` | Creates a policy granting only `*.get` and `*.list` permissions for specified services. | `services_list` |\n| `gcp_k8s_status` | Retrieves aggregated status across all GKE clusters in a region. | `region` |\n| `gcp_cleanup_stale_resources` | Identifies and generates commands to delete resources older than a threshold. | `resource_type`, `age_days` |\n\n## Development \u0026 Contribution\n\nThis project welcomes contributions. The development workflow prioritizes code quality and testing.\n\n### Setup\n\nUse the provided `Makefile` targets for standardized tasks:\n\nbash\nmake setup-dev  # Installs all required dependencies, including testing tools\nmake check-code   # Runs static analysis (linting)\nmake run-tests    # Executes all unit tests\n\n\n### Integration Testing\n\nIntegration tests validate against live GCP infrastructure. Before running:\n\n1.  Ensure environment variables like `CLOUDSDK_CORE_PROJECT` and authentication are set.\n2.  Define a dedicated test project/bucket.\n3.  Execute with the appropriate pytest marker:\n    bash\n    pytest --marker=integration\n    \n\n## Licensing\n\nDistributed under the Apache License 2.0. Refer to the LICENSE file for comprehensive details.\n\n---\n\n**Contextual Reference (Non-Tool Content)**: Google Cloud Platform (GCP) is Google's suite of public cloud services, built upon the same infrastructure powering Google Search and Gmail. It offers IaaS, PaaS, and serverless capabilities. Key compute offerings include App Engine, Compute Engine (VMs), Google Kubernetes Engine (GKE), and Cloud Functions. Storage solutions encompass Cloud Storage (object), Cloud SQL (relational), and Cloud Spanner (horizontally scalable relational DB). GCP is now commonly referred to simply as Google Cloud.",
      "stars": 165,
      "updated_at": "2025-10-02T23:00:13Z",
      "url": "https://github.com/alexei-led/aws-mcp-server"
    },
    "alexei-led--k8s-mcp-server": {
      "category": "cloud-platforms",
      "description": "A lightweight yet robust server that empowers AI assistants to securely execute Kubernetes CLI commands (`kubectl`, `helm`, `istioctl`, and `argocd`) using Unix pipes in a safe Docker environment with multi-architecture support.",
      "forks": 32,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "docker",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "k8s-mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "alexei-led",
      "readme_content": "# K8s MCP Server\n\n[![CI Status](https://github.com/alexei-led/k8s-mcp-server/actions/workflows/ci.yml/badge.svg)](https://github.com/alexei-led/k8s-mcp-server/actions/workflows/ci.yml)\n[![Release Status](https://github.com/alexei-led/k8s-mcp-server/actions/workflows/release.yml/badge.svg)](https://github.com/alexei-led/k8s-mcp-server/actions/workflows/release.yml)\n[![codecov](https://codecov.io/gh/alexei-led/k8s-mcp-server/graph/badge.svg?token=eCaXPJ0olS)](https://codecov.io/gh/alexei-led/k8s-mcp-server)\n[![Image Tags](https://ghcr-badge.egpl.dev/alexei-led/k8s-mcp-server/tags?color=%2344cc11\u0026ignore=latest\u0026n=4\u0026label=image+tags\u0026trim=)](https://github.com/alexei-led/k8s-mcp-server/pkgs/container/k8s-mcp-server/versions)\n[![Image Size](https://ghcr-badge.egpl.dev/alexei-led/k8s-mcp-server/size?color=%2344cc11\u0026tag=latest\u0026label=image+size\u0026trim=)](https://github.com/alexei-led/k8s-mcp-server/pkgs/container/k8s-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nK8s MCP Server is a Docker-based server implementing [Anthropic's Model Context Protocol (MCP)](https://www.anthropic.com/news/introducing-mcp) that enables Claude to run Kubernetes CLI tools (`kubectl`, `istioctl`, `helm`, `argocd`) in a secure, containerized environment.\n\n## Demo: Deploy and Troubleshoot WordPress\n\n**Session 1:** Using k8s-mcp-server and Helm CLI to deploy a WordPress application in the claude-demo namespace, then intentionally breaking it by scaling the MariaDB StatefulSet to zero.\n\n**Session 2:** Troubleshooting session where we use k8s-mcp-server to diagnose the broken WordPress site through kubectl commands, identify the missing database issue, and fix it by scaling up the StatefulSet and configuring ingress access..\n\n[Demo](https://private-user-images.githubusercontent.com/1898375/428398164-5ddce5bc-ec92-459b-a506-5d4442618a81.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNDE0OTEsIm5iZiI6MTc0MzM0MTE5MSwicGF0aCI6Ii8xODk4Mzc1LzQyODM5ODE2NC01ZGRjZTViYy1lYzkyLTQ1OWItYTUwNi01ZDQ0NDI2MThhODEubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDMzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAzMzBUMTMyNjMxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmUyNDExMGUzOGRlN2QxNWViMzhhOTE4Y2U1ZmRjMTQxYTI0OGNlNTFjNTRlMjFjNmQ3NTNhNGFmODNkODIzMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.hwKERwuQRXxHEYJ9d_fQ__XL1gj8l76nO6Yy6M4Uov8)\n\n## How It Works\n\n```mermaid\nflowchart LR\n    A[User] --\u003e |Asks K8s question| B[Claude]\n    B --\u003e |Sends command via MCP| C[K8s MCP Server]\n    C --\u003e |Executes kubectl, helm, etc.| D[Kubernetes Cluster]\n    D --\u003e |Returns results| C\n    C --\u003e |Returns formatted results| B\n    B --\u003e |Analyzes \u0026 explains| A\n```\n\nClaude can help users by:\n- Explaining complex Kubernetes concepts\n- Running commands against your cluster\n- Troubleshooting issues\n- Suggesting optimizations\n- Crafting Kubernetes manifests\n\n## Quick Start with Claude Desktop\n\nGet Claude helping with your Kubernetes clusters in under 2 minutes:\n\n1. **Create or update your Claude Desktop configuration file**:\n   - **macOS**: Edit `$HOME/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: Edit `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - **Linux**: Edit `$HOME/.config/Claude/claude_desktop_config.json`\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"kubernetes\": {\n         \"command\": \"docker\",\n         \"args\": [\n           \"run\",\n           \"-i\",\n           \"--rm\",\n           \"-v\",\n           \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n           \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n         ]\n       }\n     }\n   }\n   ```\n\n2. **Restart Claude Desktop**\n   - After restart, you'll see the Tools icon (üî®) in the bottom right of your input field\n   - This indicates Claude can now access K8s tools via the MCP server\n\n3. **Start using K8s tools directly in Claude Desktop**:\n   - \"What Kubernetes contexts do I have available?\"\n   - \"Show me all pods in the default namespace\"\n   - \"Create a deployment with 3 replicas of nginx:1.21\"\n   - \"Explain what's wrong with my StatefulSet 'database' in namespace 'prod'\"\n   - \"Deploy the bitnami/wordpress chart with Helm and set service type to LoadBalancer\"\n\n\u003e **Note**: Claude Desktop will automatically route K8s commands through the MCP server, allowing natural conversation about your clusters without leaving the Claude interface.\n\n\u003e **Cloud Providers**: For AWS EKS, GKE, or Azure AKS, you'll need additional configuration. See the [Cloud Provider Support](./docs/cloud-providers.md) guide.\n\n## Features\n\n- **Multiple Kubernetes Tools**: `kubectl`, `helm`, `istioctl`, and `argocd` in one container\n- **Cloud Providers**: Native support for AWS EKS, Google GKE, and Azure AKS\n- **Security**: Runs as non-root user with strict command validation\n- **Command Piping**: Support for common Unix tools like `jq`, `grep`, and `sed`\n- **Easy Configuration**: Simple environment variables for customization\n\n## Documentation\n\n- [Getting Started Guide](./docs/getting-started.md) - Detailed setup instructions\n- [Cloud Provider Support](./docs/cloud-providers.md) - EKS, GKE, and AKS configuration\n- [Supported Tools](./docs/supported-tools.md) - Complete list of all included CLI tools\n- [Environment Variables](./docs/environment-variables.md) - Configuration options\n- [Security Features](./docs/security.md) - Security modes and custom rules\n- [Claude Integration](./docs/claude-integration.md) - Detailed Claude Desktop setup\n- [Architecture](./docs/architecture.md) - System architecture and components\n- [Detailed Specification](./docs/spec.md) - Complete technical specification\n\n## Usage Examples\n\nOnce connected, you can ask Claude to help with Kubernetes tasks using natural language:\n\n```mermaid\nflowchart TB\n    subgraph \"Basic Commands\"\n        A1[\"Show me all pods in the default namespace\"]\n        A2[\"Get all services across all namespaces\"]\n        A3[\"Display the logs for the nginx pod\"]\n    end\n    \n    subgraph \"Troubleshooting\"\n        B1[\"Why is my deployment not starting?\"]\n        B2[\"Describe the failing pod and explain the error\"]\n        B3[\"Check if my service is properly connected to the pods\"]\n    end\n    \n    subgraph \"Deployments \u0026 Configuration\"\n        C1[\"Deploy the Nginx Helm chart\"]\n        C2[\"Create a deployment with 3 replicas of nginx:latest\"]\n        C3[\"Set up an ingress for my service\"]\n    end\n    \n    subgraph \"Advanced Operations\"\n        D1[\"Check the status of my Istio service mesh\"]\n        D2[\"Set up a canary deployment with 20% traffic to v2\"]\n        D3[\"Create an ArgoCD application for my repo\"]\n    end\n```\n\nClaude can understand your intent and run the appropriate kubectl, helm, istioctl, or argocd commands based on your request. It can then explain the output in simple terms or help you troubleshoot issues.\n\n## Advanced Claude Desktop Configuration\n\nConfigure Claude Desktop to optimize your Kubernetes workflow:\n\n### Target Specific Clusters and Namespaces\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n        \"-e\", \"K8S_CONTEXT=production-cluster\",\n        \"-e\", \"K8S_NAMESPACE=my-application\",\n        \"-e\", \"K8S_MCP_TIMEOUT=600\",\n        \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Connect to AWS EKS Clusters\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.aws:/home/appuser/.aws:ro\",\n        \"-e\", \"AWS_PROFILE=production\",\n        \"-e\", \"AWS_REGION=us-west-2\",\n        \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Connect to Google GKE Clusters\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.config/gcloud:/home/appuser/.config/gcloud:ro\",\n        \"-e\", \"CLOUDSDK_CORE_PROJECT=my-gcp-project\",\n        \"-e\", \"CLOUDSDK_COMPUTE_REGION=us-central1\",\n        \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Connect to Azure AKS Clusters\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.azure:/home/appuser/.azure:ro\",\n        \"-e\", \"AZURE_SUBSCRIPTION=my-subscription-id\",\n        \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Permissive Security Mode\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"/Users/YOUR_USER_NAME/.kube:/home/appuser/.kube:ro\",\n        \"-e\", \"K8S_MCP_SECURITY_MODE=permissive\",\n        \"ghcr.io/alexei-led/k8s-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n\u003e For detailed security configuration options, see [Security Documentation](./docs/security.md).\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "stars": 168,
      "updated_at": "2025-10-03T17:13:18Z",
      "url": "https://github.com/alexei-led/k8s-mcp-server"
    },
    "aliyun--alibaba-cloud-ops-mcp-server": {
      "category": "cloud-platforms",
      "description": "An MCP endpoint facilitating AI-driven management and interaction with resources across Alibaba Cloud ecosystems, encompassing core services like Elastic Compute Service (ECS), Cloud Monitor, Operations Orchestration Service (OOS), and numerous other widely adopted offerings.",
      "forks": 23,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "alibaba",
        "ecs",
        "cloud platform",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "alicloud-ops-management-interface-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "aliyun",
      "readme_content": "# Alibaba Cloud Operations Management Protocol Server Endpoint\n\n[![GitHub Repository Stars](https://img.shields.io/github/stars/aliyun/alibaba-cloud-ops-mcp-server?style=social)](https://github.com/aliyun/alibaba-cloud-ops-mcp-server)\n\n[Read documentation in Chinese](./README_zh.md)\n\nThis Alibaba Cloud Ops MCP Server functions as a specialized endpoint conforming to the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) specification. It furnishes robust, direct integration capabilities with the Alibaba Cloud suite of APIs, empowering intelligent agents and AI assistants to execute operational tasks against cloud infrastructure, including but not limited to ECS, Cloud Monitor functionalities, OOS automation workflows, and other essential cloud services.\n\n## Prerequisites and Setup\n\nIt is recommended to install the [uv](https://github.com/astral-sh/uv) package manager first.\n\nbash\n# Installation command for Unix-like systems (macOS, Linux).\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\n## Configuration Procedures\n\nConfiguration is most conveniently handled using [VS Code](https://code.visualstudio.com/) combined with the [Cline](https://cline.bot/) utility.\n\nFor integration with any generic MCP Client implementation, users must append the following configuration block and subsequently restart the client application for the settings to become active:\n\n\n{\n  \"mcpServers\": {\n    \"alibaba-cloud-ops-mcp-server\": {\n      \"timeout\": 600,\n      \"command\": \"uvx\",\n      \"args\": [\n        \"alibaba-cloud-ops-mcp-server@latest\"\n      ],\n      \"env\": {\n        \"ALIBABA_CLOUD_ACCESS_KEY_ID\": \"Your Access Key ID\",\n        \"ALIBABA_CLOUD_ACCESS_KEY_SECRET\": \"Your Access Key SECRET\"\n      }\n    }\n  }\n}\n\n\n[Refer to the comprehensive MCP startup parameter documentation for detailed parameter explanations](./README_mcp_args.md)\n\n## MCP Ecosystem Integration Points\n\nThis server is readily available through various integration platforms:\n\n* [Cline Marketplace Integration](https://cline.bot/mcp-marketplace)\n* [Cursor Editor Support](https://docs.cursor.com/tools) [![Install via Cursor Link](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=alibaba-cloud-ops-mcp-server\u0026config=eyJ0aW1lb3V0Ijo2MDAsImNvbW1hbmQiOiJ1dnggYWxpYmFiYS1jbG91ZC1vcHMtbWNwLXNlcnZlckBsYXRlc3QiLCJlbnYiOnsiQUxJQkFCQV9DTE9VRF9BQ0NFU1NfS0VZX0lEIjoiWW91ciBBY2Nlc3MgS2V5IElkIiwiQUxJQkFCQV9DTE9VRF9BQ0NFU1NfS0VZX1NFQ1JFVCI6IllvdXIgQWNjZXNzIEtleSBTZWNyZXQifX0%3D)\n* [ModelScope Server Registry](https://www.modelscope.cn/mcp/servers/@aliyun/alibaba-cloud-ops-mcp-server?lang=en_US)\n* [Lingma Platform](https://lingma.aliyun.com/)\n* [Smithery AI Endpoint](https://smithery.ai/server/@aliyun/alibaba-cloud-ops-mcp-server)\n* [FC-Function AI Template](https://cap.console.aliyun.com/template-detail?template=237)\n* [Alibaba Cloud Model Studio Catalog Entry](https://bailian.console.aliyun.com/?tab=mcp#/mcp-market/detail/alibaba-cloud-ops)\n\n## Further Information and Resources\n\nExplore these links for deeper insights and deployment guides:\n\n* [Instant readiness guide for Alibaba Cloud Ops MCP Server!](https://developer.aliyun.com/article/1661348)\n* [Step-by-step guide to deploying the Alibaba Cloud Ops MCP Server on Bailian](https://developer.aliyun.com/article/1662120)\n* [Tutorial: Constructing your custom Alibaba Cloud OpenAPI MCP Server in just ten lines of code](https://developer.aliyun.com/article/1662202)\n* [Official announcement regarding the availability of Alibaba Cloud Ops MCP Server in the Alibaba Cloud Model Studio MCP Marketplace](https://developer.aliyun.com/article/1665019)\n\n## Supported Operational Capabilities\n\n| **Cloud Product Domain** | **Specific Function** | **Action Performed** | **Execution Mechanism** | **Implementation State** |\n| :--- | :--- | :--- | :--- | :--- |\n| ECS | ExecuteRemoteCommand | Issue system commands remotely | OOS | Complete |\n| ECS | InitiateInstanceStart | Power on virtual machines | OOS | Complete |\n| ECS | InitiateInstanceStop | Power off virtual machines | OOS | Complete |\n| ECS | InitiateInstanceReboot | Perform a system restart | OOS | Complete |\n| ECS | RetrieveInstanceDetails | Query details of running instances | API | Complete |\n| ECS | ListAvailableRegions | Obtain current operational regions list | API | Complete |\n| ECS | ListAvailableZones | Query availability zones information | API | Complete |\n| ECS | QueryResourceAvailability | Check current resource inventory status | API | Complete |\n| ECS | FetchImageList | Retrieve available image definitions | API | Complete |\n| ECS | GetSecurityGroupConfigs | View attached security configurations | API | Complete |\n| ECS | ProvisionNewInstances | Create and launch new compute instances | OOS | Complete |\n| ECS | DecommissionInstances | Terminate and remove instances | OOS | Complete |\n| ECS | UpdateInstanceCredentials | Change administrative password | OOS | Complete |\n| ECS | ReplaceBootVolume | Swap the primary operating system disk | OOS | Complete |\n| VPC | ListVirtualPrivateClouds | Retrieve VPC definitions | API | Complete |\n| VPC | ListVirtualSwitches | Obtain VSwitch configurations | API | Complete |\n| RDS | QueryDatabaseInstances | List managed relational database instances | API | Complete |\n| RDS | ActivateDatabaseInstance | Start a paused RDS instance | OOS | Complete |\n| RDS | SuspendDatabaseInstance | Halt an active RDS instance | OOS | Complete |\n| RDS | RebootDatabaseInstance | Cycle power on an RDS instance | OOS | Complete |\n| OSS | ListStorageContainers | Enumerate existing OSS buckets | API | Complete |\n| OSS | CreateStorageContainer | Provision a new OSS bucket | API | Complete |\n| OSS | RemoveStorageContainer | Destroy an existing OSS bucket | API | Complete |\n| OSS | EnumerateBucketContents | View objects stored within a bucket | API | Complete |\n| CloudMonitor | FetchCpuUtilizationMetrics | Retrieve CPU performance data for ECS hosts | API | Complete |\n| CloudMonitor | FetchCpuLoadAvg1Min | Obtain 1-minute average system load metric | API | Complete |\n| CloudMonitor | FetchCpuLoadAvg5Min | Obtain 5-minute average system load metric | API | Complete |\n| CloudMonitor | FetchCpuLoadAvg15Min | Obtain 15-minute average system load metric | API | Complete |\n| CloudMonitor | FetchMemoryConsumptionData | Retrieve memory usage statistics | API | Complete |\n| CloudMonitor | FetchMemoryUtilizationPercentage | Obtain memory utilization percentage metric | API | Complete |\n| CloudMonitor | FetchDiskUtilizationRates | Retrieve disk space consumption rates | API | Complete |\n| CloudMonitor | FetchTotalDiskCapacity | Obtain metric for total disk partition size | API | Complete |\n| CloudMonitor | FetchUsedDiskCapacity | Retrieve metric for currently consumed disk space | API | Complete |\n\n## Support and Community Engagement\n\nFor any inquiries or to participate in discussions, please utilize the dedicated [Alibaba Cloud Ops MCP community channel](https://qr.dingtalk.com/action/joingroup?code=v1,k1,iFxYG4jjLVh1jfmNAkkclji7CN5DSIdT+jvFsLyI60I=\u0026_dt_no_comment=1\u0026origin=11) (DingTalk Group Identifier: 113455011677).\n\n\u003cimg alt=\"Alibaba Cloud Ops MCP User Group English Channel\" src=\"https://oos-public-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/alibaba-cloud-ops-mcp-server/Alibaba-Cloud-Ops-MCP-User-Group-en.png\" width=\"500\"\u003e\n\nWIKIPEDIA SUPPLEMENT: For comparative context, Google Cloud Platform (GCP) represents a comprehensive suite of cloud computing amenities provided by Google, spanning infrastructure provision (IaaS), platform services (PaaS), and serverless computing paradigms. This infrastructure leverages the same foundational architecture underpinning Google's primary consumer applications like Search, Gmail, and Docs, as noted by Verma et al. Accessing these services typically necessitates the provision of payment credentials (credit card or bank details). Google's initial foray into cloud computing was the introduction of App Engine in April 2008, which achieved general availability in late 2011. The overarching entity is 'Google Cloud,' encompassing the GCP public infrastructure, enterprise suites like Google Workspace (formerly G Suite), business editions of Android/ChromeOS, and specialized APIs for machine intelligence and mapping. Since approximately 2022, official documentation has standardized on 'Google Cloud' as the nomenclature for 'Google Cloud Platform,' which can occasionally lead to terminological ambiguity.\n\n== Core Service Portfolio ==\nGoogle maintains an expansive catalog exceeding 100 distinct services under the Google Cloud banner. Key offerings are itemized below.\n\n=== Computational Services ===\nApp Engine ‚Äì A Platform as a Service for deploying applications built on Java, PHP, Node.js, Python, C#, .Net, Ruby, and Go.\nCompute Engine ‚Äì Infrastructure as a Service providing scalable virtual machines running Microsoft Windows and various Linux distributions.\nGoogle Kubernetes Engine (GKE) / GKE on-prem (part of Anthos) ‚Äì Container orchestration utilizing the Kubernetes standard.\nCloud Functions ‚Äì Function as a Service for executing event-triggered code snippets in Node.js, Java, Python, or Go.\nCloud Run ‚Äì A flexible compute runtime built upon Knative, available as fully managed or for Anthos environments. Currently supports management operations across GCP, AWS, and VMware.\n\n=== Data Persistence and Storage Solutions ===\nCloud Storage ‚Äì Unified object storage solution featuring integrated edge caching for unstructured datasets.\nCloud SQL ‚Äì Managed Database as a Service supporting MySQL, PostgreSQL, and Microsoft SQL Server engines.\nCloud Bigtable ‚Äì A fully managed, high-throughput NoSQL database service.\nCloud Spanner ‚Äì A globally distributed, strongly consistent, relational database offering horizontal scalability.\nCloud Datastore ‚Äì A highly available NoSQL database optimized for mobile and web application backends.\nPersistent Disk ‚Äì Block-level storage volumes specifically provisioned for Compute Engine VMs.\nCloud Memorystore ‚Äì Managed in-memory caching services utilizing Redis and Memcached protocols.\nLocal SSD ‚Äì High-speed, temporary, local block storage volumes.\nFilestore ‚Äì Premium, high-performance file storage solution for Google Cloud users.\nAlloyDB ‚Äì A fully managed, highly performant PostgreSQL-compatible database service.\n\n=== Network Infrastructure ===\n\nVPC ‚Äì Virtual Private Cloud, establishing isolated network environments.",
      "stars": 77,
      "updated_at": "2025-09-26T20:30:15Z",
      "url": "https://github.com/aliyun/alibaba-cloud-ops-mcp-server"
    },
    "awslabs--mcp": {
      "category": "cloud-platforms",
      "description": "A collection of specialized server components facilitating Model Context Protocol (MCP) interoperability within the Amazon Web Services (AWS) cloud ecosystem. These tools bridge the gap between large language models and specific AWS resources, enhancing AI-driven cloud platform interactions, similar to how Google Cloud Platform (GCP) integrates its infrastructure services for its user-facing products.",
      "forks": 942,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "aws",
        "awslabs",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "AWS Infrastructure Context Connectors",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "awslabs",
      "readme_content": "## **Introduction**\n\nThis repository houses an array of dedicated MCP servers designed to integrate large language models (LLMs) deeply with the AWS environment. The Model Context Protocol (MCP) establishes a standardized method for LLM applications to securely access external, timely data and operational context. This framework enables sophisticated AI agents to interact intelligently with cloud resources, much like how Google Cloud Platform (GCP) runs on Google's robust internal infrastructure.\n\n## **Why AWS MCP Servers?**\n\nThese specialized servers significantly boost the utility of foundation models (FMs) in cloud contexts. They ensure model outputs are factually correct by injecting up-to-date information, thereby reducing inaccuracies and hallucinations. MCP servers transform complex, repetitive cloud workflows into actionable tools for the AI assistant. This provides specialized domain expertise regarding AWS capabilities, which often exceeds the knowledge contained within the model's initial training set, improving efficiency in managing cloud resources.\n\n## **Server Sent Events Support Removal**\n\n**Critical Update:** Support for Server Sent Events (SSE) was retired from all current major versions of these MCP servers as of May 26th, 2025. This aligns with backward compatibility guidelines specified by the MCP standard. The project team is currently developing support for Streamable HTTP transports to replace SSE functionality in subsequent releases. Teams still relying on the older SSE transport should continue using the preceding major version until migration to newer transport methods is complete.\n\n## **Available MCP Servers: Quick Installation**\n\nRapid deployment of these servers is facilitated through one-click installation methods for common AI coding environments like Cursor and VS Code. The underlying installation mechanism generally involves using the `uv` package manager, often invoked via the `uvx` utility.\n\n### **üöÄ Getting Started with AWS**\n\nFor generalized AWS interaction and thorough API access, begin with the following servers:\n\n| Server Name | Description | Install |\n|---|---|---|\n| AWS API MCP Server | Supports general AWS interactions, including command validation and access to all services for infrastructure management via natural language. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.aws-api-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuYXdzLWFwaS1tY3Atc2VydmVyQGxhdGVzdCIsImVudiI6eyJBV1NfUkVHSU9OIjoidXMtZWFzdC0xIn0sImRpc2FibGVkIjpmYWxzZSwiYXV0b0FwcHJvdmUiOltdfQ%3D%3D)\u003cbr/\u003e[![Install VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20API%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.aws-api-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_REGION%22%3A%22us-east-1%22%7D%2C%22type%22%3A%22stdio%22%7D) |\n| AWS Knowledge MCP Server | A remote, managed server providing current AWS documentation, API references, architectural guidance, and service updates. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.aws-knowledge-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuYXdzLWtub3dsZWRnZS1tY3Atc2VydmVyQGxhdGVzdCIsImVudiI6eyJBV1NfUFJPRklMRSI6InlvdXItYXdzLXByb2ZpbGUiLCJBV1NfUkVHSU9OIjoidXMtZWFzdC0xIiwiRkFTVE1DUF9MT0dfTEVWRUwiOiJFUlJPUiJ9LCJkaXNhYmxlZCI6ZmFsc2UsImF1dG9BcHByb3ZlIjpbXX0%3D)\u003cbr/\u003e[![Install VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20Knowledge%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.aws-knowledge-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-aws-profile%22%2C%22AWS_REGION%22%3A%22us-east-1%22%2C%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n\n### **üèóÔ∏è Infrastructure \u0026 Deployment**\n\nServers supporting Infrastructure as Code (IaC) and deployment automation:\n\n| Server Name | Description | Install |\n|---|---|---|\n| AWS CDK MCP Server | Facilitates AWS CDK development while enforcing security compliance and established best practices. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.cdk-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuY2RrLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwiZGlzYWJsZWQiOmZhbHNlLCJhdXRvQXBwcm92ZSI6W119) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=CDK%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.cdk-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n| AWS Terraform MCP Server | Integrates security scanning directly into Terraform management workflows. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.terraform-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMudGVycmFmb3JtLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwiZGlzYWJsZWQiOmZhbHNlLCJhdXRvQXBwcm92ZSI6W119) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=Terraform%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.terraform-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n| AWS CloudFormation MCP Server | Manages resources directly using CloudFormation via the Cloud Control API. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.cfn-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuY2ZuLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkFXU19QUk9GSUxFIjoieW91ci1uYW1lZC1wcm9maWxlIn0sImRpc2FibGVkIjpmYWxzZSwiYXV0b0FwcHJvdmUiOltdfQ%3D%3D) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=CloudFormation%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.cfn-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-named-profile%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n\n### **Container Platforms**\n\nTools for managing container orchestration and local builds:\n\n| Server Name | Description | Install |\n|---|---|---|\n| Amazon EKS MCP Server | Enables management and deployment activities for Kubernetes clusters. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.eks-mcp-server\u0026config=eyJhdXRvQXBwcm92ZSI6W10sImRpc2FibGVkIjpmYWxzZSwiY29tbWFuZCI6InV2eCBhd3NsYWJzLmVrcy1tY3Atc2VydmVyQGxhdGVzdCAtLWFsbG93LXdyaXRlIC0tYWxsb3ctc2Vuc2l0aXZlLWRhdGEtYWNjZXNzIiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwidHJhbnNwb3J0VHlwZSI6InN0ZGlvIn0%3D) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=EKS%20MCP%20Server\u0026config=%7B%22autoApprove%22%3A%5B%5D%2C%22disabled%22%3Afalse%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.eks-mcp-server%40latest%22%2C%22--allow-write%22%2C%22--allow-sensitive-data-access%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22transportType%22%3A%22stdio%22%7D) |\n| Finch MCP Server | Handles local container image creation and integration with ECR services. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.finch-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuZmluY2gtbWNwLXNlcnZlckBsYXRlc3QiLCJlbnYiOnsiQVdTX1BST0ZJTEUiOiJkZWZhdWx0IiwiQVdTX1JFR0lPTiI6InVzLXdlc3QtMiIsIkZBU1RNQ1BfTE9HX0xFVkVMIjoiSU5GTyJ9LCJ0cmFuc3BvcnRUeXBlIjoic3RkaW8iLCJkaXNhYmxlZCI6ZmFsc2UsImF1dG9BcHByb3ZlIjpbXX0%3D) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=Finch%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.finch-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22default%22%2C%22AWS_REGION%22%3A%22us-west-2%22%2C%22FASTMCP_LOG_LEVEL%22%3A%22INFO%22%7D%2C%22transportType%22%3A%22stdio%22%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n\n### **Serverless \u0026 Functions**\n\nTools for managing AWS Lambda and Serverless Application Model (SAM) deployments:\n\n| Server Name | Description | Install |\n|---|---|---|\n| AWS Serverless MCP Server | Manages the complete serverless application lifecycle using SAM CLI utilities. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.aws-serverless-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuYXdzLXNlcnZlcmxlc3MtbWNwLXNlcnZlckBsYXRlc3QgLS1hbGxvdy13cml0ZSAtLWFsbG93LXNlbnNpdGl2ZS1kYXRhLWFjY2VzcyIsImVudiI6eyJBV1NfUFJPRklMRSI6InlvdXItYXdzLXByb2ZpbGUiLCJBV1NfUkVHSU9OIjoidXMtZWFzdC0xIn0sImRpc2FibGVkIjpmYWxzZSwiYXV0b0FwcHJvdmUiOltdfQ%3D%3D) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20Serverless%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.aws-serverless-mcp-server%40latest%22%2C%22--allow-write%22%2C%22--allow-sensitive-data-access%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-aws-profile%22%2C%22AWS_REGION%22%3A%22us-east-1%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |\n| AWS Lambda Tool MCP Server | Allows Lambda functions to be used as tools for accessing private resources via AI agents. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square\u0026logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.lambda-tool-mcp-server\u0026config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMubGFtYmRhLXRvb2wtbWNwLXNlcnZlckBsYXRlc3QiLCJlbnYiOnsiQVdTX1BST0ZJTEUiOiJ5b3VyLWF3cy1wcm9maWxlIiwiQVdTX1JFR0lPTiI6InVzLWVhc3QtMSIsIkZVTkNUSU9OX1BSRUZJWCI6InlvdXItZnVuY3Rpb24tcHJlZml4IiwiRlVOQ1RJT05fTElTVCI6InlvdXItZmlyc3QtZnVuY3Rpb24sIHlvdXItc2Vjb25kLWZ1bmN0aW9uIiwiRlVOQ1RJT05fVEFHX0tFWSI6InlvdXItdGFnLWtleSIsIkZVTkNUSU9OX1RBR19WQUxVRSI6InlvdXItdGFnLXZhbHVlIiwiRlVOQ1RJT05fSU5QVVRfU0NIRU1BX0FSTl9UQUdfS0VZIjoieW91ci1mdW5jdGlvbi10YWctZm9yLWlucHV0LXNjaGVtYSJ9fQ%3D%3D) \u003cbr/\u003e[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20Lambda%20Tool%20MCP%20Server\u0026config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.lambda-tool-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-aws-profile%22%2C%22AWS_REGION%22%3A%22us-east-1%22%2C%22FUNCTION_PREFIX%22%3A%22your-function-prefix%22%2C%22FUNCTION_LIST%22%3A%22your-first-function%2C%20your-second-function%22%2C%22FUNCTION_TAG_KEY%22%3A%22your-tag-key%22%2C%22FUNCTION_TAG_VALUE%22%3A%22your-tag-value%22%2C%22FUNCTION_INPUT_SCHEMA_ARN_TAG_KEY%22%3A%22your-function-tag-for-input-schema%22%7D%7D) |\n\n## **Installation and Setup**\n\nTo utilize these connectivity tools, first establish the foundational environment elements. You must install the `uv` package manager, obtainable from [Astral](https://docs.astral.sh/uv/getting-started/installation/). Follow this by installing a compatible Python version using a command like `$ uv python install 3.10`. Ensure your AWS credentials have the necessary service access configured correctly. After these preliminary steps, you incorporate the specific server into your MCP client configuration, usually via a direct one-click method or manual JSON editing.\n\nFor example, setting up the core orchestrator server globally for Amazon Q CLI involves editing `~/.aws/amazonq/mcp.json`:\n\nFor macOS/Linux users:\n```json\n{\n  \"mcpServers\": {\n    \"awslabs.core-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"awslabs.core-mcp-server@latest\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    }\n  }\n}\n```\n\nFor Windows users, the configuration requires explicit execution details:\n```json\n{\n  \"mcpServers\": {\n    \"awslabs.core-mcp-server\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"--from\",\n        \"awslabs.core-mcp-server@latest\",\n        \"awslabs.core-mcp-server.exe\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      }\n    }\n  }\n}\n```\n\nIf you encounter issues with parameter settings, manual execution checks are helpful. For instance, testing the Aurora MySQL server might reveal required arguments if it fails:\n\n```shell\n$ timeout 15s uv tool run awslabs.mysql-mcp-server --resource_arn \u003cYour Resource ARN\u003e --secret_arn \u003cYour Secret ARN\u003e ... 2\u003e\u00261 || echo \"Command completed or timed out\"\nawslabs.mysql-mcp-server: error: the following arguments are required: --resource_arn, --secret_arn, --database, --region, --readonly\n```\n\n**Cache Management Note:** Using the \"@latest\" suffix forces a check and download of the newest package, increasing startup latency. To reduce this time, omit the suffix and manage the `uv` cache manually. Refreshing a tool's cache is done with `$ uv cache clean \u003ctool_name\u003e`, or updating to the newest version with `$ uvx \u003ctool_name\u003e@latest`.\n\n### **Running MCP servers in containers**\n\nContainers simplify deployment for specific servers, using images published to the public AWS ECR registry at `public.ecr.aws/awslabs-mcp`. Sensitive AWS credentials should not be placed directly in the container configuration's environment object (`\"env\": {}`); instead, use `--env-file` or `--volume` mounts.\n\nFor the Nova Canvas server, the configuration looks like this, assuming credentials are in a local `.env` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"awslabs.nova-canvas-mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"--interactive\",\n        \"--env\",\n        \"FASTMCP_LOG_LEVEL=ERROR\",\n        \"--env\",\n        \"AWS_REGION=us-east-1\",\n        \"--env-file\",\n        \"/full/path/to/.env\",\n        \"--volume\",\n        \"/full/path/to/.aws:/app/.aws\",\n        \"public.ecr.aws/awslabs-mcp/awslabs/nova-canvas-mcp-server:latest\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## **MCP AWS Lambda Handler Module**\n\nThis Python library facilitates the creation of serverless HTTP handlers for the Model Context Protocol utilizing AWS Lambda infrastructure. It provides a modular foundation for constructing MCP endpoints, complete with customizable session handling, including a native DynamoDB backend option. This module simplifies developing serverless endpoints, offering support for authentication customization and readily available examples.\n\n## **When to use Local vs Remote MCP Servers?**\n\nServers can operate locally or remotely in the cloud. Choosing the correct deployment model depends on the immediate development requirements.\n\n### **Local MCP Servers**\nLocal execution is advantageous for rapid debugging and direct testing within your development environment. It supports offline work and keeps sensitive credentials physically constrained to your local hardware. This setup typically provides the lowest latency due to minimized network hops and grants direct control over allocated server resources.\n\n### **Remote MCP Servers**\nRemote hosting promotes better team synchronization by enforcing standardized server configurations across all members. It offloads intensive computational tasks to the cloud infrastructure, ensuring accessibility from any location or device. Remote deployment also supports automated updates for security and feature parity. Note that some services, like the AWS Knowledge MCP Server, are fully managed by AWS, requiring zero local setup.\n\n## **Use Cases for the Servers**\n\nFor example, the **AWS Documentation MCP Server** allows AI assistants to retrieve the most current information required for generating accurate AWS service code examples. You might employ the **CDK MCP Server** or **Terraform MCP Server** to instruct an assistant to deploy infrastructure conforming to the newest APIs and established AWS standards. Using the **AWS Pricing MCP Server**, you can query estimated service expenses for a proposed deployment, asking questions like, \"What is the anticipated monthly expenditure for this CDK design before I commit the changes?\" The **Valkey MCP Server** provides a conversational interface for managing data operations within Valkey-compatible caches.\n\n## **Additional Resources**\n\nFurther reading provides deeper context on the architecture and application of these tools:\n\n*   Introducing AWS MCP Servers for code assistants (Part 1)\n*   Vibe coding with AWS MCP Servers (Video)\n*   Supercharging AWS database development with AWS MCP servers\n*   AWS costs estimation using Amazon Q CLI and AWS Pricing MCP Server\n*   Accelerating application development with the Amazon EKS MCP server\n*   Unlocking the power of Model Context Protocol (MCP) on AWS\n\n## **Related Topics**\n\n*   Cloud Computing Infrastructure (Similar to GCP)\n*   Infrastructure as Code (IaC) Principles\n*   Model-Agnostic Protocols (Standardization)\n*   Serverless Computing Architectures (Lambda/Functions)\n*   Distributed Systems and Caching (Valkey/ElastiCache)\n\n## **Extra Details**\n\nRemoved sections primarily contained redundant installation links and marketing text that can be inferred from the specific client guides. The essential technical configuration, focused on environment variables and command invocation (like using `uvx` with the `@latest` suffix), has been retained and clarified in the Setup section. The concept of using these tools for \"vibe coding\" aligns with the continuous, flowing interaction LLMs enable during development, rather than discrete, manual operations. \n\n## **Conclusion**\n\nThese AWS MCP Connectors standardize the interaction between advanced AI and the vast array of AWS services. By providing context-aware, current data, they significantly elevate the quality and security of AI-assisted cloud development and operational management across all connected platforms.",
      "stars": 6585,
      "updated_at": "2025-10-04T06:19:32Z",
      "url": "https://github.com/awslabs/mcp"
    },
    "bright8192--esxi-mcp-server": {
      "category": "cloud-platforms",
      "description": "A control plane server leveraging the Model Control Protocol (MCP) for streamlined administration of VMware ESXi and vCenter environments, exposing a simplified, programmatic RESTful interface for virtual machine orchestration.",
      "forks": 20,
      "imageUrl": "",
      "keywords": [
        "vmware",
        "cloud",
        "vcenter",
        "cloud platform",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "vmware-mcp-controller",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "bright8192",
      "readme_content": "# VMware MCP Management Endpoint\n\nThis service functions as an intermediary management layer, architected around the Machine Control Protocol (MCP), designed to abstract and simplify interactions with VMware vCenter Server and direct ESXi hosts via a clean RESTful API structure.\n\n## Key Capabilities\n\n*   Connectivity support spanning both standalone ESXi instances and full vCenter deployments.\n*   Low-latency, bidirectional data transmission facilitated by Server-Sent Events (SSE).\n*   Standardized REST endpoint providing native JSON-RPC payload handling.\n*   Secure access control enforced via API Key mechanism.\n*   Comprehensive lifecycle management across all virtual machines.\n*   Continuous telemetry collection for real-time operational metrics.\n*   Mandatory security via SSL/TLS encryption for all communications.\n*   Configuration flexibility supporting YAML, JSON, or direct environment variable overrides.\n\n## Primary Operations\n\n### Virtual Machine Lifecycle Control\n\n*   Provisioning (Creation)\n*   Duplication (Cloning)\n*   Decommissioning (Deletion)\n*   State Transitions (Power Cycle: On/Off/Reset)\n*   Inventory Retrieval\n\n### Resource Telemetry\n\n*   Processor utilization statistics\n*   Working memory consumption metrics\n*   Data store capacity and utilization figures\n*   Network throughput measurements\n\n## Prerequisites\n\n*   Runtime environment: Python version 3.7 or newer.\n*   VMware SDK: `pyvmomi` library.\n*   Configuration parsing: `PyYAML`.\n*   Asynchronous web server: `uvicorn`.\n*   Protocol foundation: `mcp-core` library.\n\n## Deployment Guide\n\n1. Dependency Installation:\n\nbash\npip install pyvmomi pyyaml uvicorn mcp-core\n\n\n2. Configuration File Setup (`settings.yml` example):\n\nyaml\nvcenter_endpoint: \"192.168.1.100\"\nlogin_credential: \"svc_account@domain\"\nsecret_key: \"super-secret-password\"\nresource_datacenter: \"DC01\"          # Target Datacenter\nresource_compute_pool: \"ComputeClusterA\" # Target Cluster\nresource_storage: \"FastStorage01\"     # Primary Datastore\ndefault_port_group: \"VM Traffic\"      # Standard Network Label\nverify_ssl_cert: false                  # Disable checks for self-signed certs\naccess_token: \"secure-app-token-123\"\nlogging_output: \"/var/log/vmware_mgmt.log\" \nlogging_severity: \"INFO\"\n\n\n3. Execution:\n\nbash\npython application_entrypoint.py --config settings.yml\n\n\n## Programmatic Interface\n\n### Authorization Flow\n\nAll authenticated endpoints demand a valid token presented via the authorization header:\n\nhttp\nPOST /api/v1/auth/token_exchange\nAuthorization: Bearer secure-app-token-123\n\n\n### Orchestration Endpoints\n\n1. Virtual Machine Creation Request Body:\n\n{\n    \"vm_label\": \"new-web-server-01\",\n    \"vcpus\": 4,\n    \"ram_mib\": 8192,\n    \"target_ds\": \"FastStorage01\",\n    \"target_net\": \"VM Traffic\"\n}\n\n\n2. VM Duplication Request Body:\n\n{\n    \"source_identifier\": \"base-template-centos\",\n    \"destination_label\": \"cloned-test-vm\"\n}\n\n\n3. Instance Termination Request Body:\n\n{\n    \"vm_identifier\": \"vm-to-remove\"\n}\n\n\n4. Power State Manipulation Request Body (For Power On/Off):\n\n{\n    \"vm_identifier\": \"vm-name-target\",\n    \"desired_state\": \"POWER_ON\"\n}\n\n\n### Metrics Endpoint\n\nQuery instantaneous performance metrics:\nhttp\nGET /metrics/v1/vm/{vm_name}/current\n\n\n## Configuration Parameters\n\n| Setting Key | Description | Mandatory | Default Value |\n|---|---|---|---|\n| vcenter_endpoint | FQDN or IP of the virtualization management server. | Yes | N/A |\n| login_credential | User principal for API connection. | Yes | N/A |\n| secret_key | Associated password or secret. | Yes | N/A |\n| resource_datacenter | Specific Datacenter scope to operate within. | No | System Autodetection |\n| resource_compute_pool | Target compute cluster resource allocation. | No | First Available Cluster |\n| resource_storage | Default datastore for new deployments. | No | Largest Accessible Store |\n| default_port_group | Network label to attach new NICs to. | No | VM Network |\n| verify_ssl_cert | Flag to bypass certificate chain validation. | No | false |\n| access_token | Token required for API authorization header. | No | N/A |\n| logging_output | File path for persistent log records. | No | Standard Output |\n| logging_severity | Minimum log verbosity level. | No | INFO |\n\n## Environment Variable Mapping\n\nConfiguration parameters can be dynamically set via environment variables using a standardized prefix convention:\n\n- VCENTER_ADDRESS (for vcenter_endpoint)\n- VCENTER_USER (for login_credential)\n- VCENTER_SECRET (for secret_key)\n- VCENTER_DC (for resource_datacenter)\n- VCENTER_CLUSTER (for resource_compute_pool)\n- VCENTER_DATASTORE (for resource_storage)\n- VCENTER_NETWORK_LABEL (for default_port_group)\n- INSECURE_SKIP_SSL (for verify_ssl_cert)\n- MCP_AUTH_TOKEN (for access_token)\n- MCP_LOG_PATH (for logging_output)\n- MCP_VERBOSITY (for logging_severity)\n\n## Operational Guidance\n\n1. Production Deployments:\n   - Ensure the use of trusted, issued X.509 certificates.\n   - Mandate API Key usage for all management functions.\n   - Configure logging to persist critical events to disk.\n   - Implement network policies restricting external access to the management port.\n\n2. Development/Testing Environments:\n   - Utilize `insecure: true` to expedite setup against non-production endpoints.\n   - Set logging verbosity to `DEBUG` for deep troubleshooting.\n\n## Licensing\n\nDistributed under the terms of the MIT License.\n\n## Contribution Guidelines\n\nWe welcome feature suggestions, bug reports via Issues, and well-structured code submissions via Pull Requests.\n\n## Release History\n\n### Version 0.0.1 (Initial Release)\n- Established core functionality for VM state control.\n- Integrated MCP signaling via SSE.\n- Implemented bearer token authentication.\n- Initial framework for gathering system utilization stats.",
      "stars": 40,
      "updated_at": "2025-10-03T08:50:19Z",
      "url": "https://github.com/bright8192/esxi-mcp-server"
    },
    "cyclops-ui--mcp-cyclops": {
      "category": "cloud-platforms",
      "description": "A Model Context Protocol (MCP) facilitator embedded within a Cyclops server instance, enabling autonomous agents to manipulate Kubernetes deployments via high-level module constructs.",
      "forks": 3,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "cyclops",
        "cloud platform",
        "services cyclops",
        "platforms cloud"
      ],
      "language": "Go",
      "license": "No License",
      "name": "mcp-cyclops-abstraction-layer",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "cyclops-ui",
      "readme_content": "\u003cp align=\"center\" width=\"100%\"\u003e\n    \u003cimg alt=\"cyclops_simplistic\" width=\"75%\" src=\"https://raw.githubusercontent.com/cyclops-ui/cyclops/main/web/static/img/cyclops-simplistic.png\"\u003e\n\u003cp/\u003e\n\n# Cyclops Model Context Protocol (MCP) Facilitator\n\nThis Cyclops MCP component empowers your preferred artificial intelligence entity to govern your cluster's Kubernetes assets. The Cyclops MCP endpoint furnishes the necessary interfaces for agents to safely instantiate and revise extant application definitions.\n\nhttps://github.com/user-attachments/assets/0c675c33-1e36-4fdb-bf8c-2fd7fb491e6b\n\nConsequently, it possesses the capacity to audit all registered manifest blueprints and their underlying schemas, thereby ensuring the generation of accurate, production-ready deployments. This drastically curtails the propensity for configuration errors by the agent, as it manipulates abstract entities (termed Cyclops Modules) rather than directly modifying granular Kubernetes objects like Deployments, Services, or Ingresses.\n\nThis mechanism facilitates accelerated development cycles while guaranteeing that no unscrutinized infrastructural flaws reach the production environment.\n\n**By leveraging Cyclops and its associated MCP, you effectively decouple Kubernetes operational complexity from both human developers and sophisticated AI operators.**\n\n## Deployment via Graphical Interface\n\nThe most straightforward method for deploying the Cyclops MCP server is through the Cyclops User Interface (UI).\n\n\u003e ‚ö†Ô∏è To utilize the UI for installing the Cyclops MCP, Cyclops must be operating on version `v0.20.1` or newer.\n\n1. Access the Cyclops MCP installation by navigating the sidebar to `‚ÄúAddon‚Äù` and then selecting `\"MCP server\"`. Initiate the installation by clicking `\"Install Cyclops MCP server\"`; the service should be operational within moments.\n\n\u003cimg width=\"762\" alt=\"addon-install\" src=\"https://github.com/user-attachments/assets/57dfd8f9-9b58-4797-a348-d2788a6fba9b\" /\u003e\n\n2. Once the MCP server is live, the subsequent task is making it accessible externally to your cluster and linking your AI agent to it.\n\n   To proxy the MCP server locally, execute the following command:\n\n    bash\n    kubectl port-forward -n cyclops svc/cyclops-mcp 8000:8000\n    \n\n   Your server endpoint will then be reachable at `localhost:8000`.\n\n3. Connect your AI entity by injecting the Cyclops MCP server address into its configuration parameters. For integration with tools like Cursor, use the following JSON snippet:\n\n    bash\n    {\n      \"mcpServers\": {\n        \"cyclops-kubernetes\": {\n          \"url\": \"http://localhost:8000/sse\"\n        }\n      }\n    }\n    \n\nYou are now prepared to commence interactions with your AI assistant. The preceding example demonstrates integration with Cursor.\n\n\u003cdetails\u003e\n\n\u003csummary\u003eInstall via stdin Binary Method\u003c/summary\u003e\n\n### 1. Prerequisite: Cyclops Installation in Kubernetes\n\nRefer to the official documentation for single-command installation instructions: https://cyclops-ui.com/docs/installation/install/manifest\n\n### 2. Fetching the MCP Server Executable\n\nObtain the Cyclops MCP server binary using this command:\n\nyaml\nGOBIN=\"$HOME/go/bin\" go install github.com/cyclops-ui/mcp-cyclops/cmd/mcp-cyclops@latest\n\n\n### 3. Configuring the MCP Client\n\n\u003e ‚ö†Ô∏è By default, the Cyclops MCP server will leverage the `.kube/config` file for cluster connectivity.\n\nConfigure your client to utilize the MCP Cyclops server:\n\n\n{\n  \"mcpServers\": {\n    \"mcp-cyclops\": {\n      \"command\": \"mcp-cyclops\"\n    }\n  }\n}\n\n\n## Configuration Directives\n\nConfiguration of the Cyclops MCP server is managed through environment variables. The subsequent JSON illustrates how to specify an alternate kubeconfig file path for the server's cluster management operations.\n\n\n{\n  \"mcpServers\": {\n    \"mcp-cyclops\": {\n      \"command\": \"mcp-cyclops\",\n      \"env\": {\n        \"KUBECONFIG\": \"/path/to/your/kubeconfig\"\n      }\n    }\n  }\n}\n\n\n\n### Environment Variables Reference\n\n| Variable Name | Purpose |\n|-----------------------------------|-----------------------------------------------------------------------------------------|\n| `KUBECONFIG`                      | Location of the kubeconfig file (optional; defaults to in-cluster configuration or $HOME/.kube/config) |\n| `CYCLOPS_KUBE_CONTEXT`            | Specific Kubernetes context to be employed (optional)                                     |\n| `CYCLOPS_MODULE_NAMESPACE`        | The designated namespace for storing module resources                                     |\n| `CYCLOPS_HELM_RELEASE_NAMESPACE`  | The namespace designated for hosting Helm release artifacts                                 |\n| `CYCLOPS_MODULE_TARGET_NAMESPACE` | The namespace where deployed modules will ultimately reside                                 |\n\n--- \n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003e\nInstallation Directly into a Kubernetes Cluster\n\u003c/summary\u003e\n\n---\n\nTo avoid requiring every developer to locally install the `mcp-cyclops` binary, you can deploy the Cyclops MCP server using SSE as the communication transport directly into your Kubernetes cluster, centralizing access for all authorized users.\n\n1. Verify prerequisite installations within your cluster:\n    1. Confirm Cyclops pods are operational:\n\n        shell\n        kubectl get pods -n cyclops\n        \n\n       Expected Output:\n\n        \n        NAME                            READY   STATUS    RESTARTS   AGE\n        cyclops-ctrl-676b5d9789-ntcls   1/1     Running   0          94s\n        cyclops-ui-7798655f97-xdg29     1/1     Running   0          94s\n        \n\n    2. Check for installed Custom Resource Definitions (CRDs) related to Cyclops:\n\n        shell\n        kubectl get crds | grep cyclops-ui\n        \n\n       Expected Output:\n\n        \n        modules.cyclops-ui.com             2025-04-26T15:28:18Z\n        templateauthrules.cyclops-ui.com   2025-04-26T15:28:18Z\n        templatestores.cyclops-ui.com      2025-04-26T15:28:18Z\n        \n\n2. Deploy the Cyclops MCP server resource using the following manifest command:\n\n    bash\n    kubectl apply -f https://raw.githubusercontent.com/cyclops-ui/mcp-cyclops/refs/heads/main/install/mcp-server.yaml\n    \n\n3. Next, expose the `cyclops-mcp` service. For initial validation, use port-forwarding:\n\n    bash\n    kubectl port-forward svc/cyclops-mcp -n cyclops 8000:8000\n    \n\n4. Update your agent's configuration to point to the exposed MCP service host, or in testing scenarios, the local forwarded address:\n\n    \n    {\n      \"mcpServers\": {\n        \"mcp-cyclops\": {\n          \"url\": \"http://localhost:8000/sse\"\n        }\n      }\n    }\n    \n---\n\n\u003c/details\u003e\n\n## Exposed Agent Interfaces\n\n| Interface Name          | Functionality Description                                                                                                            |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------|\n| `create_module`       | Instantiates a novel Module. Precondition: Execution of `get_template_schema` is mandatory to verify parameter compliance for the target template.     |\n| `get_module`          | Retrieves a Module based on its designated identifier.                                                                                |\n| `list_modules`        | Generates a manifest of all active Cyclops Modules.                                                                                |\n| `update_module`       | Modifies an existing Module by its Name. Precondition: Execution of `get_template_schema` is mandatory to validate supplied values. |n| `get_template_schema` | Returns the JSON schema definition for a specified template. Crucial validation step before invoking `create_module`.                                |n| `get_template_store`  | Fetches metadata for a specified Template Store resource.                                                                        |\n| `list_template_store` | Enumerates all Template Stores currently present within the cluster.                                                                   |\n",
      "stars": 29,
      "updated_at": "2025-08-21T07:53:35Z",
      "url": "https://github.com/cyclops-ui/mcp-cyclops"
    },
    "elementfm--mcp": {
      "category": "cloud-platforms",
      "description": "Open source podcast hosting platform",
      "forks": 0,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "platform",
        "podcast",
        "platforms cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "Unknown",
      "license": "Unknown",
      "name": "mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "elementfm",
      "readme_content": "",
      "stars": 0,
      "updated_at": "",
      "url": "https://gitlab.com/elementfm/mcp"
    },
    "erikhoward--adls-mcp-server": {
      "category": "cloud-platforms",
      "description": "An implementation of the Model Context Protocol (MCP) server specifically tailored for interaction with Azure Data Lake Storage Gen2. This service facilitates comprehensive management of storage containers alongside robust capabilities for file ingress, egress, modification, and attribute inspection within those containers.",
      "forks": 5,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "azure",
        "platform",
        "cloud platforms",
        "platforms cloud",
        "cloud platform"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "azure-data-lake-storage-mcp-adapter",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "erikhoward",
      "readme_content": "# ADLS Gen2 MCP Endpoint Implementation üöÄ\n\nThis repository hosts the necessary software stack to expose Azure Data Lake Storage Gen2 functionality through the standardized Model Context Protocol (MCP). It acts as a bridge, translating abstract MCP calls into concrete Azure SDK operations for storage manipulation.\n\n[![License](https://img.shields.io/github/license/erikhoward/adls-mcp-server)](https://opensource.org/licenses/MIT) [![Language Version](https://img.shields.io/badge/python-3.13%2B-blue.svg)](https://www.python.org/) [![Package Manager](https://img.shields.io/badge/uv-dependency%20handler-blueviolet)](https://docs.astral.sh/uv/) [![Protocol Compliance](https://img.shields.io/badge/MCP-compliant-green.svg)](https://github.com/modelcontextprotocol/spec)\n\n## Configuration and Deployment üõ†Ô∏è\n\n### Installation Procedure üì¶\n\nRequires a contemporary Python environment (version 3.13 or newer).\n\nInstall the requisite package utilizing the `uv` toolchain:\n\nbash\nuv pip install adls2-mcp-server\n\n\n### MCP Interface Setup ‚öôÔ∏è\n\n#### Integrating with Claude Desktop\n\n1.  **Configuration Modification:** Access and modify your local `claude_desktop_config.json` file. Append the subsequent configuration block to define the new storage endpoint:\n\n    *   **macOS Path:** `~/Library/Application Support/Claude Desktop/claude_desktop_config.json`\n    *   **Windows Path:** `%APPDATA%\\Claude Desktop\\claude_desktop_config.json`\n\n    \n    {\n        \"mcpServers\": {\n            \"adls2\": {\n                \"command\": \"adls2-mcp-server\",\n                \"env\": {\n                    \"LOG_LEVEL\": \"DEBUG\",\n                    \"UPLOAD_ROOT\": \"/path/to/store/uploads\",\n                    \"DOWNLOAD_ROOT\": \"/path/to/store/downloads\",\n                    \"AZURE_STORAGE_ACCOUNT_NAME\": \"your-azure-adls2-storage-account-name\",\n                    \"READ_ONLY_MODE\": \"false\"\n                }\n            }\n        }\n    }\n    \n\nEnvironment Configuration Variables Reference Table:\n\n| Parameter | Purpose | Default Value |\n| :--- | :--- | :--- |\n| `LOG_LEVEL` | Verbosity setting for logging output | `INFO` |\n| `UPLOAD_ROOT` | Local staging directory for inbound transfers | `./uploads` |\n| `DOWNLOAD_ROOT` | Local target directory for outbound transfers | `./downloads` |\n| `AZURE_STORAGE_ACCOUNT_NAME` | Identifier for the targeted Azure Storage resource | `None` |\n| `AZURE_STORAGE_ACCOUNT_KEY` | Secret key for authentication (optional) | `None` |\n| `READ_ONLY_MODE` | Constraint flag to prevent write operations | `true` |\n\n**Authentication Note:** If `AZURE_STORAGE_ACCOUNT_KEY` is omitted, the system defaults to leveraging established Azure CLI credentials. Verify successful authentication beforehand:\n\nbash\naz login\n\n\n2 - Execute a restart of the Claude Desktop application to load the new service definition.\n\n### Accessible Utility Functions üîß\n\n#### Filesystem (Container) Administration\n\n*   `list_filesystems` - Enumerate all available filesystems within the configured storage account.\n*   `create_filesystem` - Provision a new filesystem resource.\n*   `delete_filesystem` - Erase a specified existing filesystem.\n\n#### Object (File) Manipulation\n\n*   `upload_file` - Initiate the transfer of a local file to ADLS2.\n*   `download_file` - Retrieve a file from ADLS2 to the local system.\n*   `file_exists` - Verification check for file presence.\n*   `rename_file` - Atomically relocate or rename a file object.\n*   `get_file_properties` - Retrieve standard system attributes of a file.\n*   `get_file_metadata` - Fetch custom metadata key/value pairs associated with a file.\n*   `set_file_metadata` - Update a single metadata attribute on a file.\n*   `set_file_metadata_json` - Bulk update multiple metadata attributes using a JSON structure.\n\n#### Hierarchical (Directory) Management\n\n*   `create_directory` - Establish a new folder structure.\n*   `delete_directory` - Remove an existing directory and its contents.\n*   `rename_directory` - Relocate or rename a directory structure.\n*   `directory_exists` - Determine if a specified path as a directory exists.\n*   `directory_get_paths` - Recursively retrieve all contained paths beneath a given directory.\n\n## Local Development Workflow üíª\n\n### Initial Repository Setup\n\n1 - Obtain the source code repository:\n\nbash\ngit clone https://github.com/erikhoward/adls2-mcp-server.git\ncd adls2-mcp-server\n\n\n2 - Isolate environment and activate:\n\n**Unix-like Systems (Linux/macOS):**\n\nbash\npython -m venv .venv\nsource .venv/bin/activate\n\n\n**Windows:**\n\nbash\n.venv\\Scripts\\activate\n\n\n3 - Install development dependencies:\n\nbash\npip install -e \".[dev]\"\n\n\n4 - Configure local runtime variables:\n\nbash\ncp .env.example .env\n\n\nModify `.env` with your specific connection details:\n\nbash\nAZURE_STORAGE_ACCOUNT_NAME=your_azure_adls2_storage_account_name\nAZURE_STORAGE_ACCOUNT_KEY=your_azure_adls2_storage_key (optional)\nDOWNLOAD_ROOT=/path/to/download/folder\nUPLOAD_ROOT=/path/to/upload/folder\nREAD_ONLY_MODE=True\nLOG_LEVEL=INFO\n\n\nIf `AZURE_STORAGE_ACCOUNT_KEY` is absent, Azure CLI authentication is attempted. Ensure prior successful execution of `az login`.\n\n5 - Claude Desktop Integration Configuration\n\nUpdate `claude_desktop_config.json` using the following structure, noting the `uv run` command pattern for development mode execution:\n\n\n{\n    \"mcpServers\": {\n        \"adls2\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/adls2-mcp-server/repo\",\n                \"run\",\n                \"adls2-mcp-server\"\n            ],\n            \"env\": {\n                \"LOG_LEVEL\": \"DEBUG\",\n                \"UPLOAD_ROOT\": \"/path/to/store/uploads\",\n                \"DOWNLOAD_ROOT\": \"/path/to/store/downloads\",\n                \"AZURE_STORAGE_ACCOUNT_NAME\": \"your-azure-adls2-storage-account-name\",\n                \"READ_ONLY_MODE\": \"false\"\n            }\n        }\n    }\n}\n\n\n6 - Relaunch Claude Desktop.\n\n## Collaborative Efforts ü§ù\n\nWe encourage community contributions! Please submit enhancements via Pull Requests.\n\n1.  Create a personal fork of the repository.\n2.  Establish a new feature branch (`git checkout -b feature/DescriptiveFeatureName`).\n3.  Commit your modifications (`git commit -m 'feat: Incorporate new functionality'`).\n4.  Push the branch upstream (`git push origin feature/DescriptiveFeatureName`).\n5.  Submit a formal Pull Request.\n\n## Licensing Information ‚öñÔ∏è\n\nThis software is distributed under the terms of the MIT License; consult the [LICENSE.md] file for details.\n\n**Disclaimer: This software project is independently developed and is not affiliated with Microsoft Corporation.**\n\nWIKIPEDIA EXCERPT (Contextual Information):\nGoogle Cloud Platform (GCP) represents a comprehensive suite of utility computing services provided by Google, encompassing modular offerings across computation, persistent data storage, advanced data analysis, and machine learning capabilities, supported by integrated management tooling. This infrastructure operates on the identical foundational hardware utilized internally by Google for its public-facing products like Google Search, Gmail, and Google Docs (as detailed by Verma et al.). Enrollment necessitates the provision of banking or credit card credentials.\nGoogle Cloud Platform offers environments spanning Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and serverless execution models.\nGoogle's initial foray into cloud computing was the announcement of App Engine in April 2008, designed for hosting and developing web applications within Google-managed data centers. General availability for this service was achieved in November 2011. Following this debut, Google progressively augmented the platform with numerous additional cloud services.\nCurrently, Google Cloud encompasses the public cloud infrastructure (GCP), the Google Workspace suite (formerly G Suite), enterprise versions of ChromeOS and Android, and various APIs for enterprise mapping and machine learning. Since at least 2022, official Google communications standardized the nomenclature to \"Google Cloud,\" which occasionally leads to ambiguity regarding the underlying GCP infrastructure component.",
      "stars": 4,
      "updated_at": "2025-06-05T06:47:58Z",
      "url": "https://github.com/erikhoward/adls-mcp-server"
    },
    "espressif--esp-rainmaker-mcp": {
      "category": "cloud-platforms",
      "description": "Official Espressif RainMaker interface server supporting Model Context Protocol interactions.",
      "forks": 5,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "espressif",
        "platform",
        "cloud platform",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "esp-rm-mcp-gateway",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "espressif",
      "readme_content": "# ESP RainMaker MCP Server Gateway\n\nThis repository furnishes a **Model Context Protocol (MCP)** server wrapper, utilizing the underlying [`esp-rainmaker-cli`](https://github.com/espressif/esp-rainmaker-cli) Python utility. It grants MCP-compliant consumer applications (such as advanced LLMs like Claude, development assistants like Cursor, or specialized clients like Windsurf) the ability to interface with and govern your personal [ESP RainMaker](https://rainmaker.espressif.com/) connected hardware using the authorized command-line toolset.\n\n## Understanding the Model Context Protocol (MCP)\n\nThe **Model Context Protocol (MCP)** establishes a standardized mechanism enabling sophisticated AI entities to communicate with external resources, data repositories, and operational services in a cohesive manner. Championed by Anthropic and adopted broadly across the AI sector, MCP functions as a universal adapter layer‚Äîanalogous to USB-C in electronics‚Äîensuring smooth interoperability across diverse software environments.\n\n### Advantages of MCP Integration for ESP RainMaker\n\n- **Standardized Access**: MCP permits artificial intelligence engines to manipulate and query IoT apparatuses using natural language instructions, fostering greater ease of use.\n- **Immediate Operational Control**: Via MCP, end-users can trigger immediate device modifications, including power state toggling, configuration adjustments, and temporal scheduling updates, directly through their AI interface.\n- **Hybrid Control Architecture**: While this specialized MCP server executes locally, securing user authentication tokens on the host machine, all device control operations are relayed through the official ESP RainMaker cloud infrastructure via the `esp-rainmaker-cli` backend.\n\nBy implementing MCP, the ESP RainMaker ecosystem significantly boosts its utility, empowering tools such as Gemini CLI, Claude, Cursor, and Windsurf to manage smart devices efficiently and with robust security.\n\n## Necessary Preconditions\n\n*   **Python Runtime**: Version 3.10 or newer is mandatory.\n*   **Package Manager**: The `uv` Python package manager is required. Installation instructions are available on [Astral's uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n*   **RainMaker CLI Authentication**: You *must* complete the initial `esp-rainmaker-cli login` sequence in your system terminal at least once. This server depends entirely on the session credentials persisted by that preparatory step.\n*   **Device Onboarding Note**: The MCP server component does not support the initial onboarding or provisioning of new RainMaker nodes into your account.\n\n## Setup and Deployment Procedure\n\n1.  **Obtain the Source Code:**\n    bash\ngit clone https://github.com/espressif/esp-rainmaker-mcp.git\ncd esp-rainmaker-mcp\n    \n\n2.  **Dependency Installation via `uv`:**\n    This command leverages `uv` to build a dedicated virtual environment and install all required packages listed in `pyproject.toml`, including `esp-rainmaker-cli` and `mcp[cli]`.\n\n    bash\nuv sync\n    \n    *(This step presumes `uv` is accessible in your PATH)*\n\n3. **Establish ESP Rainmaker Credentials via CLI**\n    bash\nuv run esp-rainmaker-cli login\n    \n\n\u003e [!CAUTION]\n\u003e For security compliance, direct credential submission (username/password) through the MCP interface is intentionally disallowed. Initial authentication must occur using the standard CLI utility first.\n\n## Client Integration Configuration\n\nTo integrate this utility as an active MCP endpoint within supported LLM clients (e.g., Claude Desktop, Cursor, Windsurf, Gemini CLI), you must insert the following identical JSON structure into each client's respective configuration file:\n\n### Universal MCP Server Definition (For All Clients)\n\nEmploy this JSON block across all compatible MCP clients:\n\n\n{\n   \"mcpServers\": {\n      \"ESP-RainMaker-MCP\": {\n         \"command\": \"uv\",\n         \"args\": [\n            \"run\",\n            \"--with\",\n            \"esp-rainmaker-cli\",\n            \"--with\",\n            \"mcp[cli]\",\n            \"mcp\",\n            \"run\",\n            \"\u003cabsolute_path_to_repo\u003e/server.py\"\n         ]\n      }\n   }\n}\n\n\n\u003e [!WARNING]\n\u003e Substitute `\u003cabsolute_path_to_repo\u003e/server.py` with the fully qualified, absolute file system path to `server.py` located within your cloned `esp-rainmaker-mcp` directory.\n\n### Configuration Guides for Specific Clients\n\n*(The detailed steps for Cursor, Claude Desktop, Windsurf, and Gemini CLI setup are retained here for completeness, focusing on where to place the configuration block.)*\n\n[... Detailed Setup Steps for Cursor, Claude Desktop, Windsurf, and Gemini CLI ...]\n\n\u003e [!TIP]\n\u003e The `--with` arguments provided to `uv` are crucial; they guarantee that the required `esp-rainmaker-cli` and `mcp[cli]` dependencies are loaded into the execution context before the `mcp run` command initiates the server.\n\n## Operational Flow Summary\n\nThis server functions strictly as a secure intermediary. It leverages the external `mcp` library to manage all Model Context Protocol communications. Upon invocation by a connected tool:\n\n1.  It calls the appropriate functions exposed by the installed `esp-rainmaker-cli` library.\n2.  The library accesses authentication tokens stored locally on your machine.\n3.  Secure API calls are routed to the ESP RainMaker cloud service.\n4.  The resulting data payload (or any encountered errors) is returned back to the calling client via the MCP framework.\n\n\n## Exposed Functional Interfaces\n\nThis server makes the following specialized tools available for controlling ESP RainMaker resources:\n\n### Authentication and Status Checking\n\n*   `login_instructions()`:\n    *   Outputs Markdown-formatted guidance instructing the user to utilize the standard `esp-rainmaker-cli login` flow in their shell. Relies on the CLI's built-in secure browser authentication method.\n*   `check_login_status()`:\n    *   Verifies the presence and validity of the locally stored credentials, confirming the server's ability to connect to the ESP RainMaker backend.\n\n### Device (Node) Orchestration\n\n*   `get_nodes()`:\n    *   Retrieves a comprehensive manifest of all device identifiers linked to the authenticated user account.\n*   `get_node_details(node_id: str = None, fields: str = None, name: str = None, type_: str = None)`:\n    *   Fetches comprehensive metadata for specified nodes, including configuration parameters, current operational status, and firmware information.\n    *   Filtering Capabilities:\n        - `fields`: A comma-separated string specifying desired output attributes (e.g., \"node_id,name,status.connectivity\").\n        - `name`: Filters results based on a partial match against the user-friendly device name.\n        - `type_`: Filters based on a partial match against the device's hardware classification.\n        - `node_id`: Targets a single, specific node, or retrieves all if set to `None`.\n    *   Return Type: A dictionary for a single node or a list of dictionaries for multiple nodes.\n*   `get_node_status(node_id: str)`:\n    *   Returns the real-time connectivity state (online/offline) for a specified device identifier.\n*   `get_params(node_id: str)`:\n    *   Retrieves the current settable values for a designated device.\n*   `set_params(node_id: str, params_dict: dict)`:\n    *   Applies new parameter settings across one or more devices.\n    *   `node_id`: Can accept a single ID or a comma-separated string of IDs (e.g., \"bulb_a,switch_b\").\n    *   `params_dict`: A nested dictionary defining the required state change, e.g., `{\"Switch\": {\"Power\": false}}`.\n\n### Temporal Scheduling Control\n\n*   `get_schedules(node_id: str)`:\n    *   Fetches all existing timed operations scheduled for a particular device.\n*   `set_schedule(node_id: str, operation: str, ...)`:\n    *   Manages the lifecycle of device schedules.\n    *   `operation`: Must be one of: \"add\", \"edit\", \"remove\", \"enable\", or \"disable\".\n    *   For creation/modification (\"add\"/\"edit\"): Requires specification of a `name`, a temporal `trigger`, and an executable `action`.\n    *   Example Triggers:\n        *   Daily at 08:00 UTC: `{\"m\": 480, \"d\": 127}`\n        *   Weekdays at 18:30 UTC: `{\"m\": 1110, \"d\": 31}`\n    *   Example Action: `{\"Light\": {\"Brightness\": 50}}`\n\n### Hierarchical Group Management (Rooms/Homes)\n\n*   `create_group(name: str, group_type: str = None, ...)`:\n    *   Provisions a new organizational container, designated as either a \"home\" or a \"room\".\n    *   Mandatory inputs: `name` and `group_type` (\"home\" or \"room\").\n    *   Rooms require the specification of a `parent_group_id`.\n    *   Example: `create_group(\"Office Area\", \"room\", parent_group_id=\"home_id_123\")`\n\n*   `get_group_details(group_id: str = None, include_nodes: bool = False)`:\n    *   Retrieves structural data for groups. If `group_id` is omitted (`None`), it returns details for the entire hierarchy.\n    *   Setting `include_nodes=True` enriches the output with the full details of member devices.\n\n*   `update_group(group_id: str, ...)`:\n    *   Modifies an existing group's properties or manages its device membership.\n    *   Optional update parameters: `name`, `description`, `add_nodes` (comma-separated list), `remove_nodes`.\n    *   Usage Examples:\n        *   Rename: `update_group(\"g_xyz\", name=\"New Name\")`\n        *   Add Devices: `update_group(\"g_xyz\", add_nodes=\"sensor4,actuator9\")`\n\n*   `add_device_to_room(device_node_id: str, room_group_id: str)`:\n    *   A convenience function to map a specific device identifier into a designated room ID, automatically handling implicit parent group association.\n    *   Example: `add_device_to_room(\"light_kitchen_1\", \"room_kitchen_id\")`\n\n## Licensing Information\n\nThis software is distributed under the terms detailed in the official [LICENSE](LICENSE) document.\n\nWIKIPEDIA: Google Cloud Platform (GCP) encompasses a comprehensive suite of cloud computing services provided by Google, offering modular capabilities in computation, data persistence, advanced analytics, and machine learning, supplemented by robust management tooling. As documented by Verma et al., this platform operates on the identical foundational infrastructure utilized internally by Google for its consumer-facing products such as Gmail, Google Docs, and Google Search. Account creation mandates the provision of payment information, either a credit card or bank account details.\n\nGCP delivers services spanning Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and serverless computational environments.\n\nGoogle's foray into cloud computing services began in April 2008 with the announcement of App Engine, a system designed for the development and hosting of web applications within Google's managed data centers. This service reached general availability in November 2011. Following App Engine's introduction, Google systematically augmented the platform with numerous additional cloud offerings.\n\nGoogle Cloud Platform constitutes a major segment of the broader 'Google Cloud' entity, which incorporates the public cloud infrastructure of GCP, the enterprise software suite Google Workspace (formerly G Suite), business editions of Android and ChromeOS, and specialized Application Programming Interfaces (APIs) for machine intelligence and enterprise geospatial services. Since at least 2022, official Google communications frequently use \"Google Cloud\" as the definitive replacement for \"Google Cloud Platform,\" a transition that can occasionally lead to nomenclature ambiguity.\n\n\n== Key Service Offerings ==\nGoogle catalog lists upwards of 100 distinct products under the Google Cloud branding. A selection of principal services follows.\n\n\n=== Computation Services ===\nApp Engine ‚Äì A Platform as a Service environment supporting application deployment written in languages including Java, PHP, Node.js, Python, C#, .Net, Ruby, and Go.\nCompute Engine ‚Äì Infrastructure as a Service providing scalable execution environments for both Linux and Microsoft Windows virtual machines.\nGoogle Kubernetes Engine (GKE) or GKE on-prem (part of Anthos) ‚Äì A Container as a Service solution built upon the Kubernetes orchestration standard.\nCloud Functions ‚Äì A Functions as a Service framework for running short-lived, event-driven code snippets in Node.js, Java, Python, or Go.\nCloud Run ‚Äì A flexible compute execution substrate based on Knative principles. Available in a fully managed configuration or as Cloud Run for Anthos, currently supporting management across GCP, AWS, and VMware environments.\n\n\n=== Data Persistence and Databases ===\nCloud Storage ‚Äì Unified object storage solution incorporating integrated edge caching for handling unstructured data.\nCloud SQL ‚Äì A managed Database as a Service supporting relational engines like MySQL, PostgreSQL, and Microsoft SQL Server.\nCloud Bigtable ‚Äì A managed, high-throughput NoSQL database service.\nCloud Spanner ‚Äì A globally distributed, strongly consistent, relational database engineered for horizontal scalability.\nCloud Datastore ‚Äì A NoSQL data store optimized for mobile and web applications.\nPersistent Disk ‚Äì Block-level storage volumes provisioned for use with Compute Engine virtual machines.\nCloud Memorystore ‚Äì Managed in-memory caching services based on Redis and Memcached technologies.\nLocal SSD: High-speed, ephemeral, locally attached block storage resources.\nFilestore: A high-performance, managed file storage service designed for Google Cloud users.\nAlloyDB: A fully managed, proprietary database service compatible with PostgreSQL.\n\n\n=== Network Infrastructure ===\n\nVPC ‚Äì Virtual Private Cloud, providing isolated network segments within the Google infrastructure.",
      "stars": 9,
      "updated_at": "2025-09-19T10:37:22Z",
      "url": "https://github.com/espressif/esp-rainmaker-mcp"
    },
    "flux159--mcp-server-kubernetes": {
      "category": "cloud-platforms",
      "description": "Typescript implementation of Kubernetes cluster operations for pods, deployments, services.",
      "forks": 180,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "pods",
        "platforms cloud",
        "cloud platforms",
        "kubernetes cluster"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "mcp-server-kubernetes",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "flux159",
      "readme_content": "# MCP Server Kubernetes\n\n[![CI](https://github.com/Flux159/mcp-server-kubernetes/actions/workflows/ci.yml/badge.svg)](https://github.com/yourusername/mcp-server-kubernetes/actions/workflows/ci.yml)\n[![Language](https://img.shields.io/github/languages/top/Flux159/mcp-server-kubernetes)](https://github.com/yourusername/mcp-server-kubernetes)\n[![Bun](https://img.shields.io/badge/runtime-bun-orange)](https://bun.sh)\n[![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=flat\u0026logo=kubernetes\u0026logoColor=white)](https://kubernetes.io/)\n[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat\u0026logo=docker\u0026logoColor=white)](https://www.docker.com/)\n[![Stars](https://img.shields.io/github/stars/Flux159/mcp-server-kubernetes)](https://github.com/Flux159/mcp-server-kubernetes/stargazers)\n[![Issues](https://img.shields.io/github/issues/Flux159/mcp-server-kubernetes)](https://github.com/Flux159/mcp-server-kubernetes/issues)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/Flux159/mcp-server-kubernetes/pulls)\n[![Last Commit](https://img.shields.io/github/last-commit/Flux159/mcp-server-kubernetes)](https://github.com/Flux159/mcp-server-kubernetes/commits/main)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Flux159/mcp-server-kubernetes)](https://archestra.ai/mcp-catalog/flux159__mcp-server-kubernetes)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Flux159/mcp-server-kubernetes)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg alt=\"icon\" src=\"https://raw.githubusercontent.com/Flux159/mcp-server-kubernetes/refs/heads/main/icon.png\" width=\"200\"\u003e\n\u003c/p\u003e\n\nMCP Server that can connect to a Kubernetes cluster and manage it. Supports loading kubeconfig from multiple sources in priority order.\n\nhttps://github.com/user-attachments/assets/f25f8f4e-4d04-479b-9ae0-5dac452dd2ed\n\n\u003ca href=\"https://glama.ai/mcp/servers/w71ieamqrt\"\u003e\u003cimg alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w71ieamqrt/badge\" /\u003e\u003c/a\u003e\n\n## Installation \u0026 Usage\n\n### Prerequisites\n\nBefore using this MCP server with any tool, make sure you have:\n\n1. kubectl installed and in your PATH\n2. A valid kubeconfig file with contexts configured\n3. Access to a Kubernetes cluster configured for kubectl (e.g. minikube, Rancher Desktop, GKE, etc.)\n4. Helm v3 installed and in your PATH (no Tiller required). Optional if you don't plan to use Helm.\n\nYou can verify your connection by running `kubectl get pods` in a terminal to ensure you can connect to your cluster without credential issues.\n\nBy default, the server loads kubeconfig from `~/.kube/config`. For additional authentication options (environment variables, custom paths, etc.), see [ADVANCED_README.md](ADVANCED_README.md).\n\n### Claude Code\n\nAdd the MCP server to Claude Code using the built-in command:\n\n```bash\nclaude mcp add kubernetes -- npx mcp-server-kubernetes\n```\n\nThis will automatically configure the server in your Claude Code MCP settings.\n\n### Claude Desktop\n\nAdd the following configuration to your Claude Desktop config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-kubernetes\"]\n    }\n  }\n}\n```\n\n### Claude Desktop Connector via mcpb\n\nMCP Server Kubernetes is also available as a [mcpb](https://github.com/anthropics/mcpb) (formerly dxt) extension. In Claude Desktop, go to Settings (`Cmd+,` on Mac) -\u003e Extensions -\u003e Browse Extensions and scroll to find mcp-server-kubernetes in the modal. Install it \u0026 it will install \u0026 utilize kubectl via command line \u0026 your kubeconfig.\n\nTo manually install, you can also get the .mcpb by going to the latest [Release](https://github.com/Flux159/mcp-server-kubernetes/releases) and downloading it.\n\n### VS Code\n\n[![Install Kubernetes MCP in VS Code](https://img.shields.io/badge/Install%20Kubernetes%20MCP%20in%20VS%20Code-blue?logo=visualstudiocode)](vscode:mcp/install?%7B%22name%22%3A%20%22kubernetes%22%2C%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22mcp-server-kubernetes%22%5D%7D)\n\nFor VS Code integration, you can use the MCP server with extensions that support the Model Context Protocol:\n\n1. Install a compatible MCP extension (such as Claude Dev or similar MCP clients)\n2. Configure the extension to use this server:\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-kubernetes\"],\n      \"description\": \"Kubernetes cluster management and operations\"\n    }\n  }\n}\n```\n\n### Cursor\n\nCursor supports MCP servers through its AI integration. Add the server to your Cursor MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-kubernetes\"]\n    }\n  }\n}\n```\n\nThe server will automatically connect to your current kubectl context. You can verify the connection by asking the AI assistant to list your pods or create a test deployment.\n\n## Usage with mcp-chat\n\n[mcp-chat](https://github.com/Flux159/mcp-chat) is a CLI chat client for MCP servers. You can use it to interact with the Kubernetes server.\n\n```shell\nnpx mcp-chat --server \"npx mcp-server-kubernetes\"\n```\n\nAlternatively, pass it your existing Claude Desktop configuration file from above (Linux should pass the correct path to config):\n\nMac:\n\n```shell\nnpx mcp-chat --config \"~/Library/Application Support/Claude/claude_desktop_config.json\"\n```\n\nWindows:\n\n```shell\nnpx mcp-chat --config \"%APPDATA%\\Claude\\claude_desktop_config.json\"\n```\n\n## Features\n\n- [x] Connect to a Kubernetes cluster\n- [x] Unified kubectl API for managing resources\n  - Get or list resources with `kubectl_get`\n  - Describe resources with `kubectl_describe`\n  - List resources with `kubectl_get`\n  - Create resources with `kubectl_create`\n  - Apply YAML manifests with `kubectl_apply`\n  - Delete resources with `kubectl_delete`\n  - Get logs with `kubectl_logs`\n  - Manage kubectl contexts with `kubectl_context`\n  - Explain Kubernetes resources with `explain_resource`\n  - List API resources with `list_api_resources`\n  - Scale resources with `kubectl_scale`\n  - Update field(s) of a resource with `kubectl_patch`\n  - Manage deployment rollouts with `kubectl_rollout`\n  - Execute any kubectl command with `kubectl_generic`\n  - Verify connection with `ping`\n- [x] Advanced operations\n  - Scale deployments with `kubectl_scale` (replaces legacy `scale_deployment`)\n  - Port forward to pods and services with `port_forward`\n  - Run Helm operations\n    - Install, upgrade, and uninstall charts\n    - Support for custom values, repositories, and versions\n    - Template-based installation (`helm_template_apply`) to bypass authentication issues\n    - Template-based uninstallation (`helm_template_uninstall`) to bypass authentication issues\n  - Pod cleanup operations\n    - Clean up problematic pods (`cleanup_pods`) in states: Evicted, ContainerStatusUnknown, Completed, Error, ImagePullBackOff, CrashLoopBackOff\n  - Node management operations\n    - Cordoning, draining, and uncordoning nodes (`node_management`) for maintenance and scaling operations\n- [x] Troubleshooting Prompt (`k8s-diagnose`)\n  - Guides through a systematic Kubernetes troubleshooting flow for pods based on a keyword and optional namespace.\n- [x] Non-destructive mode for read and create/update-only access to clusters\n- [x] Secrets masking for security (masks sensitive data in `kubectl get secrets` commands, does not affect logs)\n\n## Prompts\n\nThe MCP Kubernetes server includes specialized prompts to assist with common diagnostic operations.\n\n### /k8s-diagnose Prompt\n\nThis prompt provides a systematic troubleshooting flow for Kubernetes pods. It accepts a `keyword` to identify relevant pods and an optional `namespace` to narrow the search.\nThe prompt's output will guide you through an autonomous troubleshooting flow, providing instructions for identifying issues, collecting evidence, and suggesting remediation steps.\n\n## Local Development\n\nMake sure that you have [bun installed](https://bun.sh/docs/installation). Clone the repo \u0026 install dependencies:\n\n```bash\ngit clone https://github.com/Flux159/mcp-server-kubernetes.git\ncd mcp-server-kubernetes\nbun install\n```\n\n### Development Workflow\n\n1. Start the server in development mode (watches for file changes):\n\n```bash\nbun run dev\n```\n\n2. Run unit tests:\n\n```bash\nbun run test\n```\n\n3. Build the project:\n\n```bash\nbun run build\n```\n\n4. Local Testing with [Inspector](https://github.com/modelcontextprotocol/inspector)\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n# Follow further instructions on terminal for Inspector link\n```\n\n5. Local testing with Claude Desktop\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-kubernetes\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/your/mcp-server-kubernetes/dist/index.js\"]\n    }\n  }\n}\n```\n\n6. Local testing with [mcp-chat](https://github.com/Flux159/mcp-chat)\n\n```bash\nbun run chat\n```\n\n## Contributing\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file for details.\n\n## Advanced\n\n### Non-Destructive Mode\n\nYou can run the server in a non-destructive mode that disables all destructive operations (delete pods, delete deployments, delete namespaces, etc.):\n\n```shell\nALLOW_ONLY_NON_DESTRUCTIVE_TOOLS=true npx mcp-server-kubernetes\n```\n\nFor Claude Desktop configuration with non-destructive mode:\n\n```json\n{\n  \"mcpServers\": {\n    \"kubernetes-readonly\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-kubernetes\"],\n      \"env\": {\n        \"ALLOW_ONLY_NON_DESTRUCTIVE_TOOLS\": \"true\"\n      }\n    }\n  }\n}\n```\n\n### Commands Available in Non-Destructive Mode\n\nAll read-only and resource creation/update operations remain available:\n\n- Resource Information: `kubectl_get`, `kubectl_describe`, `kubectl_logs`, `explain_resource`, `list_api_resources`\n- Resource Creation/Modification: `kubectl_apply`, `kubectl_create`, `kubectl_scale`, `kubectl_patch`, `kubectl_rollout`\n- Helm Operations: `install_helm_chart`, `upgrade_helm_chart`, `helm_template_apply`, `helm_template_uninstall`\n- Connectivity: `port_forward`, `stop_port_forward`\n- Context Management: `kubectl_context`\n\n### Commands Disabled in Non-Destructive Mode\n\nThe following destructive operations are disabled:\n\n- `kubectl_delete`: Deleting any Kubernetes resources\n- `uninstall_helm_chart`: Uninstalling Helm charts\n- `cleanup`: Cleanup of managed resources\n- `cleanup_pods`: Cleaning up problematic pods\n- `node_management`: Node management operations (can drain nodes)\n- `kubectl_generic`: General kubectl command access (may include destructive operations)\n\nFor additional advanced features, see the [ADVANCED_README.md](ADVANCED_README.md) and also the [docs](https://github.com/Flux159/mcp-server-kubernetes/tree/main/docs) folder for specific information on `helm_install`, `helm_template_apply`, node management \u0026 pod cleanup.\n\n## Architecture\n\nSee this [DeepWiki link](https://deepwiki.com/Flux159/mcp-server-kubernetes) for a more indepth architecture overview created by Devin.\n\nThis section describes the high-level architecture of the MCP Kubernetes server.\n\n### Request Flow\n\nThe sequence diagram below illustrates how requests flow through the system:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Transport Layer\n    participant Server as MCP Server\n    participant Filter as Tool Filter\n    participant Handler as Request Handler\n    participant K8sManager as KubernetesManager\n    participant K8s as Kubernetes API\n\n    Note over Transport: StdioTransport or\u003cbr\u003eSSE Transport\n\n    Client-\u003e\u003eTransport: Send Request\n    Transport-\u003e\u003eServer: Forward Request\n\n    alt Tools Request\n        Server-\u003e\u003eFilter: Filter available tools\n        Note over Filter: Remove destructive tools\u003cbr\u003eif in non-destructive mode\n        Filter-\u003e\u003eHandler: Route to tools handler\n\n        alt kubectl operations\n            Handler-\u003e\u003eK8sManager: Execute kubectl operation\n            K8sManager-\u003e\u003eK8s: Make API call\n        else Helm operations\n            Handler-\u003e\u003eK8sManager: Execute Helm operation\n            K8sManager-\u003e\u003eK8s: Make API call\n        else Port Forward operations\n            Handler-\u003e\u003eK8sManager: Set up port forwarding\n            K8sManager-\u003e\u003eK8s: Make API call\n        end\n\n        K8s--\u003e\u003eK8sManager: Return result\n        K8sManager--\u003e\u003eHandler: Process response\n        Handler--\u003e\u003eServer: Return tool result\n    else Resource Request\n        Server-\u003e\u003eHandler: Route to resource handler\n        Handler-\u003e\u003eK8sManager: Get resource data\n        K8sManager-\u003e\u003eK8s: Query API\n        K8s--\u003e\u003eK8sManager: Return data\n        K8sManager--\u003e\u003eHandler: Format response\n        Handler--\u003e\u003eServer: Return resource data\n    end\n\n    Server--\u003e\u003eTransport: Send Response\n    Transport--\u003e\u003eClient: Return Final Response\n```\n\nSee this [DeepWiki link](https://deepwiki.com/Flux159/mcp-server-kubernetes) for a more indepth architecture overview created by Devin.\n\n## Publishing new release\n\nGo to the [releases page](https://github.com/Flux159/mcp-server-kubernetes/releases), click on \"Draft New Release\", click \"Choose a tag\" and create a new tag by typing out a new version number using \"v{major}.{minor}.{patch}\" semver format. Then, write a release title \"Release v{major}.{minor}.{patch}\" and description / changelog if necessary and click \"Publish Release\".\n\nThis will create a new tag which will trigger a new release build via the cd.yml workflow. Once successful, the new release will be published to [npm](https://www.npmjs.com/package/mcp-server-kubernetes). Note that there is no need to update the package.json version manually, as the workflow will automatically update the version number in the package.json file \u0026 push a commit to main.\n\n## Not planned\n\nAdding clusters to kubectx.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Flux159/mcp-server-kubernetes\u0026type=Date)](https://www.star-history.com/#Flux159/mcp-server-kubernetes\u0026Date)\n\n## üñäÔ∏è Cite\n\nIf you find this repo useful, please cite:\n\n```\n@software{Patel_MCP_Server_Kubernetes_2024,\nauthor = {Patel, Paras and Sonwalkar, Suyog},\nmonth = jul,\ntitle = {{MCP Server Kubernetes}},\nurl = {https://github.com/Flux159/mcp-server-kubernetes},\nversion = {2.5.0},\nyear = {2024}\n}\n```\n",
      "stars": 1108,
      "updated_at": "2025-10-04T08:36:38Z",
      "url": "https://github.com/Flux159/mcp-server-kubernetes"
    },
    "hardik-id--azure-resource-graph-mcp-server": {
      "category": "cloud-platforms",
      "description": "A Model Context Protocol server for querying and analyzing Azure resources at scale using Azure Resource Graph, enabling AI assistants to explore and monitor Azure infrastructure.",
      "forks": 6,
      "imageUrl": "",
      "keywords": [
        "azure",
        "cloud",
        "platform",
        "azure infrastructure",
        "platforms cloud",
        "cloud platforms"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "azure-resource-graph-mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "hardik-id",
      "readme_content": "# Demo\n\n\n# Flow\n\n\n\n# Azure Resource Graph MCP Server\n\nThis is a Model Context Protocol (MCP) server that provides access to Azure Resource Graph queries. It allows you to retrieve information about Azure resources across your subscriptions using Resource Graph queries.\n\u003ca href=\"https://glama.ai/mcp/servers/@hardik-id/azure-resource-graph-mcp-server\"\u003e\n  \u003cimg alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@hardik-id/azure-resource-graph-mcp-server/badge\" /\u003e\n\u003c/a\u003e\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hardik-id-azure-resource-graph-mcp-server-badge.png)](https://mseep.ai/app/hardik-id-azure-resource-graph-mcp-server)\n\n## Features\n\n- Query Azure resources using Resource Graph queries\n- Default query returns resource ID, name, type, and location\n- Supports custom Resource Graph queries\n- Uses Azure DefaultAzureCredential for authentication\n\n## Prerequisites\n\n- Node.js installed\n- Azure subscription\n- Azure CLI installed and logged in, or other Azure credentials configured\n\n## Running the MCP Server\n\nYou can run the MCP server using either Cursor IDE or Visual Studio Code.\n\n### Option 1: Cursor IDE Integration\n\nTo integrate the MCP server with Cursor IDE:\n\n1. Clone this repository to your local machine (e.g., `C:\\YOUR_WORKSPACE\\azure-resource-graph-mcp-server`)\n2. Build the project:\n```bash \nnpm install\nnpm run build\n```\n3. Open Cursor Settings (JSON) and add the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"azure-resource-graph-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\YOUR_WORKSPACE\\\\azure-resource-graph-mcp-server\\\\build\\\\index.js\"\n      ],\n      \"env\": {\n        \"SUBSCRIPTION_ID\": \"xxxxxx-xx-xx-xx-xxxxxx\"\n      },\n    }\n  }\n}\n```\n\u003e **Note**: Make sure to update the path to match your local repository location.\n\n4. Restart Cursor IDE to apply the changes\n\n### Option 2: VS Code Integration\n\nTo integrate the MCP server with Visual Studio Code:\n\n1. Clone this repository to your local machine\n2. Build the project:\n```bash \nnpm install\nnpm run build\n```\n3. Open VS Code Settings (JSON) by pressing `Ctrl+Shift+P`, type \"Settings (JSON)\" and select \"Preferences: Open User Settings (JSON)\"\n4. Add the following configuration:\n```json\n{\n    \"mcp\": {\n        \"servers\": {\n            \"azure-resource-graph\": {\n                \"type\": \"stdio\",\n                \"command\": \"node\",\n                \"args\": [\n                    \"C:\\\\YOUR_WORKSPACE\\\\azure-resource-graph-mcp-server\\\\build\\\\index.js\"\n                ],\n                \"env\": {\n                  \"SUBSCRIPTION_ID\": \"xxxxxx-xx-xx-xx-xxxxxx\"\n                },\n            }\n        }\n    }\n}\n```\n\u003e **Note**: Make sure to update the path to match your local repository location.\n\n5. Save the settings.json file\n6. Restart VS Code to apply the changes\n\nThe MCP server will now be available to use within VS Code with cursor integration.\n\n## Usage\n\nThe server provides the following tool:\n\n### query-resources\n\nRetrieves resources and their details from Azure Resource Graph.\n\nParameters:\n- `subscriptionId` (optional): Azure subscription ID (defaults to configured ID)\n- `query` (optional): Custom Resource Graph query (defaults to \"Resources | project id, name, type, location\")\n\n## Environment Setup\n\n1. First, make sure you're logged in to Azure CLI by running:\n   ```bash\n   az login\n   ```\n   This step is crucial for local development as the DefaultAzureCredential will automatically use your Azure CLI credentials.\n\n2. Set up your environment variables:\n   - Copy `.env.example` to `.env`\n   - Update `AZURE_SUBSCRIPTION_ID` in `.env` with your actual subscription ID\n   - Other variables (`AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`) are optional when using Azure CLI authentication\n\n3. Make sure you have proper Azure credentials configured. The server uses DefaultAzureCredential which supports:\n   - Azure CLI\n   - Managed Identity\n   - Visual Studio Code credentials\n   - Environment variables\n\n4. If using environment variables, set up:\n   - AZURE_SUBSCRIPTION_ID\n   - AZURE_TENANT_ID\n   - AZURE_CLIENT_ID\n   - AZURE_CLIENT_SECRET\n\n## Error Handling\n\nThe server includes robust error handling for:\n- Azure client initialization failures\n- Query execution errors\n- Invalid queries or parameters\n\n## Development\n\nTo work on this project:\n\n1. Make changes in the `src` directory\n2. Build using `npm run build`\n3. Test your changes by running the server\n\n## License\nThis project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.",
      "stars": 12,
      "updated_at": "2025-09-28T23:08:28Z",
      "url": "https://github.com/hardik-id/azure-resource-graph-mcp-server"
    },
    "jdubois--azure-cli-mcp": {
      "category": "cloud-platforms",
      "description": "An abstraction layer built atop the Azure Command Line Interface, enabling direct interaction with Azure services via the Model Context Protocol (MCP).",
      "forks": 17,
      "imageUrl": "",
      "keywords": [
        "azure",
        "cloud",
        "platform",
        "azure cli",
        "jdubois azure",
        "cloud platforms"
      ],
      "language": "Java",
      "license": "MIT License",
      "name": "azure-cli-tooling-wrapper",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "jdubois",
      "readme_content": "# Azure Command Line Interface Abstraction Layer (MCP Server)\n\nThis MCP Server component functions as a sophisticated proxy for the standard [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/). It integrates the native CLI capabilities within the Model Context Protocol framework, enhancing usability through an interactive prompt structure.\n\n\u003e [!IMPORTANT]\n\u003e Microsoft maintains an officially sanctioned Azure MCP implementation, which leverages the codebase originating from this project. For production environments or enterprise use, the official offering is strongly advised due to dedicated maintenance. This community version might be preferred only in scenarios demanding minimal resource footprint (e.g., optimized packaging via GraalVM) or for developers seeking granular customization of the prompt generation logic. Further details on the official server are available in its [Quick Start Guide](https://learn.microsoft.com/en-us/azure/developer/azure-mcp-server/get-started) or its [source repository](https://github.com/Azure/azure-mcp).\n\n## Demonstration Media\n\n### Brief Overview (2 Minutes) utilizing Claude Desktop\n\n[![Short Tutorial Video](https://img.youtube.com/vi/y_OexCcfhW0/0.jpg)](https://www.youtube.com/watch?v=y_OexCcfhW0)\n\n### Comprehensive Walkthrough (18 Minutes) in VS Code\n\n[![Extended Tutorial Video](https://img.youtube.com/vi/NZxTr32A9lY/0.jpg)](https://www.youtube.com/watch?v=NZxTr32A9lY)\n\n## Operational Capabilities\n\nSince this server exposes the entirety of the Azure CLI's functionality, its scope of action is comprehensive across Azure. Examples include:\n\n*   **Resource Inspection and Auditing:** Retrieving details on existing infrastructure, such as querying the throughput limits imposed on an Azure OpenAI model deployment.\n*   **Security and Remediation:** Applying necessary hardening measures, like securing a misconfigured Azure Blob Storage instance.\n*   **Infrastructure Provisioning:** Orchestrating the deployment of new resources, for instance, establishing an Azure Container Apps service, provisioning an associated Azure Container Registry, and linking them securely via Managed Identity.\n\n## Security and Trust Assessment\n\nAs the command generation is mediated by a Large Language Model (LLM), rigorous validation of all suggested `az` commands is prudent. Our empirical testing indicates superior reliability when coupled with advanced models such as Claude 4 or GPT-4o, given their extensive training on Azure documentation. \n\nPlease refer to the accompanying [License](LICENSE) file; it explicitly states the software is offered \"AS IS,\" meaning usage is undertaken at the user's sole discretion and risk.\n\n## Remote Accessibility and Security Posture\n\nCrucial Answer: **ABSOLUTELY NOT**. \n\nThis server executes arbitrary `az` operations on your behalf, creating a significant security vulnerability if compromised, allowing an attacker to run arbitrary system commands. It is strictly intended for local deployment on a personal workstation, operating with your already established Azure CLI session credentials.\n\nThis component utilizes unsecured `http` transport and relies on Azure token authentication, which permits remote access. Exposing this endpoint over the public internet is strongly discouraged as it would grant external entities control over your cloud assets.\n\n## Deployment Procedures\n\n*This service can be initiated either via a Docker container or as a standalone Java runtime executable (JAR file).*\n\nThe default operational port is `8085`. This port can be modified using standard Spring Boot configuration mechanisms (e.g., setting the `SERVER_PORT=8085` environment variable).\n\n### Containerized Deployment (Docker)\n\nPrerequisite: Establish an Azure Service Principal and ensure the resulting credentials are set in the `AZURE_CREDENTIALS` environment variable. This is achieved using:\n\nbash\naz ad sp create-for-rbac --name \"azure-cli-mcp\" --role contributor --scopes /subscriptions/\u003cyour-subscription-id\u003e/resourceGroups/\u003cyour-resource-group\u003e --json-auth\n\n\nThis command generates a Service Principal with the designated permissions, outputting its configuration details in JSON format. Subsequently, launch the server within Docker, ensuring the `AZURE_CREDENTIALS` variable is populated with the preceding output:\n\nbash\ndocker run --rm -p 8085:8085 -e AZURE_CREDENTIALS=\"{\\\"clientId\\\":\\\"....\\\",\\\"clientSecret\\\":\\\"....\\\",...}\" -it ghcr.io/jdubois/azure-cli-mcp:latest\n\n\n### Native Java Execution\n\nThis mode facilitates local execution, generally simpler to configure than Docker, though slightly less isolated as it directly leverages the host machine's existing Azure CLI authentication context.\n\n1.  Install the Azure CLI tools; consult the [official installation guide](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) if necessary.\n2.  Authenticate against your Azure account by running `az login` in your terminal.\n3.  Verify that Java Runtime Environment version 17 or higher is installed (`java -version`).\n\nBinaries are published to the [GitHub Releases page](https://github.com/jdubois/azure-cli-mcp/releases). Use the GitHub CLI to fetch the latest artifact:\n\n-   Fetch the newest release artifact: `gh release download --repo jdubois/azure-cli-mcp --pattern='azure-cli-mcp.jar'`\n\nThis MCP server is structured as a Spring Boot application, allowing execution via standard Maven commands, for example:\n\nbash\nmvn spring-boot:run\n\n\n## Integration within VS Code\n\nTo utilize this server within the VS Code environment:\n\n1.  Ensure the GitHub Copilot extension is installed.\n2.  Invoke the Command Palette and select `MCP: Add Server...` to register this service.\n    -   The configuration defaults to the `http` transport protocol.\n    -   The server must be actively running as detailed in the installation sections above.\n3.  Set GitHub Copilot operational mode to `Agent` by clicking the designated selector at the bottom of the chat interface.\n4.  The `azure-cli-mcp` server configuration should then appear as an available tool above the Copilot chat window.\n\nExample configuration snippet for VS Code settings:\n\n\n{\n  \"servers\": {\n    \"azure-cli-http\": {\n      \"url\": \"http://localhost:8085/mcp\",\n      \"type\": \"http\"\n    }\n  },\n  \"inputs\": []\n}\n",
      "stars": 81,
      "updated_at": "2025-09-19T00:58:31Z",
      "url": "https://github.com/jdubois/azure-cli-mcp"
    },
    "johnneerdael--netskope-mcp": {
      "category": "cloud-platforms",
      "description": "A comprehensive Model Context Protocol (MCP) service designed to furnish complete programmatic access to every facet of the Netskope Private Access (NPA) environment, including exhaustive configuration blueprints, operational procedures, and illustrative large language model interaction examples.",
      "forks": 5,
      "imageUrl": "",
      "keywords": [
        "netskope",
        "cloud",
        "platform",
        "platforms cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "TypeScript",
      "license": "No License",
      "name": "ns-npa-orchestrator-mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "johnneerdael",
      "readme_content": "# Netskope NPA Automation \u0026 Orchestration Hub\n\nThis specialized Model Context Protocol (MCP) gateway provides unified, AI-driven control over the entire Netskope Private Access infrastructure stack.\n\n## üìö Comprehensive Repository Guide\n\nNavigate the extensive documentation provided for rapid deployment and expert usage:\n\n**‚û°Ô∏è Access the Full Documentation Suite Here](./docs/README.md)** - The central starting point and navigation index.\n\n### Essential Reference Pointers\n\n| Domain | Summary | Location Reference |\n|:---|:---|:---|\n| **System Blueprint** | Detailed server topology and design paradigms | [Architectural Specification](./docs/architecture/server-architecture.md) |\n| **Utility Catalog** | Definitive documentation for all available functions | [Publisher Utilities](./docs/tools/publisher-tools.md), [Private Application Functions](./docs/tools/private-app-tools.md), [Enforcement Policy Utilities](./docs/tools/policy-tools.md) |\n| **Automation Patterns** | Curated collection of repeatable operational sequences | [Standardized Workflows](./docs/workflows/common-workflows.md) |\n| **Practical Scenarios** | Real-world deployments and solved problems | [Verified Use Cases](./docs/examples/real-world-examples.md) |\n\n## Core Tool Capabilities Summary\n\nThe Orchestrator exposes **84 fine-grained instruments** categorized across 10 functional domains:\n\n| Area | Count | Key Responsibilities |\n|:---|:---|:---|\n| **Publisher Management** | 9 | Provisioning and lifecycle management of infrastructure gateways |\n| **Application Definition** | 15 | Configuration, modification, and management of accessible private resources |\n| **Local Brokerage** | 7 | Handling client-side connectivity, routing integrity, and tunnel establishment |\n| **Access Policies** | 6 | Defining and enforcing granular security postures and user entitlements |\n| **Identity Provisioning (SCIM)** | 5 | Synchronization and management of identity sources and user groups |\n| **Maintenance Profiles** | 7 | Orchestrating system updates, patching, and version control automation |\n| **Traffic Steering** | 3 | Controlling ingress/egress routing directives and service associations |\n| **Event Notification** | 2 | Monitoring system telemetry and generating actionable alerts |\n| **Data Discovery** | 2 | Querying the environment for existing assets and configuration states |\n| **Integrity Checks** | 2 | Automated verification of configuration adherence and compliance status |\n\n## Illustrative Operational Scenarios\n\n### üè¢ Scenario: Deploying NPA Foundation for the New EMEA Regional Hub (London)\n**LLM Generated Action Sequence**: Executes the end-to-end regional setup playbook.\n- Infrastructure provisioning: Deploy gateway and assign continuous upgrade profile.\n- Network establishment: Configure necessary local broker instances for regional traffic isolation.\n- Service mapping: Onboard critical corporate assets (e.g., SAP, SharePoint endpoints).\n- Security enforcement: Establish access control lists referencing synchronized identity groups.\n- Visibility: Activate comprehensive monitoring and network resource indexing.\n- Token issuance: Generate secure registration credentials for local device bootstrapping.\n\n### üö® Scenario: Immediate Security Lockdown for Sensitive Financial Systems\n**LLM Generated Action Sequence**: Initiates emergency containment protocol.\n- Asset identification: Automatically locate all resources tagged as HR or Finance.\n- Policy imposition: Institute a temporary, highest-precedence block rule across targeted applications.\n- Access restriction: Revoke access globally, exempting only designated incident response teams.\n- Enhanced telemetry: Intensify logging and alerting thresholds for related security events.\n- Remediation tracking: Apply metadata tags for subsequent forensic analysis.\n\n### üìä Scenario: Automated Regulatory Posture Assessment of the Entire NPA Deployment\n**LLM Generated Action Sequence**: Executes the full compliance validation routine.\n- Version audit: Verify all deployed publishers meet mandated software levels.\n- Gap analysis: Identify any registered applications lacking associated access mandates.\n- Dependency check: Validate that all policies correctly reference active SCIM identities.\n- Reporting: Calculate compliance metrics and devise a prioritized remediation roadmap.\n- Documentation: Produce a formal findings report suitable for governance review.\n\n## Initial Setup Protocol\n\n1. **Credential Configuration**\n   ```bash\n   export NETSKOPE_TENANT_URL=\"https://your-corp.goskope.com\"\n   export NETSKOPE_API_KEY=\"your-secure-key-here\"\n   ```\n\n2. **Installation and Runtime Compilation**\n   ```bash\n   npm install\n   npm run compile\n   npm start\n   ```\n\n3. **Integration with MCP Host Client**\n   ```json\n   {\n     \"mcpServers\": {\n       \"netskope-npa\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/npa-orchestrator/dist/main.js\"],\n         \"env\": {\n           \"NETSKOPE_TENANT_URL\": \"https://your-corp.goskope.com\",\n           \"NETSKOPE_API_KEY\": \"your-secure-key-here\"\n         }\n       }\n     }\n   }\n   ```\n\n## Distinguished Characteristics\n\n### ü§ñ LLM-Optimized Interface\n- Function definitions are meticulously described for accurate AI interpretation.\n- Automatic inference and conversion of input parameters.\n- Provision of rich diagnostic context upon execution failure.\n\n### üåê Transactional Workflow Engine\n- Functions inherently coordinate sequencing for complex operations.\n- Integrated mechanisms for automated retry with adaptive backoff.\n- Support for atomic transactions where state consistency is paramount.\n\n### ‚úÖ Enterprise-Grade Reliability\n- Strict runtime validation enforced via detailed schema definitions (Zod).\n- Native handling of API rate limits and quota constraints.\n- Comprehensive logging infrastructure for observability and auditing.\n\n### üß© Interoperability Frameworks\n- Seamless interfaces for identity federation via SCIM standards.\n- Utility functions dedicated to dynamic resource mapping and querying.\n- Built-in checkers to ensure configuration adherence to standards.\n\n## Deployment Options\n\n### Via NPM Registry\n```bash\nnpm install @ns-mcp/npa-orchestrator\n```\n\n### Local Source Control\n```bash\ngit clone https://github.com/vendor/ns-npa-orchestrator.git\ncd ns-npa-orchestrator\nnpm install\nnpm run compile\n```\n\n## Internal Architecture Insights\n\n### Utility Composition\nModules interact through clearly defined interfaces, ensuring operational atomicity:\n\n```typescript\n// Example: Establishing a new protected application endpoint\n1. checkAppNameValidity() -\u003e Enforce naming convention compliance\n2. locateTargetPublisher() -\u003e Identify required gateway infrastructure\n3. provisionNewApplication() -\u003e Register the service entity\n4. affixResourceTags() -\u003e Apply organizational metadata\n5. bindToGateway() -\u003e Finalize association mapping\n```\n\n### Schema-Driven Type Safety\nData integrity is guaranteed by leveraging precise Zod definitions for every endpoint interaction:\n\n```typescript\nconst provisionAppSchema = z.object({\n  friendly_name: z.string().min(3).max(100),\n  FQDN_or_IP: z.string().ip().or(z.string().url()),\n  access_protocols: z.array(protocolSchemaDefinition),\n  supports_clientless: z.boolean()\n});\n```\n\n### Resiliency Features\nMechanisms engineered to absorb operational variances:\n- Contextual parameter injection derived directly from the MCP state model.\n- Automated retry logic employing exponential decay pacing.\n- Graceful fallback procedures for non-critical operational deviations.\n\n## Contributors Acknowledgment\n\n- **[NPA Architect Lead]** (Primary Development)\n- **[Solutions Engineering Team]** (Operational Validation \u0026 Examples)\n\n## Support Channels\n\n- **Documentation Discrepancies**: File an issue via the repository tracker.\n- **Feature Enhancement Requests**: Submit a formal request detailing the requirement.\n- **Software Defects**: Utilize the standardized bug reporting template.\n- **Security Vulnerabilities**: Consult the dedicated [SECURITY.md](./docs/SECURITY.md) guidelines.\n\n---\n\n*This specialized Orchestrator abstracts complex Netskope NPA administration into intuitive, LLM-driven strategic commands.*",
      "stars": 3,
      "updated_at": "2025-09-03T22:13:17Z",
      "url": "https://github.com/johnneerdael/netskope-mcp"
    },
    "kestra-io--mcp-server-python": {
      "category": "cloud-platforms",
      "description": "A server implementation for Kestra's Metadata \u0026 Control Plane (MCP) utilizing Python, designed for integration with workflow orchestration capabilities.",
      "forks": 1,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "platform",
        "platforms",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "kestra-mcp-py-gateway",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "kestra-io",
      "readme_content": "## Kestra Python MCP Endpoint\n\nThis gateway component facilitates interaction with the Kestra orchestration engine via the MCP protocol. Deployment is often streamlined using containerization.\n\n### Docker Deployment Strategy\n\nRunning the gateway within a Docker container is recommended to isolate environments and manage dependencies effortlessly.\n\n#### Kestra AI Agent Integration\n\nRefer to `docs/flows/kestra_mcp_docker.yaml` for examples concerning Kestra AI Agent usage.\n\n#### Configuration for Open Source (OSS) Users\n\nIncorporate the subsequent JSON structure into your MCP configuration settings (e.g., within Cursor, Claude, or VS Code):\n\n\n{\n  \"mcpServers\": {\n    \"kestra\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--pull\",\n        \"always\",\n        \"-e\",\n        \"KESTRA_BASE_URL\",\n        \"-e\",\n        \"KESTRA_TENANT_ID\",\n        \"-e\",\n        \"KESTRA_MCP_DISABLED_TOOLS\",\n        \"-e\",\n        \"KESTRA_MCP_LOG_LEVEL\",\n        \"-e\",\n        \"KESTRA_USERNAME\",\n        \"-e\",\n        \"KESTRA_PASSWORD\",\n        \"ghcr.io/kestra-io/mcp-server-python:latest\"\n      ],\n      \"env\": {\n        \"KESTRA_BASE_URL\": \"http://host.docker.internal:8080/api/v1\",\n        \"KESTRA_TENANT_ID\": \"main\",\n        \"KESTRA_MCP_DISABLED_TOOLS\": \"ee\",\n        \"KESTRA_MCP_LOG_LEVEL\": \"ERROR\",\n        \"KESTRA_USERNAME\": \"admin@kestra.io\",\n        \"KESTRA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n\n\n#### Configuration for Enterprise Edition (EE) Users\n\n\n{\n  \"mcpServers\": {\n    \"kestra\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--pull\",\n        \"always\",\n        \"-e\", \"KESTRA_BASE_URL\",\n        \"-e\", \"KESTRA_API_TOKEN\",\n        \"-e\", \"KESTRA_TENANT_ID\",\n        \"-e\", \"KESTRA_MCP_LOG_LEVEL\",\n        \"ghcr.io/kestra-io/mcp-server-python:latest\"\n      ],\n      \"env\": {\n        \"KESTRA_BASE_URL\": \"http://host.docker.internal:8080/api/v1\",\n        \"KESTRA_API_TOKEN\": \"\u003cyour_kestra_api_token\u003e\",\n        \"KESTRA_TENANT_ID\": \"main\",\n        \"KESTRA_MCP_LOG_LEVEL\": \"ERROR\"\n      }\n    }\n  }\n}\n\n\n#### Comprehensive Docker Setup\n\n\n{\n  \"mcpServers\": {\n    \"kestra\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--pull\",\n        \"always\",\n        \"-e\", \"KESTRA_BASE_URL\",\n        \"-e\", \"KESTRA_API_TOKEN\",\n        \"-e\", \"KESTRA_TENANT_ID\",\n        \"-e\", \"KESTRA_USERNAME\",\n        \"-e\", \"KESTRA_PASSWORD\",\n        \"-e\", \"KESTRA_MCP_DISABLED_TOOLS\",\n        \"-e\", \"KESTRA_MCP_LOG_LEVEL\",\n        \"ghcr.io/kestra-io/mcp-server-python:latest\"\n      ],\n      \"env\": {\n        \"KESTRA_BASE_URL\": \"http://host.docker.internal:8080/api/v1\",\n        \"KESTRA_API_TOKEN\": \"\u003cyour_kestra_api_token\u003e\",\n        \"KESTRA_TENANT_ID\": \"main\",\n        \"KESTRA_USERNAME\": \"admin\",\n        \"KESTRA_PASSWORD\": \"admin\",\n        \"KESTRA_MCP_DISABLED_TOOLS\": \"ee\",\n        \"KESTRA_MCP_LOG_LEVEL\": \"ERROR\"\n      }\n    }\n  }\n}\n\n\n**Configuration Notes:**\n- Substitute placeholders like `\u003cyour_kestra_api_token\u003e` with active credentials. Note that OSS setups favor `KESTRA_USERNAME`/`KESTRA_PASSWORD` over `KESTRA_API_TOKEN`.\n- To suppress Enterprise Edition tools when running OSS, specify `KESTRA_MCP_DISABLED_TOOLS=ee`.\n- The `host.docker.internal` DNS resolves to the host machine's network interface, enabling container-to-host communication (e.g., reaching Kestra on port 8080) on standard Docker desktop environments (macOS/Windows). Linux users might require network bridge configuration.\n- Environment variables are injected into the Docker runtime via the `-e` arguments.\n\n### Tool Manifest\n\nThis gateway exposes the following operational tools:\n\n- üîÑ backfill\n- ‚öôÔ∏è ee (Enterprise functionality)\n- ‚ñ∂Ô∏è execution\n- üìÅ files\n- üîÄ flow\n- üóùÔ∏è kv\n- üåê namespace\n- üîÅ replay\n- ‚ôªÔ∏è restart\n- ‚è∏Ô∏è resume\n\n**Note on EE Tools:** The `ee` toolset is restricted to EE/Cloud subscriptions. OSS users should either omit credentials that grant EE access or explicitly disable these tools via `KESTRA_MCP_DISABLED_TOOLS=ee` in their configuration.\n\nOptionally, you may maintain an environment file (e.g., `.env`) to specify tools you wish to exclude globally. For instance, to disable file operations:\n\ndotenv\nKESTRA_MCP_DISABLED_TOOLS=files\n\n\nMultiple tool exclusions are comma-separated:\n\ndotenv\nKESTRA_MCP_DISABLED_TOOLS=ee,kv\n\n\n### Verbosity Control\n\nThe logging threshold defaults to `ERROR` to minimize operational chatter. Adjust verbosity using `KESTRA_MCP_LOG_LEVEL`:\n\ndotenv\n# Default: Only critical errors reported\nKESTRA_MCP_LOG_LEVEL=ERROR\n\n# Include warnings\nKESTRA_MCP_LOG_LEVEL=WARNING\n\n# Include informational messages\nKESTRA_MCP_LOG_LEVEL=INFO\n\n# Full diagnostic output\nKESTRA_MCP_LOG_LEVEL=DEBUG\n\n\nWhen deployed via Docker, this variable must be set within the MCP configuration block:\n\n\n{\n  \"mcpServers\": {\n    \"kestra\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"--pull\",\n        \"always\",\n        \"-e\", \"KESTRA_BASE_URL\",\n        \"-e\", \"KESTRA_MCP_LOG_LEVEL\",\n        \"ghcr.io/kestra-io/mcp-server-python:latest\"\n      ],\n      \"env\": {\n        \"KESTRA_BASE_URL\": \"http://host.docker.internal:8080/api/v1\",\n        \"KESTRA_MCP_LOG_LEVEL\": \"ERROR\"\n      }\n    }\n  }\n}\n\n\n--- \n\n### Local Development Setup\n\nTo facilitate direct modification and debugging, you can execute the server outside of a container. Ensure a Python virtual environment is established first:\n\nbash\nuv venv --python 3.13\nuv pip install -r requirements.txt\n\n\nEstablish an environment configuration file (e.g., `.env`) mirroring the structure of `.env_example`. Authentication preference is dictated by your Kestra setup: use `KESTRA_API_TOKEN` for EE/Cloud, or `KESTRA_USERNAME`/`KESTRA_PASSWORD` for OSS. Remember to include `KESTRA_MCP_DISABLED_TOOLS=ee` in the `.env` file if running OSS and wishing to restrict EE tools.\n\nFollowing setup, proceed to test the locally hosted endpoint within your preferred AI client (Cursor, Windsurf, VS Code, or Claude Desktop).\n\n--- \n\n### Client Integration (Cursor, Windsurf, VS Code, or Claude Desktop)\n\nTo enable Python MCP interaction with compatible AI environments, first ascertain the path to your `uv` executable:\n\nbash\nwhich uv\n\n\nUse the returned path in the `command` section of your configuration. Critically, substitute the placeholder `--directory` argument with the absolute location of the cloned Kestra MCP Server repository source directory (e.g., `/Users/annageller/gh/mcp-server-python/src`). Example configuration:\n\n\n{\n  \"mcpServers\": {\n    \"kestra\": {\n      \"command\": \"/Users/annageller/.local/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/annageller/gh/mcp-server-python/src\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n\n\nThis configuration snippet is suitable for direct insertion into Cursor or Claude Developer settings.\n\n#### VS Code Specific Configuration\n\nWithin your project root, create the directory structure `.vscode/` and place the configuration within a file named `mcp.json`. Note that VS Code uses the top-level key `servers` instead of `mcpServers`:\n\n\n{\n  \"servers\": {\n    \"kestra\": {\n      \"command\": \"/Users/annageller/.local/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/annageller/gh/mcp-server-python/src\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n\n\nA dedicated 'Start' button should materialize; activate it to initialize the server.\n\nSubsequently, navigate to the Agent mode within the GitHub Copilot interface to begin tool interaction. An exemplary prompt: \"Retrieve all workflows defined in the 'tutorial' tenant space.\"\n\nExecution will yield the command output directly in the designated output panel.\n\n### Frequently Asked Questions\n\n**Q: Is continuous manual server operation necessary?**\n\nA: Negative. When employing the standard `stdio` transport mechanism, the AI client (Cursor, Windsurf, VS Code, Claude Desktop) automatically manages the MCP server as a child process. Communication‚ÄîJSON-RPC messages‚Äîis exchanged via standard input (server receives) and standard output (server transmits).\n\n**Q: Must the Python virtual environment be manually activated?**\n\nA: No, this is circumvented by leveraging `uv`. Unlike conventional shell activation which modifies environment variables like `PATH`, `uv` directly targets the Python interpreter and dependencies housed within the `.venv` directory. Ensure the environment is provisioned via `uv venv` and dependencies installed with `uv pip install` as previously detailed.",
      "stars": 13,
      "updated_at": "2025-10-02T22:59:38Z",
      "url": "https://github.com/kestra-io/mcp-server-python"
    },
    "liveblocks--liveblocks-mcp-server": {
      "category": "cloud-platforms",
      "description": "Create, modify, and delete different aspects of [Liveblocks](https://liveblocks.io) such as rooms, threads, comments, notifications, and more. Additionally, it has read access to Storage and Yjs.",
      "forks": 7,
      "imageUrl": "",
      "keywords": [
        "liveblocks",
        "cloud",
        "platform",
        "platforms cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "TypeScript",
      "license": "Apache License 2.0",
      "name": "liveblocks-mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "liveblocks",
      "readme_content": "\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://liveblocks.io#gh-light-mode-only\"\u003e\n    \u003cimg src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-light.svg\" alt=\"Liveblocks\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://liveblocks.io#gh-dark-mode-only\"\u003e\n    \u003cimg src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-dark.svg\" alt=\"Liveblocks\" /\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\n# `liveblocks-mcp-server`\n\n[![smithery badge](https://smithery.ai/badge/@liveblocks/liveblocks-mcp-server)](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server)\n\nThis MCP server allows AI to use a number of functions from our [REST API](https://liveblocks.io/docs/api-reference/rest-api-endpoints). For example, it can create, modify, and delete different aspects of Liveblocks such as rooms, threads, comments, notifications, and more. It also has read access to Storage and Yjs. [Learn more in our docs](https://liveblocks.io/docs/tools/mcp-server).\n\n## Automatic setup\n\nTo install automatically, copy your Liveblocks secret key from a project in [your dashboard](https://liveblocks.io/dashboard) and run one of the following commands, replacing `[key]` with your secret key.\n\n### Cursor\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client cursor --key [key]\n```\n\n### Claude Desktop\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client claude --key [key]\n```\n\n### VS Code\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client vscode --key [key]\n```\n\n### Other clients\n\nFind installation information for other clients on [Smithery](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server).\n\n## Manual setup\n\n\u003cdetails\u003e\u003csummary\u003eRead more\u003c/summary\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\n1. Clone this repo.\n\n```bash\ngit clone https://github.com/liveblocks/liveblocks-mcp-server.git\n```\n\n2. Build the project.\n\n```bash\nnpm install\nnpm run build\n```\n\n3. Get your Liveblocks secret key from the [dashboard](https://liveblocks.io/dashboard).\n\n```\nsk_dev_Ns35f5G...\n```\n\n### Cursor\n\n4. Go to File ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new server.\n\n5. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n6. Check it's enabled in the MCP menu.\n\n### Claude Desktop\n\n4. Go to File ‚Üí Settings ‚Üí Developer ‚Üí Edit Config.\n\n5. Open the JSON file, `claude_desktop_config.json`.\n\n6. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n",
      "stars": 11,
      "updated_at": "2025-08-24T01:49:07Z",
      "url": "https://github.com/liveblocks/liveblocks-mcp-server"
    },
    "manusa--Kubernetes-MCP-Server": {
      "category": "cloud-platforms",
      "description": "powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for **any** Kubernetes resource, this server provides specialized tools to interact with your cluster.",
      "forks": 0,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "platforms",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Unknown",
      "license": "Unknown",
      "name": "Kubernetes-MCP-Server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "manusa",
      "readme_content": "",
      "stars": 0,
      "updated_at": "",
      "url": "https://github.com/manusa/kubernetes-mcp-server"
    },
    "nwiizo--tfmcp": {
      "category": "cloud-platforms",
      "description": "ü¶Ä üè† - A Terraform MCP server allowing AI assistants to manage and operate Terraform environments, enabling reading configurations, analyzing plans, applying configurations, and managing Terraform state.",
      "forks": 22,
      "imageUrl": "",
      "keywords": [
        "terraform",
        "cloud",
        "tfmcp",
        "tfmcp terraform",
        "terraform mcp",
        "cloud platforms"
      ],
      "language": "Rust",
      "license": "MIT License",
      "name": "tfmcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "nwiizo",
      "readme_content": "# tfmcp: Terraform Model Context Protocol Tool\n\n*‚ö†Ô∏è  This project includes production-ready security features but is still under active development. While the security system provides robust protection, please review all operations carefully in production environments. ‚ö†Ô∏è*\n\ntfmcp is a command-line tool that helps you interact with Terraform via the Model Context Protocol (MCP). It allows LLMs to manage and operate your Terraform environments, including:\n\n## üéÆ Demo\n\nSee tfmcp in action with Claude Desktop:\n\n\n\n- Reading Terraform configuration files\n- Analyzing Terraform plan outputs\n- Applying Terraform configurations\n- Managing Terraform state\n- Creating and modifying Terraform configurations\n\n## üéâ Latest Release\n\nThe latest version of tfmcp (v0.1.3) is now available on Crates.io! You can easily install it using Cargo:\n\n```bash\ncargo install tfmcp\n```\n\n### üÜï What's New in v0.1.3\n- **üîê Comprehensive Security System**: Production-ready security controls with audit logging\n- **üìä Enhanced Terraform Analysis**: Detailed validation and best practice recommendations  \n- **üõ°Ô∏è Access Controls**: File pattern-based restrictions and resource limits\n- **üìù Audit Logging**: Complete operation tracking for compliance and monitoring\n\n## Features\n\n- üöÄ **Terraform Integration**  \n  Deeply integrates with the Terraform CLI to analyze and execute operations.\n\n- üìÑ **MCP Server Capabilities**  \n  Runs as a Model Context Protocol server, allowing AI assistants to access and manage Terraform.\n\n- üîê **Enterprise Security**  \n  Production-ready security controls with configurable policies, audit logging, and access restrictions.\n\n- üìä **Advanced Analysis**  \n  Detailed Terraform configuration analysis with best practice recommendations and security checks.\n\n- ‚ö°Ô∏è **Blazing Fast**  \n  High-speed processing powered by the Rust ecosystem with optimized parsing and caching.\n\n- üõ†Ô∏è **Automatic Setup**  \n  Automatically creates sample Terraform projects when needed, ensuring smooth operation even for new users.\n\n- üê≥ **Docker Support**  \n  Run tfmcp in a containerized environment with all dependencies pre-installed.\n\n## Installation\n\n### From Source\n```bash\n# Clone the repository\ngit clone https://github.com/nwiizo/tfmcp\ncd tfmcp\n\n# Build and install\ncargo install --path .\n```\n\n### From Crates.io\n```bash\ncargo install tfmcp\n```\n\n### Using Docker\n```bash\n# Clone the repository\ngit clone https://github.com/nwiizo/tfmcp\ncd tfmcp\n\n# Build the Docker image\ndocker build -t tfmcp .\n\n# Run the container\ndocker run -it tfmcp\n```\n\n## Requirements\n\n- Rust (edition 2021)\n- Terraform CLI installed and available in PATH\n- Claude Desktop (for AI assistant integration)\n- Docker (optional, for containerized deployment)\n\n## Usage\n\n```bash\n$ tfmcp --help\n‚ú® A CLI tool to manage Terraform configurations and operate Terraform through the Model Context Protocol (MCP).\n\nUsage: tfmcp [OPTIONS] [COMMAND]\n\nCommands:\n  mcp       Launch tfmcp as an MCP server\n  analyze   Analyze Terraform configurations\n  help      Print this message or the help of the given subcommand(s)\n\nOptions:\n  -c, --config \u003cPATH\u003e    Path to the configuration file\n  -d, --dir \u003cPATH\u003e       Terraform project directory\n  -V, --version          Print version\n  -h, --help             Print help\n```\n\n### Using Docker\n\nWhen using Docker, you can run tfmcp commands like this:\n\n```bash\n# Run as MCP server (default)\ndocker run -it tfmcp\n\n# Run with specific command and options\ndocker run -it tfmcp analyze --dir /app/example\n\n# Mount your Terraform project directory\ndocker run -it -v /path/to/your/terraform:/app/terraform tfmcp --dir /app/terraform\n\n# Set environment variables\ndocker run -it -e TFMCP_LOG_LEVEL=debug tfmcp\n```\n\n### Integrating with Claude Desktop\n\nTo use tfmcp with Claude Desktop:\n\n1. If you haven't already, install tfmcp:\n   ```bash\n   cargo install tfmcp\n   ```\n\n   Alternatively, you can use Docker:\n   ```bash\n   docker build -t tfmcp .\n   ```\n\n2. Find the path to your installed tfmcp executable:\n   ```bash\n   which tfmcp\n   ```\n\n3. Add the following configuration to `~/Library/Application\\ Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"tfmcp\": {\n      \"command\": \"/path/to/your/tfmcp\",  // Replace with the actual path from step 2\n      \"args\": [\"mcp\"],\n      \"env\": {\n        \"HOME\": \"/Users/yourusername\",  // Replace with your username\n        \"PATH\": \"/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\",\n        \"TERRAFORM_DIR\": \"/path/to/your/terraform/project\"  // Optional: specify your Terraform project\n      }\n    }\n  }\n}\n```\n\nIf you're using Docker with Claude Desktop, you can set up the configuration like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"tfmcp\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"--rm\", \"-v\", \"/path/to/your/terraform:/app/terraform\", \"tfmcp\", \"mcp\"],\n      \"env\": {\n        \"TERRAFORM_DIR\": \"/app/terraform\"\n      }\n    }\n  }\n}\n```\n\n4. Restart Claude Desktop and enable the tfmcp tool.\n\n5. tfmcp will automatically create a sample Terraform project in `~/terraform` if one doesn't exist, ensuring Claude can start working with Terraform right away. The sample project is based on the examples included in the `example/demo` directory of this repository.\n\n## Logs and Troubleshooting\n\nThe tfmcp server logs are available at:\n```\n~/Library/Logs/Claude/mcp-server-tfmcp.log\n```\n\nCommon issues and solutions:\n\n- **Claude can't connect to the server**: Make sure the path to the tfmcp executable is correct in your configuration\n- **Terraform project issues**: tfmcp automatically creates a sample Terraform project if none is found\n- **Method not found errors**: MCP protocol support includes resources/list and prompts/list methods\n- **Docker issues**: If using Docker, ensure your container has proper volume mounts and permissions\n\n## Environment Variables\n\n### Core Configuration\n- `TERRAFORM_DIR`: Set this to specify a custom Terraform project directory. If not set, tfmcp will use the directory provided by command line arguments, configuration files, or fall back to `~/terraform`. You can also change the project directory at runtime using the `set_terraform_directory` tool.\n- `TFMCP_LOG_LEVEL`: Set to `debug`, `info`, `warn`, or `error` to control logging verbosity.\n- `TFMCP_DEMO_MODE`: Set to `true` to enable demo mode with additional safety features.\n\n### Security Configuration\n- `TFMCP_ALLOW_DANGEROUS_OPS`: Set to `true` to enable apply/destroy operations (default: `false`)\n- `TFMCP_ALLOW_AUTO_APPROVE`: Set to `true` to enable auto-approve for dangerous operations (default: `false`)\n- `TFMCP_MAX_RESOURCES`: Set maximum number of resources that can be managed (default: 50)\n- `TFMCP_AUDIT_ENABLED`: Set to `false` to disable audit logging (default: `true`)\n- `TFMCP_AUDIT_LOG_FILE`: Custom path for audit log file (default: `~/.tfmcp/audit.log`)\n- `TFMCP_AUDIT_LOG_SENSITIVE`: Set to `true` to include sensitive information in audit logs (default: `false`)\n\n## Security Considerations\n\ntfmcp includes comprehensive security features designed for production use:\n\n### üîí Built-in Security Features\n- **Access Controls**: Automatic blocking of production/sensitive file patterns\n- **Operation Restrictions**: Dangerous operations (apply/destroy) disabled by default\n- **Resource Limits**: Configurable maximum resource count protection\n- **Audit Logging**: Complete operation tracking with timestamps and user identification\n- **Directory Validation**: Security policy enforcement for project directories\n\n### üõ°Ô∏è Security Best Practices\n- **Default Safety**: Apply/destroy operations are disabled by default - explicitly enable only when needed\n- **Review Plans**: Always review Terraform plans before applying, especially AI-generated ones\n- **IAM Boundaries**: Use appropriate IAM permissions and role boundaries in cloud environments\n- **Audit Monitoring**: Regularly review audit logs at `~/.tfmcp/audit.log`\n- **File Patterns**: Built-in protection against accessing `prod*`, `production*`, and `secret*` patterns\n- **Docker Security**: When using containers, carefully consider volume mounts and exposed data\n\n### ‚öôÔ∏è Production Configuration\n```bash\n# Recommended production settings\nexport TFMCP_ALLOW_DANGEROUS_OPS=false    # Keep disabled for safety\nexport TFMCP_ALLOW_AUTO_APPROVE=false     # Require manual approval\nexport TFMCP_MAX_RESOURCES=10             # Limit resource scope\nexport TFMCP_AUDIT_ENABLED=true           # Enable audit logging\nexport TFMCP_AUDIT_LOG_SENSITIVE=false    # Don't log sensitive data\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## Roadmap\n\nHere are some planned improvements and future features for tfmcp:\n\n### Completed\n- [x] **Basic Terraform Integration**  \n  Core integration with Terraform CLI for analyzing and executing operations.\n\n- [x] **MCP Server Implementation**  \n  Initial implementation of the Model Context Protocol server for AI assistants.\n\n- [x] **Automatic Project Creation**  \n  Added functionality to automatically create sample Terraform projects when needed.\n\n- [x] **Claude Desktop Integration**  \n  Support for seamless integration with Claude Desktop.\n\n- [x] **Core MCP Methods**  \n  Implementation of essential MCP methods including resources/list and prompts/list.\n\n- [x] **Error Handling Improvements**  \n  Better error handling and recovery mechanisms for robust operation.\n\n- [x] **Dynamic Project Directory Switching**  \n  Added ability to change the active Terraform project directory without restarting the service.\n\n- [x] **Crates.io Publication**  \n  Published the package to Crates.io for easy installation via Cargo.\n  \n- [x] **Docker Support**  \n  Added containerization support for easier deployment and cross-platform compatibility.\n\n- [x] **Security Enhancements**  \n  Comprehensive security system with configurable policies, audit logging, access controls, and production-ready safety features.\n\n### In Progress\n- [ ] **Enhanced Terraform Analysis**  \n  Implement deeper parsing and analysis of Terraform configurations, plans, and state files.\n\n- [ ] **Comprehensive Testing Framework**  \n  Expand test coverage including integration tests with real Terraform configurations.\n\n### Planned\n- [ ] **Multi-Environment Support**  \n  Add support for managing multiple Terraform environments, workspaces, and modules.\n\n- [ ] **Expanded MCP Protocol Support**  \n  Implement additional MCP methods and capabilities for richer integration with AI assistants.\n\n- [ ] **Performance Optimization**  \n  Optimize resource usage and response times for large Terraform projects.\n\n- [ ] **Cost Estimation**  \n  Integrate with cloud provider pricing APIs to provide cost estimates for Terraform plans.\n\n- [ ] **Interactive TUI**  \n  Develop a terminal-based user interface for easier local usage and debugging.\n\n- [ ] **Integration with Other AI Platforms**  \n  Extend beyond Claude to support other AI assistants and platforms.\n\n- [ ] **Plugin System**  \n  Develop a plugin architecture to allow extensions of core functionality.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "stars": 342,
      "updated_at": "2025-09-28T23:09:30Z",
      "url": "https://github.com/nwiizo/tfmcp"
    },
    "openstack-kr--python-openstackmcp-server": {
      "category": "cloud-platforms",
      "description": "OpenStack MCP server for cloud infrastructure management based on openstacksdk.",
      "forks": 6,
      "imageUrl": "",
      "keywords": [
        "openstackmcp",
        "openstack",
        "cloud",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "python-openstackmcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "openstack-kr",
      "readme_content": "# python-openstackmcp-server\n\nOpenstack mcp server is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro) server that provides an interface for AI assistants to interact with OpenStack services.\n\n```mermaid\nflowchart LR\n    AI[AI Assistant] \u003c--\u003e|MCP Protocol| Server[OpenStack MCP Server]\n    Server \u003c--\u003e|OpenStack SDK| SDK[OpenStack SDK]\n    SDK \u003c--\u003e|REST API| Cloud[OpenStack Cloud]\n```\n\n# Table of Contents\n- [Table of Contents](#table-of-contents)\n- [Features](#features)\n- [Quick Start with Claude Desktop](#quick-start-with-claude-desktop)\n  - [Requirements](#requirements)\n  - [Using python](#using-python)\n  - [Using uvx](#using-uvx)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n# Features\n- **MCP Protocol Support**: Implements the Model Context Protocol for AI assistants.\n- **Compute Tools**: Manage OpenStack compute resources (servers, flavors).\n- **Image Tools**: Manage OpenStack images.\n- **Identity Tools**: Handle OpenStack identity and authentication.\n- **Network Tools**: Manage OpenStack networking resources.\n- **Block Storage Tools**: Manage OpenStack block storage resources.\n\n# Quick Start with Claude Desktop\n\nGet started quickly with the OpenStack MCP server using Claude Desktop\n\n## Requirements\n- Python 3.10 or higher\n- OpenStack credentials configured in `clouds.yaml` file\n- Claude Desktop installed\n\n1. **Create or update your `clouds.yaml` file with your OpenStack credentials.**\n   \n   Example `clouds.yaml`:\n   ```yaml\n   clouds:\n     openstack:\n       auth:\n         auth_url: https://your-openstack-auth-url.com\n         username: your-username\n         password: your-password\n         project_name: your-project-name\n         user_domain_name: Default\n         project_domain_name: Default\n       region_name: your-region\n       interface: public\n       identity_api_version: 3\n   ```\n\n2. **Create or update your Claude Desktop configuration file**:\n   - **macOS**: Edit `$HOME/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: Edit `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - **Linux**: Edit `$HOME/.config/Claude/claude_desktop_config.json`\n   \n### Using python\n \n   ```json\n   {\n     \"mcpServers\": {\n       \"openstack-mcp-server\": {\n         \"command\": \"/path/to/your/python\",\n         \"args\": [\n           \"python-openstackmcp-server\"\n         ],\n         \"env\" : {\n           \"OS_CLIENT_CONFIG_FILE\": \"/path/to/your/clouds.yaml\"\n         }\n       }\n     }\n   }\n   ```\n\n### Using uvx\n  \n   ```json\n   {\n     \"mcpServers\": {\n       \"openstack-mcp-server\": {\n         \"command\": \"uvx\",\n         \"args\": [\n           \"python-openstackmcp-server\"\n         ],\n         \"env\" : {\n           \"OS_CLIENT_CONFIG_FILE\": \"/path/to/your/clouds.yaml\"\n         }\n       }\n     }\n   }\n   ```\n\n# Contributing\nContributions are welcome! Please see the [CONTRIBUTING](CONTRIBUTING.rst) file for details on how to contribute to this project.\n\n# License\nThis project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.",
      "stars": 14,
      "updated_at": "2025-10-02T22:59:42Z",
      "url": "https://github.com/openstack-kr/python-openstackmcp-server"
    },
    "pibblokto--cert-manager-mcp-server": {
      "category": "cloud-platforms",
      "description": "A dedicated MCP intermediary for administering and diagnosing Certificate Authority resources managed by the cert-manager system within Kubernetes.",
      "forks": 1,
      "imageUrl": "",
      "keywords": [
        "cert",
        "cloud",
        "pibblokto",
        "cert manager",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "Python",
      "license": "Apache License 2.0",
      "name": "cert-mgr-k8s-interface-srv",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "pibblokto",
      "readme_content": "# cert-mgr-k8s-interface-srv\n\nThis is the backend server component for the MCP interface, specifically designed to facilitate the management and deep inspection of certificate objects and related resources governed by the [cert-manager](https://github.com/cert-manager/cert-manager) project running inside a Kubernetes cluster.\n\n## Configuration for Desktop Environment\n\nUse the following snippet in your local Claude Desktop configuration file to establish connectivity:\n\n```json\n{\n  \"mcpServers\": {\n    \"cert-manager-mcp-server\": {\n      \"command\": \"sh\",\n      \"args\": [\n        \"-c\",\n        \"docker run -i --rm -v ~/.kube:/home/app/.kube:ro -v ~/.config/gcloud:/home/app/.config/gcloud piblokto/cert-manager-mcp-server:v0.0.5\"\n      ]\n    }\n  }\n}\n```\n\n## Configuration for GKE Clusters\n\nFor environments leveraging Google Kubernetes Engine (GKE), the server configuration requires environment variable injection for project and region context:\n\n```json\n{\n  \"mcpServers\": {\n    \"cert-manager-mcp-server\": {\n      \"command\": \"sh\",\n      \"args\": [\n        \"-c\",\n        \"docker run -i --rm -v ~/.kube:/home/app/.kube:ro -v ~/.config/gcloud:/home/app/.config/gcloud -e CLOUDSDK_CORE_PROJECT=\u003cDEFAULT_PROJECT_ID\u003e -e CLOUDSDK_COMPUTE_REGION=\u003cDEFAULT_COMPUTE_REGION\u003e piblokto/cert-manager-mcp-server:v0.0.5\"\n      ]\n    }\n  }\n}\n```\n\n# Available Functionality\n\nThis service exposes a set of powerful functions tailored for interacting with `cert-manager` resources and general cluster context introspection.\n\n## Certificate Lifecycle Operations\n- **list_certificates**: Provides a comprehensive roster of certificates, optionally scoped to a specific namespace or the entire cluster. Supports filtering for lapsed certificates and includes domain details upon request (default behavior omits domains for conciseness).\n- **get_certificate**: Retrieves granular, detailed status information for an individual certificate entity.\n- **renew_certificate**: Triggers an immediate, forceful reconciliation cycle to prompt the issuance of a new certificate.\n\n## Certificate Authority (Issuer) Administration\n- **list_issuers**: Displays configuration and current operational status for both namespaced Issuers and ClusterIssuers. Filtering is restricted to namespace selection or explicit cluster-level listing.\n\n## Cluster Environment Probing\n- **list_namespaces**: Enumerates all active Kubernetes namespaces.\n- **list_contexts**: Reports the full set of configuration contexts available in the loaded kubeconfig file.\n- **get_current_context**: Identifies the context currently in use.\n- **switch_context**: Modifies the active kubeconfig context setting within the session's memory space.\n\n## Function Inventory\n\n| Tool Name | Purpose Summary | Access Mode | Input Arguments | \n|---|---|---|---| \n| `list_certificates` | Retrieve certificate listings with filtering capabilities. | Read Only | `namespace_name`, `all_namespaces`, `include_domains`, `list_expired`, `cursor`, `page_size` | \n| `get_certificate` | Fetch detailed attributes of a specific certificate. | Read Only | `namespace_name`, `certificate_name` | \n| `renew_certificate` | Initiate an expedited certificate renewal process. | Write | `namespace_name`, `certificate_name` | \n| `list_issuers` | Display status and definition of Issuers/ClusterIssuers. | Read Only | `list_cluster_issuers`, `all_namespaces`, `namespace_name` | \n| `list_namespaces` | Show all namespaces within the target cluster. | Read Only | None | \n| `list_contexts` | Report available kubeconfig contexts. | Read Only | None | \n| `get_current_context` | Determine the currently active context name. | Read Only | None | \n| `switch_context` | Alter the active kubeconfig context setting. | Read Only | `ctx` | \n\n\n## Contextual Note on GCP\n\nGoogle Cloud Platform (GCP) encompasses a comprehensive portfolio of cloud computing amenities provided by Google, offering modular services across computation, data persistence, advanced analytics, and machine learning. This infrastructure mirrors the same backbone utilized for Google's core consumer services (Search, Gmail, etc.). Access typically necessitates identity verification via banking details. GCP furnishes IaaS, PaaS, and serverless execution models. Google's initial foray into cloud computing was App Engine in 2008. The overarching brand is now 'Google Cloud', which integrates the public infrastructure with Google Workspace and specialized enterprise APIs. Key compute offerings include Compute Engine (VMs), GKE (Kubernetes), and various Function/Run services.",
      "stars": 16,
      "updated_at": "2025-10-02T22:59:46Z",
      "url": "https://github.com/pibblokto/cert-manager-mcp-server"
    },
    "portainer--portainer-mcp": {
      "category": "cloud-platforms",
      "description": "A robust intermediary server facilitating natural language interaction between sophisticated AI agents and Portainer environments, granting conversational control over container orchestration, deployment workflows, and system telemetry oversight.",
      "forks": 15,
      "imageUrl": "",
      "keywords": [
        "portainer",
        "cloud",
        "platform",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Go",
      "license": "zlib License",
      "name": "portainer-ai-bridge",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "portainer",
      "readme_content": "# Portainer AI Bridge (Portainer MCP Implementation)\n[![Go Report Card](https://goreportcard.com/badge/github.com/portainer/portainer-mcp)](https://goreportcard.com/report/github.com/portainer/portainer-mcp)\n![coverage](https://raw.githubusercontent.com/portainer/portainer-mcp/badges/.badges/main/coverage.svg)\n\nImagine querying the status of your container fleet using plain English.\n\nThis is now achievable! Portainer AI Bridge establishes a direct conduit between your chosen artificial intelligence assistant and your managed Portainer instances. Perform administrative tasks concerning users and environments, or execute arbitrary Docker or Kubernetes operational commands entirely via natural language prompts.\n\n![portainer-mcp-demo](https://downloads.portainer.io/mcp-demo5.gif)\n\n## Conceptual Framework\n\nPortainer AI Bridge represents an ongoing effort to implement the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) specifically tailored for Portainer ecosystems. The core objective is to standardize the integration pathway, enabling Large Language Models (LLMs) and associated services to interface securely with Portainer's comprehensive container management functionalities.\n\nMCP (Model Context Protocol) is an open standard designed to streamline how software furnishes contextual data to LLMs. Analogous to universal standards like USB-C connecting diverse peripherals, MCP standardizes the connection between AI models and various operational data sources and actionable tools.\n\nThis specific integration focuses on securely surfacing Portainer environment metadata through the MCP framework, thus empowering AI agents to manage containerized infrastructure using prescribed, secure methodologies.\n\n\u003e [!NOTE]\n\u003e Compatibility is strictly tied to certain Portainer release versions. Should your running Portainer version deviate from the expected set, utilizing the `--disable-version-check` switch permits an attempt at connection bypass. Consult the [Version Compatibility Matrix](#version-compatibility-matrix) for supported ranges and [Bypassing Version Validation](#bypassing-version-validation) for instructions on overriding this safeguard.\n\nReview the [Supported Operations Index](#supported-operations-index) for granular details regarding feature availability and version alignment.\n\n*Disclaimer: This utility remains actively under development.*\n\nOperation currently necessitates a valid Portainer administrator API credential.\n\n## Acquisition and Setup\n\nPrecompiled executables targeting Linux (amd64, arm64) and macOS (arm64) are accessible on the [**Latest Release Artifacts Page**](https://github.com/portainer/portainer-mcp/releases/latest). Select the appropriate compressed archive corresponding to your target operating system and processor architecture within the \"Assets\" listing.\n\n**Downloading the Package:**\nDirect download via the release page is typical. Alternatively, `curl` can be employed. Below is an example for macOS (ARM64), targeting release `v0.2.0`:\n\nbash\n# Adjust version tag and architecture as necessary\ncurl -Lo portainer-mcp-v0.2.0-darwin-arm64.tar.gz https://github.com/portainer/portainer-mcp/releases/download/v0.2.0/portainer-mcp-v0.2.0-darwin-arm64.tar.gz\n\n\n(Binaries for Linux AMD64 are also provisioned on the release page.)\n\n**(Optional but Strongly Advised) Integrity Check:**\nFirst, retrieve the accompanying `.md5` checksum file from the release page. Example for macOS (ARM64) `v0.2.0`:\n\nbash\n# Fetch the checksum file (update version/arch)\ncurl -Lo portainer-mcp-v0.2.0-darwin-arm64.tar.gz.md5 https://github.com/portainer/portainer-mcp/releases/download/v0.2.0/portainer-mcp-v0.2.0-darwin-arm64.tar.gz.md5\n# Execute verification (expected output should match the .md5 file content)\nif [ \"$(md5 -q portainer-mcp-v0.2.0-darwin-arm64.tar.gz)\" = \"$(cat portainer-mcp-v0.2.0-darwin-arm64.tar.gz.md5)\" ]; then echo \"Integrity Verified\"; else echo \"Verification Failed\"; fi\n\n\n(For Linux environments, utilize `md5sum -c \u003cchecksum_file_name\u003e.md5`)\nSuccessful execution resulting in \"Integrity Verified\" confirms the file has not been corrupted.\n\n**Decompressing the Archive:**\n\nbash\n# Adjust filename based on your downloaded version/OS/architecture\ntar -xzf portainer-mcp-v0.2.0-darwin-arm64.tar.gz\n\n\nThis action extracts the primary `portainer-mcp` executable.\n\n**Relocating the Executable:**\nMove the binary to a directory listed in your system's `$PATH` (e.g., `/usr/local/bin`) or record its current path for the subsequent configuration stage.\n\n# Deployment Protocol\n\nWhen integrating with an assistant platform like Claude Desktop, configure it using a structure resembling this:\n\n\n{\n    \"mcpServers\": {\n        \"portainer\": {\n            \"command\": \"/path/to/portainer-mcp\",\n            \"args\": [\n                \"-server\",\n                \"[IP]:[PORT]\",\n                \"-token\",\n                \"[TOKEN]\",\n                \"-tools\",\n                \"/tmp/tools.yaml\"\n            ]\n        }\n    }\n}\n\n\nSubstitute `[IP]`, `[PORT]`, and `[TOKEN]` with the precise network address, listening port, and administrative access credential for your Portainer instance.\n\n\u003e [!NOTE]\n\u003e By default, the application attempts to locate or generate the \"tools.yaml\" definition file in the same directory as the executable. If this file is absent, it is created with default tool definitions. Adjusting this path via the `-tools` argument is critical, especially when utilizing AI environments (like Claude) that impose restrictions on write operations within the execution context.\n\n## Bypassing Version Validation\n\nOrdinarily, the utility validates the connected Portainer server version against its internal compatibility list and halts startup upon mismatch. To force a connection attempt despite version disparity, include the `-disable-version-check` argument:\n\n\n{\n    \"mcpServers\": {\n        \"portainer\": {\n            \"command\": \"/path/to/portainer-mcp\",\n            \"args\": [\n                \"-server\",\n                \"[IP]:[PORT]\",\n                \"-token\",\n                \"[TOKEN]\",\n                \"-disable-version-check\"\n            ]\n        }\n    }\n}\n\n\n\u003e [!WARNING]\n\u003e Circumventing version validation carries a risk of unpredictable operational anomalies or complete failure due to unforeseen API revisions between Portainer versions. Partial or total functionality loss is possible with unsupported server targets.\n\nWhen this flag is active:\n- Startup sequence skips Portainer server version verification.\n- Features might behave erratically due to version-specific API divergence.\n- Newer Portainer releases might trigger execution errors from API expectation mismatches.\n- Older Portainer versions might lack required API endpoints.\n\nThis override is beneficial when:\n- You operate a Portainer release newer than officially supported by the current MCP bridge version.\n- You wish to experimentally test compatibility with an older Portainer instance.\n\n## Tool Definition Customization\n\nTool definitions are intrinsically bundled within the binary initially. The program will materialize this file at its standard location if it does not exist.\n\nYou gain control over these definitions by pointing to an external YAML file using the `-tools` option:\n\n\n{\n    \"mcpServers\": {\n        \"portainer\": {\n            \"command\": \"/path/to/portainer-mcp\",\n            \"args\": [\n                \"-server\",\n                \"[IP]:[PORT]\",\n                \"-token\",\n                \"[TOKEN]\",\n                \"-tools\",\n                \"/path/to/custom/tools.yaml\"\n            ]\n        }\n    }\n}\n\n\nThe reference default tool definitions reside at `internal/tooldef/tools.yaml` in the source repository. Modifying the descriptive text associated with tools allows fine-tuning how AI agents interpret and select their invocation. You retain the option to deactivate specific tools entirely.\n\n\u003e [!WARNING]\n\u003e Crucially, maintain the exact naming convention for tool identifiers and their mandated parameter structures (descriptions are the only safe element to alter). Deviation will result in registration failure and render the tools inert.\n\n## Egress Control (Read-Only Mode)\n\nFor environments prioritizing strict data integrity, the utility supports a read-only operational mode. This configuration completely neutralizes all write capabilities, restricting the AI to observation-only actions on Portainer assets.\n\nActivate read-only status by appending the `-read-only` argument:\n\n\n{\n    \"mcpServers\": {\n        \"portainer\": {\n            \"command\": \"/path/to/portainer-mcp\",\n            \"args\": [\n                \"-server\",\n                \"[IP]:[PORT]\",\n                \"-token\",\n                \"[TOKEN]\",\n                \"-read-only\"\n            ]\n        }\n    }\n}\n\n\nWhen operating in read-only restriction:\n- Only query operations (list, retrieve) are exposed to the language model.\n- All mutation tools (create, modify, erase) are suppressed from the exposed toolset.\n- The Docker API proxy utility is disabled.\n- The Kubernetes API proxy utility is disabled.\n\n# Version Compatibility Matrix\n\nThis integration is version-locked to specific Portainer releases. The startup validation step ensures alignment; failure to match results in termination.\n\n| Portainer AI Bridge Version  | Compatible Portainer Version |\n|--------------|----------------------------|\n| 0.1.0 | 2.28.1 |\n| 0.2.0 | 2.28.1 |\n| 0.3.0 | 2.28.1 |\n| 0.4.0 | 2.29.2 |\n| 0.4.1 | 2.29.2 |\n| 0.5.0 | 2.30.0 |\n| 0.6.0 | 2.31.2 |\n\n\u003e [!NOTE]\n\u003e For connectivity to unsupported Portainer versions, reference the [Bypassing Version Validation](#bypassing-version-validation) section for guidance and associated caveats.\n\n# Supported Operations Index\n\nThe ensuing table catalogs the functionalities currently exposed (as of the latest Bridge release) via the MCP interface:\n\n| Resource | Operation | Rationale | Initial Version |\n|----------|-----------|-------------|----------------------|\n| **Environments** | | | |\n| | ListEnvironments | Retrieve all accessible environments | 0.1.0 |\n| | UpdateEnvironmentTags | Modify descriptive metadata tags on an environment | 0.1.0 |\n| | UpdateEnvironmentUserAccesses | Adjust user permissions linked to an environment | 0.1.0 |\n| | UpdateEnvironmentTeamAccesses | Adjust team access rights for an environment | 0.1.0 |\n| **Environment Groups (Edge Groups)** | | | |\n| | ListEnvironmentGroups | Enumerate defined environment groupings | 0.1.0 |\n| | CreateEnvironmentGroup | Provision a new edge group | 0.1.0 |\n| | UpdateEnvironmentGroupName | Rename an existing edge group | 0.1.0 |\n| | UpdateEnvironmentGroupEnvironments | Modify the environment membership of a group | 0.1.0 |\n| | UpdateEnvironmentGroupTags | Modify edge group metadata tags | 0.1.0 |\n| **Access Groups (Endpoint Groups)** | | | |\n| | ListAccessGroups | List endpoint grouping structures | 0.1.0 |\n| | CreateAccessGroup | Establish a new endpoint group | 0.1.0 |\n| | UpdateAccessGroupName | Rename an existing endpoint group | 0.1.0 |\n| | UpdateAccessGroupUserAccesses | Modify user permissions within an endpoint group | 0.1.0 |\n| | UpdateAccessGroupTeamAccesses | Modify team permissions within an endpoint group | 0.1.0 |\n| | AddEnvironmentToAccessGroup | Associate an environment with a group | 0.1.0 |\n| | RemoveEnvironmentFromAccessGroup | Dissociate an environment from a group | 0.1.0 |\n| **Stacks (Edge Stacks)** | | | |\n| | ListStacks | Retrieve all deployed stacks | 0.1.0 |\n| | GetStackFile | Fetch the underlying compose definition for a stack | 0.1.0 |\n| | CreateStack | Deploy a new Docker stack | 0.1.0 |\n| | UpdateStack | Modify an active Docker stack configuration | 0.1.0 |\n| **Tags** | | | |\n| | ListEnvironmentTags | List all tags applied across environments | 0.1.0 |\n| | CreateEnvironmentTag | Register a new environment-specific tag | 0.1.0 |\n| **Teams** | | | |\n| | ListTeams | Enumerate established teams | 0.1.0 |\n| | CreateTeam | Form a new team entity | 0.1.0 |\n| | UpdateTeamName | Modify a team's designated name | 0.1.0 |\n| | UpdateTeamMembers | Modify the roster of team members | 0.1.0 |\n| **Users** | | | |\n| | ListUsers | Retrieve the registry of system users | 0.1.0 |\n| | UpdateUser | Modify an existing user's profile details | 0.1.0 |\n| | GetSettings | Query the global Portainer instance configuration | 0.1.0 |\n| **Docker** | | | |\n| | DockerProxy | Forward arbitrary Docker API requests | 0.2.0 |\n| **Kubernetes** | | | |\n| | KubernetesProxy | Forward arbitrary Kubernetes API requests | 0.3.0 |\n| | getKubernetesResourceStripped | Forward GET K8s requests, auto-pruning excessive metadata fields | 0.6.0 |\n\n# Development Guidelines\n\n## Source Code Metrics\n\nA utility script, `cloc.sh`, is present in the repository root to compute source line counts and related statistics using the `cloc` utility. Ensure `cloc` is installed (e.g., via `sudo apt install cloc` or `brew install cloc`).\n\nExecute the script from the repository root for the default summarized output:\n\nbash\n./cloc.sh\n\n\nRefer to the introductory comments within `cloc.sh` for flags that allow retrieval of specific measurement aggregates.\n\n## Prompt Token Cost Estimation\n\nTo approximate the token consumption of your current tool definitions within prompts, utilize the bundled Go program alongside a shell script designed to interface with the Anthropic API token counting endpoint.\n\n**1. Artifact Generation (Tools JSON):**\nFirst, employ the `token-count` Go program to translate your YAML tool specifications into the JSON payload format mandated by the Anthropic API. Execute this from the repository root:\n\nbash\n# Assume input YAML is at internal/tooldef/tools.yaml\n# Specify your desired output JSON location\ngo run ./cmd/token-count -input internal/tooldef/tools.yaml -output .tmp/tools.json\n\n\nThis command parses the input YAML tool definitions and writes a JSON array detailing each tool's `name`, `description`, and `input_schema` to the specified destination file.\n\n**2. API Query Execution:**\nSubsequently, leverage the `token.sh` script to transmit these tool definitions alongside a representative sample message to the Anthropic service. A valid Anthropic API key is mandatory.\n\nbash\n# Prerequisite: jq must be installed\n# Replace sk-ant-xxxxxxxx with your active Anthropic API authentication key\n# Ensure .tmp/tools.json points to the file generated in Step 1\n./token.sh -k sk-ant-xxxxxxxx -i .tmp/tools.json\n\n\nThe script will output the resulting JSON payload from the Anthropic API, explicitly detailing the estimated token expenditure for the supplied toolset and message within the `usage.input_tokens` field.\n\nThis methodical approach allows for precise comprehension of the token overhead introduced by the toolset provided to the language model.\n\nWIKIPEDIA: Google Cloud Platform (GCP) represents a comprehensive suite of distributed computing utilities offered by Google, encompassing modular services such as computation, persistent data storage, advanced data analysis, and machine learning capabilities, complemented by integrated management utilities. This infrastructure mirrors the foundational architecture Google employs internally for high-volume consumer products like Google Search, Gmail, and Google Docs, as documented by Verma et al. Service enrollment mandates verified credit card or bank account credentials.\nGoogle Cloud Platform furnishes Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and serverless execution paradigms.\nIn the second quarter of 2008, Google unveiled App Engine, a foundational platform facilitating the development and hosting of web applications within Google-operated data centers, marking the corporation's inaugural cloud offering. General availability commenced in late 2011. Following App Engine's introduction, Google systematically expanded its portfolio of associated cloud services.\nGoogle Cloud Platform constitutes a segment of the broader Google Cloud umbrella, which integrates the public cloud infrastructure of GCP with Google Workspace (formerly G Suite), enterprise editions of Android and ChromeOS, and specialized Application Programming Interfaces (APIs) for advanced mapping and machine learning functions. Since at least 2022, official Google documentation has standardized the nomenclature, asserting \"Google Cloud\" as the updated designation for \"Google Cloud Platform,\" which can occasionally lead to terminological ambiguity.\n\n\n== Principal Offerings ==\nGoogle catalogs upwards of 100 distinct products under the Google Cloud branding. Key services are enumerated below.\n\n\n=== Computation Resources ===\nApp Engine ‚Äì A PaaS environment for deploying applications built using Java, PHP, Node.js, Python, C#, .Net, Ruby, and Go.\nCompute Engine ‚Äì An IaaS layer supporting the execution of virtual machines running both Microsoft Windows and Linux operating systems.\nGoogle Kubernetes Engine (GKE) or GKE on-prem (available via Anthos) ‚Äì A container orchestration service built upon Kubernetes technology.\nCloud Functions ‚Äì A Functions as a Service (FaaS) offering for executing event-driven code segments written in Node.js, Java, Python, or Go.\nCloud Run ‚Äì A container execution environment leveraging Knative. It is offered as a fully managed service or integrated with Anthos. Currently supports unified management across GCP, AWS, and VMware environments.\n\n\n=== Data Persistence and Databases ===\nCloud Storage ‚Äì Scalable object storage featuring integrated edge caching for unstructured data repositories.\nCloud SQL ‚Äì A managed Database as a Service supporting MySQL, PostgreSQL, and Microsoft SQL Server.\nCloud Bigtable ‚Äì A managed, high-throughput NoSQL database service.\nCloud Spanner ‚Äì A globally distributed, strongly consistent, relational database service designed for horizontal scaling.\nCloud Datastore ‚Äì A NoSQL database optimized for web and mobile application backends.\nPersistent Disk ‚Äì Block storage volumes provisioned for Compute Engine virtual machines.\nCloud Memorystore ‚Äì Managed in-memory caching services based on Redis and Memcached protocols.\nLocal SSD: High-speed, ephemeral, local block storage attachments.\nFilestore: Managed, high-performance file storage solutions for Google Cloud consumers.\nAlloyDB: A fully managed database compatible with PostgreSQL standards.\n\n\n=== Network Infrastructure ===\n\nVPC ‚Äì Virtual Private Cloud, providing isolated network segments.",
      "stars": 68,
      "updated_at": "2025-10-04T02:42:28Z",
      "url": "https://github.com/portainer/portainer-mcp"
    },
    "pulumi--mcp-server": {
      "category": "cloud-platforms",
      "description": "MCP server for interacting with Pulumi using the Pulumi Automation API and Pulumi Cloud API. Enables MCP clients to perform Pulumi operations like retrieving package information, previewing changes, deploying updates, and retrieving stack outputs programmatically.",
      "forks": 0,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "pulumi",
        "mcp",
        "pulumi cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "Unknown",
      "license": "Unknown",
      "name": "mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "pulumi",
      "readme_content": "",
      "stars": 0,
      "updated_at": "",
      "url": "https://github.com/pulumi/mcp-server"
    },
    "pythonanywhere--pythonanywhere-mcp-server": {
      "category": "cloud-platforms",
      "description": "Bridge implementation for the Model Context Protocol targeting the PythonAnywhere cloud hosting environment.",
      "forks": 4,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "pythonanywhere",
        "platform",
        "pythonanywhere cloud",
        "cloud platform",
        "cloud platforms"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "pyanywhere-mcp-adapter",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "pythonanywhere",
      "readme_content": "# PythonAnywhere Model Context Protocol Adapter\n\nThis component serves as a conduit, adhering to the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction), to facilitate secure, programmatic interaction with your [PythonAnywhere](https://www.pythonanywhere.com/) resources. It exposes a uniform interface allowing autonomous agents and language models to orchestrate actions‚Äîlike modifying source code, deploying web applications, or scheduling recurrent jobs‚Äîunder controlled and auditable conditions.\n\n## Core Capabilities\n- **Resource Manipulation**: Perform CRUD operations on filesystem assets, including listing directories, uploading new content, and deleting existing entries. This is instrumental for log file inspection and debugging.\n- **ASGI Application Orchestration**: Full lifecycle management for ASGI web services: creation, deletion, listing available deployments, and initiating restarts.\n  _(Reference: [PythonAnywhere ASGI Configuration Guide](https://help.pythonanywhere.com/pages/ASGICommandLine))_\n- **WSGI Application Control**: Currently limited to triggering service reloads.\n- **Task Scheduling Management**: Comprehensive control over scheduled operations, encompassing listing existing jobs, setting up new ones, modifying parameters, and removal.\n  _(**Security Note**: Automated scheduling/deletion carries inherent risk; an LLM might create a task that executes code immediately upon creation. For sensitive automated workloads, consider pairing this adapter with tools like [mcp-server-time](https://pypi.org/project/mcp-server-time/) to mitigate temporal confusion by the model.)_\n\n## Deployment Instructions\nThe MCP standard is client-agnostic, but setup procedures vary. Below are the verified methods.\n\nPrerequisite: Ensure the `uv` package manager is installed and accessible from your system's command path.\n\nAuthentication requires both your PythonAnywhere username and a corresponding API access key. Obtain or generate this key via the [API management portal on PythonAnywhere](https://www.pythonanywhere.com/account/#api_token).\n\n### Claude Desktop Extension Integration\nThis offers the simplest installation path when utilizing the Claude Desktop client.\n\n1. Launch the Claude Desktop application.\n2. Retrieve the latest distribution package: **[Download the .dxt file here](https://github.com/pythonanywhere/pythonanywhere-mcp-server/releases/latest/download/pythonanywhere-mcp-server.dxt)**.\n3. Import the file by double-clicking it or dragging it into the application window.\n4. Input your provided PythonAnywhere API key and account identifier.\n5. Reinitialize Claude Desktop to activate the integration.\n\n### Direct Claude Console Invocation\nExecute the following command structure:\n   bash\n   claude mcp add pythonanywhere-mcp-server \\\n   -e API_TOKEN=yourpythonanywhereapitoken \\\n   -e LOGNAME=yourpythonanywhereusername \\\n   -- uvx pythonanywhere-mcp-server\n   \n\n### Configuration for GitHub Copilot within PyCharm\nIncorporate the server definition into your local `mcp.json` file:\n\n\n{\n  \"servers\": {\n    \"pythonanywhere-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\",\n      \"args\": [\"pythonanywhere-mcp-server\"],\n      \"env\": {\n        \"API_TOKEN\": \"yourpythonanywhereapitoken\",\n        \"LOGNAME\": \"yourpythonanywhereusername\"\n      }\n    }\n  }\n}\n\n\n### Manual Setup for Cursor and Claude Desktop\nFor Cursor, use `mcp.json`. For Claude Desktop, target `claude_desktop_config.json`. Add the following object structure:\n\n\n{\n  \"mcpServers\": {\n    \"pythonanywhere-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\",\n      \"args\": [\"pythonanywhere-mcp-server\"],\n      \"env\": {\n        \"API_TOKEN\": \"yourpythonanywhereapitoken\",\n        \"LOGNAME\": \"yourpythonanywhereusername\"\n      }\n    }\n  }\n}\n\n\n## Operational Warnings\nIntegrating an arbitrary AI agent directly with your live PythonAnywhere hosting environment grants substantial power, which mandates corresponding caution. Human oversight remains critical, particularly for destructive or irreversible operations like filesystem alterations or deletion tasks.\n\nBe extremely wary when running multiple active MCP endpoints concurrently. If any adapter has access to external, untrusted resources (e.g., external issue trackers), these connections can be exploited as pathways for malicious execution. Consult security advisories like [this detailed analysis](https://simonwillison.net/2025/Jul/6/supabase-mcp-lethal-trifecta/) for deeper context on multi-server risks.\n\n## Architectural Details\nThis service is built upon the [python mcp sdk](https://github.com/modelcontextprotocol/python-sdk). It leverages the companion package, [pythonanywhere-core](https://github.com/pythonanywhere/pythonanywhere-core) ([documentation](https://core.pythonanywhere.com/)), which provides a localized abstraction layer over the [PythonAnywhere REST API](https://help.pythonanywhere.com/pages/API/). This core library is extensible and subject to future enhancements as API capabilities evolve.\n\nWIKIPEDIA: Google Cloud Platform (GCP) is a comprehensive suite of public cloud computing resources provided by Google, encompassing modular services for computation, data persistence, analytical processing, and artificial intelligence models. It operates on the identical underlying infrastructure that powers Google's consumer-facing services, such as Google Search, Gmail, and Google Docs, as documented by Verma et al. Account activation mandates verifiable financial information, such as a credit card or banking credentials.\nGoogle Cloud Platform furnishes environments across Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and serverless computing paradigms.\nGoogle first entered the cloud computing domain in April 2008 with the introduction of App Engine, a platform dedicated to the development and hosting of web applications within Google-managed datacenters. General availability for this service was achieved in November 2011. Following App Engine's debut, Google systematically integrated numerous additional cloud services into the platform.\nGoogle Cloud Platform forms a component of the broader Google Cloud ecosystem, which includes the public cloud infrastructure, enterprise productivity suites like Google Workspace (formerly G Suite), corporate versions of Android and ChromeOS, and specialized APIs for machine intelligence and enterprise geospatial services. As of 2022, official communications indicate that \"Google Cloud\" is the preferred nomenclature, replacing \"Google Cloud Platform,\" which can occasionally lead to terminological ambiguity.\n\n== Offerings ==\nGoogle catalogues over one hundred distinct products under the Google Cloud banner. Key services are summarized below.\n\n=== Computational Resources ===\nApp Engine ‚Äì A PaaS environment supporting application deployment across languages including Java, PHP, Node.js, Python, C#, .Net, Ruby, and Go.\nCompute Engine ‚Äì An IaaS offering for provisioning and operating virtual machines running Microsoft Windows or Linux.\nGoogle Kubernetes Engine (GKE) or GKE on-prem (part of Anthos) ‚Äì A managed Container Orchestration service built on Kubernetes.\nCloud Functions ‚Äì A Function as a Service (FaaS) solution for executing event-triggered code written in Node.js, Java, Python, or Go.\nCloud Run ‚Äì A serverless compute engine based on Knative architecture, available in fully managed mode or via Cloud Run for Anthos. It currently supports unified management across GCP, AWS, and VMware environments.\n\n=== Data Persistence and Storage ===\nCloud Storage ‚Äì Scalable object storage featuring integrated edge caching for unstructured data repositories.\nCloud SQL ‚Äì A managed Database as a Service supporting MySQL, PostgreSQL, and Microsoft SQL Server backends.\nCloud Bigtable ‚Äì A high-throughput, managed NoSQL database service.\nCloud Spanner ‚Äì A globally distributed, strongly consistent, relational database solution offering horizontal scalability.\nCloud Datastore ‚Äì A NoSQL database optimized for web and mobile application backends.\nPersistent Disk ‚Äì Attachable block storage volumes provisioned for Compute Engine VMs.\nCloud Memorystore ‚Äì Managed in-memory caching services leveraging Redis and Memcached protocols.\nLocal SSD: High-performance, temporary, locally attached block storage devices.\nFilestore: A managed, high-throughput file storage service tailored for Google Cloud workloads.\nAlloyDB: A fully managed database service compatible with PostgreSQL.\n\n=== Interconnectivity ===\nVPC ‚Äì Virtual Private Cloud network isolation service.",
      "stars": 8,
      "updated_at": "2025-09-15T07:33:30Z",
      "url": "https://github.com/pythonanywhere/pythonanywhere-mcp-server"
    },
    "qiniu--qiniu-mcp-server": {
      "category": "cloud-platforms",
      "description": "A MCP built on Qiniu Cloud products, supporting access to Qiniu Cloud Storage, media processing services, etc.",
      "forks": 13,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "platforms",
        "platform",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "qiniu-mcp-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "qiniu",
      "readme_content": "# Qiniu MCP Server\n\n## Ê¶ÇËø∞\n\nÂü∫‰∫é‰∏ÉÁâõ‰∫ë‰∫ßÂìÅÊûÑÂª∫ÁöÑ Model Context Protocol (MCP) ServerÔºåÊîØÊåÅÁî®Êà∑Âú® AI Â§ßÊ®°ÂûãÂÆ¢Êà∑Á´ØÁöÑ‰∏ä‰∏ãÊñá‰∏≠ÈÄöËøáËØ• MCP\nServer Êù•ËÆøÈóÆ‰∏ÉÁâõ‰∫ëÂ≠òÂÇ®„ÄÅÊô∫ËÉΩÂ§öÂ™í‰ΩìÊúçÂä°Á≠â„ÄÇ\n\nÂÖ≥‰∫éËÆøÈóÆ‰∏ÉÁâõ‰∫ëÂ≠òÂÇ®ËØ¶ÁªÜÊÉÖÂÜµËØ∑ÂèÇËÄÉ [Âü∫‰∫é MCP ‰ΩøÁî®Â§ßÊ®°ÂûãËÆøÈóÆ‰∏ÉÁâõ‰∫ëÂ≠òÂÇ®](https://developer.qiniu.com/kodo/12914/mcp-aimodel-kodo)„ÄÇ\n\nËÉΩÂäõÈõÜÔºö\n- Â≠òÂÇ®\n  - Ëé∑Âèñ Bucket ÂàóË°®\n  - Ëé∑Âèñ Bucket ‰∏≠ÁöÑÊñá‰ª∂ÂàóË°®\n  - ‰∏ä‰º†Êú¨Âú∞Êñá‰ª∂Ôºå‰ª•ÂèäÁªôÂá∫Êñá‰ª∂ÂÜÖÂÆπËøõË°å‰∏ä‰º†\n  - ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπ\n  - Ëé∑ÂèñÊñá‰ª∂‰∏ãËΩΩÈìæÊé•\n- Êô∫ËÉΩÂ§öÂ™í‰Ωì\n  - ÂõæÁâáÁº©Êîæ\n  - ÂõæÁâáÂàáÂúÜËßí\n- CDN\n  - Ê†πÊçÆÈìæÊé•Âà∑Êñ∞Êñá‰ª∂\n  - Ê†πÊçÆÈìæÊé•È¢ÑÂèñÊñá‰ª∂\n\n## ÁéØÂ¢ÉË¶ÅÊ±Ç\n\n- Python 3.12 ÊàñÊõ¥È´òÁâàÊú¨\n- uv ÂåÖÁÆ°ÁêÜÂô®\n\nÂ¶ÇÊûúËøòÊ≤°ÊúâÂÆâË£Ö uvÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£ÖÔºö\n```bash\n# MacÔºåÊé®Ëçê‰ΩøÁî® brew ÂÆâË£Ö\nbrew install uv\n\n\n# Linux \u0026 Mac\n# 1. ÂÆâË£Ö\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# 2. ÂÆâË£ÖÂÆåÊàêÂêéÔºåËØ∑Á°Æ‰øùÂ∞ÜËΩØ‰ª∂ÂåÖÂÆâË£ÖË∑ØÂæÑÔºàÂåÖÂê´ uv Âíå uvx ÂèØÊâßË°åÊñá‰ª∂ÁöÑÁõÆÂΩïÔºâÊ∑ªÂä†Âà∞Á≥ªÁªüÁöÑ PATH ÁéØÂ¢ÉÂèòÈáè‰∏≠„ÄÇ\n# ÂÅáËÆæÂÆâË£ÖÂåÖË∑ØÂæÑ‰∏∫ /Users/xxx/.local/binÔºàËßÅÂÆâË£ÖÊâßË°åËæìÂá∫Ôºâ\n### ‰∏¥Êó∂ÁîüÊïàÔºàÂΩìÂâç‰ºöËØùÔºâÔºåÂú®ÂΩìÂâçÁªàÁ´Ø‰∏≠ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\nexport PATH=\"/Users/xxx/.local/bin:$PATH\"\n### Ê∞∏‰πÖÁîüÊïàÔºàÊé®ËçêÔºâÔºåÂú®ÂΩìÂâçÁªàÁ´Ø‰∏≠ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\necho 'export PATH=\"/Users/xxx/.local/bin:$PATH\"' \u003e\u003e ~/.bash_profile\nsource ~/.bash_profile\n\n\n# Windows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nÂÖ∑‰ΩìÂÆâË£ÖÊñπÂºèÂèÇËÄÉ [uv ÂÆâË£Ö](https://docs.astral.sh/uv/getting-started/installation/#pypi)\n\n## Âú® Cline ‰∏≠‰ΩøÁî®Ôºö\n\nÊ≠•È™§Ôºö\n\n1. Âú® vscode ‰∏ãËΩΩ Cline Êèí‰ª∂Ôºà‰∏ãËΩΩÂêé Cline Êèí‰ª∂ÂêéÂú®‰æßËæπÊ†è‰ºöÂ¢ûÂä† Cline ÁöÑÂõæÊ†áÔºâ\n2. ÈÖçÁΩÆÂ§ßÊ®°Âûã\n3. ÈÖçÁΩÆ qiniu MCP\n    1. ÁÇπÂáª Cline ÂõæÊ†áËøõÂÖ• Cline Êèí‰ª∂ÔºåÈÄâÊã© MCP Server Ê®°Âùó\n    2. ÈÄâÊã© installedÔºåÁÇπÂáª Advanced MCP Settings ÈÖçÁΩÆ MCP ServerÔºåÂèÇËÄÉ‰∏ãÈù¢ÈÖçÁΩÆ‰ø°ÊÅØ\n   ```\n   {\n     \"mcpServers\": {\n       \"qiniu\": {\n         \"command\": \"uvx\",\n         \"args\": [\n           \"qiniu-mcp-server\"\n         ],\n         \"env\": {\n           \"QINIU_ACCESS_KEY\": \"YOUR_ACCESS_KEY\",\n           \"QINIU_SECRET_KEY\": \"YOUR_SECRET_KEY\",\n           \"QINIU_REGION_NAME\": \"YOUR_REGION_NAME\",\n           \"QINIU_ENDPOINT_URL\": \"YOUR_ENDPOINT_URL\",\n           \"QINIU_BUCKETS\": \"YOUR_BUCKET_A,YOUR_BUCKET_B\"\n        },\n         \"disabled\": false\n       }\n     }\n   }\n   ```\n    3. ÁÇπÂáª qiniu MCP Server ÁöÑÈìæÊé•ÂºÄÂÖ≥ËøõË°åËøûÊé•\n4. Âú® Cline ‰∏≠ÂàõÂª∫‰∏Ä‰∏™ËÅäÂ§©Á™óÂè£ÔºåÊ≠§Êó∂Êàë‰ª¨ÂèØ‰ª•Âíå AI ËøõË°å‰∫§‰∫íÊù•‰ΩøÁî® qiniu-mcp-server Ôºå‰∏ãÈù¢ÁªôÂá∫Âá†‰∏™Á§∫‰æãÔºö\n    - Âàó‰∏æ qiniu ÁöÑËµÑÊ∫ê‰ø°ÊÅØ\n    - Âàó‰∏æ qiniu ‰∏≠ÊâÄÊúâÁöÑ Bucket\n    - Âàó‰∏æ qiniu ‰∏≠ xxx Bucket ÁöÑÊñá‰ª∂\n    - ËØªÂèñ qiniu xxx Bucket ‰∏≠ yyy ÁöÑÊñá‰ª∂ÂÜÖÂÆπ\n    - ÂØπ qiniu xxx Bucket ‰∏≠ yyy ÁöÑÂõæÁâáÂàá‰∏™ÂÆΩ200ÂÉèÁ¥†ÁöÑÂúÜËßí\n    - Âà∑Êñ∞‰∏ã qiniu ÁöÑËøô‰∏™ CDN ÈìæÊé•Ôºöhttps://developer.qiniu.com/test.txt\n\nÊ≥®Ôºö\ncursor ‰∏≠ÂàõÂª∫ MCP Server ÂèØÁõ¥Êé•‰ΩøÁî®‰∏äËø∞ÈÖçÁΩÆ„ÄÇ\nclaude ‰∏≠‰ΩøÁî®Êó∂ÂèØËÉΩ‰ºöÈÅáÂà∞ÔºöError: spawn uvx ENOENT ÈîôËØØÔºåËß£ÂÜ≥ÊñπÊ°àÔºöcommand ‰∏≠ ÂèÇÊï∞Â°´ÂÜô uvx ÁöÑÁªùÂØπË∑ØÂæÑÔºåeg: /usr/local/bin/uvx\n\n## ÂºÄÂèë\n1. ÂÖãÈöÜ‰ªìÂ∫ìÔºö\n\n```bash\n# ÂÖãÈöÜÈ°πÁõÆÂπ∂ËøõÂÖ•ÁõÆÂΩï\ngit clone git@github.com:qiniu/qiniu-mcp-server.git\ncd qiniu-mcp-server\n```\n\n2. ÂàõÂª∫Âπ∂ÊøÄÊ¥ªËôöÊãüÁéØÂ¢ÉÔºö\n\n```bash\nuv venv\nsource .venv/bin/activate  # Linux/macOS\n# Êàñ\n.venv\\Scripts\\activate  # Windows\n```\n\n3. ÂÆâË£Ö‰æùËµñÔºö\n\n```bash\nuv pip install -e .\n```\n\n4. ÈÖçÁΩÆ\n\nÂ§çÂà∂ÁéØÂ¢ÉÂèòÈáèÊ®°ÊùøÔºö\n```bash\ncp .env.example .env\n```\n\nÁºñËæë `.env` Êñá‰ª∂ÔºåÈÖçÁΩÆ‰ª•‰∏ãÂèÇÊï∞Ôºö\n```bash\n# S3/Kodo ËÆ§ËØÅ‰ø°ÊÅØ\nQINIU_ACCESS_KEY=your_access_key\nQINIU_SECRET_KEY=your_secret_key\n\n# Âå∫Âüü‰ø°ÊÅØ\nQINIU_REGION_NAME=your_region\nQINIU_ENDPOINT_URL=endpoint_url # eg:https://s3.your_region.qiniucs.com\n\n# ÈÖçÁΩÆ bucketÔºåÂ§ö‰∏™ bucket ‰ΩøÁî®ÈÄóÂè∑ÈöîÂºÄÔºåÂª∫ËÆÆÊúÄÂ§öÈÖçÁΩÆ 20 ‰∏™ bucket\nQINIU_BUCKETS=bucket1,bucket2,bucket3\n```\n\nÊâ©Â±ïÂäüËÉΩÔºåÈ¶ñÂÖàÂú® core ÁõÆÂΩï‰∏ãÊñ∞Â¢û‰∏Ä‰∏™‰∏öÂä°ÂåÖÁõÆÂΩïÔºàeg: Â≠òÂÇ® -\u003e storageÔºâÔºåÂú®Ê≠§‰∏öÂä°ÂåÖÁõÆÂΩï‰∏ãÂÆåÊàêÂäüËÉΩÊãìÂ±ï„ÄÇ\nÂú®‰∏öÂä°ÂåÖÁõÆÂΩï‰∏ãÁöÑ `__init__.py` Êñá‰ª∂‰∏≠ÂÆö‰πâ load ÂáΩÊï∞Áî®‰∫éÊ≥®ÂÜå‰∏öÂä°Â∑•ÂÖ∑ÊàñËÄÖËµÑÊ∫êÔºåÊúÄÂêéÂú® `core` ÁõÆÂΩï‰∏ãÁöÑ `__init__.py`\n‰∏≠Ë∞ÉÁî®Ê≠§ load ÂáΩÊï∞ÂÆåÊàêÂ∑•ÂÖ∑ÊàñËµÑÊ∫êÁöÑÊ≥®ÂÜå„ÄÇ\n\n```shell\ncore\n‚îú‚îÄ‚îÄ __init__.py # ÂêÑ‰∏™‰∏öÂä°Â∑•ÂÖ∑ÊàñËÄÖËµÑÊ∫êÂä†ËΩΩ\n‚îî‚îÄ‚îÄ storage # Â≠òÂÇ®‰∏öÂä°ÁõÆÂΩï\n    ‚îú‚îÄ‚îÄ __init__.py # Âä†ËΩΩÂ≠òÂÇ®Â∑•ÂÖ∑ÊàñËÄÖËµÑÊ∫ê\n    ‚îú‚îÄ‚îÄ resource.py # Â≠òÂÇ®ËµÑÊ∫êÊâ©Â±ï\n    ‚îú‚îÄ‚îÄ storage.py # Â≠òÂÇ®Â∑•ÂÖ∑Á±ª\n    ‚îî‚îÄ‚îÄ tools.py # Â≠òÂÇ®Â∑•ÂÖ∑Êâ©Â±ï\n```\n\n## ÊµãËØï\n\n### ‰ΩøÁî® Model Control Protocol Inspector ÊµãËØï\n\nÂº∫ÁÉàÊé®Ëçê‰ΩøÁî® [Model Control Protocol Inspector](https://github.com/modelcontextprotocol/inspector) ËøõË°åÊµãËØï„ÄÇ\n\n```shell\n# node ÁâàÊú¨‰∏∫Ôºöv22.4.0\nnpx @modelcontextprotocol/inspector uv --directory . run qiniu-mcp-server\n```\n\n### Êú¨Âú∞ÂêØÂä® MCP Server Á§∫‰æã\n\n1. ‰ΩøÁî®Ê†áÂáÜËæìÂÖ•ËæìÂá∫ÔºàstdioÔºâÊ®°ÂºèÂêØÂä®ÔºàÈªòËÆ§ÔºâÔºö\n\n```bash\nuv --directory . run qiniu-mcp-server\n```\n\n2. ‰ΩøÁî® SSE Ê®°ÂºèÂêØÂä®ÔºàÁî®‰∫é Web Â∫îÁî®ÔºâÔºö\n\n```bash\nuv --directory . run qiniu-mcp-server --transport sse --port 8000\n```\n\n\n\n\n",
      "stars": 28,
      "updated_at": "2025-09-25T15:54:39Z",
      "url": "https://github.com/qiniu/qiniu-mcp-server"
    },
    "redis--mcp-redis-cloud": {
      "category": "cloud-platforms",
      "description": "Facilitate comprehensive administration of your Redis Cloud tenancy using intuitive, natural language directives. Provision data stores, inspect service entitlements, and fine-tune infrastructure deployments via simple conversational inputs.",
      "forks": 15,
      "imageUrl": "",
      "keywords": [
        "redis",
        "cloud",
        "platforms",
        "redis cloud",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "TypeScript",
      "license": "MIT License",
      "name": "mcp-redis-manager-nx",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "redis",
      "readme_content": "# Redis Cloud Management Protocol (MCP) Gateway\n\nThis implementation provides a Model Context Protocol (MCP) Gateway specifically engineered to interface with the Redis Cloud API ecosystem. MCP establishes a standardized communication layer between sophisticated Language Models (LLMs) and external operational systems. This server component enables users to leverage natural language capabilities through clients like Claude Desktop or any compliant MCP interface to automate complex Redis Cloud account operations.\n\nExamples of attainable natural language actions:\n\n- \"Provision a fresh Redis data store within the Azure cloud environment.\"\n- \"Enumerate my active service subscriptions and associated billing details.\"\n- \"Recommend an optimal Redis tier configuration for handling high-volume transactional data in e-commerce workloads.\"\n\n## Core Functional Modules\n\n### Identity and Account Services\n- `get_current_account`: Retrieve detailed metadata pertaining to the currently authenticated Redis Cloud account.\n- `get_current_payment_methods`: Catalog all registered payment instruments linked to the account.\n\n### Entitlement and Plan Management\n\n#### Premium Service Tiers\n- `get_pro_subscriptions`: Obtain a listing of all provisioned Premium service entitlements.\n- `create_pro_subscription`: Initiate the creation of a new Premium entitlement featuring advanced customization parameters:\n  - Support for heterogeneous, multi-cloud deployment topologies.\n  - Granular control over memory allocation, data persistence policies, and extension modules.\n  - Setup and configuration of Active-Active redundancy paradigms.\n  - Specification of custom virtual private cloud networking parameters.\n\n#### Standard Service Tiers\n- `get_essential_subscriptions`: Fetch a paginated list of all Essential service entitlements.\n- `get_essential_subscription_by_id`: Secure detailed specifications for a singular Essential entitlement via its identifier.\n- `create_essential_subscription`: Provision a new Standard tier service instance.\n- `delete_essential_subscription`: Terminate and decommission an existing Essential service instance.\n\n### Feature and Capability Discovery\n- `get_database_modules`: List all foundational database modules (capabilities) accessible within the current account scope:\n  - Official Redis extension modules.\n  - Core database operational features.\n  - Supported performance optimization settings.\n\n### Geographical and Infrastructure Mapping\n- `get_pro_plans_regions`: Ascertain the catalogue of available deployment regions across supported hyperscalers:\n  - Amazon Web Services (AWS) regions.\n  - Google Cloud Platform (GCP) regions.\n  - Supported networking adjacency options.\n  - Availability Zone groupings.\n\n### Tiers and Costing Structure\n- `get_essentials_plans`: Display the catalogue of available Standard subscription plans (supports pagination):\n  - Coverage for AWS, GCP, and Microsoft Azure platforms.\n  - Information regarding 'Redis Flex' pricing models.\n  - Details on fixed-term subscription packages.\n\n### Asynchronous Operation Tracking\n- `get_tasks`: Present a comprehensive ledger of all pending or completed operations within the tenant environment.\n- `get_task_by_id`: Retrieve granular status updates for a specific asynchronous task:\n  - Tracking the deployment realization pipeline status.\n  - Monitoring progress of subscription modifications.\n  - Viewing overall task execution progression metrics.\n\n\n## Operational Deployment Guide\n\n#### Prerequisites\n- Possession of valid Redis Cloud API authentication materials (API Key and Secret Key).\n- Understanding that substantial provisioning operations yield Task IDs for subsequent status polling.\n- Recognition that retrieval of complete datasets often necessitates iterating through paginated API responses.\n\n\n### Integration with Claude Desktop\n\nTo deploy and initialize the MCP gateway agent within the Claude Desktop environment:\n\n1. Compile the project artifacts:\n   bash\n   npm run build\n   \n\n2. Configure the server connection within Claude Desktop:\n    - Access Claude Desktop configuration settings.\n    - Navigate to the 'Developer' panel (ensure 'Developer Mode' is toggled on).\n    - Select 'Edit config' to access the configuration manifest.\n    - Append the subsequent configuration structure to the `claude_desktop_config.json` file, substituting placeholder values:\n   \n   {\n     \"mcpServers\": {\n       \"mcp-redis-cloud\": {\n         \"command\": \"node\",\n         \"args\": [\"--experimental-fetch\", \"\u003cabsolute_path_to_project_root\u003e/dist/index.js\"],\n         \"env\": {\n           \"API_KEY\": \"\u003credis_cloud_api_key\u003e\",\n           \"SECRET_KEY\": \"\u003credis_cloud_api_secret_key\u003e\"\n         }\n       }\n     }\n   }\n   \n\n3. Terminate Claude Desktop and relaunch the application to register the new MCP resource.\n\n### Integration with Cursor IDE\n\nSteps to establish the MCP gateway connectivity within the Cursor IDE environment:\n\n1. Build the distributable files:\n   bash\n   npm run build\n   \n\n2. Register the server component within Cursor:\n    - Open Cursor IDE preferences/settings.\n    - Navigate to the dedicated MCP configuration section.\n    - Choose the option to 'Add new global MCP Server'.\n    - Modify the auto-generated `mcp.json` file to incorporate the following directive block:\n   \n   {\n     \"mcpServers\": {\n       \"mcp-redis-cloud\": {\n         \"command\": \"node\",\n         \"args\": [\"--experimental-fetch\", \"\u003cabsolute_path_to_project_root\u003e/dist/index.js\"],\n         \"env\": {\n           \"API_KEY\": \"\u003credis_cloud_api_key\u003e\",\n           \"SECRET_KEY\": \"\u003credis_cloud_api_secret_key\u003e\"\n         }\n       }\n     }\n   }\n   \n\n3. Restart Cursor to finalize the server initialization.\n\n\n## Development Cycle\n\n### Local Environment Requirements\n\n1. Node Version Manager (nvm) must be installed.\n2. Target Node.js Runtime: Version 22.14.0.\n3. Target npm Utility: Version 10.9.2.\n\n### Initialization Procedure\n\n1. Secure dependencies installation:\n   bash\n   nvm use v22.14.0\n   npm install\n   \n\n2. Compile the source code for execution:\n   bash\n   npm run build\n   \n\n3. Validation via the MCP Inspector tool:\n    bash\n    npx @modelcontextprotocol/inspector node dist/index.js --api-key=\u003capi_key\u003e --secret-key=\u003csecret_key\u003e\n    \n\n### Source Code Organization\n\n\nsrc/\n‚îú‚îÄ‚îÄ index.ts              # Primary application bootstrap file\n‚îú‚îÄ‚îÄ clients/              # Module housing external API integration logic\n‚îÇ   ‚îî‚îÄ‚îÄ generated         # Artifacts generated from the Redis Cloud API schema definitions\n‚îî‚îÄ‚îÄ tools/                # Implementation logic for each exposed MCP tool\n    ‚îî‚îÄ‚îÄ accounts/         # Tools related to account identity management\n    ‚îî‚îÄ‚îÄ subscriptions/    # Tools managing service entitlements\n    ‚îî‚îÄ‚îÄ tasks/            # Tools for monitoring asynchronous workflows\n\n\nCrucial Reminder: Any modification to the source code necessitates a rebuild and subsequent restart of the integrating client (Claude Desktop / Cursor):\nbash\nnpm run build\n\n\n## Containerized Operations (Docker)\n\n### Image Construction\nExecute the following command to build the distributable Docker image:\n\nbash\ndocker build -t mcp/redis-cloud .\n\n\n### Container Execution\nTo launch the service within a container instance:\n\nbash\ndocker run -i --rm \\\n  -e API_KEY=\u003cyour_redis_cloud_api_key\u003e \\\n  -e SECRET_KEY=\u003cyour_redis_cloud_api_secret_key\u003e \\\n  mcp/redis-cloud\n\n\n### Docker Integration with Claude Desktop\nTo integrate the containerized gateway with Claude Desktop:\n\n1. Ensure the Docker image is built (if not already completed).\n   bash\n   docker build -t mcp/redis-cloud .\n   \n\n2. Update the Claude Desktop server configuration:\n   - Open Claude Desktop settings and navigate to the Developer section (Developer Mode required).\n   - Select 'Edit config' and modify the `claude_desktop_config.json` file.\n   - Embed the following Docker execution configuration:\n\n   \n   {\n     \"mcpServers\": {\n       \"redis-cloud\": {\n         \"command\": \"docker\",\n         \"args\": [\n           \"run\",\n           \"-i\",\n           \"--rm\",\n           \"-e\",\n           \"API_KEY=\u003cyour_redis_cloud_api_key\u003e\",\n           \"-e\",\n           \"SECRET_KEY=\u003cyour_redis_cloud_api_secret_key\u003e\",\n           \"mcp/redis-cloud\"\n         ]\n       }\n     }\n   }\n   \n\n3. Substitute the API credential placeholders with your active credentials.\n\n4. Persist the changes and relaunch Claude Desktop to establish the connection.\n\n\n### Operational Notes\n- Validation is required to confirm that the requisite environment parameters (`API_KEY`, `SECRET_KEY`) are correctly populated and accessible to the process.",
      "stars": 36,
      "updated_at": "2025-10-02T23:00:04Z",
      "url": "https://github.com/redis/mcp-redis-cloud"
    },
    "reza-gholizade--k8s-mcp-server": {
      "category": "cloud-platforms",
      "description": "A Model Context Protocol (MCP) agent designed to facilitate bidirectional interaction with live Kubernetes clusters. It exposes standardized interfaces for resource introspection, state retrieval (logs, metrics), configuration management (create/update/delete), and advanced operational tooling like rolling restarts, all secured via non-root execution contexts.",
      "forks": 19,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "pod",
        "cloud platform",
        "cloud platforms",
        "server kubernetes"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "kube-context-agent",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "reza-gholizade",
      "readme_content": "# Kube Context Interactor Agent (k8s-mcp-server Rewrite)\n\nThis agent serves as a highly configurable Model Context Protocol (MCP) endpoint, offering robust, standardized tooling for manipulating and querying Kubernetes environments. It prioritizes operational safety by default, supporting various communication transports tailored for CLI and web integration.\n\n## Core Capabilities\n\nThis utility provides comprehensive control over K8s objects:\n\n*   **Discovery \u0026 Inquiry**: Enumerating available API schemas, fetching lists of resources (filterable by namespace/labels), and retrieving granular details or descriptive summaries (analogous to `kubectl describe`).\n*   **State Retrieval**: Accessing live container outputs (`pod logs`), fetching performance utilization data (`nodeMetrics`, `podMetrics`), and tracking cluster events.\n*   **Manipulation**: Executing imperative operations such as resource provisioning, modification via manifests (JSON/YAML), and resource deprecation (`deleteResource`).\n*   **Advanced Operations**: Includes utility functions like triggering configuration updates via `rolloutRestart` for Deployments, DaemonSets, etc.\n\n## Transport Modes \u0026 Configuration\n\nThe agent supports flexible deployment strategies:\n\n1.  **`stdio`**: Ideal for piping through standard command-line interfaces or integrating with local shell tools.\n2.  **`sse` (Server-Sent Events)**: Enables persistent, one-way real-time updates over HTTP.\n3.  **`streamable-http`**: A bi-directional streaming HTTP transport conforming strictly to the MCP specification.\n\nDeployment defaults to `sse` on port 8080 if no mode is specified.\n\n### Safety First: Read-Only Operation\n\nBy enabling the `--readonly` flag, all mutation capabilities (resource creation, updates, deletion) are immediately suppressed, confining the agent to monitoring and data retrieval functions.\n\n### Tool Scoping\n\nGranular control over exposed toolsets is possible:\n\n*   `--no-k8s`: Deactivates all native Kubernetes interaction tools.\n*   `--no-helm`: Deactivates all integrated Helm management utilities (installation, upgrades, rollbacks, etc.).\n\nAttempting to disable both scopes results in an immediate exit with an error.\n\n## Prerequisites \u0026 Building\n\nRequires Go version 1.23 or newer. Building involves standard Go tooling:\n\nbash\ngit clone [REPO_URL]\ncd k8s-mcp-server\ngo mod download\ngo build -o k8s-mcp-server main.go\n\n\n## Execution Examples\n\n**Starting in Streamable HTTP Mode:**\n\nbash\n./k8s-mcp-server --mode streamable-http --port 9999\n\n\n**Starting in Read-Only Stdio Mode:**\n\nbash\nSERVER_MODE=stdio SERVER_READONLY=true ./k8s-mcp-server\n\n\n## Docker Deployment\n\nPre-built images are available on Docker Hub (`ginnux/k8s-mcp-server:latest`). The container is secured by default, running as a non-root user (`appuser` UID 1001) and expecting cluster credentials to be mounted read-only into `/home/appuser/.kube/config`.\n\n**Standard SSE Docker Run:**\n\nbash\ndocker run -p 8080:8080 -v ~/.kube/config:/home/appuser/.kube/config:ro ginnux/k8s-mcp-server:latest\n\n\n## Integrated Tool Specifications (JSON-RPC Methods)\n\nInteraction occurs via the MCP standard JSON-RPC `tools/call` method. Below are key operations:\n\n| Tool Name | Primary Function | Key Arguments | Write Operation? |\n| :--- | :--- | :--- | :--- |\n| `getAPIResources` | Inventory of available K8s API groups/versions. | `includeNamespaceScoped`, `includeClusterScoped` | No |\n| `listResources` | Fetch multiple instances of a resource type. | `Kind`, `namespace`, `labelSelector` | No |\n| `getResource` | Fetch a single object by name/kind/namespace. | `kind`, `name`, `namespace` | No |\n| `describeResource` | Yields detailed observational summary. | `Kind`, `name`, `namespace` | No |\n| `getPodsLogs` | Stream container output logs. | `Name`, `namespace`, `containerName` | No |\n| `getNodeMetrics` | Retrieve resource usage statistics for nodes. | `Name` | No |\n| `getPodMetrics` | Retrieve CPU/Memory consumption for pods. | `namespace`, `podName` | No |\n| `getEvents` | Capture cluster or resource-specific events. | `namespace`, `resourceName` | No |\n| `createOrUpdateResource` | Provision or modify resources using JSON payload. | `manifest`, `namespace` | Yes |\n| `createOrUpdateResourceYAML` | Provision or modify resources using YAML payload. | `manifest`, `namespace` | Yes |\n| `deleteResource` | Terminate a specified object. | `kind`, `name`, `namespace` | Yes |\n| `rolloutRestart` | Force a rolling update annotation on stateful resources. | `kind`, `name`, `namespace` | Yes |\n| `getIngresses` | Fetch ingress definitions, optionally filtered by host. | `host` | No |\n\n### Helm Operations (If `--no-helm` is not set)\n\n*   `helmInstall`, `helmUpgrade`, `helmRollback`, `helmUninstall`: Standard Helm lifecycle management.\n*   `helmList`, `helmGet`, `helmHistory`: Release status retrieval.\n\n## VS Code Connectivity\n\nThis agent is optimized for seamless integration with the **Model Context Protocol (MCP) VS Code Extension**. Configuration involves setting the server command to the binary path (e.g., `k8s-mcp-server`), specifying the transport mode (typically `stdio`), and correctly mapping the `KUBECONFIG` environment variable within VS Code's `mcp.mcpServers` settings object.\n\n## Licensing and Support\n\nLicensed under the MIT License. Direct inquiries or contributions can be sent to the maintainer via gholizade.net@gmail.com. Contributions are detailed in `CONTRIBUTING.md`.",
      "stars": 97,
      "updated_at": "2025-10-03T09:48:50Z",
      "url": "https://github.com/reza-gholizade/k8s-mcp-server"
    },
    "rohitg00--kubectl-mcp-server": {
      "category": "cloud-platforms",
      "description": "An implementation of the Model Context Protocol (MCP) acting as a secure intermediary, allowing advanced generative AI agents (e.g., Claude, Cursor) to issue sophisticated, natural-language-driven commands against live Kubernetes environments.",
      "forks": 133,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "kubectl",
        "cloud",
        "cloud platforms",
        "cloud platform",
        "platform service"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "k8s-ai-bridge-server",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "rohitg00",
      "readme_content": "# Kubernetes AI Interaction Gateway (k8s-ai-bridge-server)\n\nThis utility establishes a robust Model Context Protocol (MCP) endpoint specifically tailored for interfacing with Kubernetes clusters. It translates abstract, intention-based requests from large language models (LLMs) into precise, verifiable `kubectl` operations, fundamentally bridging the gap between human intent and container orchestration.\n\n[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)\n[![Runtime: Python 3.10+](https://img.shields.io/badge/Runtime-Python%203.10%2B-orange.svg)](https://www.python.org/)\n[![Target Environment: Kubernetes](https://img.shields.io/badge/K8s-Native-326ce5.svg?style=flat\u0026logo=kubernetes\u0026logoColor=white)](https://kubernetes.io/)\n[![Protocol Compliance: MCP v0.5](https://img.shields.io/badge/MCP-v0.5%20Compliant-green.svg)](https://github.com/modelcontextprotocol/spec)\n[![Package Index Status](https://img.shields.io/pypi/v/k8s-ai-bridge.svg)](https://pypi.org/project/k8s-ai-bridge/)\n\n## üì∫ Demonstrations: AI Assistants Controlling K8s via the Bridge\n\nSee how agents interact with cluster state:\n\n*   **Watch an interaction with a major LLM agent...**\n*   **See Cursor orchestrating deployments...**\n*   **Observe Windsurf managing networking resources...**\n\n## Feature Matrix\n\n### Cluster Resource Management (kubectl Equivalents)\n\n- [x] Comprehensive CRUD operations across standard resources (Pods, Services, Deployments, etc.).\n- [x] Retrieval of diagnostic artifacts: events, detailed logs, and resource definitions.\n- [x] Advanced utility functions: port-forwarding, scaling (Deployments/StatefulSets), and exec into containers.\n- [x] Configuration artifact handling: ConfigMaps and Secrets lifecycle management.\n- [x] Full support for GitOps integration via Helm charts (install, upgrade, teardown).\n- [x] Contextual awareness: Retains namespace state across conversational turns and supports context switching.\n\n### AI / Intent Translation Layer\n\n- [x] High-fidelity translation of conversational prompts into executable K8s API calls.\n- [x] Memory module for maintaining state and dependency tracking across sequential requests.\n- [x] Semantic explanation generation for complex K8s objects or operational outcomes.\n- [x] Intelligent tool selection based on user intent, defaulting to raw `kubectl` invocation if no specialized tool matches.\n- [x] Namespace context automatically inferred or explicitly requested.\n- [x] Offline simulation capabilities using simulated cluster responses.\n\n### Observability and Health\n\n- [x] Real-time surveillance of cluster health and node resource consumption.\n- [x] Detailed tracking of container readiness/liveness probe outcomes.\n- [x] Utilization of `kubectl top` for resource metrics acquisition.\n- [x] Event stream monitoring for anomaly detection and alerting.\n- [x] Historical performance logging and analysis.\n\n### Security Posture Assessment\n\n- [x] Auditing of Role-Based Access Control (RBAC) bindings and service account permissions.\n- [x] Evaluation of Pod Security Context configurations against best practices.\n- [x] Secure handling and isolation of API credentials.\n- [x] Network Policy conformance checking.\n- [x] Automated identification of insecure defaults.\n\n### Troubleshooting and Diagnostics\n\n- [x] Automated root cause analysis for common operational failures.\n- [x] Validation checks against declared resource manifests for configuration drift.\n- [x] Proactive identification of resource bottlenecks (CPU/Memory limits).\n- [x] Detailed analysis of container startup failures (e.g., probe timeouts).\n\n### Extensibility and Transport\n\n- [x] Support for various transport mechanisms (e.g., standard I/O streams, Server-Sent Events).\n- [x] Framework designed for easy integration of new, specialized operational tools.\n- [x] Native handling of Custom Resource Definitions (CRDs).\n- [x] Efficient batch processing for bulk resource modifications.\n\n## System Architecture Overview\n\n### MCP Integration Framework\n\nThe core of this service leverages the **Model Context Protocol (MCP)** specification to provide a standardized communication channel to external LLM systems. The architecture is layered:\n\n1.  **MCP Endpoint**: The primary server component, built using a specialized SDK, ensuring broad compatibility across client applications.\n2.  **Tool Registry**: A dynamic catalog mapping high-level AI requests to executable Python functions that interface with the K8s API.\n3.  **Communication Fabric**: Handles data exchange using reliable protocols (e.g., JSON-over-Stream).\n4.  **K8s Abstraction Layer**: Responsible for securely executing calls against the Kubernetes API server.\n5.  **Response Standardization**: Ensures all output conforms strictly to the required MCP response schema.\n\n## Operational Modes\n\nThe binary supports two distinct operational patterns:\n\n1.  **Interactive CLI**: Used for direct, immediate execution of cluster management tasks from the terminal.\n2.  **Daemon Server**: Initiates an MCP listener, waiting for asynchronous requests from integrated AI assistants.\n\n## Deployment and Setup\n\nDetailed setup documentation is located in the [Development Guide](./docs/SETUP_GUIDE.md).\n\nInstallation via the Python Package Index (PyPI) is the recommended path:\n\nbash\npip install k8s-ai-bridge\n\n\nTo fetch a specific version:\n\nbash\npip install k8s-ai-bridge==2.0.0\n\n\nAccess the package details here: [PyPI Project Page](https://pypi.org/project/k8s-ai-bridge/)\n\n### Prerequisites for Operation\n\n*   Python interpreter version 3.10 or newer.\n*   The `kubectl` command-line utility must be accessible in the system PATH.\n*   Authenticated network access to the target Kubernetes cluster.\n*   The Python dependency manager (`pip`).\n\n### Standard Installation\n\nbash\n# Install the latest stable release\npip install k8s-ai-bridge\n\n# Alternatively, install the cutting-edge version directly from the repository\npip install git+https://github.com/your-org/k8s-ai-bridge-server.git\n\n\n### Local Development Setup\n\nbash\n# Clone the source repository\ngit clone https://github.com/your-org/k8s-ai-bridge-server.git\ncd k8s-ai-bridge-server\n\n# Install in editable/development mode\npip install -e .\n\n\n### Verifying Functionality\n\nConfirm the server is operational and correctly configured:\n\nbash\nk8s-ai-bridge --diagnose\n\n\n*Crucial Note*: This tool is primarily engineered as an MCP server. The main execution method is initiating the server process via `k8s-ai-bridge serve`.\n\n## Containerized Deployment\n\nA production-ready, multi-architecture Docker image is hosted on Docker Hub:\n\nbash\n# Obtain the latest build\ndocker pull yourrepo/k8s-ai-bridge-server:latest\n\n\n### Running the Server via Docker\n\nThe internal service exposes the MCP interface on port **8000**. You must map a host port and provide the Kubeconfig file:\n\nbash\n# Map host port 9090 to container port 8000\n# Mount your KUBECONFIG file so the container can authenticate to the cluster\n\ndocker run -d --name k8s-server -p 9090:8000 \\\n           -v $HOME/.kube/config:/mnt/config/kubeconfig:ro \\\n           yourrepo/k8s-ai-bridge-server:latest\n\n\n*   `-p 9090:8000`: Host accessibility on 9090.\n*   `-v ...`: Provides credentials. The server will be configured to use this path via an environment variable.\n\n### Building for Heterogeneous Architectures (Multi-Arch)\n\nUse Docker Buildx to create a manifest supporting both AMD64 and ARM64 platforms:\n\nbash\n# Setup Buildx environment if necessary\n# docker buildx create --name multiarch --use\n# docker buildx inspect --bootstrap\n\n# Build and push combined image\ndocker buildx build \n  --platform linux/amd64,linux/arm64 \n  -t yourrepo/k8s-ai-bridge-server:latest \n  --push .\n\n\n## Configuration Parameters\n\nThe MCP server requires specific access controls, particularly concerning Kubernetes configuration files. This is typically managed via container volumes or environment settings.\n\nyaml\n# Example configuration snippet for orchestration platforms (e.g., Terraform, Helm)\nrun:\n  volumes:\n    - '{{k8s-ai-bridge-server.kubeconfig_path}}:/access/kubeconfig'\nconfig:\n  description: Defines parameters required for the MCP agent to establish cluster connectivity.\n  parameters:\n    type: object\n    properties:\n      kubeconfig_path:\n        type: string\n        description: The absolute path where the kubeconfig file is mounted inside the container.\n        default: /access/kubeconfig\n    required:\n      - kubeconfig_path\n\n\nThis setup explicitly grants the server read-only access to the necessary authentication materials.\n\n## Integration Guide for AI Clients\n\n### Executing the MCP Bridge\n\nThe `k8s-ai-bridge.mcp_server` module, powered by the FastAPI-based FastMCP framework, ensures high-throughput, standards-compliant communication.\n\n\u003e **Troubleshooting Note**: If unexpected connection behavior occurs, you can temporarily switch the AI client configuration to use `k8s-ai-bridge.basic_listener` instead. The basic listener offers reduced functionality but isolates potential protocol implementation issues.\n\n1.  **Generic Client Configuration (e.g., Custom Tooling)**\n    \n    {\n      \"mcpServices\": {\n        \"clusterOps\": {\n          \"launcher\": \"python\",\n          \"invocation\": [\"-m\", \"k8s_ai_bridge.mcp_server\"],\n          \"environment\": {\n            \"KUBECONFIG_SOURCE\": \"/path/to/your/.kube/config\",\n            \"EXECUTION_LOG_PATH\": \"/var/log/agent/debug.log\",\n            \"VERBOSE_MODE\": \"True\",\n            \"MCP_SESSION_TIMEOUT\": \"300\"\n          }\n        }\n      }\n    }\n    \n\n2.  **Key Environment Variables for Fine-Tuning**\n    -   `EXECUTION_LOG_PATH`: Directs detailed operational logs away from stdout.\n    -   `VERBOSE_MODE`: Enables detailed logging (`True`/`False`).\n    -   `MCP_SESSION_TIMEOUT`: Sets the maximum duration for an active agent session (seconds).\n    -   `KUBECONFIG_SOURCE`: Specifies the location of the configuration file.\n    -   `K8S_LOG_LEVEL`: Controls verbosity: `TRACE`, `INFO`, `WARNING`, `CRITICAL`.\n\n3.  **Connectivity Check (Health Test)**\n    Verify the server is listening and responsive using a simple health check utility:\n    bash\n    python -m k8s_ai_bridge.health_check\n    \n    This performs a minimal connection handshake.\n\n### Client-Specific Integration Snippets\n\n**Claude Desktop (Configuration File: `claude_desktop_config.json`)**\n\n\n{\n  \"mcpServices\": {\n    \"clusterOps\": {\n      \"launcher\": \"python\",\n      \"invocation\": [\"-m\", \"k8s_ai_bridge.mcp_server\"], \n      \"environment\": {\n        \"KUBECONFIG_SOURCE\": \"$HOME/.kube/config\" \n      }\n    }\n  }\n}\n\n\n**Cursor AI (Global MCP Settings: `~/.cursor/mcp.json`)**\n\n\n{\n  \"mcpServices\": {\n    \"clusterOps\": {\n      \"launcher\": \"python\",\n      \"invocation\": [\"-m\", \"k8s_ai_bridge.mcp_server\"],\n      \"environment\": {\n        \"KUBECONFIG_SOURCE\": \"/srv/k8s/prod-cluster.yaml\",\n        \"EXECUTION_PATH_HINTS\": \"/usr/bin:/opt/homebrew/bin\"\n      }\n    }\n  }\n}\n\n\n**Windsurf Agent (Configuration File: `~/.config/windsurf/mcp.json`)**\n\n\n{\n  \"mcpServices\": {\n    \"clusterOps\": {\n      \"launcher\": \"python\",\n      \"invocation\": [\"-m\", \"k8s_ai_bridge.mcp_server\"],\n      \"environment\": {\n        \"KUBECONFIG_SOURCE\": \"/path/to/your/.kube/config\"\n      }\n    }\n  }\n}\n\n\n*Remember to substitute placeholder paths (`/path/to/your/.kube/config`) with the actual location accessible to the running process.*\n\n### Automated Client Configuration\n\nFor streamlined setup across supported assistants, utilize the provided setup script:\n\nbash\n./deploy/configure_clients.sh\n\nThis script attempts to inject the necessary server command details into the configuration files of recognized AI tools.",
      "stars": 726,
      "updated_at": "2025-10-03T16:43:16Z",
      "url": "https://github.com/rohitg00/kubectl-mcp-server"
    },
    "silenceper--mcp-k8s": {
      "category": "cloud-platforms",
      "description": "MCP-K8S is an AI-driven Kubernetes resource management tool that allows users to operate any resources in Kubernetes clusters through natural language interaction, including native resources (like Deployment, Service) and custom resources (CRD). No need to memorize complex commands - just describe your needs, and AI will accurately execute the corresponding cluster operations, greatly enhancing the usability of Kubernetes.",
      "forks": 24,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "silenceper",
        "cloud",
        "services silenceper",
        "kubernetes resource",
        "silenceper mcp"
      ],
      "language": "Go",
      "license": "Apache License 2.0",
      "name": "mcp-k8s",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "silenceper",
      "readme_content": "# mcp-k8s\n\n[![Go Version](https://img.shields.io/github/go-mod/go-version/silenceper/mcp-k8s)](https://github.com/silenceper/mcp-k8s/blob/main/go.mod)\n[![License](https://img.shields.io/github/license/silenceper/mcp-k8s)](https://github.com/silenceper/mcp-k8s/blob/main/LICENSE)\n[![Latest Release](https://img.shields.io/github/v/release/silenceper/mcp-k8s)](https://github.com/silenceper/mcp-k8s/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/silenceper/mcp-k8s)](https://goreportcard.com/report/github.com/silenceper/mcp-k8s)\n[![Go CI](https://github.com/silenceper/mcp-k8s/actions/workflows/go-ci.yml/badge.svg)](https://github.com/silenceper/mcp-k8s/actions/workflows/go-ci.yml)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/silenceper/mcp-k8s/pulls)\n\nA Kubernetes MCP (Model Control Protocol) server that enables interaction with Kubernetes clusters through MCP tools.\n\n## Features\n\n- Query supported Kubernetes resource types (built-in resources and CRDs)\n- Kubernetes resource operations with fine-grained control\n  - Read operations: get resource details, list resources by type with filtering options\n  - Write operations: create, update, and delete resources (each can be independently enabled/disabled)\n  - Support for all Kubernetes resource types, including custom resources\n- Connects to Kubernetes cluster using kubeconfig\n- Helm support with fine-grained control\n  - Helm releases management (list, get, install, upgrade, uninstall)\n  - Helm repositories management (list, add, remove)\n  - Each operation can be independently enabled/disabled\n\n## Preview\n\u003e Interaction through cursor\n\n\n\n## Use Cases\n\n### 1. Kubernetes Resource Management via LLM\n\n- **Interactive Resource Management**: Manage Kubernetes resources through natural language interaction with LLM, eliminating the need to memorize complex kubectl commands\n- **Batch Operations**: Describe complex batch operation requirements in natural language, letting LLM translate them into specific resource operations\n- **Resource Status Queries**: Query cluster resource status using natural language and receive easy-to-understand responses\n\n### 2. Automated Operations Scenarios\n\n- **Intelligent Operations Assistant**: Serve as an intelligent assistant for operators in daily cluster management tasks\n- **Problem Diagnosis**: Assist in cluster problem diagnosis through natural language problem descriptions\n- **Configuration Review**: Leverage LLM's understanding capabilities to help review and optimize Kubernetes resource configurations\n\n### 3. Development and Testing Support\n\n- **Quick Prototype Validation**: Developers can quickly create and validate resource configurations through natural language\n- **Environment Management**: Simplify test environment resource management, quickly create, modify, and clean up test resources\n- **Configuration Generation**: Automatically generate resource configurations that follow best practices based on requirement descriptions\n\n### 4. Education and Training Scenarios\n\n- **Interactive Learning**: Newcomers can learn Kubernetes concepts and operations through natural language interaction\n- **Best Practice Guidance**: LLM provides best practice suggestions during resource operations\n- **Error Explanation**: Provide easy-to-understand error explanations and correction suggestions when operations fail\n\n## Architecture\n\n### 1. Project Overview\n\nAn stdio-based MCP server that connects to Kubernetes clusters and provides the following capabilities:\n- Query Kubernetes resource types (including built-in resources and CRDs)\n- CRUD operations on Kubernetes resources (with configurable write operations)\n- Helm operations for release and repository management\n\n### 2. Technical Stack\n\n- Go\n- [mcp-go](https://github.com/mark3labs/mcp-go) SDK\n- Kubernetes client-go library\n- Helm v3 client library\n- Stdio for communication\n\n### 3. Core Components\n\n1. **MCP Server**: Uses mcp-go's `mcp-k8s` package to create an stdio-based MCP server\n2. **K8s Client**: Uses client-go to connect to Kubernetes clusters\n3. **Helm Client**: Uses Helm v3 library for Helm operations\n4. **Tool Implementations**: Implements various MCP tools for different Kubernetes operations\n\n### 4. Available Tools\n\n#### Resource Type Query Tools\n- `get_api_resources`: Get all supported API resource types in the cluster\n\n#### Resource Operation Tools\n- `get_resource`: Get detailed information about a specific resource\n- `list_resources`: List all instances of a resource type\n- `create_resource`: Create new resources (can be disabled)\n- `update_resource`: Update existing resources (can be disabled)\n- `delete_resource`: Delete resources (can be disabled)\n\n#### Helm Operation Tools\n- `list_helm_releases`: List all Helm releases in the cluster\n- `get_helm_release`: Get detailed information about a specific Helm release\n- `install_helm_chart`: Install a Helm chart (can be disabled)\n- `upgrade_helm_chart`: Upgrade a Helm release (can be disabled)\n- `uninstall_helm_chart`: Uninstall a Helm release (can be disabled)\n- `list_helm_repositories`: List configured Helm repositories\n- `add_helm_repository`: Add a new Helm repository (can be disabled)\n- `remove_helm_repository`: Remove a Helm repository (can be disabled)\n\n## Usage\n\nmcp-k8s supports two communication modes:\n\n### 1. Stdio Mode (Default)\n\nIn stdio mode, mcp-k8s communicates with the client through standard input/output streams. This is the default mode and is suitable for most use cases.\n\n```bash\n# Run in stdio mode (default)\n{\n    \"mcpServers\":\n    {\n        \"mcp-k8s\":\n        {\n            \"command\": \"/path/to/mcp-k8s\",\n            \"args\":\n            [\n                \"-kubeconfig\",\n                \"/path/to/kubeconfig\",\n                \"-enable-create\",\n                \"-enable-delete\",\n                \"-enable-update\",\n                \"-enable-list\",\n                \"-enable-helm-install\",\n                \"-enable-helm-upgrade\"\n            ]\n        }\n    }\n}\n```\n\n### 2. SSE Mode\n\nIn SSE (Server-Sent Events) mode, mcp-k8s exposes an HTTP endpoint to mcp client.\nYou can deploy the service on a remote server (but you need to pay attention to security)\n\n```bash\n# Run in SSE mode\n./bin/mcp-k8s -kubeconfig=/path/to/kubeconfig -transport=sse -port=8080 -host=localhost -enable-create -enable-delete -enable-list -enable-update -enable-helm-install\n# This command will open all operations\n```\n\nmcp config\n```json\n{\n  \"mcpServers\": {\n    \"mcp-k8s\": {\n      \"url\": \"http://localhost:8080/sse\",\n      \"args\": []\n    }\n  }\n}\n```\n\nSSE mode configuration:\n- `-transport`: Set to \"sse\" to enable SSE mode\n- `-port`: HTTP server port (default: 8080)\n- `--host`: HTTP server host (default: \"localhost\")\n\n### 3. Docker environment\n#### SSE Mode\n\n1. Complete Example\nAssuming your image name is mcp-k8s and you need to map ports and set environment parameters, you can run:\n```bash\ndocker run --rm -p 8080:8080 -i -v ~/.kube/config:/root/.kube/config ghcr.io/silenceper/mcp-k8s:latest -transport=sse\n```\n#### stdio Mode\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-k8s\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"-v\",\n        \"~/.kube/config:/root/.kube/config\",\n        \"--rm\",\n        \"ghcr.io/silenceper/mcp-k8s:latest\"\n      ]\n    }\n  }\n}\n```\n\n\n## Getting Started\n\n### Direct Usage\nYou can directly download the binary for your platform from the [releases page](https://github.com/silenceper/mcp-k8s/releases) and use it immediately.\n\n### Go Install\n\n```bash\ngo install github.com/silenceper/mcp-k8s/cmd/mcp-k8s@latest\n```\n\n### Build\n\n```bash\ngit clone https://github.com/silenceper/mcp-k8s.git\ncd mcp-k8s\ngo build -o bin/mcp-k8s cmd/mcp-k8s/main.go\n```\n\n### Command Line Arguments\n\n#### Kubernetes Resource Operations\n- `-kubeconfig`: Path to Kubernetes configuration file (uses default config if not specified)\n- `-enable-create`: Enable resource creation operations (default: false)\n- `-enable-update`: Enable resource update operations (default: false)\n- `-enable-delete`: Enable resource deletion operations (default: false)\n- `-enable-list`: Enable resource list operations (default: true)\n\n#### Helm Operations\n- `-enable-helm-release-list`: Enable Helm release list operations (default: true)\n- `-enable-helm-release-get`: Enable Helm release get operations (default: true)\n- `-enable-helm-install`: Enable Helm chart installation (default: false)\n- `-enable-helm-upgrade`: Enable Helm chart upgrade (default: false)\n- `-enable-helm-uninstall`: Enable Helm chart uninstallation (default: false)\n- `-enable-helm-repo-list`: Enable Helm repository list operations (default: true)\n- `-enable-helm-repo-add`: Enable Helm repository add operations (default: false)\n- `-enable-helm-repo-remove`: Enable Helm repository remove operations (default: false)\n\n#### Transport Configuration\n- `-transport`: Transport type (stdio or sse) (default: \"stdio\")\n- `-host`: Host for SSE transport (default \"localhost\")\n- `-port`: TCP port for SSE transport (default 8080)\n\n### Integration with MCP Clients\n\nmcp-k8s is an stdio-based MCP server that can be integrated with any MCP-compatible LLM client. Refer to your MCP client's documentation for integration instructions.\n\n## Security Considerations\n\n- Write operations are strictly controlled through independent configuration switches\n- Uses RBAC to ensure K8s client has only necessary permissions\n- Validates all user inputs to prevent injection attacks\n- Helm operations follow the same security principles with read operations enabled by default and write operations disabled by default\n\n## Follow WeChat Official Account",
      "stars": 126,
      "updated_at": "2025-09-30T05:55:48Z",
      "url": "https://github.com/silenceper/mcp-k8s"
    },
    "strowk--mcp-k8s-go": {
      "category": "cloud-platforms",
      "description": "Golang implementation of the Model Context Protocol (MCP) server facilitating interaction with Kubernetes clusters.",
      "forks": 49,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "cluster",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "mcp-k8s-go-bridge",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "strowk",
      "readme_content": "\u003ch4 align=\"center\"\u003eGolang-based MCP gateway interfacing with Kubernetes APIs\u003c/h4\u003e\n\n\u003ch1 align=\"center\"\u003e\n   \n   \u003cbr/\u003e\n   K8s MCP Server (Go)\n\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"#key-capabilities\"\u003eCapabilities\u003c/a\u003e ‚öô\n  \u003ca href=\"#inspector-integration\"\u003eInspect with Inspector\u003c/a\u003e ‚öô\n  \u003ca href=\"#claude-usage\"\u003eInteract via Claude\u003c/a\u003e ‚öô\n  \u003ca href=\"https://github.com/strowk/mcp-k8s-go/blob/main/CONTRIBUTING.md\"\u003eContribute ‚Üó\u003c/a\u003e ‚öô\n  \u003ca href=\"https://modelcontextprotocol.io\"\u003eAbout MCP ‚Üó\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n    \u003ca href=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/dependabot/dependabot-updates\"\u003e\u003cimg alt=\"Dependency Updates Status\" src=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/dependabot/dependabot-updates/badge.svg\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/test.yaml\"\u003e\u003cimg alt=\"Unit Test Coverage\" src=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/test.yaml/badge.svg\"\u003e\u003c/a\u003e\n\t  \u003ca href=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/golangci-lint.yaml\"\u003e\u003cimg alt=\"Linting Status\" src=\"https://github.com/strowk/mcp-k8s-go/actions/workflows/golangci-lint.yaml/badge.svg\"/\u003e\u003c/a\u003e\n    \u003cbr/\u003e\n    \u003ca href=\"https://github.com/strowk/mcp-k8s-go/releases/latest\"\u003e\u003cimg src=\"https://img.shields.io/github/v/release/strowk/mcp-k8s-go?logo=github\u0026color=90EE90\" alt=\"Latest Release Version\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://goreportcard.com/report/github.com/strowk/mcp-k8s-go\"\u003e\u003cimg src=\"https://goreportcard.com/badge/github.com/strowk/mcp-k8s-go\" alt=\"Go Quality Score\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://github.com/strowk/mcp-k8s-go/blob/main/LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/github/license/strowk/mcp-k8s-go\" alt=\"Software License\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n## Key Capabilities\n\nMCP üí¨ context input üóÇÔ∏è resource interaction ü§ñ automated execution \n\n- üóÇÔ∏èü§ñ Context Enumeration for Kubernetes environments\n- üí¨ü§ñ Namespace Discovery for current cluster setup\n- ü§ñ Comprehensive API access for any K8s resource type\n  - Includes specialized handlers for common objects like Pods, Services, Deployments\n- ü§ñ Node inventory retrieval\n- üí¨ Pod listing functionality\n- ü§ñ Retrieval of diagnostic Kubernetes Events\n- ü§ñ Extraction of container logs from specified Pods\n- ü§ñ Execution of arbitrary shell commands within a running Pod container\n\n## Inspect Integration\n\nTo leverage the most recent released artifact via the Inspector client:\n\n```bash\nnpx @modelcontextprotocol/inspector npx @strowk/mcp-k8s\n```\n\n## Interact With Claude\n\n\u003cdetails\u003e\u003csummary\u003e\u003cb\u003e\nDemonstration Snippet\n\u003c/b\u003e\u003c/summary\u003e\n\nBelow is an illustration of the workflow in Claude Desktop, where a specific cluster context is selected, and then a command is issued to examine logs for error indicators within the 'kube-system' namespace:\n\n\n\n\u003c/details\u003e\n\nTo integrate this MCP server with Claude Desktop (or alternative compliant clients), choose your preferred installation method.\n\n| Deployment Method | \u003ca href=\"#using-smithery\"\u003eSmithery\u003c/a\u003e | \u003ca href=\"#using-mcp-get\"\u003emcp-get\u003c/a\u003e | \u003ca href=\"#prebuilt-from-npm\"\u003eNPM Package\u003c/a\u003e | \u003ca href=\"#from-github-releases\"\u003eGitHub Binary\u003c/a\u003e | \u003ca href=\"#building-from-source\"\u003eSource Build\u003c/a\u003e | \u003ca href=\"#using-docker\"\u003eDocker Image\u003c/a\u003e |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Client Setup | Automatic | Automatic | Manual | Manual | Manual | Manual |\n| Dependency | Node.js | Node.js | Node.js | None | Golang Toolchain | Docker Engine |\n\n### Smithery Integration\n\nAutomated setup for Claude Desktop using [Smithery](https://smithery.ai/server/@strowk/mcp-k8s):\n\n```bash\nnpx -y @smithery/cli install @strowk/mcp-k8s --client claude\n```\n\n### mcp-get Utility\n\nAutomated installation for Claude Desktop via [mcp-get](https://mcp-get.com/packages/%40strowk%2Fmcp-k8s):\n\n```bash\nnpx @michaellatman/mcp-get@latest install @strowk/mcp-k8s\n```\n\n### Manual Installation via Binaries\n\n#### Pre-compiled npm Distribution\n\nIf Node Package Manager (npm) is available, use this for pre-built binaries:\n\n```bash\nnpm install -g @strowk/mcp-k8s\n```\n\nVerify installation with `mcp-k8s --version`. Once confirmed, update your client configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_k8s\": {\n      \"command\": \"mcp-k8s\",\n      \"args\": []\n    }\n  }\n}\n```\n\nAlternatively, execute via `npx`:\n\n```bash\nnpx @strowk/mcp-k8s\n```\n\nConfiguration example for Claude:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_k8s\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@strowk/mcp-k8s\"\n      ]\n    }\n  }\n}\n```\n\n#### Binaries from GitHub Releases\n\nObtain the latest binary package from the [GitHub releases page](https://github.com/strowk/mcp-k8s-go/releases). Unzip the archive containing the executable named `mcp-k8s-go`, place it in a directory within your system's PATH, and then configure your client config file (`claude_desktop_config.json`) as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_k8s\": {\n      \"command\": \"mcp-k8s-go\",\n      \"args\": []\n    }\n  }\n}\n```\n\n### Compilation from Source Code\n\nRequires the Golang development environment:\n\n```bash\ngo get github.com/strowk/mcp-k8s-go\ngo install github.com/strowk/mcp-k8s-go\n```\n\nThen, configure the client using the installed binary name:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_k8s_go\": {\n      \"command\": \"mcp-k8s-go\",\n      \"args\": []\n    }\n  }\n}\n```\n\n### Docker Containerization\n\nThis server image is available on Docker Hub since version 0.3.1-beta.2, supporting linux/amd64 and linux/arm64 multi-arch builds.\n\nUse the 'latest' tag, ensuring Kubeconfig volume mounting:\n\n```bash\ndocker run -i -v ~/.kube/config:/home/nonroot/.kube/config --rm mcpk8s/server:latest\n```\n\nNote for Windows users (Git Bash): You may need to translate `~/.kube/config` to a full path like `//c/Users/\u003cusername\u003e/.kube/config` for volume mapping.\n\nClient configuration example for Claude:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_k8s_go\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"-v\",\n        \"~/.kube/config:/home/nonroot/.kube/config\",\n        \"--rm\",\n        \"mcpk8s/server:latest\"\n      ]\n    }\n  }\n}\n```\n\n### Environment Variables and Runtime Arguments\n\nThe MCP server respects the following environment settings:\n\n- `KUBECONFIG`: Specifies the path to the Kubernetes configuration file (defaults to `~/.kube/config` if unset).\n\nThe server accepts these command-line flags during invocation:\n\n- `--allowed-contexts=\u003clist\u003e`: Limits operational scope to a comma-separated set of Kubernetes contexts (e.g., `dev,prod`). If omitted, all contexts are accessible.\n- `--readonly`: Activates read-only mode, blocking any operations that modify cluster state.\n- `--help`: Prints usage instructions.\n- `--version`: Outputs the current version information.\n\nExample configuration in `claude_desktop_config.json` to restrict context access and enforce read-only operations:\n\n```json\n{\n    \"mcpServers\": {\n        \"mcp_k8s\": {\n            \"command\": \"mcp-k8s\",\n            \"args\": [\n                \"--allowed-contexts=dev,prod\",\n                \"--readonly\"\n            ]\n        }\n    }\n}\n```",
      "stars": 348,
      "updated_at": "2025-10-03T16:13:05Z",
      "url": "https://github.com/strowk/mcp-k8s-go"
    },
    "thunderboltsid--mcp-nutanix": {
      "category": "cloud-platforms",
      "description": "A server implemented in Go designed to facilitate interaction between Large Language Models (LLMs) utilizing the Model Context Protocol (MCP) and backend resources managed via Nutanix Prism Central (PC).",
      "forks": 4,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "thunderboltsid",
        "nutanix",
        "platforms cloud",
        "cloud platforms",
        "cloud platform"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "mcp-nutanix-pc-connector",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "thunderboltsid",
      "readme_content": "# Nutanix Prism Central MCP Interface\n\nThis utility acts as a Model Context Protocol (MCP) gateway, enabling generative AI models to query and retrieve state information from a Nutanix Prism Central deployment.\n\n## :warning: Experimental Status Notice\n\n**BE ADVISED: THIS IS NOT PRODUCTION-READY SOFTWARE.**\n\nThis component originated as an exercise to explore Go-based implementations of the MCP framework. Key characteristics:\n\n*   It is **not** an official utility provided or supported by Nutanix.\n*   It carries **no official endorsement** or maintenance commitment from the vendor.\n*   It should **never** be deployed in mission-critical or production environments.\n*   It is supplied strictly **\"AS IS,\"** without any form of warranty or assurance regarding fitness for purpose or stability.\n\n**USER ASSUMES ALL RISK**: The creator is not liable for any operational disruption, data inconsistencies, or system failures resulting from the deployment or utilization of this source code.\n\n## Core Functionality Summary\n\nThis MCP server bridges LLMs to the Nutanix ecosystem by:\n\n1.  Establishing authenticated connectivity to a specified Prism Central instance using supplied credentials.\n2.  Enumerating inventory elements (e.g., Virtual Machines, Cluster definitions, Host hypervisors, Image repositories).\n3.  Fetching granular configuration details for entities identified by unique resource locators (URIs).\n\nThe underlying mechanism leverages the official [Prism Go Client Library](https://github.com/nutanix-cloud-native/prism-go-client) for API calls and the [MCP Go Abstraction Layer](https://github.com/mark3labs/mcp-go) for protocol adherence.\n\n## Initial Setup Guide\n\n### Prerequisites\n\n*   Go compiler version 1.23 or newer.\n*   Network access to a functional Nutanix Prism Central deployment.\n*   Standard development utilities such as `make` and `go fmt`.\n\n### Compilation Process\n\nbash\n# Obtain the source repository\ngit clone https://github.com/thunderboltsid/mcp-nutanix.git\ncd mcp-nutanix\n\n# Compile the executable server application\nmake build\n\n\n## Authentication Methods\n\nThe server accommodates two distinct credential injection strategies:\n\n1.  **Interactive Mode (Default)**: Suitable for clients like Claude that can manage sequential prompting exchanges.\n2.  **Static Configuration**: Necessary for tools (e.g., Cursor) lacking interactive prompt capabilities, relying on pre-set environment variables or configuration files.\n\n## MCP Client Integration Details\n\nConfiguration specifics depend on the consuming MCP client software.\n\n### Configuration for Claude Desktop/Code\n\nModify or establish the configuration file located at `~/.anthropic/claude_desktop.json`:\n\n\n{\n  \"mcpServers\": {\n    \"nutanix\": {\n      \"command\": \"/path/to/mcp-nutanix\"\n    }\n  }\n}\n\n\nUpon first invocation, Claude will automatically initiate the credential dialogue.\n\n### Configuration for Cursor\n\nSince Cursor prohibits interactive input, static credentials must be injected via environment mappings within `~/.cursor/mcp.json`:\n\n\n{\n  \"mcpServers\": {\n    \"nutanix\": {\n      \"command\": \"/path/to/mcp-nutanix\",\n      \"env\": {\n        \"NUTANIX_ENDPOINT\": \"your-prism-central-ip-or-hostname\",\n        \"NUTANIX_USERNAME\": \"your-username\", \n        \"NUTANIX_PASSWORD\": \"your-password\",\n        \"NUTANIX_INSECURE\": \"true\" \n      }\n    }\n  }\n}\n\n\n**Required Environment Variables:**\n- `NUTANIX_ENDPOINT`: The FQDN or IP address of the Prism Central instance (mandatory).\n- `NUTANIX_USERNAME`: The API user identifier (mandatory).\n- `NUTANIX_PASSWORD`: The corresponding secret for the API user (mandatory).\n- `NUTANIX_INSECURE`: Setting this to `\"true\"` bypasses certificate validation for self-signed CAs (optional).\n\n### Third-Party MCP Clients\n\nThis implementation strictly adheres to the established MCP specification, ensuring compatibility with any client utilizing standard stdio communication. Consult your specific client's documentation for integration steps.\n\n## Operational Use Cases\n\nAfter successful client setup and connection to the targeted Prism Central deployment, LLMs can issue requests.\n\n### Resource Index Retrieval\n\nExecute the following tool calls to fetch collection summaries:\n\n\nvms\nclusters\nhosts\nimages\nsubnets\n\n\nThe LLM payload will subsequently contain a parsable JSON array detailing the retrieved collection members.\n\n### Entity Detail Fetching\n\nTo query the specific metadata for an object, utilize its unique identifier within the corresponding resource scheme:\n\n\nvm://{uuid}\ncluster://{uuid}\nhost://{uuid}\n\n\nThe resulting response will deliver comprehensive JSON attributes pertaining to the requested entity.\n\n## Internal Development Structure\n\n### Directory Layout\n\n\nmcp-nutanix/\n‚îú‚îÄ‚îÄ bin/                  # Built executable artifacts\n‚îú‚îÄ‚îÄ internal/             # Private implementation packages\n‚îÇ   ‚îú‚îÄ‚îÄ client/           # Logic for PC API communication\n‚îÇ   ‚îú‚îÄ‚îÄ codegen/          # Utilities for automated code generation\n‚îÇ   ‚îî‚îÄ‚îÄ json/             # Specialized JSON utility functions\n‚îú‚îÄ‚îÄ pkg/                  # Publicly releasable components\n‚îÇ   ‚îú‚îÄ‚îÄ prompts/          # Handlers for MCP prompt interpretation\n‚îÇ   ‚îú‚îÄ‚îÄ resources/        # Modules responsible for resource abstraction\n‚îÇ   ‚îî‚îÄ‚îÄ tools/            # Implementations of exposed MCP tools\n‚îî‚îÄ‚îÄ Makefile              # Build targets and helper scripts\n\n\n### Code Re-generation Procedure\n\nIf modifications necessitate updating auto-generated files (e.g., interface definitions), execute the regeneration target:\n\nbash\nmake generate\n\n\n## Current Constraints\n\n*   Operation throughput is constrained by the maximum payload size defined within the MCP specification.\n*   Retrieving entities with exceptionally large attribute sets might trigger data truncation or process failure.\n*   The current feature set **lacks** support for data pagination; fetching large collections may yield incomplete results.\n*   This implementation is restricted to **read-only** actions; write operations (create, modify, delete) are not supported.\n\n## Licensing Information\n\nThis project is distributed under the terms of the MIT License (refer to the `LICENSE` file).\n\n## Acknowledgements\n\n*   Gratitude to [Nutanix](https://www.nutanix.com/) for providing the underlying Prism API.\n*   Appreciation to [Mark3Labs](https://github.com/mark3labs) for developing the foundational MCP Go library.\n*   Thanks to the [Nutanix Cloud Native](https://github.com/nutanix-cloud-native) organization for the official Prism Go Client SDK.\n\n## Contribution Policy\n\nAs this remains an experimental side project, there is no formal contribution pipeline. Suggestions, bug reports (issues), or proposed code changes (pull requests) are welcome.",
      "stars": 10,
      "updated_at": "2025-08-13T18:19:23Z",
      "url": "https://github.com/thunderboltsid/mcp-nutanix"
    },
    "trilogy-group--aws-pricing-mcp": {
      "category": "cloud-platforms",
      "description": "Get up-to-date EC2 pricing information with one call. Fast. Powered by a pre-parsed AWS pricing catalogue.",
      "forks": 8,
      "imageUrl": "",
      "keywords": [
        "cloud",
        "aws",
        "ec2",
        "cloud platforms",
        "cloud platform",
        "platforms cloud"
      ],
      "language": "Python",
      "license": "MIT License",
      "name": "aws-pricing-mcp",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "trilogy-group",
      "readme_content": "# AWS Pricing MCP\n\nA Model Context Protocol (MCP) server that provides AWS EC2 instance pricing data. This project includes both a traditional server implementation and a serverless Lambda function.\n\n## Quick Start\n\n### Lambda Deployment (Recommended)\n\nThe Lambda function provides the same functionality as the server but with serverless benefits:\n\n```bash\n# Build and deploy\nsam build\nsam deploy --guided\n\n# Get the Function URL\naws cloudformation describe-stacks \\\n  --stack-name aws-pricing-mcp \\\n  --query 'Stacks[0].Outputs[?OutputKey==`FunctionUrl`].OutputValue' \\\n  --output text\n```\n\nFor detailed Lambda documentation, see [LAMBDA.md](LAMBDA.md).\n\n### Server Deployment\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Run the server\npython src/server.py\n```\n\n## Features\n\n- **EC2 Pricing Data**: Find the cheapest EC2 instances based on specifications\n- **Multiple Pricing Models**: On Demand, Reserved Instances, CloudFix RightSpend\n- **Flexible Filtering**: Region, platform, tenancy, vCPU, RAM, GPU, etc.\n- **JSON-RPC 2.0**: Full MCP protocol compliance\n- **Serverless Option**: Lambda function with Function URL\n- **Dynamic Data**: Always up-to-date pricing from S3\n\n## Documentation\n\n- [LAMBDA.md](LAMBDA.md) - Comprehensive Lambda documentation\n- [MCP.md](MCP.md) - MCP protocol examples\n- [PRICING.md](PRICING.md) - Pricing data format and sources\n- [BUILD.md](BUILD.md) - Build instructions\n\n## License\n\n[LICENSE](LICENSE)\n",
      "stars": 16,
      "updated_at": "2025-09-22T18:33:10Z",
      "url": "https://github.com/trilogy-group/aws-pricing-mcp"
    },
    "weibaohui--k8m": {
      "category": "cloud-platforms",
      "description": "Provides MCP multi-cluster Kubernetes management and operations, featuring a management interface, logging, and nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.",
      "forks": 108,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "k8m",
        "kubernetes management",
        "cloud platform",
        "cluster kubernetes"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "k8m",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "weibaohui",
      "readme_content": "\u003cdiv align=\"center\"\u003e\n\u003ch1\u003eK8M\u003c/h1\u003e\n\u003c/div\u003e\n\n\n[English](README_en.md) | [‰∏≠Êñá](README.md)\n\n[![k8m](https://img.shields.io/badge/License-MIT-blue?style=flat-square)](https://github.com/weibaohui/k8m/blob/master/LICENSE)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/weibaohui/k8m)](https://archestra.ai/mcp-catalog/weibaohui__k8m)\n\n![Alt](https://repobeats.axiom.co/api/embed/9fde094e5c9a1d4c530e875864ee7919b17d0690.svg \"Repobeats analytics image\")\n\n**k8m** ÊòØ‰∏ÄÊ¨æAIÈ©±Âä®ÁöÑ Mini Kubernetes AI Dashboard ËΩªÈáèÁ∫ßÊéßÂà∂Âè∞Â∑•ÂÖ∑Ôºå‰∏ì‰∏∫ÁÆÄÂåñÈõÜÁæ§ÁÆ°ÁêÜËÆæËÆ°„ÄÇÂÆÉÂü∫‰∫é AMIS ÊûÑÂª∫ÔºåÂπ∂ÈÄöËøá  [\n`kom`](https://github.com/weibaohui/kom)  ‰Ωú‰∏∫ Kubernetes API ÂÆ¢Êà∑Á´ØÔºå**k8m** ÂÜÖÁΩÆ‰∫Ü\nQwen2.5-Coder-7BÔºåÊîØÊåÅdeepseek-ai/DeepSeek-R1-Distill-Qwen-7BÊ®°Âûã\nÊ®°Âûã‰∫§‰∫íËÉΩÂäõÔºåÂêåÊó∂ÊîØÊåÅÊé•ÂÖ•ÊÇ®Ëá™Â∑±ÁöÑÁßÅÊúâÂåñÂ§ßÊ®°ÂûãÔºàÂåÖÊã¨ollamaÔºâ„ÄÇ\n\n### ÊºîÁ§∫DEMO\n\n[DEMO](http://107.150.119.151:3618)\n[DEMO-InClusterÊ®°Âºè](http://107.150.119.151:31999)\nÁî®Êà∑ÂêçÂØÜÁ†Å demo/demo\n\n### ÊñáÊ°£\n\n- ËØ¶ÁªÜÁöÑÈÖçÁΩÆÂíå‰ΩøÁî®ËØ¥ÊòéËØ∑ÂèÇËÄÉ[ÊñáÊ°£](docs/README.md)„ÄÇ\n- Êõ¥Êñ∞Êó•ÂøóËØ∑ÂèÇËÄÉ[Êõ¥Êñ∞Êó•Âøó](CHANGELOG.md)„ÄÇ\n- Â¶ÇÈúÄËá™ÂÆö‰πâÂ§ßÊ®°ÂûãÂèÇÊï∞„ÄÅÈÖçÁΩÆÁßÅÊúâÂåñÂ§ßÊ®°ÂûãÔºåËØ∑ÂèÇËÄÉ[Ëá™ÊâòÁÆ°/Ëá™ÂÆö‰πâÂ§ßÊ®°ÂûãÊîØÊåÅ](docs/use-self-hosted-ai.md)\n  Âíå [OllamaÈÖçÁΩÆ](docs/ollama.md)„ÄÇ\n- ËØ¶ÁªÜÁöÑÈÖçÁΩÆÈÄâÈ°πËØ¥ÊòéËØ∑ÂèÇËÄÉ[ÈÖçÁΩÆÈÄâÈ°πËØ¥Êòé](docs/config.md)„ÄÇ\n- Êï∞ÊçÆÂ∫ìÈÖçÁΩÆËØ∑ÂèÇËÄÉ[Êï∞ÊçÆÂ∫ìÈÖçÁΩÆËØ¥Êòé](docs/database.md)„ÄÇ\n- DeepWiki ÊñáÊ°£Ôºö[ÂºÄÂèëËÆæËÆ°ÊñáÊ°£](https://deepwiki.com/weibaohui/k8m)\n\n### ‰∏ªË¶ÅÁâπÁÇπ\n\n- **Ëø∑‰Ω†ÂåñËÆæËÆ°**ÔºöÊâÄÊúâÂäüËÉΩÊï¥ÂêàÂú®‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂèØÊâßË°åÊñá‰ª∂‰∏≠ÔºåÈÉ®ÁΩ≤‰æøÊç∑Ôºå‰ΩøÁî®ÁÆÄÂçï„ÄÇ\n- **ÁÆÄ‰æøÊòìÁî®**ÔºöÂèãÂ•ΩÁöÑÁî®Êà∑ÁïåÈù¢ÂíåÁõ¥ËßÇÁöÑÊìç‰ΩúÊµÅÁ®ãÔºåËÆ© Kubernetes ÁÆ°ÁêÜÊõ¥Âä†ËΩªÊùæ„ÄÇÊîØÊåÅÊ†áÂáÜk8s„ÄÅaws eks„ÄÅk3s„ÄÅkind„ÄÅk0sÁ≠âÈõÜÁæ§Á±ªÂûã„ÄÇ\n- **È´òÊïàÊÄßËÉΩ**ÔºöÂêéÁ´ØÈááÁî® Golang ÊûÑÂª∫ÔºåÂâçÁ´ØÂü∫‰∫éÁôæÂ∫¶ AMISÔºå‰øùËØÅËµÑÊ∫êÂà©Áî®ÁéáÈ´ò„ÄÅÂìçÂ∫îÈÄüÂ∫¶Âø´„ÄÇ\n- **AIÈ©±Âä®ËûçÂêà**\n  ÔºöÂü∫‰∫éChatGPTÂÆûÁé∞ÂàíËØçËß£Èáä„ÄÅËµÑÊ∫êÊåáÂçó„ÄÅYAMLÂ±ûÊÄßËá™Âä®ÁøªËØë„ÄÅDescribe‰ø°ÊÅØËß£ËØª„ÄÅÊó•ÂøóAIÈóÆËØä„ÄÅËøêË°åÂëΩ‰ª§Êé®Ëçê,Âπ∂ÈõÜÊàê‰∫Ü[k8s-gpt](https://github.com/k8sgpt-ai/k8sgpt)\n  ÂäüËÉΩÔºåÂÆûÁé∞‰∏≠ÊñáÂ±ïÁé∞Ôºå‰∏∫ÁÆ°ÁêÜk8sÊèê‰æõÊô∫ËÉΩÂåñÊîØÊåÅ„ÄÇ\n- **MCPÈõÜÊàê**:ÂèØËßÜÂåñÁÆ°ÁêÜMCPÔºåÂÆûÁé∞Â§ßÊ®°ÂûãË∞ÉÁî®ToolsÔºåÂÜÖÁΩÆk8sÂ§öÈõÜÁæ§MCPÂ∑•ÂÖ∑49ÁßçÔºåÂèØÁªÑÂêàÂÆûÁé∞Ë∂ÖÁôæÁßçÈõÜÁæ§Êìç‰ΩúÔºåÂèØ‰Ωú‰∏∫MCP Server\n  ‰æõÂÖ∂‰ªñÂ§ßÊ®°ÂûãËΩØ‰ª∂‰ΩøÁî®„ÄÇËΩªÊùæÂÆûÁé∞Â§ßÊ®°ÂûãÁÆ°ÁêÜk8s„ÄÇÂèØËØ¶ÁªÜËÆ∞ÂΩïÊØè‰∏ÄÊ¨°MCPË∞ÉÁî®„ÄÇÊîØÊåÅmcp.so‰∏ªÊµÅÊúçÂä°„ÄÇ\n- **MCPÊùÉÈôêÊâìÈÄö**:Â§öÈõÜÁæ§ÁÆ°ÁêÜÊùÉÈôê‰∏éMCPÂ§ßÊ®°ÂûãË∞ÉÁî®ÊùÉÈôêÊâìÈÄöÔºå‰∏ÄÂè•ËØùÊ¶ÇËø∞ÔºöË∞Å‰ΩøÁî®Â§ßÊ®°ÂûãÔºåÂ∞±Áî®Ë∞ÅÁöÑÊùÉÈôêÊâßË°åMCP„ÄÇÂÆâÂÖ®‰ΩøÁî®ÔºåÊó†ÂêéÈ°æ‰πãÂøßÔºåÈÅøÂÖçÊìç‰ΩúË∂äÊùÉ„ÄÇ\n- **Â§öÈõÜÁæ§ÁÆ°ÁêÜ**ÔºöËá™Âä®ËØÜÂà´ÈõÜÁæ§ÂÜÖÈÉ®‰ΩøÁî®InClusterÊ®°ÂºèÔºåÈÖçÁΩÆkubeconfigË∑ØÂæÑÂêéËá™Âä®Êâ´ÊèèÂêåÁ∫ßÁõÆÂΩï‰∏ãÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂêåÊó∂Ê≥®ÂÜåÁÆ°ÁêÜÂ§ö‰∏™ÈõÜÁæ§„ÄÇ\n- **Â§öÈõÜÁæ§ÊùÉÈôêÁÆ°ÁêÜ**ÔºöÊîØÊåÅÂØπÁî®Êà∑„ÄÅÁî®Êà∑ÁªÑËøõË°åÊéàÊùÉÔºåÂèØÊåâÈõÜÁæ§ÊéàÊùÉÔºåÂåÖÊã¨ÈõÜÁæ§Âè™ËØª„ÄÅExecÂëΩ‰ª§„ÄÅÈõÜÁæ§ÁÆ°ÁêÜÂëò‰∏âÁßçÊùÉÈôê„ÄÇÂØπÁî®Êà∑ÁªÑÊéàÊùÉÂêéÔºåÁªÑÂÜÖÁî®Êà∑ÂùáËé∑ÂæóÁõ∏Â∫îÊéàÊùÉ„ÄÇÊîØÊåÅËÆæÁΩÆÂëΩÂêçÁ©∫Èó¥ÈªëÁôΩÂêçÂçï„ÄÇ\n- **ÊîØÊåÅk8sÊúÄÊñ∞ÁâπÊÄß**:ÊîØÊåÅAPIGateway„ÄÅOpenKruiseÁ≠âÂäüËÉΩÁâπÊÄß„ÄÇ\n- **Pod Êñá‰ª∂ÁÆ°ÁêÜ**ÔºöÊîØÊåÅ Pod ÂÜÖÊñá‰ª∂ÁöÑÊµèËßà„ÄÅÁºñËæë„ÄÅ‰∏ä‰º†„ÄÅ‰∏ãËΩΩ„ÄÅÂà†Èô§ÔºåÁÆÄÂåñÊó•Â∏∏Êìç‰Ωú„ÄÇ\n- **Pod ËøêË°åÁÆ°ÁêÜ**ÔºöÊîØÊåÅÂÆûÊó∂Êü•Áúã Pod Êó•ÂøóÔºå‰∏ãËΩΩÊó•ÂøóÔºåÂπ∂Âú® Pod ÂÜÖÁõ¥Êé•ÊâßË°å Shell ÂëΩ‰ª§„ÄÇÊîØÊåÅgrep -A -BÈ´ò‰∫ÆÊêúÁ¥¢\n- **APIÂºÄÊîæ**:ÊîØÊåÅÂàõÂª∫API KEYÔºå‰ªéÁ¨¨‰∏âÊñπÂ§ñÈÉ®ËÆøÈóÆÔºåÊèê‰æõswaggerÊé•Âè£ÁÆ°ÁêÜÈ°µÈù¢„ÄÇ\n- **ÈõÜÁæ§Â∑°Ê£ÄÊîØÊåÅ**ÔºöÊîØÊåÅÂÆöÊó∂Â∑°Ê£Ä„ÄÅËá™ÂÆö‰πâÂ∑°Ê£ÄËßÑÂàôÔºåÊîØÊåÅluaËÑöÊú¨ËßÑÂàô„ÄÇÊîØÊåÅÂèëÈÄÅÂà∞ÈíâÈíâÁæ§„ÄÅÂæÆ‰ø°Áæ§„ÄÅÈ£û‰π¶Áæ§„ÄÇ\n- **CRD ÁÆ°ÁêÜ**ÔºöÂèØËá™Âä®ÂèëÁé∞Âπ∂ÁÆ°ÁêÜ CRD ËµÑÊ∫êÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéá„ÄÇ\n- **Helm Â∏ÇÂú∫**ÔºöÊîØÊåÅHelmËá™Áî±Ê∑ªÂä†‰ªìÂ∫ìÔºå‰∏ÄÈîÆÂÆâË£Ö„ÄÅÂç∏ËΩΩ„ÄÅÂçáÁ∫ß Helm Â∫îÁî®ÔºåÊîØÊåÅËá™Âä®Êõ¥Êñ∞„ÄÇ\n- **Ë∑®Âπ≥Âè∞ÊîØÊåÅ**ÔºöÂÖºÂÆπ Linux„ÄÅmacOS Âíå WindowsÔºåÂπ∂ÊîØÊåÅ x86„ÄÅARM Á≠âÂ§öÁßçÊû∂ÊûÑÔºåÁ°Æ‰øùÂ§öÂπ≥Âè∞Êó†ÁºùËøêË°å„ÄÇ\n- **Â§öÊï∞ÊçÆÂ∫ìÊîØÊåÅ**ÔºöÊîØÊåÅSQLite„ÄÅMySql„ÄÅPostgreSqlÁ≠âÂ§öÁßçÊï∞ÊçÆÂ∫ì„ÄÇ\n- **ÂÆåÂÖ®ÂºÄÊ∫ê**ÔºöÂºÄÊîæÊâÄÊúâÊ∫êÁ†ÅÔºåÊó†‰ªª‰ΩïÈôêÂà∂ÔºåÂèØËá™Áî±ÂÆöÂà∂ÂíåÊâ©Â±ïÔºåÂèØÂïÜ‰∏ö‰ΩøÁî®„ÄÇ\n\n**k8m** ÁöÑËÆæËÆ°ÁêÜÂøµÊòØ‚ÄúAIÈ©±Âä®ÔºåËΩª‰æøÈ´òÊïàÔºåÂåñÁπÅ‰∏∫ÁÆÄ‚ÄùÔºåÂÆÉÂ∏ÆÂä©ÂºÄÂèëËÄÖÂíåËøêÁª¥‰∫∫ÂëòÂø´ÈÄü‰∏äÊâãÔºåËΩªÊùæÁÆ°ÁêÜ Kubernetes ÈõÜÁæ§„ÄÇ\n\n![0951d6c1_389c_49cb_b247_84de15b6ec0e](https://github.com/user-attachments/assets/0951d6c1-389c-49cb-b247-84de15b6ec0e)\n\n## **ËøêË°å**\n\n1. **‰∏ãËΩΩ**Ôºö‰ªé [GitHub release](https://github.com/weibaohui/k8m/releases) ‰∏ãËΩΩÊúÄÊñ∞ÁâàÊú¨„ÄÇ\n2. **ËøêË°å**Ôºö‰ΩøÁî® `./k8m` ÂëΩ‰ª§ÂêØÂä®,ËÆøÈóÆ[http://127.0.0.1:3618](http://127.0.0.1:3618)„ÄÇ\n3. **ÁôªÂΩïÁî®Êà∑ÂêçÂØÜÁ†Å**Ôºö\n    - Áî®Êà∑ÂêçÔºö`k8m`\n    - ÂØÜÁ†ÅÔºö`k8m`\n    - ËØ∑Ê≥®ÊÑè‰∏äÁ∫øÂêé‰øÆÊîπÁî®Êà∑ÂêçÂØÜÁ†Å„ÄÅÂêØÁî®‰∏§Ê≠•È™åËØÅ„ÄÇ\n4. **ÂèÇÊï∞**Ôºö\n\n```shell\nUsage of ./k8m:\n      --enable-temp-admin                ÊòØÂê¶ÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÔºåÈªòËÆ§ÂÖ≥Èó≠\n      --admin-password string            ÁÆ°ÁêÜÂëòÂØÜÁ†ÅÔºåÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÂêéÁîüÊïà \n      --admin-username string            ÁÆ°ÁêÜÂëòÁî®Êà∑ÂêçÔºåÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÂêéÁîüÊïà\n      --print-config                     ÊòØÂê¶ÊâìÂç∞ÈÖçÁΩÆ‰ø°ÊÅØ (default false)\n      --connect-cluster                  ÂêØÂä®ÈõÜÁæ§ÊòØÊòØÂê¶Ëá™Âä®ËøûÊé•Áé∞ÊúâÈõÜÁæ§ÔºåÈªòËÆ§ÂÖ≥Èó≠\n  -d, --debug                            Ë∞ÉËØïÊ®°Âºè\n      --in-cluster                       ÊòØÂê¶Ëá™Âä®Ê≥®ÂÜåÁ∫≥ÁÆ°ÂÆø‰∏ªÈõÜÁæ§ÔºåÈªòËÆ§ÂêØÁî®\n      --jwt-token-secret string          ÁôªÂΩïÂêéÁîüÊàêJWT token ‰ΩøÁî®ÁöÑSecret (default \"your-secret-key\")\n  -c, --kubeconfig string                kubeconfigÊñá‰ª∂Ë∑ØÂæÑ (default \"/root/.kube/config\")\n      --kubectl-shell-image string       Kubectl Shell ÈïúÂÉè„ÄÇÈªòËÆ§‰∏∫ bitnami/kubectl:latestÔºåÂøÖÈ°ªÂåÖÂê´kubectlÂëΩ‰ª§ (default \"bitnami/kubectl:latest\")\n      --log-v int                        klogÁöÑÊó•ÂøóÁ∫ßÂà´klog.V(2) (default 2)\n      --login-type string                ÁôªÂΩïÊñπÂºèÔºåpassword, oauth, tokenÁ≠â,default is password (default \"password\")\n      --image-pull-timeout               Node Shell„ÄÅKubectl Shell ÈïúÂÉèÊãâÂèñË∂ÖÊó∂Êó∂Èó¥„ÄÇÈªòËÆ§‰∏∫ 30 Áßí\n      --node-shell-image string          NodeShell ÈïúÂÉè„ÄÇ ÈªòËÆ§‰∏∫ alpine:latestÔºåÂøÖÈ°ªÂåÖÂê´`nsenter`ÂëΩ‰ª§ (default \"alpine:latest\")\n  -p, --port int                         ÁõëÂê¨Á´ØÂè£ (default 3618)\n  -v, --v Level                          klogÁöÑÊó•ÂøóÁ∫ßÂà´ (default 2)\n```\n\n‰πüÂèØ‰ª•Áõ¥Êé•ÈÄöËøádocker-compose(Êé®Ëçê)ÂêØÂä®Ôºö\n\n```yaml\nservices:\n  k8m:\n    container_name: k8m\n    image: registry.cn-hangzhou.aliyuncs.com/minik8m/k8m\n    restart: always\n    ports:\n      - \"3618:3618\"\n    environment:\n      TZ: Asia/Shanghai\n    volumes:\n      - ./data:/app/data\n```\n\nÂêØÂä®‰πãÂêéÔºåËÆøÈóÆ`3618`Á´ØÂè£ÔºåÈªòËÆ§Áî®Êà∑Ôºö`k8m`ÔºåÈªòËÆ§ÂØÜÁ†Å`k8m`„ÄÇ\nÂ¶ÇÊûú‰Ω†ÊÉ≥ÈÄöËøáÂú®Á∫øÁéØÂ¢ÉÂø´ÈÄüÊãâËµ∑‰ΩìÈ™åÔºåÂèØ‰ª•ËÆøÈóÆÔºö[k8m](https://cnb.cool/znb/qifei/-/tree/main/letsfly/justforfun/k8m)\n\n## **ChatGPT ÈÖçÁΩÆÊåáÂçó**\n\n### ÂÜÖÁΩÆGPT\n\n‰ªév0.0.8ÁâàÊú¨ÂºÄÂßãÔºåÂ∞ÜÂÜÖÁΩÆGPTÔºåÊó†ÈúÄÈÖçÁΩÆ„ÄÇ\nÂ¶ÇÊûúÊÇ®ÈúÄË¶Å‰ΩøÁî®Ëá™Â∑±ÁöÑGPTÔºåËØ∑ÂèÇËÄÉ‰ª•‰∏ãÊñáÊ°£„ÄÇ\n\n- [Ëá™ÊâòÁÆ°/Ëá™ÂÆö‰πâÂ§ßÊ®°ÂûãÊîØÊåÅ](use-self-hosted-ai.md) - Â¶Ç‰Ωï‰ΩøÁî®Ëá™ÊâòÁÆ°ÁöÑ\n- [OllamaÈÖçÁΩÆ](ollama.md) - Â¶Ç‰ΩïÈÖçÁΩÆ‰ΩøÁî®OllamaÂ§ßÊ®°Âûã„ÄÇ\n\n### **ChatGPT Áä∂ÊÄÅË∞ÉËØï**\n\nÂ¶ÇÊûúËÆæÁΩÆÂèÇÊï∞ÂêéÔºå‰æùÁÑ∂Ê≤°ÊúâÊïàÊûúÔºåËØ∑Â∞ùËØï‰ΩøÁî®`./k8m -v 6`Ëé∑ÂèñÊõ¥Â§öÁöÑË∞ÉËØï‰ø°ÊÅØ„ÄÇ\n‰ºöËæìÂá∫‰ª•‰∏ã‰ø°ÊÅØÔºåÈÄöËøáÊü•ÁúãÊó•ÂøóÔºåÁ°ÆËÆ§ÊòØÂê¶ÂêØÁî®ChatGPT„ÄÇ\n\n```go\nChatGPT ÂºÄÂêØÁä∂ÊÄÅ:true\nChatGPT ÂêØÁî® key:sk-hl**********************************************, url:https: // api.siliconflow.cn/v1\nChatGPT ‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè‰∏≠ËÆæÁΩÆÁöÑÊ®°Âûã:Qwen/Qwen2.5-7B-Instruc\n```\n\n### **ChatGPT Ë¥¶Êà∑**\n\nÊú¨È°πÁõÆÈõÜÊàê‰∫Ü[github.com/sashabaranov/go-openai](https://github.com/sashabaranov/go-openai)SDK„ÄÇ\nÂõΩÂÜÖËÆøÈóÆÊé®Ëçê‰ΩøÁî®[Á°ÖÂü∫ÊµÅÂä®](https://cloud.siliconflow.cn/)ÁöÑÊúçÂä°„ÄÇ\nÁôªÂΩïÂêéÔºåÂú®[https://cloud.siliconflow.cn/account/ak](https://cloud.siliconflow.cn/account/ak)ÂàõÂª∫API_KEY\n\n## **k8m ÊîØÊåÅÁéØÂ¢ÉÂèòÈáèËÆæÁΩÆ**\n\nk8m ÊîØÊåÅÈÄöËøáÁéØÂ¢ÉÂèòÈáèÂíåÂëΩ‰ª§Ë°åÂèÇÊï∞ÁÅµÊ¥ªÈÖçÁΩÆÔºå‰∏ªË¶ÅÂèÇÊï∞Â¶Ç‰∏ãÔºö\n\n| ÁéØÂ¢ÉÂèòÈáè              | ÈªòËÆ§ÂÄº                   | ËØ¥Êòé                                                           |\n| --------------------- | ------------------------ | -------------------------------------------------------------- |\n| `PORT`                | `3618`                   | ÁõëÂê¨ÁöÑÁ´ØÂè£Âè∑                                                   |\n| `KUBECONFIG`          | `~/.kube/config`         | `kubeconfig` Êñá‰ª∂Ë∑ØÂæÑÔºå‰ºöËá™Âä®Êâ´ÊèèËØÜÂà´ÂêåÁ∫ßÁõÆÂΩï‰∏ãÊâÄÊúâÁöÑÈÖçÁΩÆÊñá‰ª∂  |\n| `ANY_SELECT`          | `\"true\"`                 | ÊòØÂê¶ÂºÄÂêØ‰ªªÊÑèÈÄâÊã©ÂàíËØçËß£ÈáäÔºåÈªòËÆ§ÂºÄÂêØ (default true)              |\n| `LOGIN_TYPE`          | `\"password\"`             | ÁôªÂΩïÊñπÂºèÔºàÂ¶Ç `password`, `oauth`, `token`Ôºâ                    |\n| `ENABLE_TEMP_ADMIN`   | `\"false\"`                | ÊòØÂê¶ÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÔºåÈªòËÆ§ÂÖ≥Èó≠„ÄÇÂàùÊ¨°ÁôªÂΩï„ÄÅÂøòËÆ∞ÂØÜÁ†ÅÊó∂‰ΩøÁî® |\n| `ADMIN_USERNAME`      |                          | ÁÆ°ÁêÜÂëòÁî®Êà∑ÂêçÔºåÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÂêéÁîüÊïà                     |\n| `ADMIN_PASSWORD`      |                          | ÁÆ°ÁêÜÂëòÂØÜÁ†ÅÔºåÂêØÁî®‰∏¥Êó∂ÁÆ°ÁêÜÂëòË¥¶Êà∑ÈÖçÁΩÆÂêéÁîüÊïà                       |\n| `DEBUG`               | `\"false\"`                | ÊòØÂê¶ÂºÄÂêØ `debug` Ê®°Âºè                                          |\n| `LOG_V`               | `\"2\"`                    | logËæìÂá∫Êó•ÂøóÔºåÂêåklogÁî®Ê≥ï                                        |\n| `JWT_TOKEN_SECRET`    | `\"your-secret-key\"`      | Áî®‰∫é JWT Token ÁîüÊàêÁöÑÂØÜÈí•                                      |\n| `KUBECTL_SHELL_IMAGE` | `bitnami/kubectl:latest` | kubectl shell ÈïúÂÉèÂú∞ÂùÄ                                         |\n| `NODE_SHELL_IMAGE`    | `alpine:latest`          | Node shell ÈïúÂÉèÂú∞ÂùÄ                                            |\n| `IMAGE_PULL_TIMEOUT`  | `30`                     | Node shell„ÄÅkubectl shell ÈïúÂÉèÊãâÂèñË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ               |\n| `CONNECT_CLUSTER`     | `\"false\"`                | ÂêØÂä®Á®ãÂ∫èÂêéÔºåÊòØÂê¶Ëá™Âä®ËøûÊé•ÂèëÁé∞ÁöÑÈõÜÁæ§ÔºåÈªòËÆ§ÂÖ≥Èó≠                   |\n| `PRINT_CONFIG`        | `\"false\"`                | ÊòØÂê¶ÊâìÂç∞ÈÖçÁΩÆ‰ø°ÊÅØ                                               |\n\nËØ¶ÁªÜÂèÇÊï∞ËØ¥ÊòéÂíåÊõ¥Â§öÈÖçÁΩÆÊñπÂºèËØ∑ÂèÇËÄÉ [docs/readme.md](docs/README.md)„ÄÇ\n\nËøô‰∫õÁéØÂ¢ÉÂèòÈáèÂèØ‰ª•ÈÄöËøáÂú®ËøêË°åÂ∫îÁî®Á®ãÂ∫èÊó∂ËÆæÁΩÆÔºå‰æãÂ¶ÇÔºö\n\n```sh\nexport PORT=8080\nexport GIN_MODE=\"release\"\n./k8m\n```\n\nÂÖ∂‰ªñÂèÇÊï∞ËØ∑ÂèÇËÄÉ [docs/readme.md](docs/README.md)„ÄÇ\n\n## ÂÆπÂô®Âåñk8sÈõÜÁæ§ÊñπÂºèËøêË°å\n\n‰ΩøÁî®[KinD](https://kind.sigs.k8s.io/docs/user/quick-start/)„ÄÅ[MiniKube](https://minikube.sigs.k8s.io/docs/start/)\nÂÆâË£Ö‰∏Ä‰∏™Â∞èÂûãk8sÈõÜÁæ§\n\n## KinDÊñπÂºè\n\n* ÂàõÂª∫ KinD Kubernetes ÈõÜÁæ§\n\n```\nbrew install kind\n```\n\n* ÂàõÂª∫Êñ∞ÁöÑ Kubernetes ÈõÜÁæ§Ôºö\n\n```\nkind create cluster --name k8sgpt-demo\n```\n\n## Â∞Ük8mÈÉ®ÁΩ≤Âà∞ÈõÜÁæ§‰∏≠‰ΩìÈ™å\n\n### ÂÆâË£ÖËÑöÊú¨\n\n```docker\nkubectl apply -f https://raw.githubusercontent.com/weibaohui/k8m/refs/heads/main/deploy/k8m.yaml\n```\n\n* ËÆøÈóÆÔºö\n  ÈªòËÆ§‰ΩøÁî®‰∫ÜnodePortÂºÄÊîæÔºåËØ∑ËÆøÈóÆ31999Á´ØÂè£„ÄÇÊàñËá™Ë°åÈÖçÁΩÆIngress\n  http://NodePortIP:31999\n\n### ‰øÆÊîπÈÖçÁΩÆ\n\nÈ¶ñÈÄâÂª∫ËÆÆÈÄöËøá‰øÆÊîπÁéØÂ¢ÉÂèòÈáèÊñπÂºèËøõË°å‰øÆÊîπ„ÄÇ ‰æãÂ¶ÇÂ¢ûÂä†deploy.yaml‰∏≠ÁöÑenvÂèÇÊï∞\n\n## ÂºÄÂèëË∞ÉËØï\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥Âú®Êú¨Âú∞ÂºÄÂèëË∞ÉËØïÔºåËØ∑ÂÖàÊâßË°å‰∏ÄÊ¨°Êú¨Âú∞ÂâçÁ´ØÊûÑÂª∫ÔºåËá™Âä®ÁîüÊàêdistÁõÆÂΩï„ÄÇÂõ†‰∏∫Êú¨È°πÁõÆÈááÁî®‰∫Ü‰∫åËøõÂà∂ÂµåÂÖ•ÔºåÊ≤°ÊúâdistÂâçÁ´Ø‰ºöÊä•Èîô„ÄÇ\n\n#### Á¨¨‰∏ÄÊ≠•ÁºñËØëÂâçÁ´Ø\n\n```bash \ncd ui\npnpm run build\n```\n\n#### ÁºñËØëË∞ÉËØïÂêéÁ´Ø\n\n```bash\n#‰∏ãËΩΩ‰æùËµñ\ngo mod tidy\n#ËøêË°å\nair\n#ÊàñËÄÖ\ngo run main.go \n# ÁõëÂê¨localhost:3618Á´ØÂè£\n```\n\n#### ÂâçÁ´ØÁÉ≠Âä†ËΩΩ\n\n```bash\ncd ui\npnpm run dev\n#ViteÊúçÂä°‰ºöÁõëÂê¨Âú®localhost:3000Á´ØÂè£\n#ViteËΩ¨ÂèëÂêéÁ´ØËÆøÈóÆÂà∞3618Á´ØÂè£\n```\n\nËÆøÈóÆhttp://localhost:3000\n\n### HELP \u0026 SUPPORT\n\nÂ¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïËøõ‰∏ÄÊ≠•ÁöÑÈóÆÈ¢òÊàñÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂ∏ÆÂä©ÔºåËØ∑ÈöèÊó∂‰∏éÊàëËÅîÁ≥ªÔºÅ\n\n### ÁâπÂà´È∏£Ë∞¢\n\n[zhaomingcheng01](https://github.com/zhaomingcheng01)ÔºöÊèêÂá∫‰∫ÜËØ∏Â§öÈùûÂ∏∏È´òË¥®ÈáèÁöÑÂª∫ËÆÆÔºå‰∏∫k8mÁöÑÊòìÁî®Â•ΩÁî®ÂÅöÂá∫‰∫ÜÂçìË∂äË¥°ÁåÆ~\n\n[La0jin](https://github.com/La0jin):Êèê‰æõÂú®Á∫øËµÑÊ∫êÂèäÁª¥Êä§ÔºåÊûÅÂ§ßÊèêÂçá‰∫Ük8mÁöÑÂ±ïÁ§∫ÊïàÊûú\n\n[eryajf](https://github.com/eryajf):‰∏∫Êàë‰ª¨Êèê‰æõ‰∫ÜÈùûÂ∏∏Â•ΩÁî®ÁöÑgithub actionsÔºå‰∏∫k8mÂ¢ûÂä†‰∫ÜËá™Âä®ÂåñÁöÑÂèëÁâà„ÄÅÊûÑÂª∫„ÄÅÂèëÂ∏ÉÁ≠âÂäüËÉΩ\n\n## ËÅîÁ≥ªÊàë\n\nÂæÆ‰ø°ÔºàÂ§ßÁΩóÈ©¨ÁöÑÂ§™Èò≥Ôºâ ÊêúÁ¥¢IDÔºödaluomadetaiyang,Â§áÊ≥®k8m„ÄÇ\n\u003cbr\u003e\u003cimg width=\"214\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/166db141-42c5-42c4-9964-8e25cf12d04c\" /\u003e\n\n## ÂæÆ‰ø°Áæ§\n![ËæìÂÖ•ÂõæÁâáËØ¥Êòé](https://foruda.gitee.com/images/1759234568793520473/ccfc8a18_77493.png \"Â±èÂπïÊà™Âõæ\")\n## QQÁæ§\n![ËæìÂÖ•ÂõæÁâáËØ¥Êòé](https://foruda.gitee.com/images/1753099785542398999/184a765a_77493.png \"Â±èÂπïÊà™Âõæ\")\n\n",
      "stars": 672,
      "updated_at": "2025-10-02T02:02:09Z",
      "url": "https://github.com/weibaohui/k8m"
    },
    "weibaohui--kom": {
      "category": "cloud-platforms",
      "description": "Provides MCP multi-cluster Kubernetes management and operations. It can be integrated as an SDK into your own project and includes nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.",
      "forks": 28,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "kom",
        "cloud platform",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "Go",
      "license": "MIT License",
      "name": "kom",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "weibaohui",
      "readme_content": "# Kom - Kubernetes Operations Manager\n\n[English](README_en.md) | [‰∏≠Êñá](README.md)\n[![kom](https://img.shields.io/badge/License-MIT-blue?style=flat-square)](https://github.com/weibaohui/kom/blob/master/LICENSE)\n\n\n## ÁÆÄ‰ªã\n\n`kom` ÊòØ‰∏Ä‰∏™Áî®‰∫é Kubernetes Êìç‰ΩúÁöÑÂ∑•ÂÖ∑ÔºåÁõ∏ÂΩì‰∫éSDKÁ∫ßÁöÑkubectl„ÄÅclient-goÁöÑ‰ΩøÁî®Â∞ÅË£Ö„ÄÇ\nÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÂäüËÉΩÊù•ÁÆ°ÁêÜ Kubernetes ËµÑÊ∫êÔºåÂåÖÊã¨ÂàõÂª∫„ÄÅÊõ¥Êñ∞„ÄÅÂà†Èô§ÂíåËé∑ÂèñËµÑÊ∫ê„ÄÇËøô‰∏™È°πÁõÆÊîØÊåÅÂ§öÁßç Kubernetes ËµÑÊ∫êÁ±ªÂûãÁöÑÊìç‰ΩúÔºåÂπ∂ËÉΩÂ§üÂ§ÑÁêÜËá™ÂÆö‰πâËµÑÊ∫êÂÆö‰πâÔºàCRDÔºâ„ÄÇ\nÈÄöËøá‰ΩøÁî® `kom`Ôºå‰Ω†ÂèØ‰ª•ËΩªÊùæÂú∞ËøõË°åËµÑÊ∫êÁöÑÂ¢ûÂà†ÊîπÊü•ÂíåÊó•ÂøóËé∑Âèñ‰ª•ÂèäÊìç‰ΩúPODÂÜÖÊñá‰ª∂Á≠âÂä®‰ΩúÔºåÁîöËá≥ÂèØ‰ª•‰ΩøÁî®SQLËØ≠Âè•Êù•Êü•ËØ¢„ÄÅÁÆ°ÁêÜk8sËµÑÊ∫ê„ÄÇ\n\n## **ÁâπÁÇπ**\n1. ÁÆÄÂçïÊòìÁî®Ôºökom Êèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÂäüËÉΩÔºåÂåÖÊã¨ÂàõÂª∫„ÄÅÊõ¥Êñ∞„ÄÅÂà†Èô§„ÄÅËé∑Âèñ„ÄÅÂàóË°®Á≠âÔºåÂåÖÊã¨ÂØπÂÜÖÁΩÆËµÑÊ∫ê‰ª•ÂèäCRDËµÑÊ∫êÁöÑÊìç‰Ωú„ÄÇ\n2. Â§öÈõÜÁæ§ÊîØÊåÅÔºöÈÄöËøáRegisterClusterÔºå‰Ω†ÂèØ‰ª•ËΩªÊùæÂú∞ÁÆ°ÁêÜÂ§ö‰∏™ Kubernetes ÈõÜÁæ§ÔºåÊîØÊåÅAWS EKSÈõÜÁæ§„ÄÇ\n3. MCPÊîØÊåÅÔºöÊîØÊåÅÂ§öÈõÜÁæ§ÁöÑMCPÁÆ°ÁêÜ,ÂêåÊó∂ÊîØÊåÅstdio„ÄÅsse‰∏§ÁßçÊ®°ÂºèÔºåÂÜÖÁΩÆ58ÁßçÂ∑•ÂÖ∑ÔºåÊîØÊåÅSSEÊ®°ÂºèÔºåÊîØÊåÅÁßÅÊúâÂåñÈÉ®ÁΩ≤ÔºåÂ§ö‰∫∫ÂÖ±‰∫´„ÄÇÊîØÊåÅË∂ÖËøáÁôæÁßçÁªÑÂêàÊìç‰Ωú„ÄÇ\n4. ÊîØÊåÅË∑®ÂëΩÂêçÁ©∫Èó¥ÔºöÈÄöËøákom.Namespace(\"default\",\"kube-system\").List(\u0026items) Ë∑®ÂëΩÂêçÁ©∫Èó¥Êü•ËØ¢ËµÑÊ∫ê„ÄÇ\n5. ÈìæÂºèË∞ÉÁî®Ôºökom Êèê‰æõ‰∫ÜÈìæÂºèË∞ÉÁî®Ôºå‰ΩøÂæóÊìç‰ΩúËµÑÊ∫êÊõ¥Âä†ÁÆÄÂçïÂíåÁõ¥ËßÇ„ÄÇ\n6. ÊîØÊåÅËá™ÂÆö‰πâËµÑÊ∫êÂÆö‰πâÔºàCRDÔºâÔºökom ÊîØÊåÅËá™ÂÆö‰πâËµÑÊ∫êÂÆö‰πâÔºàCRDÔºâÔºå‰Ω†ÂèØ‰ª•ËΩªÊùæÂú∞ÂÆö‰πâÂíåÊìç‰ΩúËá™ÂÆö‰πâËµÑÊ∫ê„ÄÇ\n7. ÊîØÊåÅÂõûË∞ÉÊú∫Âà∂ÔºåËΩªÊùæÊãìÂ±ï‰∏öÂä°ÈÄªËæëÔºåËÄå‰∏çÂøÖË∑ük8sÊìç‰ΩúÂº∫ËÄ¶Âêà„ÄÇ\n8. ÊîØÊåÅPODÂÜÖÊñá‰ª∂Êìç‰ΩúÔºåËΩªÊùæ‰∏ä‰º†„ÄÅ‰∏ãËΩΩ„ÄÅÂà†Èô§Êñá‰ª∂„ÄÇ\n9. ÊîØÊåÅÈ´òÈ¢ëÊìç‰ΩúÂ∞ÅË£ÖÔºåÂ¶ÇdeploymentÁöÑrestartÈáçÂêØ„ÄÅscaleÊâ©Áº©ÂÆπ„ÄÅÂêØÂÅúÁ≠â20‰ΩôÈ°πÊìç‰ΩúÂäüËÉΩ„ÄÇ\n10. ÊîØÊåÅSQLÊü•ËØ¢k8sËµÑÊ∫ê„ÄÇselect * from pod where metadata.namespace='kube-system' or metadata.namespace='default' order by  metadata.creationTimestamp desc \n11. ÊîØÊåÅÊü•ËØ¢ÁºìÂ≠òÔºåÂú®È´òÈ¢ë„ÄÅÊâπÈáèÊü•ËØ¢Âú∫ÊôØ‰∏ãÔºåÂèØËÆæÁΩÆÁºìÂ≠òËøáÊúüÊó∂Èó¥ÔºåÊèêÂçáÊü•ËØ¢ÊÄßËÉΩ„ÄÇÂàóË°®ËøáÊª§Êù°‰ª∂‰∏çÂèóÁºìÂ≠òÂΩ±Âìç„ÄÇ\n\n\n\n## Á§∫‰æãÁ®ãÂ∫è\n**k8m** ÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑ Kubernetes ÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÂÆÉÂü∫‰∫ékom„ÄÅamisÂÆûÁé∞ÔºåÂçïÊñá‰ª∂ÔºåÊîØÊåÅÂ§öÂπ≥Âè∞Êû∂ÊûÑ„ÄÇ\n1. **‰∏ãËΩΩ**Ôºö‰ªé [https://github.com/weibaohui/k8m](https://github.com/weibaohui/k8m) ‰∏ãËΩΩÊúÄÊñ∞ÁâàÊú¨„ÄÇ\n2. **ËøêË°å**Ôºö‰ΩøÁî® `./k8m` ÂëΩ‰ª§ÂêØÂä®,ËÆøÈóÆ[http://127.0.0.1:3618](http://127.0.0.1:3618)„ÄÇ\n\n\n\n\n## ÂÆâË£Ö\n\n```bash\nimport (\n    \"github.com/weibaohui/kom\"\n    \"github.com/weibaohui/kom/callbacks\"\n)\nfunc main() {\n    // Ê≥®ÂÜåÂõûË∞ÉÔºåÂä°ÂøÖÂÖàÊ≥®ÂÜå\n    callbacks.RegisterInit()\n    // Ê≥®ÂÜåÈõÜÁæ§\n\tdefaultKubeConfig := os.Getenv(\"KUBECONFIG\")\n\tif defaultKubeConfig == \"\" {\n\t\tdefaultKubeConfig = filepath.Join(homedir.HomeDir(), \".kube\", \"config\")\n\t}\n\t_, _ = kom.Clusters().RegisterInCluster()\n\t_, _ = kom.Clusters().RegisterByPathWithID(defaultKubeConfig, \"default\")\n\tkom.Clusters().Show()\n\t// ÂÖ∂‰ªñÈÄªËæë\n}\n```\n\n## ‰ΩøÁî®Á§∫‰æã\n\n### 0. Â§öÈõÜÁæ§ k8s MCP ÊîØÊåÅ\nÂêåÊó∂ÊîØÊåÅstdio„ÄÅsse‰∏§ÁßçÊ®°Âºè\nÊîØÊåÅÂ§ö‰∏™tools ÊîØÊåÅ„ÄÇÂåÖÊã¨ÂØπ‰ªªÊÑèËµÑÊ∫êÁöÑÊü•ËØ¢ÂàóË°®Âà†Èô§ÊèèËø∞Êìç‰ΩúÔºå‰ª•ÂèäPODÊó•ÂøóËØªÂèñÊìç‰Ωú„ÄÇ\n#### 1.ÈõÜÊàêÂà∞‰ª£Á†Å‰∏≠\n```go\n// ‰∏ÄË°å‰ª£Á†ÅÂêØÂä®MCP Server\nmcp.RunMCPServer(\"kom mcp server\", \"0.0.1\", 9096)\n\n\n\n```\n#### 2. ÁºñËØë\n```shell\n# Ê∫êÁ†ÅÂêØÂä®\ngo build main.go \n//ÁºñËØë‰∏∫kom\n```\n#### 3. ÂêØÂä®\nÂêØÂä®ÂêéÊîØÊåÅ‰∏§ÁßçÊ®°ÂºèÔºå‰∏ÄÁßç‰∏∫stdioÔºå‰∏ÄÁßçsse„ÄÇ\nÁÆ°ÁêÜk8sÈªòËÆ§‰ΩøÁî®KUBECONFIG envÁéØÂ¢ÉÂèòÈáè„ÄÇ\n```shell\n# ËÆæÁΩÆKUBECONFIGÁéØÂ¢ÉÂèòÈáè\nexport KUBECONFIG = /Users/xxx/.kube/config\n```\n```shell\n# ËøêË°å\n./kom \n# MCP Server ËÆøÈóÆÂú∞ÂùÄ\nhttp://IP:9096/sse\n```\nÊ≠§Êó∂ÔºåÁºñËØëÂæóÂà∞ÁöÑ‰∫åËøõÂà∂Êñá‰ª∂ÔºåÂèØÂΩìÂÅöstdio Ê®°Âºè‰ΩøÁî®„ÄÇ\nhttp://IP:9096/sse Ê®°ÂºèÔºåÂèØ‰ª•ÂΩìÂÅösse Ê®°Âºè‰ΩøÁî®„ÄÇ\n\n\n#### 4. ÈõÜÊàêÂà∞MCPÂ∑•ÂÖ∑‰∏≠\nÊîØÊåÅstdio\\sse ‰∏§ÁßçÊñπÂºèÈõÜÊàê„ÄÇ\nÈÄÇÂêàMCPÂ∑•ÂÖ∑ÈõÜÊàêÔºåÂ¶ÇCursor„ÄÅClaude Desktop(‰ªÖÊîØÊåÅstdioÊ®°Âºè)„ÄÅWindsurfÁ≠âÔºåÊ≠§Â§ñ‰πüÂèØ‰ª•‰ΩøÁî®Ëøô‰∫õËΩØ‰ª∂ÁöÑUIÊìç‰ΩúÁïåÈù¢ËøõË°åÊ∑ªÂä†„ÄÇ\n```json\n{\n  \"mcpServers\": {\n    \"kom\": {\n      \"type\": \"sse\",\n      \"url\": \"http://IP:9096/sse\"\n    }\n  }\n}\n```\n```json\n{\n    \"mcpServers\": {\n        \"k8m\": {\n            \"command\": \"path/to/kom\",\n            \"args\": []\n        }\n    }\n}\n```\n\n####  MCPÂ∑•ÂÖ∑ÂàóË°®Ôºà59ÁßçÔºâ\n\n| Á±ªÂà´                       | ÊñπÊ≥ï                                 | ÊèèËø∞                                                  |\n| -------------------------- | ------------------------------------ | ----------------------------------------------------- |\n| **ÈõÜÁæ§ÁÆ°ÁêÜÔºà1Ôºâ**          | `list_k8s_clusters`                  | ÂàóÂá∫ÊâÄÊúâÂ∑≤Ê≥®ÂÜåÁöÑKubernetesÈõÜÁæ§                        |\n| **DaemonSetÁÆ°ÁêÜÔºà1Ôºâ**     | `restart_k8s_daemonset`              | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞,ÈáçÂêØDaemonSet                |\n| **ÈÉ®ÁΩ≤ÁÆ°ÁêÜÔºà12Ôºâ**         | `scale_k8s_deployment`               | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥„ÄÅÂêçÁß∞ Êâ©Áº©ÂÆπDeploymentÔºåËÆæÁΩÆÂâØÊú¨Êï∞ |\n|                            | `restart_k8s_deployment`             | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞,ÈáçÂêØDeployment               |\n|                            | `stop_k8s_deployment`                | ÂÅúÊ≠¢Deployment                                        |\n|                            | `restore_k8s_deployment`             | ÊÅ¢Â§çDeploymentÂâØÊú¨Êï∞                                  |\n|                            | `update_k8s_deployment_image_tag`    | Êõ¥Êñ∞Deployment‰∏≠ÂÆπÂô®ÁöÑÈïúÂÉèTag                         |\n|                            | `get_k8s_deployment_rollout_history` | Êü•ËØ¢ÂçáÁ∫ßÂéÜÂè≤                                          |\n|                            | `undo_k8s_deployment_rollout`        | ÂõûÊªö                                                  |\n|                            | `pause_k8s_deployment_rollout`       | ÊöÇÂÅúÂçáÁ∫ß                                              |\n|                            | `resume_k8s_deployment_rollout`      | ÊÅ¢Â§çÂçáÁ∫ß                                              |\n|                            | `get_k8s_deployment_rollout_status`  | Êü•ËØ¢ÂçáÁ∫ßÁä∂ÊÄÅ                                          |\n|                            | `get_k8s_deployment_hpa_list`        | Êü•ËØ¢DeploymentÁöÑHPAÂàóË°®                               |\n|                            | `list_k8s_deploy_event`              | ÂàóÂá∫DeploymentÁõ∏ÂÖ≥ÁöÑ‰∫ã‰ª∂                              |\n| **Âä®ÊÄÅËµÑÊ∫êÁÆ°ÁêÜ(Âê´CRDÔºå8)** | `get_k8s_resource`                   | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞Ëé∑ÂèñKubernetesËµÑÊ∫êËØ¶ÊÉÖ        |\n|                            | `describe_k8s_resource`              | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞Ëé∑ÂèñKubernetesËµÑÊ∫êËØ¶ÊÉÖ        |\n|                            | `delete_k8s_resource`                | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞Âà†Èô§KubernetesËµÑÊ∫ê            |\n|                            | `list_k8s_resource`                  | ÊåâÈõÜÁæ§ÂíåËµÑÊ∫êÁ±ªÂûãÂàóÂá∫KubernetesËµÑÊ∫ê                    |\n|                            | `annotate_k8s_resource`              | ‰∏∫KubernetesËµÑÊ∫êÊ∑ªÂä†ÊàñÂà†Èô§Ê≥®Ëß£                        |\n|                            | `label_k8s_resource`                 | ‰∏∫KubernetesËµÑÊ∫êÊ∑ªÂä†ÊàñÂà†Èô§Ê†áÁ≠æ                        |\n|                            | `patch_k8s_resource`                 | ÈÄöËøáÈõÜÁæ§„ÄÅÂëΩÂêçÁ©∫Èó¥ÂíåÂêçÁß∞Êõ¥Êñ∞KubernetesËµÑÊ∫ê            |\n|                            | `GetDynamicResource`                 | Ëé∑ÂèñÂä®ÊÄÅËµÑÊ∫ê                                          |\n| **ËäÇÁÇπÁÆ°ÁêÜÔºà11Ôºâ**         | `taint_k8s_node`                     | ‰∏∫ËäÇÁÇπÊ∑ªÂä†Ê±°ÁÇπ                                        |\n|                            | `untaint_k8s_node`                   | ‰∏∫ËäÇÁÇπÁßªÈô§Ê±°ÁÇπ                                        |\n|                            | `cordon_k8s_node`                    | ËÆæÁΩÆËäÇÁÇπ‰∏∫‰∏çÂèØË∞ÉÂ∫¶Áä∂ÊÄÅ                                |\n|                            | `uncordon_k8s_node`                  | ËÆæÁΩÆËäÇÁÇπ‰∏∫ÂèØË∞ÉÂ∫¶Áä∂ÊÄÅ                                  |\n|                            | `drain_k8s_node`                     | Ê∏ÖÁ©∫ËäÇÁÇπ‰∏äÁöÑPodÂπ∂Èò≤Ê≠¢Êñ∞ÁöÑPodË∞ÉÂ∫¶                      |\n|                            | `get_k8s_node_ip_usage`              | Êü•ËØ¢ËäÇÁÇπIPËµÑÊ∫ê‰ΩøÁî®ÊÉÖÂÜµ                                |\n|                            | `list_k8s_node`                      | Ëé∑ÂèñNodeÂàóË°®                                          |\n|                            | `get_k8s_top_node`                   | Ëé∑ÂèñNodeËäÇÁÇπCPUÂíåÂÜÖÂ≠òËµÑÊ∫êÁî®ÈáèÊéíÂêçÂàóË°®                 |\n|                            | `get_k8s_pod_count_running_on_node`  | Êü•ËØ¢Êüê‰∏™ËäÇÁÇπ‰∏äËøêË°åÁöÑPodÊï∞ÈáèÁªüËÆ°                       |\n|                            | `get_k8s_node_resource_usage`        | Êü•ËØ¢ËäÇÁÇπËµÑÊ∫ê‰ΩøÁî®ÊÉÖÂÜµÁªüËÆ°                              |\n|                            | `TaintNodeTool`                      | ‰∏∫ËäÇÁÇπÊ∑ªÂä†Ê±°ÁÇπ                                        |\n| **‰∫ã‰ª∂ÁÆ°ÁêÜÔºà1Ôºâ**          | `list_k8s_event`                     | ÊåâÈõÜÁæ§ÂíåÂëΩÂêçÁ©∫Èó¥ÂàóÂá∫Kubernetes‰∫ã‰ª∂                    |\n| **IngressÁÆ°ÁêÜÔºà1Ôºâ**       | `set_default_k8s_ingressclass`       | ËÆæÁΩÆIngressClass‰∏∫ÈªòËÆ§                                |\n| **Pod ÁÆ°ÁêÜÔºà18Ôºâ**         | `run_command_in_k8s_pod`             | Âú®PodÂÜÖÊâßË°åÂëΩ‰ª§                                       |\n|                            | `list_k8s_pod_event`                 | ÂàóÂá∫PodÁõ∏ÂÖ≥ÁöÑ‰∫ã‰ª∂                                     |\n|                            | `list_files_in_k8s_pod`              | Ëé∑ÂèñPod‰∏≠ÊåáÂÆöË∑ØÂæÑ‰∏ãÁöÑÊñá‰ª∂ÂàóË°®                         |\n|                            | `list_pod_all_files`                 | Ëé∑ÂèñPod‰∏≠ÊåáÂÆöË∑ØÂæÑ‰∏ãÁöÑÊâÄÊúâÊñá‰ª∂ÂàóË°®ÔºåÂåÖÂê´Â≠êÁõÆÂΩï         |\n|                            | `delete_k8s_pod`                     | Âà†Èô§Pod                                               |\n|                            | `delete_pod_file`                    | Âà†Èô§Pod‰∏≠ÁöÑÊåáÂÆöÊñá‰ª∂                                   |\n|                            | `get_k8s_pod_linked_env`             | Ëé∑ÂèñPodËøêË°åÊó∂ÁöÑÁéØÂ¢ÉÂèòÈáè‰ø°ÊÅØ                           |\n|                            | `get_pod_linked_env_from_yaml`       | ÈÄöËøáPod yaml ÂÆö‰πâ Ëé∑ÂèñPodÂÆö‰πâ‰∏≠ÁöÑÁéØÂ¢ÉÂèòÈáè‰ø°ÊÅØ         |\n|                            | `get_k8s_pod_linked_services`        | Ëé∑Âèñ‰∏éPodÂÖ≥ËÅîÁöÑService                                |\n|                            | `get_pod_linked_ingresses`           | Ëé∑Âèñ‰∏éPodÂÖ≥ËÅîÁöÑIngress                                |\n|                            | `get_pod_linked_endpoints`           | Ëé∑Âèñ‰∏éPodÂÖ≥ËÅîÁöÑEndpoints                              |\n|                            | `list_k8s_pod`                       | Ëé∑ÂèñPodÂàóË°®                                           |\n|                            | `get_k8s_top_pod`                    | Ëé∑ÂèñPod CPU ÂÜÖÂ≠ò ËµÑÊ∫êÁî®ÈáèÊéíÂêç ÂàóË°®                    |\n|                            | `ListPodFilesTool`                   | ÂàóÂá∫PodÊñá‰ª∂                                           |\n|                            | `ListAllPodFilesTool`                | ÂàóÂá∫PodÊâÄÊúâÊñá‰ª∂                                       |\n|                            | `DeletePodFileTool`                  | Âà†Èô§PodÊñá‰ª∂                                           |\n|                            | `UploadPodFileTool`                  | ‰∏ä‰º†PodÊñá‰ª∂                                           |\n|                            | `GetPodLogsTool`                     | Ëé∑ÂèñPodÊó•Âøó                                           |\n|                            | `describe_k8s_pod`                   | ÊèèËø∞PodÂÆπÂô®ÁªÑ                                         |\n| **Â≠òÂÇ®ÁÆ°ÁêÜÔºà3Ôºâ**          | `set_k8s_default_storageclass`       | ËÆæÁΩÆStorageClass‰∏∫ÈªòËÆ§                                |\n|                            | `get_k8s_storageclass_pvc_count`     | Ëé∑ÂèñStorageClass‰∏ãÁöÑPVCÊï∞Èáè                           |\n|                            | `get_k8s_storageclass_pv_count`      | Ëé∑ÂèñStorageClass‰∏ãÁöÑPVÊï∞Èáè                            |\n| **YAMLÁÆ°ÁêÜÔºà2Ôºâ**          | `apply_k8s_yaml`                     | ÈÄöËøáYAMLÂàõÂª∫ÊàñÊõ¥Êñ∞KubernetesËµÑÊ∫ê                      |\n|                            | `delete_k8s_yaml`                    | ÈÄöËøáYAMLÂà†Èô§KubernetesËµÑÊ∫ê                            |\n\n#### ÂêØÂä®ÂëΩ‰ª§\n```go\nmcp.RunMCPServer(\"kom mcp server\", \"0.0.1\", 3619)\n```\n \n#### AIÂ∑•ÂÖ∑ÈõÜÊàê\n\n##### Claude Desktop\n1. ÊâìÂºÄClaude DesktopËÆæÁΩÆÈù¢Êùø\n2. Âú®APIÈÖçÁΩÆÂå∫ÂüüÊ∑ªÂä†MCP ServerÂú∞ÂùÄ\n3. ÂêØÁî®SSE‰∫ã‰ª∂ÁõëÂê¨ÂäüËÉΩ\n4. È™åËØÅËøûÊé•Áä∂ÊÄÅ\n```json\n{\n  \"mcpServers\": {\n    \"k8m\": {\n      \"command\": \"path/to/kom\",\n      \"args\": []\n    }\n  }\n}\n```\n\n##### Cursor\n1. ËøõÂÖ•CursorËÆæÁΩÆÁïåÈù¢\n2. ÊâæÂà∞Êâ©Â±ïÊúçÂä°ÈÖçÁΩÆÈÄâÈ°π\n3. ÊîØÊåÅsse„ÄÅstdio‰∏§ÁßçÊñπÂºè„ÄÇsse ÊñπÂºèÂ°´ÂÜôhttp://localhost:9096/sse,stdioÊñπÂºèÂ°´ÂÜôkomÁöÑÊñá‰ª∂‰ΩçÁΩÆ„ÄÇ\n\n##### Windsurf\n1. ËÆøÈóÆÈÖçÁΩÆ‰∏≠ÂøÉ\n2. ËÆæÁΩÆAPIÊúçÂä°Âô®Âú∞ÂùÄ\n3. ÊîØÊåÅsse„ÄÅstdio‰∏§ÁßçÊñπÂºè„ÄÇsse ÊñπÂºèÂ°´ÂÜôhttp://localhost:9096/sse,stdioÊñπÂºèÂ°´ÂÜôkomÁöÑÊñá‰ª∂‰ΩçÁΩÆ„ÄÇ\n\n#### cherry studio\n1. ÁÇπÂáªÂ∑¶‰∏ãËßíËÆæÁΩÆ\n2. ÁÇπÂáªMCP ÊúçÂä°Âô®\n3. ÁÇπÂáªÊ∑ªÂä†ÊúçÂä°Âô®\n4. ÊîØÊåÅsse„ÄÅstdio‰∏§ÁßçÊñπÂºè„ÄÇsse ÊñπÂºèÂ°´ÂÜôhttp://localhost:9096/sse,stdioÊñπÂºèÂ°´ÂÜôkomÁöÑÊñá‰ª∂‰ΩçÁΩÆ„ÄÇ\n\n\n### 1. Â§öÈõÜÁæ§ÁÆ°ÁêÜ\n#### Ê≥®ÂÜåÂ§öÈõÜÁæ§\n```go\n// Ê≥®ÂÜåInClusterÈõÜÁæ§ÔºåÂêçÁß∞‰∏∫InCluster\nkom.Clusters().RegisterInCluster()\n// Ê≥®ÂÜå‰∏§‰∏™Â∏¶ÂêçÁß∞ÁöÑÈõÜÁæ§,ÂàÜÂà´Âêç‰∏∫orbÂíådocker-desktop\nkom.Clusters().RegisterByPathWithID(\"/Users/kom/.kube/orb\", \"orb\")\nkom.Clusters().RegisterByPathWithID(\"/Users/kom/.kube/config\", \"docker-desktop\")\n// Ê≥®ÂÜå‰∏Ä‰∏™Âêç‰∏∫defaultÁöÑÈõÜÁæ§ÔºåÈÇ£‰πàkom.DefaultCluster()Âàô‰ºöËøîÂõûËØ•ÈõÜÁæ§„ÄÇ\nkom.Clusters().RegisterByPathWithID(\"/Users/kom/.kube/config\", \"default\")\n```\n#### Ê≥®ÂÜåAWS EKSÈõÜÁæ§\n```go\n// ÈÖçÁΩÆ EKS ÈõÜÁæ§‰ø°ÊÅØ\nconfig := aws.EKSAuthConfig{\n    AccessKey:       \"XXX\",        // AWS Access Key ID\n    SecretAccessKey: \"yyy\",        // AWS Secret Access Key\n    Region:          \"us-east-1\",  // AWS Âå∫Âüü\n    ClusterName:     \"k8m\",        // EKS ÈõÜÁæ§ÂêçÁß∞\n}\n\n// Ê≥®ÂÜå AWS EKS ÈõÜÁæ§\n_, err := kom.Clusters().RegisterAWSCluster(config)\nif err != nil {\n    fmt.Printf(\"Ê≥®ÂÜå EKS ÈõÜÁæ§Â§±Ë¥•: %v\", err)\n    return\n}\n\n// ‰ΩøÁî®Ê≥®ÂÜåÁöÑ EKS ÈõÜÁæ§\nvar pods []corev1.Pod\nclusterID := fmt.Sprintf(\"%s-%s\", config.Region, config.ClusterName) // ÈõÜÁæ§IDÊ†ºÂºè: {Region}-{ClusterName}\nerr = kom.Cluster(clusterID).Resource(\u0026corev1.Pod{}).Namespace(\"kube-system\").List(\u0026pods).Error\n```\n\n**AWS EKS ÈõÜÁæ§Ê≥®ÂÜåËØ¥ÊòéÔºö**\n- `AccessKey`: AWS ËÆøÈóÆÂØÜÈí• ID\n- `SecretAccessKey`: AWS ÁßòÂØÜËÆøÈóÆÂØÜÈí•  \n- `Region`: AWS Âå∫ÂüüÔºåÂ¶Ç `us-east-1`„ÄÅ`ap-southeast-1` Á≠â\n- `ClusterName`: EKS ÈõÜÁæ§ÂêçÁß∞\n- `RoleARN`: (ÂèØÈÄâ) Ë¶ÅÊâøÊãÖÁöÑ IAM ËßíËâ≤ ARNÔºåÁî®‰∫éË∑®Ë¥¶Êà∑ËÆøÈóÆ\n- ÈõÜÁæ§Ê≥®ÂÜåÂêé‰ºöËá™Âä®ÁîüÊàê IDÔºåÊ†ºÂºè‰∏∫ `{Region}-{ClusterName}`\n- ÊîØÊåÅ IAM ËßíËâ≤ÊâøÊãÖÊú∫Âà∂ÂÆûÁé∞Ë∑®Ë¥¶Êà∑ÈõÜÁæ§ËÆøÈóÆ\n- AWS Âá≠ËØÅ‰ø°ÊÅØ‰ªÖÂú®ÂÜÖÂ≠ò‰∏≠‰ΩøÁî®ÔºåÁ®ãÂ∫èÈáçÂêØÂêéËá™Âä®Ê∏ÖÁêÜ\n#### ÊòæÁ§∫Â∑≤Ê≥®ÂÜåÈõÜÁæ§\n```go\nkom.Clusters().Show()\n```\n#### ÈÄâÊã©ÈªòËÆ§ÈõÜÁæ§\n```go\n// ‰ΩøÁî®ÈªòËÆ§ÈõÜÁæ§,Êü•ËØ¢ÈõÜÁæ§ÂÜÖkube-systemÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑpod\n// È¶ñÂÖàÂ∞ùËØïËøîÂõû ID ‰∏∫ \"InCluster\" ÁöÑÂÆû‰æãÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºå\n// ÂàôÂ∞ùËØïËøîÂõû ID ‰∏∫ \"default\" ÁöÑÂÆû‰æã„ÄÇ\n// Â¶ÇÊûú‰∏äËø∞‰∏§‰∏™ÂêçÁß∞ÁöÑÂÆû‰æãÈÉΩ‰∏çÂ≠òÂú®ÔºåÂàôËøîÂõû clusters ÂàóË°®‰∏≠ÁöÑ‰ªªÊÑè‰∏Ä‰∏™ÂÆû‰æã„ÄÇ\nvar pods []corev1.Pod\nerr = kom.DefaultCluster().Resource(\u0026corev1.Pod{}).Namespace(\"kube-system\").List(\u0026pods).Error\n```\n#### ÈÄâÊã©ÊåáÂÆöÈõÜÁæ§\n```go\n// ÈÄâÊã©orbÈõÜÁæ§,Êü•ËØ¢ÈõÜÁæ§ÂÜÖkube-systemÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑpod\nvar pods []corev1.Pod\nerr = kom.Cluster(\"orb\").Resource(\u0026corev1.Pod{}).Namespace(\"kube-system\").List(\u0026pods).Error\n```\n\n### 2. ÂÜÖÁΩÆËµÑÊ∫êÂØπË±°ÁöÑÂ¢ûÂà†ÊîπÊü•‰ª•ÂèäWatchÁ§∫‰æã\nÂÆö‰πâ‰∏Ä‰∏™ Deployment ÂØπË±°ÔºåÂπ∂ÈÄöËøá kom ËøõË°åËµÑÊ∫êÊìç‰Ωú„ÄÇ\n```go\nvar item v1.Deployment\nvar items []v1.Deployment\n```\n#### ÂàõÂª∫Êüê‰∏™ËµÑÊ∫ê\n```go\nitem = v1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      \"nginx\",\n\t\t\tNamespace: \"default\",\n\t\t},\n\t\tSpec: v1.DeploymentSpec{\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{Name: \"test\", Image: \"nginx:1.14.2\"},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\nerr := kom.DefaultCluster().Resource(\u0026item).Create(\u0026item).Error\n```\n#### GetÊü•ËØ¢Êüê‰∏™ËµÑÊ∫ê\n```go\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ãÂêç‰∏∫ nginx ÁöÑ Deployment\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").Get(\u0026item).Error\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ãÂêç‰∏∫ nginx ÁöÑ DeploymentÔºåÂπ∂‰ΩøÁî®ÁºìÂ≠ò 5 Áßí\n// 5ÁßíÂÜÖÔºå‰∏ç‰ºöÂÜçÊ¨°Êü•ËØ¢ÔºåÊâπÈáèÊìç‰Ωú„ÄÅÈ´òÈ¢ëÊìç‰Ωú‰∏ãÔºåÂª∫ËÆÆÂêØÁî®ÁºìÂ≠ò\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").WithCache(5 * time.Second).Get(\u0026item).Error\n```\n#### ListÊü•ËØ¢ËµÑÊ∫êÂàóË°®\n```go\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑ Deployment ÂàóË°®\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").List(\u0026items).Error\n// Êü•ËØ¢ default„ÄÅkube-system ÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑ Deployment ÂàóË°®\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\",\"kube-system\").List(\u0026items).Error\n// Êü•ËØ¢ ÊâÄÊúâ ÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑ Deployment ÂàóË°®\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"*\").List(\u0026items).Error\nerr := kom.DefaultCluster().Resource(\u0026item).AllNamespace().List(\u0026items).Error\n// ËÆæÁΩÆ5ÁßíÁºìÂ≠òÔºåÂØπÂàóË°®ÁîüÊïà\nerr := kom.DefaultCluster().Resource(\u0026item).WithCache(5 * time.Second).List(\u0026nodeList).Error\n```\n#### ÈÄöËøáLabelÊü•ËØ¢ËµÑÊ∫êÂàóË°®\n```go\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ã Ê†áÁ≠æ‰∏∫ app:nginx ÁöÑ Deployment ÂàóË°®\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").WithLabelSelector(\"app=nginx\").List(\u0026items).Error\n```\n#### ÈÄöËøáÂ§ö‰∏™LabelÊü•ËØ¢ËµÑÊ∫êÂàóË°®\n```go\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ã Ê†áÁ≠æ‰∏∫ app:nginx m:n ÁöÑ Deployment ÂàóË°®\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").WithLabelSelector(\"app=nginx\").WithLabelSelector(\"m=n\").List(\u0026items).Error\n```\n#### ÈÄöËøáFieldÊü•ËØ¢ËµÑÊ∫êÂàóË°®\n```go\n// Êü•ËØ¢ default ÂëΩÂêçÁ©∫Èó¥‰∏ã Ê†áÁ≠æ‰∏∫ metadata.name=test-deploy ÁöÑ Deployment ÂàóË°®\n// filedSelector ‰∏ÄËà¨ÊîØÊåÅÂéüÁîüÁöÑÂ≠óÊÆµÂÆö‰πâ„ÄÇÂ¶Çmetadata.name,metadata.namespace,metadata.labels,metadata.annotations,metadata.creationTimestamp,spec.nodeName,spec.serviceAccountName,spec.schedulerName,status.phase,status.hostIP,status.podIP,status.qosClass,spec.containers.nameÁ≠âÂ≠óÊÆµ\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").WithFieldSelector(\"metadata.name=test-deploy\").List(\u0026items).Error\n```\n#### ÂàÜÈ°µÊü•ËØ¢ËµÑÊ∫ê\n```go\nvar list []corev1.Pod\nvar total int64\nsql := \"select * from pod where metadata.namespace=? or metadata.namespace=?     order by  metadata.creationTimestamp desc \"\nerr := kom.DefaultCluster().Sql(sql, \"kube-system\", \"default\").\n\t\tFillTotalCount(\u0026total).\n\t\tLimit(5).\n\t\tOffset(10).\n\t\tList(\u0026list).Error\nfmt.Printf(\"total %d\\n\", total)  //ËøîÂõûÊÄªÊï∞ 480\nfmt.Printf(\"Count %d\\n\", len(list)) //ËøîÂõûÊù°ÁõÆÊï∞=limit=5\n```\n#### Êõ¥Êñ∞ËµÑÊ∫êÂÜÖÂÆπ\n```go\n// Êõ¥Êñ∞Âêç‰∏∫nginx ÁöÑ DeploymentÔºåÂ¢ûÂä†‰∏Ä‰∏™Ê≥®Ëß£\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").Get(\u0026item).Error\nif item.Spec.Template.Annotations == nil {\n\titem.Spec.Template.Annotations = map[string]string{}\n}\nitem.Spec.Template.Annotations[\"kom.kubernetes.io/restartedAt\"] = time.Now().Format(time.RFC3339)\nerr = kom.DefaultCluster().Resource(\u0026item).Update(\u0026item).Error\n```\n#### PATCH Êõ¥Êñ∞ËµÑÊ∫ê\n```go\n// ‰ΩøÁî® Patch Êõ¥Êñ∞ËµÑÊ∫ê,‰∏∫Âêç‰∏∫ nginx ÁöÑ Deployment Â¢ûÂä†‰∏Ä‰∏™Ê†áÁ≠æÔºåÂπ∂ËÆæÁΩÆÂâØÊú¨Êï∞‰∏∫5\npatchData := `{\n    \"spec\": {\n        \"replicas\": 5\n    },\n    \"metadata\": {\n        \"labels\": {\n            \"new-label\": \"new-value\"\n        }\n    }\n}`\nerr := kom.DefaultCluster().Resource(\u0026item).Patch(\u0026item, types.StrategicMergePatchType, patchData).Error\n```\n#### Âà†Èô§ËµÑÊ∫ê\n```go\n// Âà†Èô§Âêç‰∏∫ nginx ÁöÑ Deployment\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").Delete().Error\n```\n#### Âº∫Âà∂Âà†Èô§ËµÑÊ∫ê\n```go\n// Âà†Èô§Âêç‰∏∫ nginx ÁöÑ Deployment\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").ForceDelete().Error\n```\n#### ÈÄöÁî®Á±ªÂûãËµÑÊ∫êÁöÑËé∑ÂèñÔºàÈÄÇÁî®‰∫ék8sÂÜÖÁΩÆÁ±ªÂûã‰ª•ÂèäCRDÔºâ\n```go\n// ÊåáÂÆöGVKËé∑ÂèñËµÑÊ∫ê\nvar list []corev1.Event\nerr := kom.DefaultCluster().GVK(\"events.k8s.io\", \"v1\", \"Event\").Namespace(\"default\").List(\u0026list).Error\n```\n#### WatchËµÑÊ∫êÂèòÊõ¥\n```go\n// watch default ÂëΩÂêçÁ©∫Èó¥‰∏ã PodËµÑÊ∫ê ÁöÑÂèòÊõ¥\nvar watcher watch.Interface\nvar pod corev1.Pod\nerr := kom.DefaultCluster().Resource(\u0026pod).Namespace(\"default\").Watch(\u0026watcher).Error\nif err != nil {\n\tfmt.Printf(\"Create Watcher Error %v\", err)\n\treturn err\n}\ngo func() {\n\tdefer watcher.Stop()\n\n\tfor event := range watcher.ResultChan() {\n\t\terr := kom.DefaultCluster().Tools().ConvertRuntimeObjectToTypedObject(event.Object, \u0026pod)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Êó†Ê≥ïÂ∞ÜÂØπË±°ËΩ¨Êç¢‰∏∫ *v1.Pod Á±ªÂûã: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t// Â§ÑÁêÜ‰∫ã‰ª∂\n\t\tswitch event.Type {\n\t\tcase watch.Added:\n\t\t\tfmt.Printf(\"Added Pod [ %s/%s ]\\n\", pod.Namespace, pod.Name)\n\t\tcase watch.Modified:\n\t\t\tfmt.Printf(\"Modified Pod [ %s/%s ]\\n\", pod.Namespace, pod.Name)\n\t\tcase watch.Deleted:\n\t\t\tfmt.Printf(\"Deleted Pod [ %s/%s ]\\n\", pod.Namespace, pod.Name)\n\t\t}\n\t}\n}()\n```\n#### DescribeÊü•ËØ¢Êüê‰∏™ËµÑÊ∫ê\n```go\n// Describe default ÂëΩÂêçÁ©∫Èó¥‰∏ãÂêç‰∏∫ nginx ÁöÑ Deployment\nvar describeResult []byte\nerr := kom.DefaultCluster().Resource(\u0026item).Namespace(\"default\").Name(\"nginx\").Describe(\u0026item).Error\nfmt.Printf(\"describeResult: %s\", describeResult)\n```\n\n### 3. YAML ÂàõÂª∫„ÄÅÊõ¥Êñ∞„ÄÅÂà†Èô§\n```go\nyaml := `apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example-config\n  namespace: default\ndata:\n  key: value\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example\n  template:\n    metadata:\n      labels:\n        app: example\n    spec:\n      containers:\n        - name: example-container\n          image: nginx\n`\n// Á¨¨‰∏ÄÊ¨°ÊâßË°åApply‰∏∫ÂàõÂª∫ÔºåËøîÂõûÊØè‰∏ÄÊù°ËµÑÊ∫êÁöÑÊâßË°åÁªìÊûú \nresults := kom.DefaultCluster().Applier().Apply(yaml)\n// Á¨¨‰∫åÊ¨°ÊâßË°åApply‰∏∫Êõ¥Êñ∞ÔºåËøîÂõûÊØè‰∏ÄÊù°ËµÑÊ∫êÁöÑÊâßË°åÁªìÊûú\nresults = kom.DefaultCluster().Applier().Apply(yaml)\n// Âà†Èô§ÔºåËøîÂõûÊØè‰∏ÄÊù°ËµÑÊ∫êÁöÑÊâßË°åÁªìÊûú\nresults = kom.DefaultCluster().Applier().Delete(yaml)\n```\n\n### 4. Pod Êìç‰Ωú\n#### Ëé∑ÂèñÊó•Âøó\n```go\n// Ëé∑ÂèñPodÊó•Âøó\nvar stream io.ReadCloser\nerr := kom.DefaultCluster().Namespace(\"default\").Name(\"random-char-pod\").Ctl().Pod().ContainerName(\"container\").GetLogs(\u0026stream, \u0026corev1.PodLogOptions{}).Error\nreader := bufio.NewReader(stream)\nline, _ := reader.ReadString('\\n')\nfmt.Println(line)\n```\n#### ÊâßË°åÂëΩ‰ª§\nÂú®PodÂÜÖÊâßË°åÂëΩ‰ª§ÔºåÈúÄË¶ÅÊåáÂÆöÂÆπÂô®ÂêçÁß∞ÔºåÂπ∂‰∏î‰ºöËß¶ÂèëExec()Á±ªÂûãÁöÑcallbacks„ÄÇ\n```go\n// Âú®PodÂÜÖÊâßË°åps -efÂëΩ‰ª§\nvar execResult string\nerr := kom.DefaultCluster().Namespace(\"default\").Name(\"random-char-pod\").Ctl().Pod().ContainerName(\"container\").Command(\"ps\", \"-ef\").ExecuteCommand(\u0026execResult).Error\nfmt.Printf(\"execResult: %s\", execResult)\n```\n#### Á´ØÂè£ËΩ¨Âèë\n```go\nerr := kom.DefaultCluster().Resource(\u0026v1.Pod{}).\n\t\tNamespace(\"default\").\n\t\tName(\"nginx-deployment-f576985cc-7czqr\").\n    Ctl().Pod().\n\t\tContainerName(\"nginx\").\n\t\tPortForward(\"20088\", \"80\", stopCh).Error\n// ÁõëÂê¨0.0.0.0‰∏äÁöÑ20088Á´ØÂè£ÔºåËΩ¨ÂèëÂà∞PodÁöÑ80Á´ØÂè£\n```\n#### ÊµÅÂºèÊâßË°åÂëΩ‰ª§\nÂú®PodÂÜÖÊâßË°åÂëΩ‰ª§ÔºåÂπ∂‰∏î‰ºöËß¶ÂèëStreamExec()Á±ªÂûãÁöÑcallbacks„ÄÇÈÄÇÂêàÊâßË°åping Á≠âÂëΩ‰ª§\n```go\ncb := func(data []byte) error {\n\t\tfmt.Printf(\"Data %s\\n\", string(data))\n\t\treturn nil\n\t}\nerr := kom.DefaultCluster().Namespace(\"kube-system\").Name(\"traefik-d7c9c5778-p9nf4\").Ctl().Pod().ContainerName(\"traefik\").Command(\"ping\", \"127.0.0.1\").StreamExecute(cb, cb).Error\n//ËæìÂá∫Ôºö\n//Data PING 127.0.0.1 (127.0.0.1): 56 data bytes\n//Data 64 bytes from 127.0.0.1: seq=0 ttl=42 time=0.023 ms\n//Data 64 bytes from 127.0.0.1: seq=1 ttl=42 time=0.011 ms\n//Data 64 bytes from 127.0.0.1: seq=2 ttl=42 time=0.012 ms\n//Data 64 bytes from 127.0.0.1: seq=3 ttl=42 time=0.016 ms\n```\n\n#### Êñá‰ª∂ÂàóË°®\n```go\n// Ëé∑ÂèñPodÂÜÖ/etcÊñá‰ª∂Â§πÂàóË°®\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").ListFiles(\"/etc\")\n```\n#### ÊâÄÊúâÊñá‰ª∂ÂàóË°®ÔºåÂåÖÊã¨ÈöêËóèÊñá‰ª∂\n```go\n// Ëé∑ÂèñPodÂÜÖ/etcÊñá‰ª∂Â§πÂàóË°®\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").ListAllFiles(\"/etc\")\n```\n#### Êñá‰ª∂‰∏ãËΩΩ\n```go\n// ‰∏ãËΩΩPodÂÜÖ/etc/hostsÊñá‰ª∂\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").DownloadFile(\"/etc/hosts\")\n```\n#### Êñá‰ª∂‰∏ãËΩΩ(TarÂéãÁº©)\n```go\n// ‰∏ãËΩΩPodÂÜÖ/etc/hostsÊñá‰ª∂Ôºå‰ª•tarÊñπÂºèËøõË°åÊâìÂåÖÂêéÔºåËé∑ÂèñÔºå‰∏ãËΩΩ\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").DownloadTarFile(\"/etc/hosts\")\n```\n#### Êñá‰ª∂‰∏ä‰º†\n```go\n// ‰∏ä‰º†Êñá‰ª∂ÂÜÖÂÆπÂà∞PodÂÜÖ/etc/demo.txtÊñá‰ª∂\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").SaveFile(\"/etc/demo.txt\", \"txt-context\")\n// os.File Á±ªÂûãÊñá‰ª∂Áõ¥Êé•‰∏ä‰º†Âà∞PodÂÜÖ/etc/ÁõÆÂΩï‰∏ã\nfile, _ := os.Open(tempFilePath)\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").UploadFile(\"/etc/\", file)\n```\n#### Êñá‰ª∂Âà†Èô§\n```go\n// Âà†Èô§PodÂÜÖ/etc/xyzÊñá‰ª∂\nkom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().ContainerName(\"nginx\").DeleteFile(\"/etc/xyz\")\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-Service\n```go\n// Ëé∑ÂèñPodÂÖ≥ËÅîÁöÑService\nsvcs, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedService()\nfor _, svc := range svcs {\n\tfmt.Printf(\"service name %v\\n\", svc.Name)\n}\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-Ingress\n```go\n// Ëé∑ÂèñPodÂÖ≥ËÅîÁöÑIngress\ningresses, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedIngress()\nfor _, ingress := range ingresses {\n\tfmt.Printf(\"ingress name %v\\n\", ingress.Name)\n}\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-PVC\n```go\n// Ëé∑ÂèñPodÂÖ≥ËÅîÁöÑPVC\npvcs, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedPVC()\nfor _, pvc := range pvcs {\n\tfmt.Printf(\"pvc name %v\\n\", pvc.Name)\n}\n``` \n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-PV\n```go\n// Ëé∑ÂèñPodÂÖ≥ËÅîÁöÑPVC\npvs, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedPV()\nfor _, pv := range pvs {\n\tfmt.Printf(\"pv name %v\\n\", pv.Name)\n}\n``` \n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-Endpoints\n```go\n// Ëé∑ÂèñPodÂÖ≥ËÅîÁöÑEndpoints\nendpoints, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedEndpoints()\nfor _, endpoint := range endpoints {\n\tfmt.Printf(\"endpoint name %v\\n\", endpoint.Name)\n}\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-ËøêË°åÊó∂Env\n‰ªéPodÂÜÖÊâßË°åenvÂëΩ‰ª§Ëé∑ÂæóENVÈÖçÁΩÆ‰ø°ÊÅØ\n```go\nenvs, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedEnv()\nfor _, env := range envs {\n\t\tfmt.Printf(\"env %s %s=%s\\n\", env.ContainerName, env.EnvName, env.EnvValue)\n\t}\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-ÂÆö‰πâEnv\n‰ªépodÂÆö‰πâ‰∏äÊèêÂèñENVÈÖçÁΩÆ‰ø°ÊÅØ\n```go\nenvs, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedEnvFromPod()\nfor _, env := range envs {\n\t\tfmt.Printf(\"env %s %s=%s\\n\", env.ContainerName, env.EnvName, env.EnvValue)\n\t}\n```\n#### Ëé∑ÂèñÂÖ≥ËÅîËµÑÊ∫ê-ËäÇÁÇπ\nÊ†πÊçÆPod ÂÆö‰πâ‰∏≠Â£∞ÊòéÁöÑNodeSelector„ÄÅNodeAffinity„ÄÅÊ±°ÁÇπÂÆπÂøçÂ∫¶„ÄÅNodeNameÁ≠âÈÖçÁΩÆ‰ø°ÊÅØÔºåËøîÂõûÂèØÁî®ËäÇÁÇπÂàóË°®„ÄÇÊöÇÊú™ËÄÉËôëPod‰∫≤ÂíåÊÄß„ÄÅCPUÂÜÖÂ≠òÁ≠âËøêË°åÊó∂Ë∞ÉÂ∫¶Âõ†Á¥†„ÄÇ\n```go\nnodes, err := kom.DefaultCluster().Namespace(\"default\").Name(\"nginx\").Ctl().Pod().LinkedNode()\nfor _, node := range nodes {\n    fmt.Printf(\"reason:%s\\t node name %s\\n\", node.Reason, node.Name)\n}\n```\n\n### 5. Ëá™ÂÆö‰πâËµÑÊ∫êÂÆö‰πâÔºàCRDÔºâÂ¢ûÂà†ÊîπÊü•ÂèäWatchÊìç‰Ωú\nÂú®Ê≤°ÊúâCRÂÆö‰πâÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ¶Ç‰ΩïËøõË°åÂ¢ûÂà†ÊîπÊü•Êìç‰Ωú„ÄÇÊìç‰ΩúÊñπÂºèÂêåk8sÂÜÖÁΩÆËµÑÊ∫ê„ÄÇ\nÂ∞ÜÂØπË±°ÂÆö‰πâ‰∏∫unstructured.UnstructuredÔºåÂπ∂‰∏îÈúÄË¶ÅÊåáÂÆöGroup„ÄÅVersion„ÄÅKind„ÄÇ\nÂõ†Ê≠§ÂèØ‰ª•ÈÄöËøákom.DefaultCluster().GVK(group, version, kind)Êù•Êõø‰ª£kom.DefaultCluster().Resource(interface{})\n‰∏∫Êñπ‰æøËÆ∞ÂøÜÂèä‰ΩøÁî®ÔºåkomÊèê‰æõ‰∫Ükom.DefaultCluster().CRD(group, version, kind)Êù•ÁÆÄÂåñÊìç‰Ωú„ÄÇ\n‰∏ãÈù¢ÁªôÂá∫Êìç‰ΩúCRDÁöÑÁ§∫‰æãÔºö\nÈ¶ñÂÖàÂÆö‰πâ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂ§ÑÁêÜÂØπË±°ÔºåÁî®Êù•Êé•Êî∂CRDÁöÑËøîÂõûÁªìÊûú„ÄÇ\n```go\nvar item unstructured.Unstructured\n```\n#### ÂàõÂª∫CRD\n```go\nyaml := `apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: crontabs.stable.example.com\nspec:\n  group: stable.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  scope: Namespaced\n  names:\n    plural: crontabs\n    singular: crontab\n    kind: CronTab\n    shortNames:\n    - ct`\nresult := kom.DefaultCluster().Applier().Apply(yaml)\n```\n#### ÂàõÂª∫CRDÁöÑCRÂØπË±°\n```go\nitem = unstructured.Unstructured{\n\t\tObject: map[string]interface{}{\n\t\t\t\"apiVersion\": \"stable.example.com/v1\",\n\t\t\t\"kind\":       \"CronTab\",\n\t\t\t\"metadata\": map[string]interface{}{\n\t\t\t\t\"name\":      \"test-crontab\",\n\t\t\t\t\"namespace\": \"default\",\n\t\t\t},\n\t\t\t\"spec\": map[string]interface{}{\n\t\t\t\t\"cronSpec\": \"* * * * */8\",\n\t\t\t\t\"image\":    \"test-crontab-image\",\n\t\t\t},\n\t\t},\n\t}\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Namespace(item.GetNamespace()).Name(item.GetName()).Create(\u0026item).Error\n```\n#### GetËé∑ÂèñÂçï‰∏™CRÂØπË±°\n```go\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Name(item.GetName()).Namespace(item.GetNamespace()).Get(\u0026item).Error\n```\n#### ListËé∑ÂèñCRÂØπË±°ÁöÑÂàóË°®\n```go\nvar crontabList []unstructured.Unstructured\n// Êü•ËØ¢defaultÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑCronTab\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Namespace(crontab.GetNamespace()).List(\u0026crontabList).Error\n// Êü•ËØ¢ÊâÄÊúâÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑCronTab\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").AllNamespace().List(\u0026crontabList).Error\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Namespace(\"*\").List(\u0026crontabList).Error\n```\n#### Êõ¥Êñ∞CRÂØπË±°\n```go\npatchData := `{\n    \"spec\": {\n        \"image\": \"patch-image\"\n    },\n    \"metadata\": {\n        \"labels\": {\n            \"new-label\": \"new-value\"\n        }\n    }\n}`\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Name(crontab.GetName()).Namespace(crontab.GetNamespace()).Patch(\u0026crontab, types.StrategicMergePatchType, patchData).Error\n```\n#### Âà†Èô§CRÂØπË±°\n```go\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Name(crontab.GetName()).Namespace(crontab.GetNamespace()).Delete().Error\n```\n#### Âº∫Âà∂Âà†Èô§CRÂØπË±°\n```go\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Name(crontab.GetName()).Namespace(crontab.GetNamespace()).ForceDelete().Error\n```\n#### Watch CRÂØπË±°\n```go\nvar watcher watch.Interface\n\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Namespace(\"default\").Watch(\u0026watcher).Error\nif err != nil {\n    fmt.Printf(\"Create Watcher Error %v\", err)\n}\ngo func() {\n    defer watcher.Stop()\n    \n    for event := range watcher.ResultChan() {\n    var item *unstructured.Unstructured\n    \n    item, err := kom.DefaultCluster().Tools().ConvertRuntimeObjectToUnstructuredObject(event.Object)\n    if err != nil {\n        fmt.Printf(\"Êó†Ê≥ïÂ∞ÜÂØπË±°ËΩ¨Êç¢‰∏∫ Unstructured Á±ªÂûã: %v\", err)\n        return\n    }\n    // Â§ÑÁêÜ‰∫ã‰ª∂\n    switch event.Type {\n        case watch.Added:\n            fmt.Printf(\"Added Unstructured [ %s/%s ]\\n\", item.GetNamespace(), item.GetName())\n        case watch.Modified:\n            fmt.Printf(\"Modified Unstructured [ %s/%s ]\\n\", item.GetNamespace(), item.GetName())\n        case watch.Deleted:\n            fmt.Printf(\"Deleted Unstructured [ %s/%s ]\\n\", item.GetNamespace(), item.GetName())\n        }\n    }\n}()\n```\n#### DescribeÊü•ËØ¢Êüê‰∏™CRDËµÑÊ∫ê\n```go\n// Describe default ÂëΩÂêçÁ©∫Èó¥‰∏ãÂêç‰∏∫ nginx ÁöÑ Deployment\nvar describeResult []byte\nerr := kom.DefaultCluster().CRD(\"stable.example.com\", \"v1\", \"CronTab\").Namespace(\"default\").Name(item.GetName()).Describe(\u0026item).Error\nfmt.Printf(\"describeResult: %s\", describeResult)\n```\n#### Ëé∑ÂèñCRD‰∏ãÁöÑPodËµÑÊ∫ê\n```go\npods, err := kom.DefaultCluster().CRD(\"apps.kruise.io\", \"v1beta1\", \"StatefulSet\").\nNamespace(\"default\").Name(\"sample\").Ctl().CRD().ManagedPods()\n\tfor _, pod := range pods {\n\t\tfmt.Printf(\"Get pods: %v\", pod.GetName())\n\t}\n```\n\n### 6. ÈõÜÁæ§ÂèÇÊï∞‰ø°ÊÅØ\n```go\n// ÈõÜÁæ§ÊñáÊ°£\nkom.DefaultCluster().Status().Docs()\n// ÈõÜÁæ§ËµÑÊ∫ê‰ø°ÊÅØ\nkom.DefaultCluster().Status().APIResources()\n// ÈõÜÁæ§Â∑≤Ê≥®ÂÜåCRDÂàóË°®\nkom.DefaultCluster().Status().CRDList()\n// ÈõÜÁæ§ÁâàÊú¨‰ø°ÊÅØ\nkom.DefaultCluster().Status().ServerVersion()\n// Ëé∑ÂèñÈõÜÁæ§ÂÜÖÂêÑËµÑÊ∫êÁßçÁ±ªÊï∞Èáè\nkom.DefaultCluster().Status().GetResourceCountSummary(10)\n```\n\n### 7. callbackÊú∫Âà∂\n* ÂÜÖÁΩÆ‰∫ÜcallbackÊú∫Âà∂ÔºåÂèØ‰ª•Ëá™ÂÆö‰πâÂõûË∞ÉÂáΩÊï∞ÔºåÂΩìÊâßË°åÂÆåÊüêÈ°πÊìç‰ΩúÂêéÔºå‰ºöË∞ÉÁî®ÂØπÂ∫îÁöÑÂõûË∞ÉÂáΩÊï∞„ÄÇ\n* Â¶ÇÊûúÂõûË∞ÉÂáΩÊï∞ËøîÂõûtrueÔºåÂàôÁªßÁª≠ÊâßË°åÂêéÁª≠Êìç‰ΩúÔºåÂê¶ÂàôÁªàÊ≠¢ÂêéÁª≠Êìç‰Ωú„ÄÇ\n* ÂΩìÂâçÊîØÊåÅÁöÑcallbackÊúâÔºöget,list,create,update,patch,delete,exec,stream-exec,logs,watch,doc.\n* ÂÜÖÁΩÆÁöÑcallbackÂêçÁß∞ÊúâÔºö\"kom:get\",\"kom:list\",\"kom:create\",\"kom:update\",\"kom:patch\",\"kom:watch\",\"kom:delete\",\"kom:pod:exec\",\"kom:pod:stream:exec\",\"kom:pod:logs\",\"kom:pod:port:forward\",\"kom:doc\"\n* ÊîØÊåÅÂõûË∞ÉÂáΩÊï∞ÊéíÂ∫èÔºåÈªòËÆ§ÊåâÊ≥®ÂÜåÈ°∫Â∫èÊâßË°åÔºåÂèØ‰ª•ÈÄöËøákom.DefaultCluster().Callback().After(\"kom:get\")ÊàñËÄÖ.Before(\"kom:get\")ËÆæÁΩÆÈ°∫Â∫è„ÄÇ\n* ÊîØÊåÅÂà†Èô§ÂõûË∞ÉÂáΩÊï∞ÔºåÈÄöËøákom.DefaultCluster().Callback().Delete(\"kom:get\")\n* ÊîØÊåÅÊõøÊç¢ÂõûË∞ÉÂáΩÊï∞ÔºåÈÄöËøákom.DefaultCluster().Callback().Replace(\"kom:get\",cb)\n```go\n// ‰∏∫GetËé∑ÂèñËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Get().Register(\"get\", cb)\n// ‰∏∫ListËé∑ÂèñËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().List().Register(\"list\", cb)\n// ‰∏∫CreateÂàõÂª∫ËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Create().Register(\"create\", cb)\n// ‰∏∫UpdateÊõ¥Êñ∞ËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Update().Register(\"update\", cb)\n// ‰∏∫PatchÊõ¥Êñ∞ËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Patch().Register(\"patch\", cb)\n// ‰∏∫DeleteÂà†Èô§ËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Delete().Register(\"delete\", cb)\n// ‰∏∫WatchËµÑÊ∫êÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Watch().Register(\"watch\",cb)\n// ‰∏∫Exec PodÂÜÖÊâßË°åÂëΩ‰ª§Ê≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Exec().Register(\"exec\", cb)\n// ‰∏∫LogsËé∑ÂèñÊó•ÂøóÊ≥®ÂÜåÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Logs().Register(\"logs\", cb)\n// Âà†Èô§ÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Get().Delete(\"get\")\n// ÊõøÊç¢ÂõûË∞ÉÂáΩÊï∞\nkom.DefaultCluster().Callback().Get().Replace(\"get\", cb)\n// ÊåáÂÆöÂõûË∞ÉÂáΩÊï∞ÊâßË°åÈ°∫Â∫èÔºåÂú®ÂÜÖÁΩÆÁöÑÂõûË∞ÉÂáΩÊï∞ÊâßË°åÂÆå‰πãÂêéÂÜçÊâßË°å\nkom.DefaultCluster().Callback().After(\"kom:get\").Register(\"get\", cb)\n// ÊåáÂÆöÂõûË∞ÉÂáΩÊï∞ÊâßË°åÈ°∫Â∫èÔºåÂú®ÂÜÖÁΩÆÁöÑÂõûË∞ÉÂáΩÊï∞ÊâßË°å‰πãÂâçÂÖàÊâßË°å\n// Ê°à‰æã1.Âú®CreateÂàõÂª∫ËµÑÊ∫êÂâçÔºåËøõË°åÊùÉÈôêÊ£ÄÊü•ÔºåÊ≤°ÊúâÊùÉÈôêÂàôËøîÂõûerrorÔºåÂêéÁª≠ÂàõÂª∫Âä®‰ΩúÂ∞Ü‰∏çÂÜçÊâßË°å\n// Ê°à‰æã2.Âú®ListËé∑ÂèñËµÑÊ∫êÂàóË°®ÂêéÔºåËøõË°åÁâπÂÆöÁöÑËµÑÊ∫êÁ≠õÈÄâÔºå‰ªéÂàóË°®(Statement.Dest)‰∏≠Âà†Èô§‰∏çÁ¨¶ÂêàË¶ÅÊ±ÇÁöÑËµÑÊ∫êÔºåÁÑ∂ÂêéËøîÂõûÁªôÁî®Êà∑\nkom.DefaultCluster().Callback().Before(\"kom:create\").Register(\"create\", cb)\n\n// Ëá™ÂÆö‰πâÂõûË∞ÉÂáΩÊï∞\nfunc cb(k *kom.Kubectl) error {\n    stmt := k.Statement\n    gvr := stmt.GVR\n    ns := stmt.Namespace\n    name := stmt.Name\n    // ÊâìÂç∞‰ø°ÊÅØ\n    fmt.Printf(\"Get %s/%s(%s)\\n\", ns, name, gvr)\n    fmt.Printf(\"Command %s/%s(%s %s)\\n\", ns, name, stmt.Command, stmt.Args)\n    return nil\n\t// return fmt.Errorf(\"error\") ËøîÂõûerrorÂ∞ÜÈòªÊ≠¢ÂêéÁª≠cbÁöÑÊâßË°å\n}\n```\n\n### 8. SQLÊü•ËØ¢k8sËµÑÊ∫ê\n* ÈÄöËøáSQL()ÊñπÊ≥ïÊü•ËØ¢k8sËµÑÊ∫êÔºåÁÆÄÂçïÈ´òÊïà„ÄÇ\n* Table ÂêçÁß∞ÊîØÊåÅÈõÜÁæ§ÂÜÖÊ≥®ÂÜåÁöÑÊâÄÊúâËµÑÊ∫êÁöÑÂÖ®Áß∞ÂèäÁÆÄÂÜôÔºåÂåÖÊã¨CRDËµÑÊ∫ê„ÄÇÂè™Ë¶ÅÊòØÊ≥®ÂÜåÂà∞ÈõÜÁæ§‰∏ä‰∫ÜÔºåÂ∞±ÂèØ‰ª•Êü•„ÄÇ\n* ÂÖ∏ÂûãÁöÑTable ÂêçÁß∞ÊúâÔºöpod,deployment,service,ingress,pvc,pv,node,namespace,secret,configmap,serviceaccount,role,rolebinding,clusterrole,clusterrolebinding,crd,cr,hpa,daemonset,statefulset,job,cronjob,limitrange,horizontalpodautoscaler,poddisruptionbudget,networkpolicy,endpoints,ingressclass,mutatingwebhookconfiguration,validatingwebhookconfiguration,customresourcedefinition,storageclass,persistentvolumeclaim,persistentvolume,horizontalpodautoscaler,podsecurity„ÄÇÁªüÁªüÈÉΩÂèØ‰ª•Êü•„ÄÇ\n* Êü•ËØ¢Â≠óÊÆµÁõÆÂâç‰ªÖÊîØÊåÅ*„ÄÇ‰πüÂ∞±ÊòØselect *\n* Êü•ËØ¢Êù°‰ª∂ÁõÆÂâçÊîØÊåÅ =Ôºå!=,\u003e=,\u003c=,\u003c\u003e,like,in,not in,and,or,between\n* ÊéíÂ∫èÂ≠óÊÆµÁõÆÂâçÊîØÊåÅÂØπÂçï‰∏ÄÂ≠óÊÆµËøõË°åÊéíÂ∫è„ÄÇÈªòËÆ§ÊåâÂàõÂª∫Êó∂Èó¥ÂÄíÂ∫èÊéíÂàó\n* \n#### Êü•ËØ¢k8sÂÜÖÁΩÆËµÑÊ∫ê\n```go\n    sql := \"select * from deploy where metadata.namespace='kube-system' or metadata.namespace='default' order by  metadata.creationTimestamp asc   \"\n\n\tvar list []v1.Deployment\n\terr := kom.DefaultCluster().Sql(sql).List(\u0026list).Error\n\tfor _, d := range list {\n\t\tfmt.Printf(\"List Items foreach %s,%s at %s \\n\", d.GetNamespace(), d.GetName(), d.GetCreationTimestamp())\n\t}\n```\n#### Êü•ËØ¢CRDËµÑÊ∫ê\n```go\n    // vm ‰∏∫kubevirt ÁöÑCRD\n    sql := \"select * from vm where (metadata.namespace='kube-system' or metadata.namespace='default' )  \"\n\tvar list []unstructured.Unstructured\n\terr := kom.DefaultCluster().Sql(sql).List(\u0026list).Error\n\tfor _, d := range list {\n\t\tfmt.Printf(\"List Items foreach %s,%s\\n\", d.GetNamespace(), d.GetName())\n\t}\n```\n#### ÈìæÂºèË∞ÉÁ†îÊü•ËØ¢SQL\n```go\n// Êü•ËØ¢pod ÂàóË°®\nerr := kom.DefaultCluster().From(\"pod\").\n\t\tWhere(\"metadata.namespace = ?  or metadata.namespace= ? \", \"kube-system\", \"default\").\n\t\tOrder(\"metadata.creationTimestamp desc\").\n\t\tList(\u0026list).Error\n```\n#### k8sËµÑÊ∫êÂµåÂ•óÂàóË°®Â±ûÊÄßÊîØÊåÅ\n```go\n// spec.containers‰∏∫ÂàóË°®ÔºåÂÖ∂‰∏ãÁöÑports‰πü‰∏∫ÂàóË°®ÔºåÊàë‰ª¨Êü•ËØ¢portsÁöÑname\nsql := \"select * from pod where spec.containers.ports.name like '%k8m%'  \"\nvar list []v1.Pod\nerr := kom.DefaultCluster().Sql(sql).List(\u0026list).Error\nfor _, d := range list {\n\tt.Logf(\"List Items foreach %s,%s\\n\", d.GetNamespace(), d.GetName())\n}\n```\n\n### 9. ÂÖ∂‰ªñÊìç‰Ωú\n#### DeploymentÈáçÂêØ\n```go\nerr = kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Restart()\n```\n#### DeploymentÊâ©Áº©ÂÆπ\n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑÂâØÊú¨Êï∞ËÆæÁΩÆ‰∏∫3\nerr = kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Scaler().Scale(3)\n```\n#### Deployment ÂÅúÊ≠¢\n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑÂâØÊú¨Êï∞ËÆæÁΩÆ‰∏∫0\n// ÂΩìÂâçËøêË°åÂâØÊú¨Êï∞ÈáèËÆ∞ÂΩïÂà∞Ê≥®Ëß£‰∏≠\nerr = kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Scaler().Stop()\n```\n#### Deployment ÊÅ¢Â§ç\n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑÂâØÊú¨Êï∞‰ªéÊ≥®Ëß£‰∏≠ÊÅ¢Â§çÔºåÂ¶ÇÊûúÊ≤°ÊúâÊ≥®Ëß£ÔºåÈªòËÆ§ÊÅ¢Â§çÂà∞1\nerr = kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Scaler().Restore()\n```\n#### DeploymentÊõ¥Êñ∞Tag\n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑ‰∏≠ÁöÑÂÆπÂô®ÈïúÂÉètagÂçáÁ∫ß‰∏∫alpine\nerr = kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Deployment().ReplaceImageTag(\"main\",\"20241124\")\n```\n#### Deployment Rollout History\n```go\n// Êü•ËØ¢ÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑÂçáÁ∫ßÂéÜÂè≤\nresult, err := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().History()\n```\n#### Deployment Rollout Undo\n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentËøõË°åÂõûÊªö\nresult, err := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Undo()\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentËøõË°åÂõûÊªöÂà∞ÊåáÂÆöÁâàÊú¨(history Êü•ËØ¢)\nresult, err := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Undo(\"6\")\n```\n#### Deployment Rollout Pause\n```go\n// ÊöÇÂÅúÂçáÁ∫ßËøáÁ®ã\nerr := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Pause()\n```\n#### Deployment Rollout Resume \n```go\n// ÊÅ¢Â§çÂçáÁ∫ßËøáÁ®ã\nerr := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Resume()\n```\n#### Deployment Rollout Status \n```go\n// Â∞ÜÂêçÁß∞‰∏∫nginxÁöÑdeploymentÁöÑ‰∏≠ÁöÑÂÆπÂô®ÈïúÂÉètagÂçáÁ∫ß‰∏∫alpine\nresult, err := kom.DefaultCluster().Resource(\u0026Deployment{}).Namespace(\"default\").Name(\"nginx\").Ctl().Rollout().Status()\n```\n#### Deployment HPA\n```go\n// ÊòæÁ§∫deploymentÁöÑhpa \nlist, err := kom.DefaultCluster().Resource(\u0026v1.Deployment{}).Namespace(\"default\").Name(\"nginx-web\").Ctl().Deployment().HPAList()\nfor _, item := range list {\n    t.Logf(\"HPA %s\\n\", item.Name)\n}\n```\n#### ËäÇÁÇπÊâìÊ±°ÁÇπ\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Node().Taint(\"dedicated=special-user:NoSchedule\")\n```\n#### ËäÇÁÇπÂéªÈô§Ê±°ÁÇπ\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Node().UnTaint(\"dedicated=special-user:NoSchedule\")\n```\n#### ËäÇÁÇπCordon\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Node().Cordon()\n```\n#### ËäÇÁÇπUnCordon\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Node().UnCordon()\n```\n#### ËäÇÁÇπDrain\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Node().Drain()\n```\n#### Êü•ËØ¢ËäÇÁÇπIPËµÑÊ∫êÊÉÖÂÜµ\nÊîØÊåÅËÆæÁΩÆÁºìÂ≠òÊó∂Èó¥ÔºåÈÅøÂÖçÈ¢ëÁπÅÊü•ËØ¢k8s API\n```go\nnodeName := \"lima-rancher-desktop\"\ntotal, used, available := kom.DefaultCluster().Resource(\u0026corev1.Node{}).WithCache(5 * time.Second).Name(nodeName).Ctl().Node().IPUsage()\nfmt.Printf(\"Total %d, Used %d, Available %d\\n\", total, used, available)\n//Total 256, Used 6, Available 250\n```\n#### ËäÇÁÇπIPËµÑÊ∫ê‰ΩøÁî®ÊÉÖÂÜµÁªüËÆ°\nÊîØÊåÅËÆæÁΩÆÁºìÂ≠òÊó∂Èó¥ÔºåÈÅøÂÖçÈ¢ëÁπÅÊü•ËØ¢k8s API\n```go\nnodeName := \"lima-rancher-desktop\"\ntotal, used, available := kom.DefaultCluster().Resource(\u0026corev1.Node{}).WithCache(5 * time.Second).Name(nodeName).Ctl().Node().PodCount()\nfmt.Printf(\"Total %d, Used %d, Available %d\\n\", total, used, available)\n//Total 110, Used 9, Available 101\n```\n#### ËäÇÁÇπËµÑÊ∫êÁî®ÈáèÊÉÖÂÜµÁªüËÆ°\nÊîØÊåÅËÆæÁΩÆÁºìÂ≠òÊó∂Èó¥ÔºåÈÅøÂÖçÈ¢ëÁπÅÊü•ËØ¢k8s API\n```go\nnodeName := \"lima-rancher-desktop\"\nusage := kom.DefaultCluster().Resource(\u0026corev1.Node{}).WithCache(5 * time.Second).Name(nodeName).Ctl().Node().ResourceUsage()\nfmt.Printf(\"Node Usage %s\\n\", utils.ToJSON(usage))\n```\nÂåÖÊã¨ÂΩìÂâçÁöÑËØ∑Ê±ÇÂÄº„ÄÅÈôêÂà∂ÂÄº„ÄÅÂèØÂàÜÈÖçÂÄº„ÄÅ‰ΩøÁî®ÊØî‰æã\n```json\n{\n  \"requests\": {\n    \"cpu\": \"200m\",\n    \"memory\": \"140Mi\"\n  },\n  \"limits\": {\n    \"memory\": \"170Mi\"\n  },\n  \"allocatable\": {\n    \"cpu\": \"4\",\n    \"ephemeral-storage\": \"99833802265\",\n    \"hugepages-1Gi\": \"0\",\n    \"hugepages-2Mi\": \"0\",\n    \"hugepages-32Mi\": \"0\",\n    \"hugepages-64Ki\": \"0\",\n    \"memory\": \"8127096Ki\",\n    \"pods\": \"110\"\n  },\n  \"usageFractions\": {\n    \"cpu\": {\n      \"requestFraction\": 5,\n      \"limitFraction\": 0\n    },\n    \"ephemeral-storage\": {\n      \"requestFraction\": 0,\n      \"limitFraction\": 0\n    },\n    \"memory\": {\n      \"requestFraction\": 1.76397571777176,\n      \"limitFraction\": 2.1419705144371375\n    }\n  }\n}\n```\n#### ÁªôËµÑÊ∫êÂ¢ûÂä†Ê†áÁ≠æ\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Label(\"name=zhangsan\")\n```\n#### ÁªôËµÑÊ∫êÂà†Èô§Ê†áÁ≠æ\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Label(\"name-\")\n```\n#### ÁªôËµÑÊ∫êÂ¢ûÂä†Ê≥®Ëß£\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Annotate(\"name=zhangsan\")\n```\n#### ÁªôËµÑÊ∫êÂà†Èô§Ê≥®Ëß£\n```go\nerr = kom.DefaultCluster().Resource(\u0026Node{}).Name(\"kind-control-plane\").Ctl().Annotate(\"name-\")\n```\n#### ÂàõÂª∫NodeSell\n```go\nns, pod, container, err  := kom.DefaultCluster().Resource(\u0026v1.Node{}).Name(\"kind-control-plane\").Ctl().Node().CreateNodeShell()\nfmt.Printf(\"Node Shell ns=%s podName=%s containerName=%s\", ns, pod, container)\n```\n#### ÂàõÂª∫kubectl Shell\n```go\nns, pod, container, err := kom.DefaultCluster().Resource(\u0026v1.Node{}).Name(name).Ctl().Node().CreateKubectlShell(kubeconfig)\nfmt.Printf(\"Kubectl Shell ns=%s podName=%s containerName=%s\", ns, pod, container)\n\n```\n#### ÁªüËÆ°StorageClass‰∏ãÁöÑPVCÊï∞Èáè\n```go\ncount, err := kom.DefaultCluster().Resource(\u0026v1.StorageClass{}).Name(\"hostpath\").Ctl().StorageClass().PVCCount()\nfmt.Printf(\"pvc count %d\\n\", count)\n```\n#### ÁªüËÆ°StorageClass‰∏ãÁöÑPVÊï∞Èáè\n```go\ncount, err := kom.DefaultCluster().Resource(\u0026v1.StorageClass{}).Name(\"hostpath\").Ctl().StorageClass().PVCount()\nfmt.Printf(\"pv count %d\\n\", count)\n```\n#### ËÆæÁΩÆStorageClass‰∏∫ÈªòËÆ§\n```go\nerr := kom.DefaultCluster().Resource(\u0026v1.StorageClass{}).Name(\"hostpath\").Ctl().StorageClass().SetDefault()\n```\n#### ËÆæÁΩÆIngressClass‰∏∫ÈªòËÆ§\n```go\nerr := kom.DefaultCluster().Resource(\u0026v1.IngressClass{}).Name(\"nginx\").Ctl().IngressClass().SetDefault()\n```\n#### ÁªüËÆ°Deployment/StatefulSet/DaemonSet‰∏ãÁöÑPodÂàóË°®\n```go\nlist, err := kom.DefaultCluster().Namespace(\"default\").Name(\"managed-pods\").Ctl().Deployment().ManagedPods()\nfor _, pod := range list {\n\tfmt.Printf(\"ManagedPod: %v\", pod.Name)\n}\n```\n#### Ëé∑ÂèñÊâÄÊúâËäÇÁÇπÁöÑÊ†áÁ≠æÈõÜÂêà\n```go\n// labels Á±ªÂûã‰∏∫map[string]string\nlabels, err := kom.DefaultCluster().Resource(\u0026v1.Node{}).Ctl().Node().AllNodeLabels()\nfmt.Printf(\"%s\", utils.ToJSON(labels))\n```\n```json\n{\n          \"beta.kubernetes.io/arch\": \"arm64\",\n          \"beta.kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/arch\": \"arm64\",\n          \"kubernetes.io/hostname\": \"kind-control-plane\",\n          \"kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/role\": \"agent\",\n          \"node-role.kubernetes.io/agent\": \"\",\n          \"node-role.kubernetes.io/control-plane\": \"\",\n          \"type\": \"kwok\",\n          \"uat\": \"test\",\n          \"x\": \"x\"\n}\n```\n#### Êü•ÁúãPodËµÑÊ∫êÂç†Áî®Áéá\n```go\npodName := \"coredns-ccb96694c-jprpf\"\nns := \"kube-system\"\nusage := kom.DefaultCluster().Resource(\u0026corev1.Pod{}).Name(podName).Namespace(ns).Ctl().Pod().ResourceUsage()\nfmt.Printf(\"Pod Usage %s\\n\", utils.ToJSON(usage))\n```\nÂåÖÊã¨ÂΩìÂâçÁöÑËØ∑Ê±ÇÂÄº„ÄÅÈôêÂà∂ÂÄº„ÄÅÂèØÂàÜÈÖçÂÄº„ÄÅ‰ΩøÁî®ÊØî‰æã\n```json\n{\n  \"requests\": {\n    \"cpu\": \"100m\",\n    \"memory\": \"70Mi\"\n  },\n  \"limits\": {\n    \"memory\": \"170Mi\"\n  },\n  \"allocatable\": {\n    \"cpu\": \"4\",\n    \"ephemeral-storage\": \"99833802265\",\n    \"hugepages-1Gi\": \"0\",\n    \"hugepages-2Mi\": \"0\",\n    \"hugepages-32Mi\": \"0\",\n    \"hugepages-64Ki\": \"0\",\n    \"memory\": \"8127096Ki\",\n    \"pods\": \"110\"\n  },\n  \"usageFractions\": {\n    \"cpu\": {\n      \"requestFraction\": 2.5,\n      \"limitFraction\": 0\n    },\n    \"memory\": {\n      \"requestFraction\": 0.88198785888588,\n      \"limitFraction\": 2.1419705144371375\n    }\n  }\n}\n```\n#### Ëé∑ÂèñÂ≠óÊÆµÊñáÊ°£Ëß£Èáä\n```go\nvar docResult []byte\n\titem := v1.Deployment{}\n\tfield := \"spec.replicas\"\n\tfield = \"spec.template.spec.containers.name\"\n\tfield = \"spec.template.spec.containers.imagePullPolicy\"\n\tfield = \"spec.template.spec.containers.livenessProbe.successThreshold\"\n\terr := kom.DefaultCluster().\n\t\tResource(\u0026item).DocField(field).Doc(\u0026docResult).Error\n\tfmt.Printf(\"Get Deployment Doc [%s] :%s\", field, string(docResult))\n```\n\n",
      "stars": 126,
      "updated_at": "2025-09-29T02:24:56Z",
      "url": "https://github.com/weibaohui/kom"
    },
    "wenhuwang--mcp-k8s-eye": {
      "category": "cloud-platforms",
      "description": "MCP Server for kubernetes management, and analyze your cluster, application health",
      "forks": 7,
      "imageUrl": "",
      "keywords": [
        "kubernetes",
        "cloud",
        "platform",
        "cloud platform",
        "cloud platforms",
        "platforms cloud"
      ],
      "language": "Go",
      "license": "Apache License 2.0",
      "name": "mcp-k8s-eye",
      "npm_downloads": 0,
      "npm_url": "",
      "owner": "wenhuwang",
      "readme_content": "## mcp-k8s-eye\n\nmcp-k8s-eye is a tool that can manage kubernetes cluster and analyze workload status.\n\n## Features\n\n### Core Kubernetes Operations\n- [x] Connect to a Kubernetes cluster\n- [x] **Generic Kubernetes Resources** management capabilities\n  - Support all navtie resources: Pod, Deployment, Service, StatefulSet, Ingress...\n  - Support CustomResourceDefinition resources\n  - Operations include: list, get, create, update, delete\n- [x] Pod management capabilities (exec, logs)\n- [x] Deployment management capabilities (scale)\n- [x] Describe Kubernetes resources\n- [ ] Explain Kubernetes resources\n\n\n### Diagnostics\n- [x] Pod diagnostics (analyze pod status, container status, pod resource utilization)\n- [x] Service diagnostics (analyze service selector configuration, not ready endpoints, events)\n- [x] Deployment diagnostics (analyze available replicas)\n- [x] StatefulSet diagnostics (analyze statefulset service if exists, pvc if exists, available replicas)\n- [x] CronJob diagnostics (analyze cronjob schedule, starting deadline, last schedule time)\n- [x] Ingress diagnostics (analyze ingress class configuration, related services, tls secrets)\n- [x] NetworkPolicy diagnostics (analyze networkpolicy configuration, affected pods)\n- [x] ValidatingWebhook diagnostics (analyze webhook configuration, referenced services and pods)\n- [x] MutatingWebhook diagnostics (analyze webhook configuration, referenced services and pods)\n- [x] Node diagnostics (analyze node conditions)\n- [ ] Cluster diagnostics and troubleshooting \n\n### Monitoring\n- [x] Pod, Deployment, ReplicaSet, StatefulSet, DaemonSet workload resource usage (cpu, memory)\n- [ ] Node capacity, utilization (cpu, memory)\n- [ ] Cluster capacity, utilization (cpu, memory)\n\n### Advanced Features\n- [x] Multiple transport protocols support (Stdio, SSE)\n- [x] Support multiple AI Clients\n\n\n## Tools Usage\n\n### Resource Operation Tools\n- `resource_get`: Get detailed resource information about a specific resource in a namespace\n- `resource_list`: List detailed resource information about all resources in a namespace \n- `resource_create_or_update`: Create or update a resource in a namespace\n- `resource_delete`: Delete a resource in a namespace\n- `resource_describe`: Describe a resource detailed information in a namespace\n- `deployment_scale`: Scale a deployment in a namespace\n- `pod_exec`: Execute a command in a pod in a namespace`\n- `pod_logs`: Get logs from a pod in a namespace\n\n###  Diagnostics Tools\n- `pod_analyze`: Diagnose all pods in a namespace\n- `deployment_analyze`: Diagnose all deployments in a namespace\n- `statefulset_analyze`: Diagnose all statefulsets in a namespace\n- `service_analyze`: Diagnose all services in a namespace\n- `cronjob_analyze`: Diagnose all cronjobs in a namespace\n- `ingress_analyze`: Diagnose all ingresses in a namespace\n- `networkpolicy_analyze`: Diagnose all networkpolicies in a namespace\n- `validatingwebhook_analyze`: Diagnose all validatingwebhooks\n- `mutatingwebhook_analyze`: Diagnose all mutatingwebhooks\n- `node_analyze`: Diagnose all nodes in cluster\n\n### Monitoring Tools\n- `workload_resource_usage`: Get pod/deployment/replicaset/statefulset resource usage in a namepace (cpu, memory)\n\n\n## Requirements\n- Go 1.23 or higher\n- kubectl configured\n\n## Installation\n```\n# clone the repository\ngit clone https://github.com/wenhuwang/mcp-k8s-eye.git\ncd mcp-k8s-eye\n\n# build the binary\ngo build -o mcp-k8s-eye\n```\n\n## Usage\n### Stdio mode\n```\n{\n  \"mcpServers\": {\n    \"k8s eye\": {\n      \"command\": \"YOUR mcp-k8s-eye PATH\",\n      \"env\": {\n        \"HOME\": \"USER HOME DIR\"\n      },\n    }\n  }\n}\n```\n`env.HOME` is used to set the HOME directory for kubeconfig file.\n\n### SSE mode\n1. start your mcp sse server\n2. config your mcp server\n\n```\n{\n  \"mcpServers\": {\n    \"k8s eye\": {\n      \"url\": \"http://localhost:8080/sse\",\n      \"env\": {}\n    }\n  }\n}\n```\n\n### cursor tools",
      "stars": 25,
      "updated_at": "2025-08-20T01:16:43Z",
      "url": "https://github.com/wenhuwang/mcp-k8s-eye"
    }
  },
  "totalRepositories": 40
}