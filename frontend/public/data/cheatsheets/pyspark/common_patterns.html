<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Common Dataframe Patterns - Code Examples</title>
    <meta name="description" content="Explore common PySpark DataFrame patterns for data manipulation, filtering, joins, column operations, casting, and handling nulls and duplicates.">
    <meta name="keywords" content="PySpark, DataFrame, data manipulation, filtering, joins, column operations, casting, null handling, duplicate removal, Python, Spark SQL">
    <link rel="canonical" href="https://example.com/pyspark/common_patterns.html">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="PySpark Common Dataframe Patterns - Code Examples">
    <meta property="og:description" content="Explore common PySpark DataFrame patterns for data manipulation, filtering, joins, column operations, casting, and handling nulls and duplicates.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://example.com/pyspark/common_patterns.html">
    <meta property="og:image" content="https://example.com/images/pyspark-common-patterns.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="PySpark Common Dataframe Patterns - Code Examples">
    <meta name="twitter:description" content="Explore common PySpark DataFrame patterns for data manipulation, filtering, joins, column operations, casting, and handling nulls and duplicates.">
    <meta name="twitter:image" content="https://example.com/images/pyspark-common-patterns.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .header {
            background: #2c3e50;
            color: white;
            padding: 20px;
            border-bottom: 1px solid #34495e;
        }
        .header h1 {
            margin: 0;
            font-size: 1.5em;
        }
        .content {
            padding: 20px;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
            margin: 0;
        }
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
        }
        .markdown-content {
            line-height: 1.7;
        }
        .markdown-content h1, .markdown-content h2, .markdown-content h3 {
            color: #2c3e50;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 8px;
        }
        .markdown-content pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
        }
        .markdown-content code {
            background: #f1f3f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        .markdown-content pre code {
            background: none;
            padding: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>PySpark Common Dataframe Patterns</h1>
        </div>
        <div class="content">
            <div class="markdown-content">
                <p>This section outlines common and essential patterns for working with PySpark DataFrames, providing concise code examples for efficient data manipulation and analysis.</p>

                <h2>Importing Functions &amp; Types</h2>
                <p>It's good practice to import PySpark SQL functions and types with aliases for cleaner code.</p>
                <pre><code class="language-python"># Easily reference these as F.my_function() and T.my_type() below
from pyspark.sql import functions as F, types as T
</code></pre>

                <h2>Filtering DataFrames</h2>
                <p>Learn how to filter rows based on various conditions, including equality, range comparisons, and multiple criteria.</p>
                <h3>Filtering by Equality</h3>
                <pre><code class="language-python"># Filter on equals condition
df = df.filter(df.is_adult == 'Y')
</code></pre>
                <h3>Filtering by Range</h3>
                <pre><code class="language-python"># Filter on >, <, >=, <= condition
df = df.filter(df.age > 25)
</code></pre>
                <h3>Filtering with Multiple Conditions</h3>
                <p>Combine conditions using logical AND (&amp;) and OR (|) operators. Ensure each condition is enclosed in parentheses.</p>
                <pre><code class="language-python"># Multiple conditions require parentheses around each condition
df = df.filter((df.age > 25) &amp; (df.is_adult == 'Y'))
</code></pre>
                <h3>Filtering Against a List</h3>
                <pre><code class="language-python"># Compare against a list of allowed values
from pyspark.sql.functions import col
df = df.filter(col('first_name').isin([3, 4, 7]))
</code></pre>
                <h3>Sorting Results</h3>
                <p>Order your DataFrame based on one or more columns in ascending or descending order.</p>
                <pre><code class="language-python"># Sort results ascending
df = df.orderBy(df.age.asc())
# Sort results descending
df = df.orderBy(df.age.desc())
</code></pre>

                <h2>Joining DataFrames</h2>
                <p>Understand different types of joins and how to match columns between DataFrames.</p>
                <h3>Left Join</h3>
                <pre><code class="language-python"># Left join in another dataset
df = df.join(person_lookup_table, 'person_id', 'left')
</code></pre>
                <h3>Joining on Different Columns</h3>
                <pre><code class="language-python"># Match on different columns in left &amp; right datasets
df = df.join(other_table, df.id == other_table.person_id, 'left')
</code></pre>
                <h3>Joining on Multiple Columns</h3>
                <pre><code class="language-python"># Match on multiple columns
df = df.join(other_table, ['first_name', 'last_name'], 'left')
</code></pre>

                <h2>Column Operations</h2>
                <p>Perform various operations on DataFrame columns, including adding, modifying, selecting, and renaming.</p>
                <h3>Adding a Static Column</h3>
                <pre><code class="language-python"># Add a new static column
df = df.withColumn('status', F.lit('PASS'))
</code></pre>
                <h3>Constructing a Dynamic Column</h3>
                <p>Create new columns based on conditional logic using <code>when</code> and <code>otherwise</code>.</p>
                <pre><code class="language-python"># Construct a new dynamic column
df = df.withColumn('full_name', F.when(
    (df.fname.isNotNull() &amp; df.lname.isNotNull()), F.concat(df.fname, df.lname)
).otherwise(F.lit('N/A')))
</code></pre>
                <h3>Selecting and Renaming Columns</h3>
                <p>Choose specific columns to keep and rename them for clarity.</p>
                <pre><code class="language-python"># Pick which columns to keep, optionally rename some
df = df.select(
    'name',
    'age',
    F.col('dob').alias('date_of_birth'),
)
</code></pre>
                <h3>Removing Columns</h3>
                <pre><code class="language-python"># Remove columns
df = df.drop('mod_dt', 'mod_username')
</code></pre>
                <h3>Renaming a Column</h3>
                <pre><code class="language-python"># Rename a column
df = df.withColumnRenamed('dob', 'date_of_birth')
</code></pre>
                <h3>Selecting Columns from Another DataFrame</h3>
                <pre><code class="language-python"># Keep all the columns which also occur in another dataset
df = df.select(*(F.col(c) for c in df2.columns))
</code></pre>
                <h3>Batch Rename/Clean Columns</h3>
                <p>A common task is to clean up column names by converting them to lowercase and replacing spaces or hyphens.</p>
                <pre><code class="language-python"># Batch Rename/Clean Columns
for col_name in df.columns:
    df = df.withColumnRenamed(col_name, col_name.lower().replace(' ', '_').replace('-', '_'))
</code></pre>

                <h2>Casting, Null Handling, and Duplicates</h2>
                <p>Manage data types, replace null values, and handle duplicate records effectively.</p>
                <h3>Casting Column Types</h3>
                <pre><code class="language-python"># Cast a column to a different type
df = df.withColumn('price', df.price.cast(T.DoubleType()))
</code></pre>
                <h3>Filling Null Values</h3>
                <p>Replace nulls in specific columns with default values.</p>
                <pre><code class="language-python"># Replace all nulls with a specific value
df = df.fillna({
    'first_name': 'Tom',
    'age': 0,
})
</code></pre>
                <h3>Coalescing Values</h3>
                <p>Select the first non-null value from a list of columns.</p>
                <pre><code class="language-python"># Take the first value that is not null
df = df.withColumn('last_name', F.coalesce(df.last_name, df.surname, F.lit('N/A')))
</code></pre>
                <h3>Dropping Duplicates</h3>
                <p>Remove duplicate rows from a DataFrame.</p>
                <pre><code class="language-python"># Drop duplicate rows in a dataset (distinct)
df = df.dropDuplicates() # or
df = df.distinct()

# Drop duplicate rows, but consider only specific columns
df = df.dropDuplicates(['name', 'height'])
</code></pre>
                <h3>Replacing Empty Strings with Null</h3>
                <pre><code class="language-python"># Replace empty strings with null (leave out subset keyword arg to replace in all columns)
df = df.replace({"": None}, subset=["name"])
</code></pre>
                <h3>Replacing NaN Values</h3>
                <pre><code class="language-python"># Convert Python/PySpark/NumPy NaN operator to null
df = df.replace(float("nan"), None)
</code></pre>

                <h2>External Resources</h2>
                <ul>
                    <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#module-pyspark.sql.functions" target="_blank" rel="noopener noreferrer">PySpark SQL Functions Documentation</a></li>
                    <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#module-pyspark.sql.types" target="_blank" rel="noopener noreferrer">PySpark SQL Types Documentation</a></li>
                    <li><a href="https://stackoverflow.com/questions/tagged/pyspark" target="_blank" rel="noopener noreferrer">PySpark Questions on Stack Overflow</a></li>
                </ul>
            </div>
        </div>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
</body>
</html>